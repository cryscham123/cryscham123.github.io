---
title: "코드 snippet"
date: 2025-09-21
categories: ["데이터 분석"]
directories: ["adp_실기"]
---

![](/img/stat-thumb.jpg){.post-thumbnail}

## 모델 정의

```{python}
#| eval: false

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_features),
    ('cat', OneHotEncoder(drop='first'), cat_features),
    ('ord', OrdinalEncoder(), ord_features)
])
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model_name', YourModel(random_state=42))
])
```

## Grid Search & Bayesian Optimization

```{python filename="grid search}
#| eval: false

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error

# 파라미터 이름 앞에 pipeline에서 사용한 '모델이름__'을 붙여야 함
params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [10, 50, 100],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=pipeline, 
                           param_grid=params, 
                           cv=5, 
                           scoring='accuracy') # 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

# ============ 분류 scoring ============

y_pred_proba = best_model.predict_proba(X_test)[:, 1] # 다중클래스인 경우 [:, 1] 제거

test_acc = accuracy_score(y_test, y_pred)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
    y_test, 
    y_pred, 
    average='binary' # 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택
)
test_auc = roc_auc_score(y_test, y_pred_proba) # 다중 클래스인 경우 multi_class='ovo', 'ovr' 중 선택

print(f"test_accuracy: {test_acc:.4f}")
print(f"test_precision: {test_precision:.4f}")
print(f"test_recall: {test_recall:.4f}")
print(f"test_f1_score: {test_f1:.4f}")
print(f"test_auc: {test_auc:.4f}")

# ============ 회귀 scoring ============

rmse = root_mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"test_rmse: {rmse:.4f}")
print(f"test_mae: {mae:.4f}")
print(f"test_r2: {r2:.4f}")
```

```{python filename="bayesian optimization"}
#| eval: false
import optuna
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error

def objective(trial):
    # 모델에 적합한 파라미터에 맞게 수정
    params = {
        'classifier__n_estimators': trial.suggest_int("n_estimators", 100, 500, 100),
        'classifier__max_features': trial.suggest_categorical("max_features", ['sqrt', 'log2']),
        'classifier__max_depth': trial.suggest_int("max_depth", 10, 110, 20),
        'classifier__min_samples_split': trial.suggest_int("min_samples_split", 2, 10, 2),
        'classifier__min_samples_leaf': trial.suggest_int("min_samples_leaf", 1, 4, 1)
    }
    pipeline.set_params(**params)

    cv_score = cross_val_score(pipeline,
                               X_train,
                               y_train,
                               cv=5,
                               scoring='accuracy') # 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error
    mean_cv_accuracy = cv_score.mean()
    return mean_cv_accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

pipeline.set_params(**study.best_params)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

# ============ 분류 scoring ============

y_pred_proba = pipeline.predict_proba(X_test)[:, 1]


test_acc = accuracy_score(y_test, y_pred)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
    y_test, 
    y_pred, 
    average='binary' # 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택해서 사용
)
test_auc = roc_auc_score(y_test, y_pred_proba) # 다중 클래스인 경우 multiclass='ovo', 'ovr' 중 선택해서 사용

print("test_accuracy:", test_acc)
print("test_precision:", test_precision)
print("test_recall:", test_recall)
print("test_f1_score:", test_f1)
print("test_auc:", test_auc)

# ============ 회귀 scoring ============

rmse = root_mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"test_rmse: {rmse:.4f}")
print(f"test_mae: {mae:.4f}")
print(f"test_r2: {r2:.4f}")
```

## 모델 평가 visualization

### ROC AUC

```{python}
#| eval: false
import matplotlib.pyplot as plt

plt.figure()
plt.plot(logistic_fpr, logistic_tpr, color='blue', lw=2, label=f'Logistic (AUC = {logistic_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC')
plt.legend(loc='lower right')
plt.show()
```

### Feature Importance

```{python}
#| eval: false

```

## 모델 파라미터

### Logistic 회귀분석

```{python}
#| eval: false

params = {
    "penalty":, 
    "C":, 
    "l1_ratio":
}

C : float, default=1.0
    Inverse of regularization strength; must be a positive float.
    Like in support vector machines, smaller values specify stronger
    regularization.


l1_ratio : float, default=None
    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only
    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent
    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent
    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a
    combination of L1 and L2.
```

### KNN

```{python}
#| eval: false

```

### Support Vector Machine

```{python}
#| eval: false

```

### Random Forest

```{python}
#| eval: false

params = {
    "n_estimators":,
    "criterion":,
    "max_depth":,
    "min_samples_split":,
    "min_samples_leaf":,
    "min_weight_fraction_leaf":,
    "max_features":,
    "max_leaf_nodes":,
    "min_impurity_decrease":,
    "min_impurity_split":,
    "ccp_alpha":,
    "max_samples":
}
```

### XGBOOST

```{python}
#| eval: false

params = {
    "max_depth" :,
    "learning_rate":,
    "n_estimators":,
    "gamma":,
    "min_child_weight":,
    "max_delta_step":,
    "subsample":,
    "colsample_bytree":,
    "colsample_bylevel":,
    "colsample_bynode":,
    "reg_alpha":,
    "reg_lambda":,
}
```

### LightGBM

```{python}
#| eval: false

```

### Catboost


```{python}
#| eval: false

```
