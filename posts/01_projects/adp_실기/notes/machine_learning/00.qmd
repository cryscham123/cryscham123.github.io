---
title: "Titanic"
date: 2025-07-03
categories: ["확률 통계"]
---

![](/img/stat-thumb.jpg){.post-thumbnail}

## Define Problem

RMS 타이타닉호의 침몰은 역사상 가장 악명 높은 난파선 사고 중 하나다.
1912년 4월 15일, 첫 항해 중 타이타닉호는 빙산과 충돌한 후 침몰하여 2224명의 승객과 승무원 중 1502명이 사망했다.
이 충격적인 비극은 국제 사회를 경악시켰고, 더 나은 선박 안전 규정으로 이어졌다.

난파선 사고가 그토록 큰 인명 손실로 이어진 이유 중 하나는 승객과 승무원을 위한 구명보트가 충분하지 않았기 때문이다.
침몰에서 살아남는 데는 어느 정도 운이 따랐지만, 여성, 어린이, 상류층과 같은 특정 집단의 사람들이 다른 사람들보다 생존할 가능성이 더 높았다.

이 챌린지에서 우리는 어떤 종류의 사람들이 살아남을 가능성이 높았는지에 대한 분석을 완료하도록 요청한다.
특히, 머신러닝 도구를 적용하여 어떤 승객이 비극에서 살아남았는지 예측하도록 요청한다.

- 이진 분류 문제

## Gather Data

## Prepare Data

### Load Library

```{python}
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings('ignore')
```

```{python}
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process
from xgboost import XGBClassifier

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn import feature_selection
from sklearn import model_selection
from sklearn import metrics

import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns
from pandas.tools.plotting import scatter_matrix

plt.rcParams['font.family'] = 'Noto Sans KR'
mpl.style.use('ggplot')
sns.set_style('white')
```

### Load Data

```{python}
train_raw = pd.read_csv('_data/train.csv')
test_df = pd.read_csv('_data/test.csv')

train_df = train_raw.copy(deep=True)
train_X = train_df.drop('Survived', axis=1)
train_y = train_df['Survived'].astype('category')
cleaner = [train_X, test_df]
```

```{python}
del_feat = ['PassengerId', 'Cabin', 'Ticket']
qual = ['Pclass', 'Name', 'Sex', 'Ticket', 'Embarked']
quan = ['Age', 'SibSp', 'Parch', 'Fare']
for target in cleaner:
    target.drop(del_feat, axis=1, inplace=True)
    for col in qual:
        target[col] = target[col].astype('category')
train_X.info()
```

```{python}
train_X.describe(include='category')
```

```{python}
freq = pd.DataFrame(train_y.value_counts())
freq['상대도수'] = train_y.value_counts(normalize=True)
freq = freq.rename(columns={'count': '도수'})
freq
```

- Survived: 종속 변수. 생존은 1, 사망은 0으로 표시되는 이진 명목형 데이터 타입.
- PassengerID, Ticket 변수는 결과 변수에 영향을 미치지 않는 무작위 고유 식별자로 간주. 제외
- Pclass: 티켓 등급을 나타내는 순서형 데이터 타입. 사회경제적 지위(SES)를 나타내는 대리 변수. 1 = 상류층, 2 = 중산층, 3 = 하류층을 의미.
- Name 명목형 데이터. 피처 엔지니어링에 사용되어 호칭(title)에서 성별을, 성(surname)에서 가족 규모를, 그리고 의사(doctor)나 도련님(master) 같은 호칭에서 사회경제적 지위를 파생시킬 수 있다. 이러한 변수들이 이미 존재하므로, 우리는 'master'와 같은 호칭이 차이를 만드는지 확인하는 데 이 변수를 활용할 것
- Sex(성별)와 Embarked(탑승 항구) 변수는 명목형 데이터 타입
- Age(나이)와 Fare(요금) 변수는 연속형 양적 데이터 타입
- SibSp는 함께 탑승한 형제자매/배우자의 수를, Parch는 함께 탑승한 부모/자녀의 수를 나타냄. 두 변수 모두 이산형 양적 데이터 타입. 이 변수들은 피처 엔지니어링에 사용되어 가족 규모(family size)와 동승자 없는 변수(is alone)를 만드는 데 사용될 수 있음.
- Cabin(객실) 변수는 명목형 데이터 타입으로, 사고 발생 시 배에서의 대략적인 위치나 갑판 등급(deck levels)에 따른 사회경제적 지위를 위한 피처 엔지니어링에 사용될 수 있다. 하지만 결측값(null values)이 많기 때문에 가치를 더하지 못하므로 분석에서 제외.

### 상관계수

- 순서척도의 경우 스피어만, 켄달 타우 상관계수 분석 방법을 사용할 수 있다.
- 카테고리의 수는 그렇게 중요하지 않아보임.
- 책에서는 Survived를 측정하고 있는데, 이건 명목척도로 볼 수 있어서 조금 애매한거 같다.

```{python}
from scipy.stats import spearmanr, kendalltau

corr, p = spearmanr(train_y, train_X['Pclass'])
print(f'스피어만: {corr:.3f}, {p:.3f}')

corr2, p2 = kendalltau(train_y, train_X['Pclass'])
print(f'켄달타우: {corr2:.3f}, {p2:.3f}')
```

- 등급이 높을 수록 생존 가능성이 높아지는 경향

### Correcting, Completing, Creating, and Converting

```{python}
train_df.describe(include='all')
```

1. 비정상적인 값과 이상치 수정하기
  - Fare의 max가 지나치게 커보이긴 함
  - Age도 크긴 하지만 상식적인 범위
1. 누락된 정보 보완하기
  - 누락된 column은 Age, Cabin, Embarked, Fare
  - 보통 질적 데이터는 최빈값으로 대체
  - 양적 데이터는 평균, 중앙값, 평균 + 무작위 표준편차로 대체
  - 일단은 Age, Fare는 중앙값, Cabin은 제거, Embarked는 최빈값으로 대체
1. 분석을 위한 새로운 피처 생성하기
  - title을 사용할 것
1. 계산과 표현을 위해 필드를 올바른 형식으로 변환하기
  - one hot encoding

```{python}
drop_column = ['PassengerId','Cabin', 'Ticket']

# train의 값만 사용해서 대치를 해줬다.
age_median = train_df['Age'].median()
fare_median = train_df['Fare'].median()
embarked_mode = train_df['Embarked'].mode()[0]

for dataset in cleaner:
    dataset['Age'].fillna(age_median, inplace = True)
    dataset['Fare'].fillna(fare_median, inplace = True)
    dataset['Embarked'].fillna(embarked_mode, inplace = True)
    dataset.drop(drop_column, axis=1, inplace = True)
```

```{python}
for dataset in cleaner:
    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1
    dataset['IsAlone'] = np.where(dataset['FamilySize'] > 1, 0, 1)
    dataset['Title'] = dataset['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0]

less_title = (train_df['Title'].value_counts() < 10)
train_df['Title'] = train_df['Title'].apply(lambda x: 'Misc' if less_title.loc[x] else x)
```
