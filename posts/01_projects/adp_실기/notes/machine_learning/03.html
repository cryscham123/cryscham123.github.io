<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="ko" xml:lang="ko"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>분류 - 산탄데르 고객 만족 예측 – 김형훈의 학습 블로그</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/05.html" rel="next">
<link href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/02.html" rel="prev">
<link href="../../../../../favicon" rel="icon">
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "일치 없음",
    "search-matching-documents-text": "일치된 문서",
    "search-copy-link-title": "검색 링크 복사",
    "search-hide-matches-text": "추가 검색 결과 숨기기",
    "search-more-match-text": "추가 검색결과",
    "search-more-matches-text": "추가 검색결과",
    "search-clear-button-title": "제거",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "취소",
    "search-submit-button-title": "검색",
    "search-label": "검색"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GRXCD70RKK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
 
  gtag('consent', 'default', {
    'ad_storage': 'denied',
    'analytics_storage': 'denied'
  });
gtag('config', 'G-GRXCD70RKK', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdn.jsdelivr.net/npm/@mariusbongarts/previewbox/dist/link/index.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../../../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">김형훈의 학습 블로그</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="탐색 전환" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">PARA</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../all.html"> 
<span class="menu-text">전체 게시글</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="사이드바 전환" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/index.html">Projects</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/index.html">ADP 실기 준비 - try 1</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/00.pdf">Notes</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/00.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/03.html">분류 - 산탄데르 고객 만족 예측</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="사이드바 전환" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Inboxes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/00_inboxes/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">인간 관계론 - 데일 카네기</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/00_inboxes/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">대학원 준비</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Projects</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/01_projects/adp_실기/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ADP 실기 준비 - try 1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth3 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/00.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ADP 정리 노트</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pandas data 구조</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA와 시각화</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">데이터 전처리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">머신 러닝</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Bayse</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">베이즈 정리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">비율 추정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">수량 추정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">공산과 가산</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">최솟값, 최댓값 그리고 혼합 분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">포아송 과정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">의사결정분석</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">검정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">비교</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">분류</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">로지스틱 회귀</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">회귀</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/17.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">켤레사전분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/bayse/18.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MCMC</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Etc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/etc/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">다차원 척도법 (Multidimensional Scaling)</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth4 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Titanic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">분류 - 결정 트리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">분류 - 앙상블</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">분류 - 산탄데르 고객 만족 예측</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">회귀</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">차원 축소</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">텍스트 분석</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">텍스트 분석 - 20 뉴스그룹 분류</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">텍스트 분석 - 감성 분석</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Nonparametric</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/Nonparametric/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">intro</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/Nonparametric/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">순열검정과 전통적인 비모수통계</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/Nonparametric/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">두 변수의 연관성과 독립성</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/statistics/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Time Series</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">단순 미래 예측</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률보행</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">이동평균과정 모델링</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">자기귀모형</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">복잡한 시계열 모델</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">계절성 고려</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/01_projects/adp_실기/notes/time_series/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">외생 변수 추가하기</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Areas</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/deep_learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/deep_learning/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">퍼셉트론</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/deep_learning/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">신경망</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/deep_learning/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">신경망 학습</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/deep_learning/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">오차역전법</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/deep_learning/notes/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">합성곱 신경망</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/hadoop/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hadoop</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/hadoop/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hadoop Ecosystem</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/42_seoul/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">42 Seoul</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-18" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/42_seoul/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ft_transcendence - github action</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/42_seoul/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">inception-of-things part 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/42_seoul/notes/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">cloud-1 개념 설명</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/42_seoul/notes/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">cloud-1 코드 설명</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/kaggle/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Kaggle</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-19" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-20" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-21" role="navigation" aria-expanded="false">
 <span class="menu-text">Titanic</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-21" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-21" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/kaggle/notes/titanic/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">titanic</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/선형대수/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">선형대수</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-22" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-22" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-23" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-23" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-23" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">what is linear algebra</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2-기초(1)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2-기초(2)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3-몰라</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">벡터와 공간</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">선형결합과 생성</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">linear independence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Subspaces and the basis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">vector dot product, cross product</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">가감법으로 연립방정식을 풀기 위한 행렬</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/선형대수/notes/10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Null space and Column space</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/helm/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helm</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-24" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-24" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-25" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-25" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-25" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/helm/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">개요</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/machine_learning/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-26" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-26" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-27" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-27" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-27" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">data preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simple Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Polynorminal Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Support Vector Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Decision Tree Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">random forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">K Nearest Neighbors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Support Vector Machine</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Naive Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Decision Tree Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">k-means clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">hierarchical clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Apriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/17.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Eclat</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/machine_learning/notes/18.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Upper Confidence Bound</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/02_areas/air_flow/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AirFlow</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-28" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-28" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-29" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-29" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-29" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/air_flow/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/02_areas/air_flow/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coding pipeline</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-30" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-30" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-30" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/problem_solve/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problem Solving</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/blog/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Blog</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-31" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-31" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-32" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-32" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-32" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/blog/notes/0.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PARA Blog 제작</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/blog/notes/1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Second Brain - 티아고 포르테</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/인생/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">인생</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-33" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-33" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-34" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-34" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-34" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/인생/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">나의 단점에 관한 고찰</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/terraform/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Terraform</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-35" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-35" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-36" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-36" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-36" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-37" role="navigation" aria-expanded="false">
 <span class="menu-text">Tfc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-37" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-37" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/terraform/notes/tfc/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Terraform Cloud</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/smart_contract/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Smart Contract</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-38" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-38" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-39" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-39" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-39" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-40" role="navigation" aria-expanded="false">
 <span class="menu-text">Block Chain Basic</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-40" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-40" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/smart_contract/notes/block_chain_basic/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">what is a blockchain?</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/금융/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">금융</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-41" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-41" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-42" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-42" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-42" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/금융/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">돈의 심리학 - 모건 하우절</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/03_resources/quantum_programming/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantum Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-43" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-43" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-44" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-44" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-44" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/quantum_programming/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantum Programming</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/03_resources/quantum_programming/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Qiskit</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-45" role="navigation" aria-expanded="false">
 <span class="menu-text">Archives</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-45" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-45" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/tofel_준비/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TOFEL 준비</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/adp_필기/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ADP 필기 준비</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-46" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-46" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-47" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-47" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-47" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - 데이터 이해</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - 데이터의 가치와 미래</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - 데이터 처리 프로세스</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - 데이터 처리 기술</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - 데이터 분석 기획의 이해</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - 분석 마스터 플랜</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - 데이터 마트</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - 통계분석</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - 정형 데이터 마이닝</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - 비정형 데이터 마이닝</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - 시각화 인사이트 프로세스</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - 시각화 디자인</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">시험을 보고 왔습니다.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/adp_필기/notes/16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">안녕하세요. 데이터 분석 전문가(진)입니다.</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/vault/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">vault</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-48" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-48" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-49" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-49" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-49" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/vault/notes/0_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/opic/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OPIc 준비</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-50" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-50" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-51" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-51" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-51" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/opic/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">오픽 구조 파악</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/opic/notes/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">설문 script 정리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/opic/notes/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tips</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/opic/notes/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">일정 정리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/opic/notes/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">돌발 script 정리</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/k8s/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">k8s</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-52" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-52" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-53" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-53" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-53" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/0_core_concept.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">k8s cluster architecture</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/1_scheduler.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">manual scheduling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/2_logging_monitoring.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">metrics server</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/3_cluster_maintainance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">fail tolerance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/4_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Authentication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/5_storage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Persistant volume</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/6_network.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">core DNS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/k8s/notes/7_design_cluster.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HA in master node</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/bs_3_1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">학부 3학년 1학기</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-54" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-54" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-55" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-55" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-55" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3학년 1학기 후기</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-56" role="navigation" aria-expanded="false">
 <span class="menu-text">Computer</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-56" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-56" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/computer/01.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">컴퓨팅적 사고 1차 발표 구현 raw script</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/computer/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">컴퓨팅적사고 발표 ppt</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-57" role="navigation" aria-expanded="false">
 <span class="menu-text">Data Mining</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-57" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-57" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">데이터 전처리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">분류</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">random forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Support vector machine</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">association rule mining</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ensemble</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XGBoost</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/11.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">데이터마이닝 1차 팀과제 script</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dataminig 1차 발표 ppt</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">classification with trees</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/data_mining/16.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">청소년기의 심리·정서적 요인을 통한 성인 진입기 진로 안정형·탐색형 성향 분류 예측</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-58" role="navigation" aria-expanded="false">
 <span class="menu-text">Dsa</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-58" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-58" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/dsa/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">시험 범위</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/dsa/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">시험 범위</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-59" role="navigation" aria-expanded="false">
 <span class="menu-text">OR</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-59" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-59" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simplex 표 계산</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Programming Algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simplex Method (part 5)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">쌍대이론과 민감도 분석 (part 6)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/06.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OR 과제 - 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/07.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OR 과제 - 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/08.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OR 과제 - 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/09.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OR 과제 - 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">선형계획을 위한 다른 알고리즘들</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">수송문제와 할당 문제들</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/12.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OR 과제 - 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">시험 범위</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">수송문제와 할당 문제들</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/OR/16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">네트워크 최적화 모형</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-60" role="navigation" aria-expanded="false">
 <span class="menu-text">Others</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-60" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-60" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/others/1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">자기 소개서</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/others/2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">성적 장학금</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/others/3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">봉사</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-61" role="navigation" aria-expanded="false">
 <span class="menu-text">Product</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-61" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-61" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Matching Supply with Demand</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">조직을 프로세스 관점에서 바라보기</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">공급 프로세스의 이해: 프로세스 처리능력 평가</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">제품 설계 기법 및 기업 프로세스 유형</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">인건비 추정과 감축</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">배치 생산 및 경제적 주문량 모형</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">프로세스 성과에 미치는 변동성의 영향: 대기시간 문제</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">프로세스 성과에 미치는 변동성의 영향: 산술 손실</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">프로젝트 관리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">예측</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">총괄생산계획</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">기준생산계획 및 자재소요계획</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/product/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">일정 계획</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-62" role="navigation" aria-expanded="false">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-62" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-62" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률과 통계 1 정리</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">통계적 추정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">통계적 가설검정</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ANOVA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regression Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Analysis of categorical data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_3_1/notes/statistics/06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률과 통계 R 실습 과제</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/aws_saa/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS SAA 준비</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-63" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-63" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-64" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-64" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-64" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/00_region.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">aws global infrastructure</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/01_IAM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Define IAM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/02_ec2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">what is EC2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/03_ebs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">what is ebs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/04_elb_asg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ELB</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Amazon RDS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/06_route53.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Route53</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/07_S3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS S3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/08_cloudfront.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CloudFront</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/09_aws_storage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS Snow Family</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/10_message_queue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS SQS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/11_serverless.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS Lambda</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/12_database.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">database choice in aws</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/13_data_analytics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Performance Improvement</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/14_machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Amazon Rekognition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/15_monitoring.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Amazon CloudWatch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/16_IAM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AWS Organization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/17_AWS_secure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KMS(Key Management Service)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/18_VPC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VPC</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/aws_saa/notes/19_DR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Disaster Recovery(DR)</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/bs_2_2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2학년 2학기 학부 정리</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-65" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-65" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-66" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-66" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-66" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-67" role="navigation" aria-expanded="false">
 <span class="menu-text">Bs Database</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-67" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-67" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/01-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">An Overview of Database</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Relational Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/04-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Database Normalization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/04-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SQL</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Modeling and the Entity-Relationship Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Database Design</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Database Administration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ASP.NET</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/work1.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">데이터베이스설계및활용 개인과제 #2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/work2.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4조 기말과제 제안서</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_database/work4.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">숭실대학교 학생식당 식자제 SCM 설계</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-68" role="navigation" aria-expanded="false">
 <span class="menu-text">Bs Human</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-68" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-68" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/0_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Human Factors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Research Method in Human Factors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/2_human_information_processing_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Human Information Processing Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sensor System (Visual)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Auditory Haptic</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Signal Detection Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/6_attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/7_display.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Display</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/8_control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Control</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_human/인간공학-보고서-초안.pdf" class="sidebar-item-text sidebar-link">
 <span class="menu-text">맥도날드 키오스크 UI 개선 보고서</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-69" role="navigation" aria-expanded="false">
 <span class="menu-text">Bs Statistics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-69" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-69" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률과 통계의 정의</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률변수와 확률분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">확률변수의 기댓값</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">이산형 확률분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">연속형 확률분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">정규 분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">표본의 분포</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">중심 극한 정리</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../../posts/04_archives/toeic_speaking/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">토익 스피킹 준비</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-70" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-70" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-71" role="navigation" aria-expanded="false">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-71" role="navigation" aria-expanded="false" aria-label="토글 섹션">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-71" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../posts/04_archives/toeic_speaking/notes/00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">토익 스피킹 후기</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link active" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#xgboost" id="toc-xgboost" class="nav-link" data-scroll-target="#xgboost">XGBoost</a>
  <ul class="collapse">
  <li><a href="#베이지안-최적화" id="toc-베이지안-최적화" class="nav-link" data-scroll-target="#베이지안-최적화">베이지안 최적화</a></li>
  <li><a href="#재-학습" id="toc-재-학습" class="nav-link" data-scroll-target="#재-학습">재 학습</a></li>
  <li><a href="#plot-importance" id="toc-plot-importance" class="nav-link" data-scroll-target="#plot-importance">plot importance</a></li>
  </ul></li>
  <li><a href="#lightgbm" id="toc-lightgbm" class="nav-link" data-scroll-target="#lightgbm">LightGBM</a>
  <ul class="collapse">
  <li><a href="#베이지안-최적화-1" id="toc-베이지안-최적화-1" class="nav-link" data-scroll-target="#베이지안-최적화-1">베이지안 최적화</a></li>
  <li><a href="#재학습" id="toc-재학습" class="nav-link" data-scroll-target="#재학습">재학습</a></li>
  </ul></li>
  <li><a href="#제출" id="toc-제출" class="nav-link" data-scroll-target="#제출">제출</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/index.html">Projects</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/index.html">ADP 실기 준비 - try 1</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/00.pdf">Notes</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/00.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/03.html">분류 - 산탄데르 고객 만족 예측</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">분류 - 산탄데르 고객 만족 예측</h1>
  <div class="quarto-categories">
    <div class="quarto-category">머신 러닝</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">공개</div>
    <div class="quarto-title-meta-contents">
      <p class="date">2025년 7월 27일</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="../../../../../img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<div id="d5947441" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.family'</span>] <span class="op">=</span> <span class="st">'Noto Sans KR'</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'_data/santander/train.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 76020 entries, 0 to 76019
Columns: 371 entries, ID to TARGET
dtypes: float64(111), int64(260)
memory usage: 215.2 MB</code></pre>
</div>
</div>
<div id="64745c05" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df.describe()</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ID</th>
<th data-quarto-table-cell-role="th">var3</th>
<th data-quarto-table-cell-role="th">var15</th>
<th data-quarto-table-cell-role="th">imp_ent_var16_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult3</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult3</th>
<th data-quarto-table-cell-role="th">var38</th>
<th data-quarto-table-cell-role="th">TARGET</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>...</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>7.602000e+04</td>
<td>76020.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>75964.050723</td>
<td>-1523.199277</td>
<td>33.212865</td>
<td>86.208265</td>
<td>72.363067</td>
<td>119.529632</td>
<td>3.559130</td>
<td>6.472698</td>
<td>0.412946</td>
<td>0.567352</td>
<td>...</td>
<td>7.935824</td>
<td>1.365146</td>
<td>12.215580</td>
<td>8.784074</td>
<td>31.505324</td>
<td>1.858575</td>
<td>76.026165</td>
<td>56.614351</td>
<td>1.172358e+05</td>
<td>0.039569</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>43781.947379</td>
<td>39033.462364</td>
<td>12.956486</td>
<td>1614.757313</td>
<td>339.315831</td>
<td>546.266294</td>
<td>93.155749</td>
<td>153.737066</td>
<td>30.604864</td>
<td>36.513513</td>
<td>...</td>
<td>455.887218</td>
<td>113.959637</td>
<td>783.207399</td>
<td>538.439211</td>
<td>2013.125393</td>
<td>147.786584</td>
<td>4040.337842</td>
<td>2852.579397</td>
<td>1.826646e+05</td>
<td>0.194945</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000000</td>
<td>-999999.000000</td>
<td>5.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.163750e+03</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>38104.750000</td>
<td>2.000000</td>
<td>23.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.787061e+04</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>76043.000000</td>
<td>2.000000</td>
<td>28.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.064092e+05</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>113748.750000</td>
<td>2.000000</td>
<td>40.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.187563e+05</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>151838.000000</td>
<td>238.000000</td>
<td>105.000000</td>
<td>210000.000000</td>
<td>12888.030000</td>
<td>21024.810000</td>
<td>8237.820000</td>
<td>11073.570000</td>
<td>6600.000000</td>
<td>6600.000000</td>
<td>...</td>
<td>50003.880000</td>
<td>20385.720000</td>
<td>138831.630000</td>
<td>91778.730000</td>
<td>438329.220000</td>
<td>24650.010000</td>
<td>681462.900000</td>
<td>397884.300000</td>
<td>2.203474e+07</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

<p>8 rows × 371 columns</p>
</div>
</div>
</div>
<div id="e314ea19" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'var3'</span>].replace(<span class="op">-</span><span class="dv">999999</span>, <span class="dv">2</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>df.drop(<span class="st">'ID'</span>, axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X_features <span class="op">=</span> df.iloc[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> df.iloc[:, <span class="op">-</span><span class="dv">1</span>]</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f9cec085" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">'_data/santander/test.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>test_df[<span class="st">'var3'</span>].replace(<span class="op">-</span><span class="dv">999999</span>, <span class="dv">2</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>test_df.drop(<span class="st">'ID'</span>, axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="05d26bff" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_features, labels, test_size<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>train, test의 label의 비율이 동일한게 좋은걸까</li>
</ul>
</section>
<section id="xgboost" class="level2">
<h2 class="anchored" data-anchor-id="xgboost">XGBoost</h2>
<div id="8340b7d0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X_tr, X_val, y_tr, y_val <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="43b1a238" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>evals <span class="op">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>xgb_clf <span class="op">=</span> XGBClassifier(n_estimators<span class="op">=</span><span class="dv">400</span>, </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>                    learning_rate<span class="op">=</span><span class="fl">0.05</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>                    early_stopping_rounds<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                    eval_metric<span class="op">=</span>[<span class="st">'auc'</span>])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>xgb_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>evals, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>xgb_roc_score <span class="op">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv">1</span>])</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>xgb_roc_score<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="베이지안-최적화" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화">베이지안 최적화</h3>
<div id="ffd62d7c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective_func(search_space):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    xgb_clf <span class="op">=</span> XGBClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                            early_stopping_rounds<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>                            eval_metric<span class="op">=</span><span class="st">'auc'</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span><span class="bu">int</span>(search_space[<span class="st">'max_depth'</span>]),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                            min_child_weight<span class="op">=</span><span class="bu">int</span>(search_space[<span class="st">'min_child_weight'</span>]),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                            colsample_bytree<span class="op">=</span>search_space[<span class="st">'colsample_bytree'</span>],</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>                            learning_rate<span class="op">=</span>search_space[<span class="st">'learning_rate'</span>])</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    roc_auc_list <span class="op">=</span> []</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tr_index, val_index <span class="kw">in</span> kf.split(X_train):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        X_tr, y_tr <span class="op">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        X_val, y_val <span class="op">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        xgb_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, <span class="dv">1</span>])</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        roc_auc_list.append(score)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> np.mean(roc_auc_list)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c5c43e8f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hyperopt <span class="im">import</span> hp, fmin, tpe, Trials</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>xgb_search_space <span class="op">=</span> {</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">'max_depth'</span>: hp.quniform(<span class="st">'max_depth'</span>, <span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">1</span>),</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'min_child_weight'</span>: hp.quniform(<span class="st">'min_child_weight'</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>),</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">'colsample_bytree'</span>: hp.uniform(<span class="st">'colsample_bytree'</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">'learning_rate'</span>: hp.uniform(<span class="st">'learning_rate'</span>, <span class="fl">0.01</span>, <span class="fl">0.2</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> Trials()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> fmin(fn<span class="op">=</span>objective_func,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>            space<span class="op">=</span>xgb_search_space,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>            algo<span class="op">=</span>tpe.suggest,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>            max_evals<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>            trials<span class="op">=</span>trials)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="재-학습" class="level3">
<h3 class="anchored" data-anchor-id="재-학습">재 학습</h3>
<div id="84dbf898" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBClassifier</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>evals <span class="op">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>xgb_clf <span class="op">=</span> XGBClassifier(n_estimators<span class="op">=</span><span class="dv">500</span>, </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>                    learning_rate<span class="op">=</span><span class="bu">round</span>(best[<span class="st">'learning_rate'</span>], <span class="dv">5</span>),</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                    max_depth<span class="op">=</span><span class="bu">int</span>(best[<span class="st">'max_depth'</span>]),</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                    min_child_weight<span class="op">=</span><span class="bu">int</span>(best[<span class="st">'min_child_weight'</span>]),</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                    colsample_bytree<span class="op">=</span><span class="bu">round</span>(best[<span class="st">'colsample_bytree'</span>], <span class="dv">5</span>),</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                    early_stopping_rounds<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>                    eval_metric<span class="op">=</span>[<span class="st">'auc'</span>])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>xgb_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>evals, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>xgb_roc_score <span class="op">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv">1</span>])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>xgb_roc_score<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="plot-importance" class="level3">
<h3 class="anchored" data-anchor-id="plot-importance">plot importance</h3>
<div id="7efd1ad9" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> plot_importance</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plot_importance(xgb_clf, max_num_features<span class="op">=</span><span class="dv">20</span>, height<span class="op">=</span><span class="fl">0.4</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="lightgbm" class="level2">
<h2 class="anchored" data-anchor-id="lightgbm">LightGBM</h2>
<div id="58532da1" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightgbm <span class="im">import</span> LGBMClassifier</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>lgbm_clf <span class="op">=</span> LGBMClassifier(n_estimators<span class="op">=</span><span class="dv">500</span>, early_stopping_rounds<span class="op">=</span><span class="dv">100</span>, eval_metric<span class="op">=</span><span class="st">'auc'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>eval_set <span class="op">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>eval_set)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>lgbm_roc_score <span class="op">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv">1</span>])</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>lgbm_roc_score<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1653, number of negative: 40918
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007850 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 13447
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 251
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978
[LightGBM] [Info] Start training from score -3.208978
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117279 valid_1's binary_logloss: 0.137813
[LightGBM] [Warning] Unknown parameter: eval_metric
0.834</code></pre>
</div>
</div>
<section id="베이지안-최적화-1" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화-1">베이지안 최적화</h3>
<div id="d3b3256c" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective_func(search_space):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    lgbm_clf <span class="op">=</span> LGBMClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                            early_stopping_rounds<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                            eval_metric<span class="op">=</span><span class="st">'auc'</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                            num_leaves<span class="op">=</span><span class="bu">int</span>(search_space[<span class="st">'num_leaves'</span>]),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                            max_depth<span class="op">=</span><span class="bu">int</span>(search_space[<span class="st">'max_depth'</span>]),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>                            min_child_samples<span class="op">=</span><span class="bu">int</span>(search_space[<span class="st">'min_child_samples'</span>]),</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>                            subsample<span class="op">=</span>search_space[<span class="st">'subsample'</span>],</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                            learning_rate<span class="op">=</span>search_space[<span class="st">'learning_rate'</span>])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    roc_auc_list <span class="op">=</span> []</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tr_index, val_index <span class="kw">in</span> kf.split(X_train):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        X_tr, y_tr <span class="op">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        X_val, y_val <span class="op">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, <span class="dv">1</span>])</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        roc_auc_list.append(score)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> np.mean(roc_auc_list)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b6d8769a" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hyperopt <span class="im">import</span> hp, fmin, tpe, Trials</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>lgbm_search_space <span class="op">=</span> {</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">'num_leaves'</span>: hp.quniform(<span class="st">'num_leaves'</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">1</span>),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'max_depth'</span>: hp.quniform(<span class="st">'max_depth'</span>, <span class="dv">100</span>, <span class="dv">160</span>, <span class="dv">1</span>),</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">'min_child_samples'</span>: hp.quniform(<span class="st">'min_child_samples'</span>, <span class="dv">60</span>, <span class="dv">100</span>, <span class="dv">1</span>),</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">'subsample'</span>: hp.uniform(<span class="st">'subsample'</span>, <span class="fl">0.7</span>, <span class="dv">1</span>),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">'learning_rate'</span>: hp.uniform(<span class="st">'learning_rate'</span>, <span class="fl">0.01</span>, <span class="fl">0.2</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> Trials()</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>best <span class="op">=</span> fmin(fn<span class="op">=</span>objective_func,</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            space<span class="op">=</span>lgbm_search_space,</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>            algo<span class="op">=</span>tpe.suggest,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>            max_evals<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>            trials<span class="op">=</span>trials)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009804 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12809
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.168309
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[36]    training's binary_logloss: 0.121676 valid_1's binary_logloss: 0.127049
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011714 seconds.
You can set `force_col_wise=true` to remove the overhead.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.194075
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[44]    training's binary_logloss: 0.115084 valid_1's binary_logloss: 0.135595
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.233233
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[50]    training's binary_logloss: 0.110571 valid_1's binary_logloss: 0.140209
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006776 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12809
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Early stopping, best iteration is:
[68]    training's binary_logloss: 0.119949 valid_1's binary_logloss: 0.127337
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008185 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12874
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:
[74]    training's binary_logloss: 0.114945 valid_1's binary_logloss: 0.135003
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005666 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12865
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.140191
  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.121998 valid_1's binary_logloss: 0.127349
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006521 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12970
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[26]    training's binary_logloss: 0.115056 valid_1's binary_logloss: 0.136143
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008614 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 13049
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[30]    training's binary_logloss: 0.110308 valid_1's binary_logloss: 0.140967
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]  6%|▌         | 3/50 [00:05&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006389 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.119702 valid_1's binary_logloss: 0.127682
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005965 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.11491  valid_1's binary_logloss: 0.13632
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006447 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[19]    training's binary_logloss: 0.113764 valid_1's binary_logloss: 0.141398
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006690 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.114179 valid_1's binary_logloss: 0.127237
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006128 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[57]    training's binary_logloss: 0.113751 valid_1's binary_logloss: 0.136174
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007084 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[58]    training's binary_logloss: 0.11113  valid_1's binary_logloss: 0.140897
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884] 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006562 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.120721 valid_1's binary_logloss: 0.127623
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005976 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.117914 valid_1's binary_logloss: 0.135692
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[18]    training's binary_logloss: 0.117142 valid_1's binary_logloss: 0.141073
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884] 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005795 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.122492 valid_1's binary_logloss: 0.127389
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005857 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12882
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.119931 valid_1's binary_logloss: 0.13599
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007378 seconds.
You can set `force_col_wise=true` to remove the overhead.
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12883
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.116742 valid_1's binary_logloss: 0.14122
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884] 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006486 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[37]    training's binary_logloss: 0.118076 valid_1's binary_logloss: 0.12711
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008708 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[27]    training's binary_logloss: 0.118244 valid_1's binary_logloss: 0.135768
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007417 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[34]    training's binary_logloss: 0.11261  valid_1's binary_logloss: 0.140798
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884] 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007634 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[93]    training's binary_logloss: 0.113871 valid_1's binary_logloss: 0.127108
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007168 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12934
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.113106 valid_1's binary_logloss: 0.135792
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008788 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12989
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[82]    training's binary_logloss: 0.109277 valid_1's binary_logloss: 0.140921
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884] 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007124 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[57]    training's binary_logloss: 0.120677 valid_1's binary_logloss: 0.127111
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009349 seconds.
You can set `force_col_wise=true` to remove the overhead.
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[50]    training's binary_logloss: 0.118347 valid_1's binary_logloss: 0.135488
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013450 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[54]    training's binary_logloss: 0.114424 valid_1's binary_logloss: 0.140196
 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884] 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006974 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12907
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[78]    training's binary_logloss: 0.111459 valid_1's binary_logloss: 0.12715
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006741 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12943
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.112371 valid_1's binary_logloss: 0.13579
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007176 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13017
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.105423 valid_1's binary_logloss: 0.141018
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884] 22%|██▏       | 11/50 [00:25&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006851 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.123258 valid_1's binary_logloss: 0.127671
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008232 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.118847 valid_1's binary_logloss: 0.135735
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006765 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.115967 valid_1's binary_logloss: 0.140884
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884] 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007459 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.118279 valid_1's binary_logloss: 0.128419
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010830 seconds.
You can set `force_col_wise=true` to remove the overhead.
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.114963 valid_1's binary_logloss: 0.136964
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008271 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.111041 valid_1's binary_logloss: 0.141811
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884] 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006716 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.117911 valid_1's binary_logloss: 0.127609
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007818 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.112846 valid_1's binary_logloss: 0.135945
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007760 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[21]    training's binary_logloss: 0.11335  valid_1's binary_logloss: 0.141758
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884] 28%|██▊       | 14/50 [00:31&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011169 seconds.
You can set `force_col_wise=true` to remove the overhead.
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[15]    training's binary_logloss: 0.123793 valid_1's binary_logloss: 0.127794
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006417 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.117509 valid_1's binary_logloss: 0.136341
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012417 seconds.
You can set `force_col_wise=true` to remove the overhead.
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.118131 valid_1's binary_logloss: 0.141827
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884] 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.116669 valid_1's binary_logloss: 0.127316
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008685 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.112195 valid_1's binary_logloss: 0.13634
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008114 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.110627 valid_1's binary_logloss: 0.141491
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:36&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884] 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006910 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[44]    training's binary_logloss: 0.11468  valid_1's binary_logloss: 0.127009
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008679 seconds.
You can set `force_col_wise=true` to remove the overhead.
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.116699 valid_1's binary_logloss: 0.136132
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009082 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.110851 valid_1's binary_logloss: 0.140914
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884] 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009004 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.133099 valid_1's binary_logloss: 0.130118
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010889 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.128591 valid_1's binary_logloss: 0.138373
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006967 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.125837 valid_1's binary_logloss: 0.144181
 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884] 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007196 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.117838 valid_1's binary_logloss: 0.127509
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.114266 valid_1's binary_logloss: 0.136132
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[44]    training's binary_logloss: 0.107097 valid_1's binary_logloss: 0.141644
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884] 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006872 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[32]    training's binary_logloss: 0.120651 valid_1's binary_logloss: 0.12748
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007690 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.117806 valid_1's binary_logloss: 0.135748
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007569 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.111183 valid_1's binary_logloss: 0.140593
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884] 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010567 seconds.
You can set `force_col_wise=true` to remove the overhead.
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127621 valid_1's binary_logloss: 0.127975
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028233 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123117 valid_1's binary_logloss: 0.136484
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007220 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120407 valid_1's binary_logloss: 0.141888
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884] 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008396 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[94]    training's binary_logloss: 0.120321 valid_1's binary_logloss: 0.12706
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007023 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.116042 valid_1's binary_logloss: 0.135298
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007237 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.112382 valid_1's binary_logloss: 0.140103
 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884] 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007562 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120691 valid_1's binary_logloss: 0.127296
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009653 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.116561 valid_1's binary_logloss: 0.135521
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007650 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.113883 valid_1's binary_logloss: 0.140482
 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328] 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009103 seconds.
You can set `force_col_wise=true` to remove the overhead.
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12506  valid_1's binary_logloss: 0.127517
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007298 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120784 valid_1's binary_logloss: 0.135909
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006853 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11795  valid_1's binary_logloss: 0.141026
 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328] 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006418 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.120728 valid_1's binary_logloss: 0.12726
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007255 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.116085 valid_1's binary_logloss: 0.13534
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006430 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.11095  valid_1's binary_logloss: 0.140369
 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328] 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.118162 valid_1's binary_logloss: 0.127362
 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006679 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[93]    training's binary_logloss: 0.114804 valid_1's binary_logloss: 0.135408
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007134 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[98]    training's binary_logloss: 0.111333 valid_1's binary_logloss: 0.140214
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328] 52%|█████▏    | 26/50 [01:05&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006927 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.11927  valid_1's binary_logloss: 0.127628
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007967 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[33]    training's binary_logloss: 0.11608  valid_1's binary_logloss: 0.136215
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007403 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.141059
 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328] 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011520 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[65]    training's binary_logloss: 0.11962  valid_1's binary_logloss: 0.127063
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007475 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[72]    training's binary_logloss: 0.114002 valid_1's binary_logloss: 0.135602
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008925 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.111386 valid_1's binary_logloss: 0.140329
 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328] 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006519 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.123961 valid_1's binary_logloss: 0.127545
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010127 seconds.
You can set `force_col_wise=true` to remove the overhead.
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[32]    training's binary_logloss: 0.117126 valid_1's binary_logloss: 0.13534
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007890 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[41]    training's binary_logloss: 0.111145 valid_1's binary_logloss: 0.140511
 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328] 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010351 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.135256 valid_1's binary_logloss: 0.131344
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007594 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.130846 valid_1's binary_logloss: 0.139185
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006775 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127867 valid_1's binary_logloss: 0.14514
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328] 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006594 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127239 valid_1's binary_logloss: 0.127384
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006879 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12288  valid_1's binary_logloss: 0.135571
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007380 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120284 valid_1's binary_logloss: 0.140863
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:18&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328] 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008255 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.119732 valid_1's binary_logloss: 0.127276
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007609 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[70]    training's binary_logloss: 0.116807 valid_1's binary_logloss: 0.135585
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008248 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[80]    training's binary_logloss: 0.112368 valid_1's binary_logloss: 0.14032
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328] 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.120657 valid_1's binary_logloss: 0.126949
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006888 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[42]    training's binary_logloss: 0.118801 valid_1's binary_logloss: 0.135645
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006469 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.113559 valid_1's binary_logloss: 0.140513
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328] 66%|██████▌   | 33/50 [01:22&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012104 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[36]    training's binary_logloss: 0.121629 valid_1's binary_logloss: 0.127166
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008693 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.117903 valid_1's binary_logloss: 0.13587
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011244 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.115553 valid_1's binary_logloss: 0.140845
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:25&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328] 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007149 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.118882 valid_1's binary_logloss: 0.128522
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[11]    training's binary_logloss: 0.12009  valid_1's binary_logloss: 0.136809
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012520 seconds.
You can set `force_col_wise=true` to remove the overhead.
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.114296 valid_1's binary_logloss: 0.141912
 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328] 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009374 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12870
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[67]    training's binary_logloss: 0.117834 valid_1's binary_logloss: 0.127248
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12934
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[55]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.135743
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008382 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12939
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[62]    training's binary_logloss: 0.112207 valid_1's binary_logloss: 0.140686
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328] 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008790 seconds.
You can set `force_col_wise=true` to remove the overhead.
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[72]    training's binary_logloss: 0.116081 valid_1's binary_logloss: 0.12713
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008642 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.117446 valid_1's binary_logloss: 0.135845
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006554 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[54]    training's binary_logloss: 0.113495 valid_1's binary_logloss: 0.140635
 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328] 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008911 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.11803  valid_1's binary_logloss: 0.126746
 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008127 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[85]    training's binary_logloss: 0.115626 valid_1's binary_logloss: 0.135332
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013095 seconds.
You can set `force_col_wise=true` to remove the overhead.
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[89]    training's binary_logloss: 0.112365 valid_1's binary_logloss: 0.140135
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328] 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007640 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[98]    training's binary_logloss: 0.123399 valid_1's binary_logloss: 0.126912
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007213 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119208 valid_1's binary_logloss: 0.135189
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011345 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.116397 valid_1's binary_logloss: 0.140343
 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017] 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006904 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.13496  valid_1's binary_logloss: 0.13089
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006719 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.130487 valid_1's binary_logloss: 0.138913
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009924 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12765  valid_1's binary_logloss: 0.144942
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017] 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008446 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[91]    training's binary_logloss: 0.119162 valid_1's binary_logloss: 0.126781
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010092 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12943
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.117526 valid_1's binary_logloss: 0.135504
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018761 seconds.
You can set `force_col_wise=true` to remove the overhead.
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[84]    training's binary_logloss: 0.113084 valid_1's binary_logloss: 0.140427
 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017] 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007948 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124667 valid_1's binary_logloss: 0.127059
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006578 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120428 valid_1's binary_logloss: 0.135621
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006144 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.117733 valid_1's binary_logloss: 0.140661
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017] 84%|████████▍ | 42/50 [01:47&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008411 seconds.
You can set `force_col_wise=true` to remove the overhead.
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.120438 valid_1's binary_logloss: 0.127484
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006109 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.118015 valid_1's binary_logloss: 0.13605
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006744 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.115064 valid_1's binary_logloss: 0.14127
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017] 86%|████████▌ | 43/50 [01:49&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008998 seconds.
You can set `force_col_wise=true` to remove the overhead.
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.117532 valid_1's binary_logloss: 0.128445
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[13]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.136088
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007065 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13049
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.10854  valid_1's binary_logloss: 0.14215
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017] 88%|████████▊ | 44/50 [01:51&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009960 seconds.
You can set `force_col_wise=true` to remove the overhead.
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[31]    training's binary_logloss: 0.120736 valid_1's binary_logloss: 0.127726
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007563 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117682 valid_1's binary_logloss: 0.135689
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010431 seconds.
You can set `force_col_wise=true` to remove the overhead.
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.112434 valid_1's binary_logloss: 0.141034
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017] 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007370 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.114951 valid_1's binary_logloss: 0.127139
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.114337 valid_1's binary_logloss: 0.136683
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007329 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.110257 valid_1's binary_logloss: 0.141881
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017] 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011444 seconds.
You can set `force_col_wise=true` to remove the overhead.
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[45]    training's binary_logloss: 0.120473 valid_1's binary_logloss: 0.127224
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007092 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[47]    training's binary_logloss: 0.115765 valid_1's binary_logloss: 0.135738
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.114087 valid_1's binary_logloss: 0.140748
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017] 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.117042 valid_1's binary_logloss: 0.127461
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006729 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.110513 valid_1's binary_logloss: 0.135839
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011866 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[61]    training's binary_logloss: 0.107023 valid_1's binary_logloss: 0.140678
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017] 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008940 seconds.
You can set `force_col_wise=true` to remove the overhead.
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12818
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.116856 valid_1's binary_logloss: 0.127122
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009554 seconds.
You can set `force_col_wise=true` to remove the overhead.
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[36]    training's binary_logloss: 0.117447 valid_1's binary_logloss: 0.13599
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006844 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[48]    training's binary_logloss: 0.110674 valid_1's binary_logloss: 0.140834
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017] 98%|█████████▊| 49/50 [02:03&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007160 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117901 valid_1's binary_logloss: 0.128002
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008421 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.11285  valid_1's binary_logloss: 0.135927
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007163 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.110102 valid_1's binary_logloss: 0.141424
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.48s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.53s/trial, best loss: -0.8368093643173017]
{'learning_rate': 0.043324531254078945, 'max_depth': 133.0, 'min_child_samples': 85.0, 'num_leaves': 36.0, 'subsample': 0.7305792288105732}</code></pre>
</div>
</div>
</section>
<section id="재학습" class="level3">
<h3 class="anchored" data-anchor-id="재학습">재학습</h3>
<div id="05fb8493" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>lgbm_clf <span class="op">=</span> LGBMClassifier(n_estimators<span class="op">=</span><span class="dv">500</span>, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                          num_leaves<span class="op">=</span><span class="bu">int</span>(best[<span class="st">'num_leaves'</span>]),</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                          max_depth<span class="op">=</span><span class="bu">int</span>(best[<span class="st">'max_depth'</span>]),</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                          min_child_samples<span class="op">=</span><span class="bu">int</span>(best[<span class="st">'min_child_samples'</span>]),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                          subsample<span class="op">=</span><span class="bu">round</span>(best[<span class="st">'subsample'</span>], <span class="dv">5</span>),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                          learning_rate<span class="op">=</span><span class="bu">round</span>(best[<span class="st">'learning_rate'</span>], <span class="dv">5</span>),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                          early_stopping_rounds<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>                          eval_metric<span class="op">=</span><span class="st">'auc'</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>eval_set <span class="op">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op">=</span>eval_set)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>lgbm_roc_score <span class="op">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv">1</span>])</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>lgbm_roc_score<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1653, number of negative: 40918
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009941 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12969
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978
[LightGBM] [Info] Start training from score -3.208978
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[62]    training's binary_logloss: 0.119449 valid_1's binary_logloss: 0.137449
[LightGBM] [Warning] Unknown parameter: eval_metric
0.838</code></pre>
</div>
</div>
</section>
</section>
<section id="제출" class="level2">
<h2 class="anchored" data-anchor-id="제출">제출</h2>
<div id="7af1ef06" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> lgbm_clf.predict(test_df)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>submit <span class="op">=</span> pd.read_csv(<span class="st">'_data/santander/sample_submission.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>submit[<span class="st">'TARGET'</span>] <span class="op">=</span> target</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>submit.to_csv(<span class="st">'_data/santander/submission.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>, index<span class="op">=</span><span class="va">False</span>)</span></code><button title="클립보드 복사" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric</code></pre>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "복사완료!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "복사완료!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/cryscham123\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="cryscham123/cryscham123.github.io" data-repo-id="R_kgDONGE9Rw" data-category="Announcements" data-category-id="DIC_kwDONGE9R84CjtIQ" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/02.html" class="pagination-link" aria-label="분류 - 앙상블">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">분류 - 앙상블</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../posts/01_projects/adp_실기/notes/machine_learning/05.html" class="pagination-link" aria-label="회귀">
        <span class="nav-page-text">회귀</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024 김형훈</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>