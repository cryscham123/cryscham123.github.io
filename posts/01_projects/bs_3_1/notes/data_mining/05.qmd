---
title: 데이터마이닝 팀과제 자료 조사
categories: ["보고서", "데이터 마이닝"]
date: last-modified
format: 
  pdf:
    fig-pos: 'H'
    fig-width: 6
    fig-height: 4
    css: styles.css
    documentclass: report
    papersize: a4
    number-sections: true
    number-depth: 3
    top-level-division: chapter
    toc: true
    fontsize: 17pt
    geometry:
      - top=20mm
    indent-size: 2
include-in-header: 
  text: |
    \date{}
    \usepackage{fontspec}
    \setmainfont{Noto Sans KR}
    \usepackage{titlesec}
    \titleformat{\chapter}{\normalfont\huge\bfseries}{\thechapter.}{0.8em}{\huge}
    \titleformat{\subsection}[block]{\normalfont\large\bfseries}{}{0pt}{}
    \titleformat{\subsubsection}[block]{\normalfont\normalsize\bfseries}{}{0pt}{}
    \titlespacing*{\chapter}{0pt}{-20pt}{5pt}
    \titlespacing*{\section}{0pt}{10pt}{10pt}
    \newcommand{\chapterbreak}{\clearpage}
    \usepackage{setspace}
    \setstretch{1.5}
    \usepackage{tabularx}
    \newcolumntype{Y}{>{\centering\arraybackslash}X}
    \usepackage[bottom]{footmisc}
    \usepackage{setspace}
    \renewcommand{\footnotesize}{\setstretch{1.2}\fontsize{9pt}{11pt}\selectfont}
    \setlength{\skip\footins}{10pt}
    \setlength{\footnotesep}{8pt}
    \renewcommand{\footnoterule}{\vspace{1em}\hrule width 0.5\linewidth\vspace{1em}}
    \makeatletter
    \renewcommand{\@makefntext}[1]{\noindent\makebox[1.5em][r]{\@makefnmark}\hspace{0.5em}#1}
    \makeatother
    \usepackage{float}
    \usepackage{multirow}
    \usepackage{booktabs}

execute:
  echo: false
  warning: false
  message: false

---

# Airflow 소개

## 개요

데이터 분석 및 엔지니어링 분야에서는 복잡한 데이터 처리 작업을 자동화하고 관리하기 위해 워크플로우 관리 도구를 도입합니다.
특히 대규모 데이터 처리 환경에서는 거의 필수적으로 사용하죠.

Apache Airflow는 현재 가장 널리 사용되는 워크플로우 관리 도구 중 하나로, Python으로 작업 흐름을 정의하고 스케줄링, 모니터링, 오류 처리 등의 기능을 제공합니다.

## workflow tool이란?

워크플로우 툴(Workflow Tool)은 복잡한 작업 과정을 자동화하고 관리하는 소프트웨어를 칭입니다.
쉽게 말해, 여러 단계의 작업을 순서대로 실행하고 모니터링할 수 있게 도와주는 도구죠.

예를 들어보겠습니다:

- 매일 아침 데이터베이스에서 정보를 가져와
- 이를 정리하고 분석한 다음
- 분석 결과를 보고서로 만들어
- 이메일로 팀원들에게 자동 발송하는 과정

이런 반복적인 작업을 수동으로 진행하면 시간도 많이 소요되고 실수할 가능성도 높습니다.
워크플로우 툴은 이런 과정을 코드로 작성하여 자동화함으로써 사람의 개입 없이도 정확하게 작업이 수행되도록 합니다.

데이터 분석 분야에서 주로 사용되는 워크플로우 툴로는 Apache Airflow, Prefect, Dagster 등이 있으며, 이들은 작업 실패 시 자동 재시도, 작업 간 의존성 관리, 스케줄링, 모니터링 등의 기능을 제공합니다.

## 데이터 분석 분야에서의 workflow tool

데이터 분석 및 엔지니어링 분야에서 워크플로우 관리 도구의 중요성은 지속적으로 대두되고 있습니다.
이는 데이터의 규모와 복잡성이 증가하면서 수동적인 작업 관리로는 한계가 있기 때문입니다.

```{python}
#| fig-cap: "데이터 팀이 직면한 문제 (Unravel 설문조사)"

import matplotlib.pyplot as plt
import numpy as np

issues = [
    'Lack of visibility across the environment',
    'lack of proactive alerts',
    'Expensive runaway jobs/pipelines',
    'Lack of experts',
    'No visibility into cost/usage'
]
percentages = [52, 37, 35, 30, 25]  # Sample data values (first one is actual data)

# Graph settings
plt.figure(figsize=(10, 6))
bars = plt.barh(issues, percentages, color=['#ff7f0e', '#1f77b4', '#2ca02c', '#d62728', '#9467bd'])
plt.xlabel('Response Rate (%)', fontsize=12)
plt.xlim(0, 100)

# Highlighting the first bar
bars[0].set_color('#ff7f0e')
bars[0].set_edgecolor('black')
bars[0].set_linewidth(1.5)

# Adding data labels
for i, v in enumerate(percentages):
    plt.text(v + 1, i, f'{v}%', va='center', fontweight='bold')

plt.tight_layout()
```

- [unravel 2022 dataops 설문조사](https://www.unraveldata.com/resources/key-findings-of-the-2022-dataops-unleashed-survey/)에 따르면, 데이터 팀이 가장 많이 겪는 문제는 환경 전체적인 가시성 부족입니다. 이는 데이터 파이프라인이 복잡해지면서 작업의 흐름을 파악하기 어려워지고, 오류 발생 시 대응이 늦어지는 문제를 의미합니다. Apache Airflow와 같은 워크플로우 관리 도구는 작업 자동화, 스케줄링, 모니터링을 통해 데이터 팀의 생산성을 향상시키고, 오류를 최소화하는 데 큰 도움을 줍니다. (최신 자료는 잘 안보이네요. 2022년 자료긴 하지만 뭐.. 이정도도 나쁘지 않죠)

- 또한 [Gartner의 2025 트렌드](https://www.gartner.com/en/newsroom/press-releases/2025-03-05-gartner-identifies-top-trends-in-data-and-analytics-for-2025)에 따르면 데이터 분석 전략에서 주목해야 할 최신 트렌드로 **다양한 형태의 데이터를 통합 관리하는 시스템**과 **쉽게 활용 가능한 데이터 상품화**를 강조하고 있습니다. 이러한 트렌드를 실현하기 위해서는 데이터를 수집하고 처리하는 과정을 자동화하고 효율적으로 관리하는 것이 필수적인데, Apache Airflow와 같은 작업 자동화 도구가 바로 이런 필요성을 충족시키는 핵심 기술이 될 수 있습니다.

## workflow 활용 사례

### 국내 기업

추가 예정

### 해외 기업

추가 예정

## Workflow Tool 다운로드 통계

```{python}
#| fig-cap: "2024.03~2025.02 데이터 관련 Workflow tool 다운로드 수 비교"

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# 저장된 이미지 파일 경로
image_path = "monthly_downloads.png"

# 이미지 불러오기 및 표시
plt.figure(figsize=(12, 6))
img = mpimg.imread(image_path)
plt.imshow(img)
plt.axis("off")  # 축 숨기기
plt.tight_layout()

# 이미지 출력
plt.show()
```

- PyPI 다운로드 통계를 통해 경쟁 기술과 비교한 상대적인 Airflow의 사용 비율을 조사해 보았습니다.
- Airflow는 데이터 분석 workflow 툴 중에서도 가장 많은 다운로드 수를 기록하고 있습니다.
- 물론 다운로드가 반드시 실사용으로 이어진다고 보장할 수는 없지만 다른 tool에 비해 상대적으로 많은 관심을 받고 있다는 것을 알 수 있습니다.

## Airflow github star 수

```{python}
#| fig-cap: "workflow tool github star 수 변화"

import pandas as pd
import matplotlib.pyplot as plt

# CSV 파일 읽기
file_path = '_data/star-history-2025324.csv'
df = pd.read_csv(file_path)

# 날짜 형식을 datetime으로 변환
df['Date'] = pd.to_datetime(df['Date'])

# 플롯 생성
plt.figure(figsize=(12, 6))

# 각 repository에 대해 플롯
for repo in df['Repository'].unique():
    repo_data = df[df['Repository'] == repo]
    plt.plot(repo_data['Date'], repo_data['Stars'], label=repo, marker='o')

# 그래프 설정
plt.xlabel('Date')
plt.ylabel('Stars')
plt.legend()
plt.xticks(rotation=45)

# 레이아웃 조정
plt.tight_layout()

# 그래프 표시
plt.show()
```

- GitHub Star 수는 오픈소스 프로젝트의 인기도와 커뮤니티 활성화 정도를 보여주는 중요한 지표입니다.
- Apache Airflow는 2015년 출시 이후 꾸준히 성장하여 2025년까지 약 32,000개의 Star를 기록했습니다.
- 후발 주자인 Prefect와 Dagster도 성장세를 보이고 있으나, Airflow가 여전히 시장을 선도하고 있습니다.
- 특히 2021년 이후 Airflow의 Star 수 증가 속도가 더욱 가속화되는 추세를 보이고 있어, 워크플로우 도구 시장에서의 입지가 더욱 공고해지고 있습니다.

## Airflow를 사용하는 직종 비율

```{python}
#| fig-cap: "2024 Airflow 이용자 중 직종별 비율"

import numpy as np

# Airflow demand data by job role
roles = ['Data Engineer', 'Analytics Engineer', 'Solutions Architect', 'DevOps Engineer', 'Software Enginner', 'Data Scientist', 'Data Analyst', 'ML Engineer', 'Business Analyst']
percentages = [61.5, 6.4, 5.3, 5.1, 4.7, 4.4, 4.1, 2.2, 1]

# 파이 차트 설정
plt.figure(figsize=(10, 8))
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22']
explode = [0.1, 0, 0, 0, 0, 0, 0, 0, 0]  # Data Engineer 부분만 강조

# 파이 차트 생성
wedges, texts, autotexts = plt.pie(
    percentages,
    explode=explode,
    labels=None, 
    colors=colors,
    autopct='%1.1f%%',  # 퍼센트 표시 형식
    startangle=90,
    shadow=True,
    textprops={'fontsize': 12}
)

# 텍스트 스타일 설정
plt.setp(autotexts, size=10, weight='bold')
plt.axis('equal')  # 원형으로 보이게 설정

# 레이블 위치 수동 조정
bbox_props = dict(boxstyle="round,pad=0.3", fc="w", ec="k", lw=0.72, alpha=0.8)
kw = dict(arrowprops=dict(arrowstyle="-"), bbox=bbox_props, zorder=0, va="center")

for i, p in enumerate(wedges):
    ang = (p.theta2 - p.theta1) / 2. + p.theta1
    y = np.sin(np.deg2rad(ang))
    x = np.cos(np.deg2rad(ang))
    
    # 작은 조각들의 경우 레이블을 더 바깥쪽으로 배치
    connectionstyle = "angle,angleA=0,angleB={}".format(ang)
    
    # ML Engineer와 Business Analyst는 작은 조각이므로 특별히 조정
    if roles[i] == "Business Analyst":
        horizontalalignment = "left" if x < 0 else "right"
        dist = 1.1  # 더 멀리 배치
    elif roles[i] == "Solutions Architect":
        horizontalalignment = "left" if x < 0 else "right"
        dist = 1.45  # 더 멀리 배치
    elif roles[i] == "Data Engineer":
        horizontalalignment = "left" if x < 0 else "right"
        dist = 1.45  # 더 멀리 배치
    else:
        horizontalalignment = "center"
        dist = 1.2  # 기본 거리
    
    plt.annotate(roles[i], 
                xy=(x, y), 
                xytext=(x * dist, y * dist), 
                horizontalalignment=horizontalalignment,
                **kw)

plt.tight_layout()
```

- [Airflow 공식 사이트 2024 설문조사 결과](https://airflow.apache.org/blog/airflow-survey-2024/)를 참고해서 작성했습니다.
- airflow 기술을 요구하는 직종 중 **데이터 엔지니어** 직무에서 요구되는 비율이 높습니다.
- 생각보다 data engineer의 비율이 아주 높지는 않습니다. Airflow 기술을 익힌다면 데이터 분야의 다양한 직무에서 활동할 수 있는 기회가 높아질 것으로 기대됩니다.

## Airflow 사용자들의 만족도

```{python}
#| fig-cap: "2024 Airflow 사용자 만족도 조사 결과"

import matplotlib.pyplot as plt
import numpy as np

# Data preparation
categories = ['Job Importance', 'Recommendation Intent']
positive = [93, 91]
negative = [7, 9]

# Set figure size
plt.figure(figsize=(10, 5))

# Set bar positions
bar_width = 0.35
r1 = np.arange(len(categories))
r2 = [x + bar_width for x in r1]

# Create bar chart
plt.bar(r1, positive, color='#1f77b4', width=bar_width, edgecolor='grey', label='Positive')
plt.bar(r2, negative, color='#ff7f0e', width=bar_width, edgecolor='grey', label='Negative')

# Set axes and labels
plt.xlabel('Category', fontweight='bold', fontsize=12)
plt.ylabel('Response Rate (%)', fontweight='bold', fontsize=12)
plt.xticks([r + bar_width/2 for r in range(len(categories))], categories, fontsize=11)
plt.yticks(np.arange(0, 101, 10), fontsize=10)

# Add percentage labels above bars
for i, v in enumerate(positive):
    plt.text(i - 0.05, v + 1, f"{v}%", fontweight='bold', fontsize=11)
    
for i, v in enumerate(negative):
    plt.text(i + bar_width - 0.05, v + 1, f"{v}%", fontweight='bold', fontsize=11)

# Add legend
plt.legend(loc='upper right', fontsize=10)

plt.tight_layout()

# Display the plot
plt.show()
```

- [Airflow 공식 사이트 2024 설문조사 결과](https://airflow.apache.org/blog/airflow-survey-2024/)에 따르면, 93%의 사용자가 자신의 직무에서 Airflow가 중요하다고 답했습니다.
- 같은 조사에서 91%의 사용자가 Airflow를 다른 사람에게 추천할 의향이 있다고 답했습니다.
- 이처럼 높은 만족도는 Airflow가 실제 업무 환경에서 효과적으로 작동하고 있음을 보여줍니다.

\clearpage

## 채용 시장에서의 Airflow 수요

학생 입장에서 실제 기업에서 활용하고 있는 기술 현황에 대해 조사하기에는 채용 공고가 가장 직접적인 정보 제공원이라고 판단해서 이쪽으로 조사해봤습니다.

뭔가 대규모로 분석해보고 싶었는데 볼 수 있는 정보가 길어봤자 2주가 끝이더라고요.
그래서 가볍게 Job Description 예시 몇개 가져왔습니다.

### 주요 기업 JD 발췌 예시

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-03-22-18-23-30.png}}
\caption{\href{https://toss.im/career/job-detail?job\_id=4071103003\&company=\%ED\%86\%A0\%EC\%8A\%A4\%EC\%A6\%9D\%EA\%B6\%8C\&detailedPosition=Infra}{토스 증권 Data Engineer JD}}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-03-22-17-31-13.png}}
\caption{\href{https://www.lgresearch.ai/careers/view?seq=228}{LG MLOps Engineer Internship}}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-03-22-18-24-36.png}}
\caption{\href{https://www.google.com/about/careers/applications/jobs/results/124798537199166150-senior-software-engineer-enterprise-data-and-engineering?q=airflow}{Google Senior Software Engineer, Enterprise Data and Engineering}}
\label{fig:toss_jd}
\end{center}
\end{figure}

국내 뿐만 아니라 해외 주요 기업에서도 Airflow를 활용한 데이터 파이프라인 구축 및 관리 역할에 수요가 있습니다. (그래봤자 3개만 발췌했지만요)

이상의 내용들을 통해 데이터 분석 직무에서 workflow tool에 대한 중요성은 대두되고 있고, Airflow는 그 중에서도 높은 위상을 가지며, 현재 다양한 데이터 분석 직무에서 Airflow 기술을 활용하고 있음을 알 수 있습니다.

# Airflow Demo

## 개요

Apache Airflow를 활용해 데이터 파이프라인을 구축해보겠습니다.
아래와 같은 흐름으로 진행될 예정입니다.

![파이프라인 프로세스](img/2025-04-06-22-30-26.png)

데이터베이스에는 각각 아래와 같은 demo 데이터를 저정했습니다.

```{.python filename=init-mysql1.sql}
CREATE TABLE shopping_data (
    amount FLOAT,
    date DATE
);

INSERT INTO shopping_data (amount, date) VALUES
    (300.00, '2025-04-01'),
    (280.00, '2025-04-02'),
    (290.00, '2025-04-03'),
    (200.00, '2025-04-04'),
    (150.00, '2025-04-05'),
    (130.00, '2025-04-06'),
    (140.00, '2025-04-07'),
    (160.00, '2025-04-08'),
    (270.00, '2025-04-09'),
    (180.00, '2025-04-10'),
    (120.00, '2025-04-11'),
    (70.00,  '2025-04-12'),
    (60.00,  '2025-04-13'),
    (65.00,  '2025-04-14'),
    (100.00, '2025-04-15'),
    (110.00, '2025-04-16'),
    (250.00, '2025-04-17'),
    (260.00, '2025-04-18'),
    (255.00, '2025-04-19'),
    (220.00, '2025-04-20');
```

```{.python filename=init-mysql2.sql}
CREATE TABLE weather_data (
    date DATE,
    temperature FLOAT
);

INSERT INTO weather_data (date, temperature) VALUES
    ('2025-04-01', 18.5),
    ('2025-04-02', 19.2),
    ('2025-04-03', 17.8),
    ('2025-04-04', 20.1),
    ('2025-04-05', 21.3),
    ('2025-04-06', 22.0),
    ('2025-04-07', 20.5),
    ('2025-04-08', 19.8),
    ('2025-04-09', 18.7),
    ('2025-04-10', 20.2),
    ('2025-04-11', 21.5),
    ('2025-04-12', 22.7),
    ('2025-04-13', 23.1),
    ('2025-04-14', 22.8),
    ('2025-04-15', 21.6),
    ('2025-04-16', 20.9),
    ('2025-04-17', 19.5),
    ('2025-04-18', 18.3),
    ('2025-04-19', 19.0),
    ('2025-04-20', 20.4);
```

\clearpage

매 달 자동으로 위의 데이터를 불러와 날씨와 판매량의 상관관계를 분석하고, 그 결과를 HTML 형식으로 이메일로 전송하는 파이프라인을 구축해보겠습니다.

## Airflow 설치

먼저 airflow를 설치합니다.
아래의 명령어로 간단하게 세팅할 수 있습니다.

```
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml'
```

그러면 airflow 세팅에 필요한 설정 파일이 자동으로 다운로드 됩니다.

저는 여기서 이번 demo를 위해 몇가지 설정을 추가했습니다.

```{.python filename=docker-compose.yml}
AIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp
AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
AIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT:-587}
AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}
AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}
AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_FROM_MAIL}
```

먼저 메일 전송을 위한 설정을 추가해줍니다.

```{.python filename=docker-compose.yml}
services:
  mysql1:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root}
      MYSQL_DATABASE: ${SHOPPING_DB_NAME:-shopping_db}
      MYSQL_USER: ${MYSQL_USER:-your_user}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-your_password}
    volumes:
      - ./mysql1-data:/var/lib/mysql
      - ./init-mysql1.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-prootpassword"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  mysql2:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root}
      MYSQL_DATABASE: ${WEATHER_DB_NAME:-weather}
      MYSQL_USER: ${MYSQL_USER:-your_user}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-your_password}
    volumes:
      - ./mysql2-data:/var/lib/mysql
      - ./init-mysql2.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-prootpassword"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
```

database 역할을 해줄 환경 두 개도 설정했습니다.

\clearpage

이제 아래의 명령어를 실행한 후 localhost:8080으로 접속하면 Airflow UI에 접속할 수 있습니다.

```
docker compose up -d
```

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-19-27.png}}
\caption{airflow 접속 화면}
\label{fig:toss_jd}
\end{center}
\end{figure}

초기 id 와 password는 airflow로 설정되어 있습니다.

## DAG 설정 (설명은 나중에)

아래와 같이 파일을 작성해서 dags 폴더에 저장합니다.

```{.python filename=dags/demo.py}
from airflow import DAG
from airflow.operators.python import PythonOperator
import pandas as pd
from datetime import datetime
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.operators.email import EmailOperator
with DAG(
    dag_id='integrate_shopping_weather_data',
    schedule='@monthly',
    start_date=datetime(2024, 4, 4),
    catchup=False,
) as dag:
```

```{.python filename=dags/demo.py}
    def extract_shopping_data():
        hook = MySqlHook(mysql_conn_id='shopping_source')
        conn = hook.get_sqlalchemy_engine().connect()
        df = pd.read_sql("SELECT * FROM shopping_data", conn)
        df.to_csv('/tmp/shopping_data.csv', index=False)

    extract_shopping_data_task = PythonOperator(
        task_id='extract_shopping_data',
        python_callable=extract_shopping_data,
    )

    def extract_weather_data():
        hook = MySqlHook(mysql_conn_id='weather_source')
        conn = hook.get_sqlalchemy_engine().connect()
        df = pd.read_sql("SELECT * FROM weather_data", conn)
        df.to_csv('/tmp/weather_data.csv', index=False)

    extract_weather_data_task = PythonOperator(
        task_id='extract_weather_data',
        python_callable=extract_weather_data,
    )
```

```{.python filename=dags/demo.py}
    def preprocess_and_integrate():
        shopping_df = pd.read_csv('/tmp/shopping_data.csv')
        weather_df = pd.read_csv('/tmp/weather_data.csv')
        shopping_df['amount'] = shopping_df['amount'].fillna(shopping_df['amount'].mean())
        weather_df['temperature'] = weather_df['temperature'].fillna(weather_df['temperature'].mean())
        integrated_df = pd.merge(shopping_df, weather_df, on='date', how='inner')
        integrated_df.to_csv('/tmp/integrated_data.csv', index=False)
        return "Data preprocessed and integrated"

    process_task = PythonOperator(
        task_id='preprocess_and_integrate',
        python_callable=preprocess_and_integrate,
    )
```

```{.python filename=dags/demo.py}
    def generate_report(**kwargs):
        df = pd.read_csv('/tmp/integrated_data.csv')
        correlation = df['temperature'].corr(df['amount'])
        bins = [0, 20, 22, 25]
        labels = ['Low (<20°C)', 'Medium (20-22°C)', 'High (>22°C)']
        df['temp_range'] = pd.cut(df['temperature'], bins=bins, labels=labels, include_lowest=True)
        temp_grouped = df.groupby('temp_range')['amount'].mean().round(2)
        html_report = f"""
        <h2>Shopping Amount vs Temperature Analysis Report</h2>
        <p><b>Data Period:</b> 2025-04-01 to 2025-04-20</p>
        <h3>1. Correlation</h3>
        <p>Correlation Coefficient: {correlation:.2f}</p>
        <p>(Negative value indicates higher temperature correlates with lower purchases)</p>
        <h3>2. Average Amount by Temperature Range</h3>
        <ul>
            <li>{labels[0]}: ${temp_grouped[labels[0]]}</li>
            <li>{labels[1]}: ${temp_grouped[labels[1]]}</li>
            <li>{labels[2]}: ${temp_grouped[labels[2]]}</li>
        </ul>
        <p><b>Conclusion:</b> Higher temperatures (>22°C) are associated with lower average purchase amounts.</p>
        """
        kwargs['ti'].xcom_push(key='html_report', value=html_report)
        return "Report generated"

    report_task = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report,
        provide_context=True,
    )
```

```{.python filename=dags/demo.py}
    email_task = EmailOperator(
        task_id='send_email_report',
        to='cryscham123@soongsil.ac.kr',
        subject='Daily Shopping vs Temperature Report',
        html_content="{{ ti.xcom_pull(task_ids='generate_report', key='html_report') }}",
    )

    [extract_shopping_data_task, extract_weather_data_task] >> process_task >> report_task >> email_task
```

\clearpage

## Connections 설정

Airflow UI에서 Connections 메뉴를 통해 데이터 소스를 위한 Connection을 설정합니다.

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-21-56.png}}
\caption{상단 메뉴의 Connections를 눌러 줍니다.}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-22-32.png}}
\caption{+ 기호를 선택}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-23-41.png}}
\caption{설정에 맞게 form을 작성한 후 save를 눌러줍니다.}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-24-55.png}}
\caption{나머지 db도 알맞게 설정해줍니다.}
\label{fig:toss_jd}
\end{center}
\end{figure}

## DAG 실행

이후 DAG를 수동으로 실행하면 아래와 같은 결과를 확인할 수 있습니다.

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-25-49.png}}
\caption{Trigger DAG를 선택}
\label{fig:toss_jd}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-26-24.png}}
\caption{결과화면에서 task간 의존 관계와 결과를 확인할 수 있습니다.}
\label{fig:toss_jd}
\end{center}
\end{figure}

\clearpage

## 결과

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{img/2025-04-06-22-27-13.png}}
\caption{메일도 정상적으로 확인해볼 수 있습니다.}
\label{fig:toss_jd}
\end{center}
\end{figure}
