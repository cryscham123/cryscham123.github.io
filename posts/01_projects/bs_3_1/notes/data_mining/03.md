---
title: "random forest"
date: 2025-06-13
categories: ["data mining"]
---

![](/img/human-thumb.jpg){.post-thumbnail}

1. bootstrap sampling
1. k개의 feature 중 랜덤하게 선택해서 노드를 추가
1. 각 tree 학습
1. 각 tree의 예측값을 투표하여 최종 예측값 결정
    - 분류: 다수결 투표
    - 회귀: 평균값
- vs bagging
    - bagging: 데이터의 행에 무작위성을 부여
    - random forest: 데이터의 행과 열에 무작위성을 부여

- oob(out-of-bag) error: bootstrap sampling으로 학습에 사용되지 않은 데이터로 평가
    - 사실상 별도의 validation set을 필요로 하지 않음

## 장점

- Classification, Regression문제 모두 해결 가능
- Accuracy, out-of-bag error에 우수한 결과
- Validation을 위한 별도의 data set이 필요하지 않음
- Built-in validation set
- Overfitting이 없다
- Outlier에 강함
- Missing data를 잘 처리
- 선처리 작업을 최소화
- Feature의 선택을 자동처리
- 변수 삭제 없이 수천 개의 입력 변수를 처리

## 단점

- 속도가 느림
- 해석이 어렵다
