---
title: "ëª¨ë¸ë§, í‰ê°€ í…œí”Œë¦¿"
date: 2025-09-21
categories: ["ë°ì´í„° ë¶„ì„"]
directories: ["adp_ì‹¤ê¸°"]
---

## ëª¨ë¸ ì •ì˜

```{python}
#| eval: false

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_features), # outlierê°€ ë§ë‹¤ë©´ RobustScalerë¥¼ ê³ ë ¤í•˜ì
    ('cat', OneHotEncoder(drop='first'), cat_features),
    ('ord', OrdinalEncoder(), ord_features) # ìˆœì„œí˜• ë³€ìˆ˜. í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ê·¸ëƒ¥ DataFrame.map()ìœ¼ë¡œ ì§ì ‘ ì¸ì½”ë”©í•´ì£¼ëŠ”ê²Œ ë” ë‚˜ì„ë“¯
])
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model_name', YourModel(random_state=42))
])
```

- category í˜•ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¶ë‹¤ë©´, pipelineë³´ë‹¤ëŠ” LGBM, CatBoost, XGBoost ë“±ì˜ ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•˜ëŠ”ê²Œ ë” ë‚˜ìŒ.
    - sklearnì€ ë°ì´í„°ë“¤ì„ numpy arrayë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì— category í˜•ì„ ìœ ì§€í•˜ì§€ ëª»í•¨

## Grid Search & Bayesian Optimization

```{python filename="grid search"}
#| eval: false

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# íŒŒë¼ë¯¸í„° ì´ë¦„ ì•ì— pipelineì—ì„œ ì‚¬ìš©í•œ 'ëª¨ë¸ì´ë¦„__'ì„ ë¶™ì—¬ì•¼ í•¨
params = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [10, 50, 100],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=pipeline, 
                           param_grid=params, 
                           cv=5, 
                           scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

# ============ ë¶„ë¥˜ scoring ============

y_pred_proba = best_model.predict_proba(X_test)[:, 1] # ë‹¤ì¤‘í´ë˜ìŠ¤ì¸ ê²½ìš° [:, 1] ì œê±°

test_acc = accuracy_score(y_test, y_pred)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
    y_test, 
    y_pred, 
    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒ
)
test_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multi_class='ovr', 'ovo' ì¤‘ ì„ íƒ

print(f"test_accuracy: {test_acc:.4f}")
print(f"test_precision: {test_precision:.4f}")
print(f"test_recall: {test_recall:.4f}")
print(f"test_f1_score: {test_f1:.4f}")
print(f"test_auc: {test_auc:.4f}")

# ============ íšŒê·€ scoring ============

rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"test_rmse: {rmse:.4f}")
print(f"test_mae: {mae:.4f}")
print(f"test_r2: {r2:.4f}")
```

- ğŸ‘‡ ì‹œí—˜ì—ì„œ ì‚¬ìš© ë¶ˆê°€. í•˜ì§€ë§Œ kaggleì´ë‚˜ daconì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë¨

```{python filename="bayesian optimization"}
#| eval: false
import optuna
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

def objective(trial):
    # ëª¨ë¸ì— ì í•©í•œ íŒŒë¼ë¯¸í„°ì— ë§ê²Œ ìˆ˜ì •
    params = {
        'classifier__n_estimators': trial.suggest_int("n_estimators", 100, 500, 100),
        'classifier__max_features': trial.suggest_categorical("max_features", ['sqrt', 'log2']),
        'classifier__max_depth': trial.suggest_int("max_depth", 10, 110, 20),
        'classifier__min_samples_split': trial.suggest_int("min_samples_split", 2, 10, 2),
        'classifier__min_samples_leaf': trial.suggest_int("min_samples_leaf", 1, 4, 1)
    }
    pipeline.set_params(**params)

    cv_score = cross_val_score(pipeline,
                               X_train,
                               y_train,
                               cv=5,
                               scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error
    mean_cv_accuracy = cv_score.mean()
    return mean_cv_accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

pipeline.set_params(**study.best_params)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

# ============ ë¶„ë¥˜ scoring ============

y_pred_proba = pipeline.predict_proba(X_test)[:, 1]


test_acc = accuracy_score(y_test, y_pred)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
    y_test, 
    y_pred, 
    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©
)
test_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multiclass='ovr', 'ovo' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©

print("test_accuracy:", test_acc)
print("test_precision:", test_precision)
print("test_recall:", test_recall)
print("test_f1_score:", test_f1)
print("test_auc:", test_auc)

# ============ íšŒê·€ scoring ============

rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"test_rmse: {rmse:.4f}")
print(f"test_mae: {mae:.4f}")
print(f"test_r2: {r2:.4f}")
```

- ğŸ‘‡ ì‹œí—˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

```{python filename="bayesian optimization hyperopt"}
#| eval: false
from hyperopt import hp, STATUS_OK, fmin, tpe, Trials
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
search_space = {
    'classifier__n_estimators': hp.quniform('n_estimators', 100, 500, 100),
    'classifier__max_features': hp.choice('max_features', ['sqrt', 'log2']),
    'classifier__max_depth': hp.quniform('max_depth', 10, 110, 20),
    'classifier__min_samples_split': hp.quniform('min_samples_split', 2, 10, 2),
    'classifier__min_samples_leaf': hp.quniform('min_samples_leaf', 1, 4, 1)
}

def objective_func(params):
    # íŒŒë¼ë¯¸í„° íƒ€ì… ë³€í™˜ (hyperoptëŠ” floatë¡œ ë°˜í™˜í•˜ë¯€ë¡œ intë¡œ ë³€í™˜ í•„ìš”)
    params_int = {
        'classifier__n_estimators': int(params['classifier__n_estimators']),
        'classifier__max_features': params['classifier__max_features'],
        'classifier__max_depth': int(params['classifier__max_depth']),
        'classifier__min_samples_split': int(params['classifier__min_samples_split']),
        'classifier__min_samples_leaf': int(params['classifier__min_samples_leaf'])
    }
    pipeline.set_params(**params_int)
    
    cv_score = cross_val_score(pipeline,
                               X_train,
                               y_train,
                               cv=5,
                               scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error
    mean_cv_score = cv_score.mean()
    
    # hyperoptëŠ” ìµœì†Œí™”í•˜ë¯€ë¡œ ìŒìˆ˜ ë°˜í™˜ (ìµœëŒ€í™”í•˜ë ¤ëŠ” ê²½ìš°)
    return {'loss': -mean_cv_score, 'status': STATUS_OK}

# ìµœì í™” ì‹¤í–‰
trials = Trials()
best = fmin(fn=objective_func,
            space=search_space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)

# ìµœì  íŒŒë¼ë¯¸í„° ì ìš© (íƒ€ì… ë³€í™˜ í¬í•¨)
best_params = {
    'classifier__n_estimators': int(best['n_estimators']),
    'classifier__max_features': ['sqrt', 'log2'][int(best['max_features'])],
    'classifier__max_depth': int(best['max_depth']),
    'classifier__min_samples_split': int(best['min_samples_split']),
    'classifier__min_samples_leaf': int(best['min_samples_leaf'])
}

pipeline.set_params(**best_params)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

# ============ ë¶„ë¥˜ scoring ============

y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

test_acc = accuracy_score(y_test, y_pred)
test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
    y_test, 
    y_pred, 
    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©
)
test_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multiclass='ovr', 'ovo' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©

print("test_accuracy:", test_acc)
print("test_precision:", test_precision)
print("test_recall:", test_recall)
print("test_f1_score:", test_f1)
print("test_auc:", test_auc)

# ============ íšŒê·€ scoring ============

rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"test_rmse: {rmse:.4f}")
print(f"test_mae: {mae:.4f}")
print(f"test_r2: {r2:.4f}")
```

## ëª¨ë¸ í‰ê°€ visualization

### ROC AUC

```{python}
#| eval: false
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# ROC ê³¡ì„  ê³„ì‚°
y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

# ROC ê³¡ì„  ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```

### Feature Importance

```{python}
#| eval: false
import matplotlib.pyplot as plt
import seaborn as sns

importances = pipeline.named_steps['classifier'].feature_importances_
ftr_importance = pd.Series(importances, index=X.columns)
ftr_top = ftr_importance.sort_values(ascending=False)[:20] # ì›í•˜ëŠ” ìˆ˜ ë§Œí¼ ì„¤ì •
sns.barplot(ftr_top20, y=ftr_top.index)
plt.show()
```

### Drop Column importance

```{python}
#| eval: false
from sklearn.base import clone

for col in X_train.columns:
    X_train_dropped = X_train.drop(columns=[col])
    X_test_dropped = X_test.drop(columns=[col])

    # ì‹œê°„ì˜ ë‹¨ì¶•ì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # ì—„ë°€í•˜ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´ ê° iterationë§ˆë‹¤ ë‹¤ì‹œ íŠœë‹
    model_dropped = clone(best_model)
    model_dropped.fit(X_train_dropped, y_train)

    score = model_dropped.score(X_test_dropped, y_test)
    print(f"{col}'s difference:", best_model.score(X_test, y_test) - score) # í˜¹ì€ ë‹¤ë¥¸ í‰ê°€ ì§€í‘œ ì‚¬ìš©
```

### Permutation importance

```{python}
#| eval: false

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(best_model).fit(X_test, y_test)
eli5.show_weights(perm, feature_names=X_test.columns.tolist())
```

- ìŒìˆ˜ ê°’ì€ ì œê±°í•´ë„ ëœë‹¤ëŠ” ëœ»
- ë‹¨ì : ìƒê´€ê´€ê³„ê°€ ë†’ì€ featureì— ëŒ€í•´ì„œ ë¹„í˜„ì‹¤ì ì¸ ë°ì´í„° ì¡°í•©ì´ ìƒì„±ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.
    - (ex. shuffleì„ í†µí•´ í‚¤ 180cm, ëª¸ë¬´ê²Œ 30kgì˜ ë°ì´í„°ê°€ ì¡°í•©ë˜ëŠ” ê²½ìš°)

## ensemble

- stacking:
    - ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„, ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë©”íƒ€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.
    - ë©”íƒ€ ëª¨ë¸ì€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
- voting, averaging:
    - ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„, ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íˆ¬í‘œ(voting)í•˜ê±°ë‚˜ í‰ê· (averaging)í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
    - ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ë‹¤ìˆ˜ê²° íˆ¬í‘œ(hard voting) ë˜ëŠ” í™•ë¥  í‰ê· (soft voting)ì„ ì‚¬ìš©í•˜ê³ , íšŒê·€ ë¬¸ì œì—ì„œëŠ” ë‹¨ìˆœ í‰ê·  ë˜ëŠ” ê°€ì¤‘ í‰ê· ì„ ì‚¬ìš©í•œë‹¤.
- bagging
    - vs cross validation:
        - cross validationì€ ì´ë¯¸ ìƒì„±ëœ ëª¨ë¸ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ë°©ë²•. ëª¨ë¸ êµ¬ì¶• ë°©ë²•ì€ ì•„ë‹˜
        - baggingì€ ë¶„ì‚°ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•¨
- boosting: sequentially í•™ìŠµ
    - ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

```{python filename="stacking"}
#| eval: false

from sklearn.ensemble import StackingClassifier, StackingRegressor

stacking_lf = StackingClassifier(estimators=[
    ('lr', LogisticRegression()),
    ('dt', DecisionTreeClassifier()),
    ('rf', RandomForestClassifier())
], final_estimator=LogisticRegression())

stacking_rf = StackingRegressor(estimators=[
    ('lr', LinearRegression()),
    ('dt', DecisionTreeRegressor()),
    ('rf', RandomForestRegressor())
], final_estimator=LinearRegression())

stacking_lf.fit(X_train, y_train)
stacking_rf.fit(X_train, y_train)
stacking_lf.predict(X_test)
stacking_rf.predict(X_test)
```

```{python filename="voting"}
#| eval: false

from sklearn.ensemble import VotingClassifier, VotingRegressor

voting_lf = VotingClassifier(estimators=[
    ('lr', LogisticRegression()),
    ('dt', DecisionTreeClassifier()),
    ('rf', RandomForestClassifier())
], voting='soft') # or hard


voting_rf = VotingRegressor(estimators=[
    ('lr', LinearRegression()),
    ('dt', DecisionTreeRegressor()),
    ('rf', RandomForestRegressor())
], weights=[1, 1, 2])


voting_lf.fit(X_train, y_train)
voting_rf.fit(X_train, y_train)
voting_lf.predict(X_test)
voting_rf.predict(X_test)
```

## ëª¨ë¸ íŒŒë¼ë¯¸í„°

- ì•„ë˜ë¶€í„°ëŠ” aiì˜ ë„ì›€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.
- ì ë‹¹íˆ ëª‡ ê°œë§Œ ê³¨ë¼ì„œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

### Logistic íšŒê·€ë¶„ì„

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__penalty": ['l1', 'l2', 'elasticnet', 'none'],  # ì •ê·œí™” ë°©ë²•
    "classifier__C": [0.001, 0.01, 0.1, 1, 10, 100],  # ì •ê·œí™” ê°•ë„ (ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™”)
    "classifier__l1_ratio": [0.1, 0.3, 0.5, 0.7, 0.9],  # elasticnetì¼ ë•Œë§Œ ì‚¬ìš©
    "classifier__solver": ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    "classifier__max_iter": [100, 500, 1000, 2000]  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__penalty": hp.choice("penalty", ['l1', 'l2', 'elasticnet']),
    "classifier__C": hp.loguniform("C", np.log(0.001), np.log(100)),
    "classifier__l1_ratio": hp.uniform("l1_ratio", 0.1, 0.9),
    "classifier__solver": hp.choice("solver", ['liblinear', 'saga']),
    "classifier__max_iter": hp.quniform("max_iter", 100, 2000, 100)
}
```

### KNN

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__n_neighbors": [3, 5, 7, 9, 11, 15, 21],  # kê°’ (ì´ì›ƒì˜ ìˆ˜)
    "classifier__weights": ['uniform', 'distance'],  # ê°€ì¤‘ì¹˜ ë°©ë²•
    "classifier__algorithm": ['auto', 'ball_tree', 'kd_tree', 'brute'],  # ì•Œê³ ë¦¬ì¦˜
    "classifier__leaf_size": [20, 30, 40, 50],  # ë¦¬í”„ í¬ê¸° (ball_tree, kd_treeìš©)
    "classifier__p": [1, 2],  # ê±°ë¦¬ ì¸¡ë„ (1: ë§¨í•˜íƒ„, 2: ìœ í´ë¦¬ë“œ)
    "classifier__metric": ['minkowski', 'euclidean', 'manhattan']  # ê±°ë¦¬ í•¨ìˆ˜
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__n_neighbors": hp.quniform("n_neighbors", 3, 21, 2),
    "classifier__weights": hp.choice("weights", ['uniform', 'distance']),
    "classifier__algorithm": hp.choice("algorithm", ['auto', 'ball_tree', 'kd_tree']),
    "classifier__leaf_size": hp.quniform("leaf_size", 20, 50, 10),
    "classifier__p": hp.choice("p", [1, 2]),
    "classifier__metric": hp.choice("metric", ['minkowski', 'euclidean', 'manhattan'])
}
```

### Support Vector Machine

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # ì •ê·œí™” íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™”)
    "classifier__kernel": ['linear', 'poly', 'rbf', 'sigmoid'],  # ì»¤ë„ í•¨ìˆ˜
    "classifier__degree": [2, 3, 4, 5],  # poly ì»¤ë„ ì°¨ìˆ˜ (polyì¼ ë•Œë§Œ ì‚¬ìš©)
    "classifier__gamma": ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],  # ì»¤ë„ ê³„ìˆ˜ (rbf, poly, sigmoidìš©)
    "classifier__coef0": [0.0, 0.1, 0.5, 1.0],  # ì»¤ë„ì˜ ë…ë¦½í•­ (poly, sigmoidìš©)
    "classifier__shrinking": [True, False],  # ìˆ˜ì¶• íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© ì—¬ë¶€
    "classifier__tol": [1e-5, 1e-4, 1e-3],  # ì •ì§€ ê¸°ì¤€ í—ˆìš© ì˜¤ì°¨
    "classifier__max_iter": [100, 500, 1000, 2000, -1]  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (-1: ì œí•œ ì—†ìŒ)
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__C": hp.loguniform("C", np.log(0.001), np.log(1000)),
    "classifier__kernel": hp.choice("kernel", ['linear', 'poly', 'rbf', 'sigmoid']),
    "classifier__degree": hp.quniform("degree", 2, 5, 1),  # polyì¼ ë•Œë§Œ
    "classifier__gamma": hp.choice("gamma", ['scale', 'auto'] + [hp.loguniform("gamma_float", np.log(0.001), np.log(10))]),
    "classifier__coef0": hp.uniform("coef0", 0.0, 1.0),
    "classifier__shrinking": hp.choice("shrinking", [True, False]),
    "classifier__tol": hp.loguniform("tol", np.log(1e-5), np.log(1e-3)),
    "classifier__max_iter": hp.quniform("max_iter", 100, 2000, 100)
}
```

### Random Forest

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__n_estimators": [50, 100, 200, 300, 500],  # íŠ¸ë¦¬ ê°œìˆ˜
    "classifier__criterion": ['gini', 'entropy', 'log_loss'],  # ë¶ˆìˆœë„ ì¸¡ì • ê¸°ì¤€
    "classifier__max_depth": [None, 5, 10, 20, 30, 50],  # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (None: ì œí•œ ì—†ìŒ)
    "classifier__min_samples_split": [2, 5, 10, 20],  # ë‚´ë¶€ ë…¸ë“œ ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    "classifier__min_samples_leaf": [1, 2, 4, 10],  # ë¦¬í”„ ë…¸ë“œì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    "classifier__min_weight_fraction_leaf": [0.0, 0.1, 0.2],  # ë¦¬í”„ ë…¸ë“œì— í•„ìš”í•œ ìµœì†Œ ê°€ì¤‘ì¹˜ ë¹„ìœ¨
    "classifier__max_features": ['sqrt', 'log2', None, 0.3, 0.5, 0.7],  # ê° ë¶„í• ì—ì„œ ê³ ë ¤í•  í”¼ì²˜ ìˆ˜
    "classifier__max_leaf_nodes": [None, 50, 100, 200],  # ìµœëŒ€ ë¦¬í”„ ë…¸ë“œ ìˆ˜
    "classifier__min_impurity_decrease": [0.0, 0.01, 0.05, 0.1],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ë¶ˆìˆœë„ ê°ì†ŒëŸ‰
    "classifier__bootstrap": [True, False],  # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‚¬ìš© ì—¬ë¶€
    "classifier__oob_score": [True, False],  # OOB ìŠ¤ì½”ì–´ ê³„ì‚° ì—¬ë¶€
    "classifier__max_samples": [None, 0.5, 0.7, 0.9],  # ê° íŠ¸ë¦¬ì— ì‚¬ìš©í•  ìƒ˜í”Œ ë¹„ìœ¨
    "classifier__ccp_alpha": [0.0, 0.01, 0.05, 0.1]  # ë³µì¡ë„ ê°€ì§€ì¹˜ê¸° íŒŒë¼ë¯¸í„°
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__n_estimators": hp.quniform("n_estimators", 50, 500, 50),
    "classifier__criterion": hp.choice("criterion", ['gini', 'entropy']),
    "classifier__max_depth": hp.choice("max_depth", [None, hp.quniform("max_depth_val", 5, 50, 5)]),
    "classifier__min_samples_split": hp.quniform("min_samples_split", 2, 20, 2),
    "classifier__min_samples_leaf": hp.quniform("min_samples_leaf", 1, 10, 1),
    "classifier__max_features": hp.choice("max_features", ['sqrt', 'log2', None]),
    "classifier__min_impurity_decrease": hp.uniform("min_impurity_decrease", 0.0, 0.1),
    "classifier__bootstrap": hp.choice("bootstrap", [True, False]),
    "classifier__max_samples": hp.choice("max_samples", [None, hp.uniform("max_samples_val", 0.5, 1.0)]),
    "classifier__ccp_alpha": hp.uniform("ccp_alpha", 0.0, 0.1)
}
```

### XGBOOST

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__n_estimators": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜
    "classifier__learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥  (eta)
    "classifier__max_depth": [3, 4, 5, 6, 7, 8],  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    "classifier__min_child_weight": [1, 3, 5, 7],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜ í•©
    "classifier__gamma": [0, 0.1, 0.2, 0.3, 0.4],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ
    "classifier__subsample": [0.6, 0.7, 0.8, 0.9, 1.0],  # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨
    "classifier__colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (íŠ¸ë¦¬ë³„)
    "classifier__colsample_bylevel": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (ë ˆë²¨ë³„)
    "classifier__colsample_bynode": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (ë…¸ë“œë³„)
    "classifier__reg_alpha": [0, 0.01, 0.1, 1, 10],  # L1 ì •ê·œí™” íŒŒë¼ë¯¸í„°
    "classifier__reg_lambda": [0, 0.01, 0.1, 1, 10],  # L2 ì •ê·œí™” íŒŒë¼ë¯¸í„°
    "classifier__max_delta_step": [0, 1, 2, 5, 10],  # ê° íŠ¸ë¦¬ ê°€ì¤‘ì¹˜ ë³€í™”ì˜ ìµœëŒ€ê°’
    "classifier__scale_pos_weight": [1, 2, 3, 5]  # ì–‘ì„± í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë°ì´í„°ìš©)
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__n_estimators": hp.quniform("n_estimators", 50, 500, 50),
    "classifier__learning_rate": hp.loguniform("learning_rate", np.log(0.01), np.log(0.3)),
    "classifier__max_depth": hp.quniform("max_depth", 3, 8, 1),
    "classifier__min_child_weight": hp.quniform("min_child_weight", 1, 7, 1),
    "classifier__gamma": hp.uniform("gamma", 0, 0.4),
    "classifier__subsample": hp.uniform("subsample", 0.6, 1.0),
    "classifier__colsample_bytree": hp.uniform("colsample_bytree", 0.6, 1.0),
    "classifier__colsample_bylevel": hp.uniform("colsample_bylevel", 0.6, 1.0),
    "classifier__colsample_bynode": hp.uniform("colsample_bynode", 0.6, 1.0),
    "classifier__reg_alpha": hp.loguniform("reg_alpha", np.log(0.01), np.log(10)),
    "classifier__reg_lambda": hp.loguniform("reg_lambda", np.log(0.01), np.log(10)),
    "classifier__max_delta_step": hp.quniform("max_delta_step", 0, 10, 1),
    "classifier__scale_pos_weight": hp.quniform("scale_pos_weight", 1, 5, 1)
}
```

### LightGBM

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__n_estimators": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜
    "classifier__learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥ 
    "classifier__max_depth": [3, 5, 7, 10, -1],  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ (-1: ì œí•œ ì—†ìŒ)
    "classifier__num_leaves": [15, 31, 63, 127, 255],  # ë¦¬í”„ ë…¸ë“œ ìˆ˜ (2^max_depth - 1ë³´ë‹¤ ì‘ê²Œ)
    "classifier__min_child_samples": [10, 20, 30, 50],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    "classifier__min_child_weight": [1e-3, 1e-2, 1e-1, 1],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜ í•©
    "classifier__subsample": [0.6, 0.7, 0.8, 0.9, 1.0],  # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨
    "classifier__colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨
    "classifier__reg_alpha": [0, 0.01, 0.1, 1, 10],  # L1 ì •ê·œí™”
    "classifier__reg_lambda": [0, 0.01, 0.1, 1, 10],  # L2 ì •ê·œí™”
    "classifier__min_split_gain": [0, 0.01, 0.1, 0.5],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ gain
    "classifier__min_data_in_leaf": [10, 20, 50, 100],  # ë¦¬í”„ì˜ ìµœì†Œ ë°ì´í„° ìˆ˜
    "classifier__boosting_type": ['gbdt', 'dart', 'goss'],  # ë¶€ìŠ¤íŒ… íƒ€ì…
    "classifier__feature_fraction": [0.6, 0.7, 0.8, 0.9, 1.0],  # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨
    "classifier__bagging_fraction": [0.6, 0.7, 0.8, 0.9, 1.0],  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
    "classifier__bagging_freq": [0, 1, 3, 5]  # ë°°ê¹… ë¹ˆë„
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__n_estimators": hp.quniform("n_estimators", 50, 500, 50),
    "classifier__learning_rate": hp.loguniform("learning_rate", np.log(0.01), np.log(0.3)),
    "classifier__max_depth": hp.choice("max_depth", [-1, hp.quniform("max_depth_val", 3, 10, 1)]),
    "classifier__num_leaves": hp.quniform("num_leaves", 15, 255, 16),
    "classifier__min_child_samples": hp.quniform("min_child_samples", 10, 50, 10),
    "classifier__min_child_weight": hp.loguniform("min_child_weight", np.log(1e-3), np.log(1)),
    "classifier__subsample": hp.uniform("subsample", 0.6, 1.0),
    "classifier__colsample_bytree": hp.uniform("colsample_bytree", 0.6, 1.0),
    "classifier__reg_alpha": hp.loguniform("reg_alpha", np.log(0.01), np.log(10)),
    "classifier__reg_lambda": hp.loguniform("reg_lambda", np.log(0.01), np.log(10)),
    "classifier__min_split_gain": hp.uniform("min_split_gain", 0, 0.5),
    "classifier__min_data_in_leaf": hp.quniform("min_data_in_leaf", 10, 100, 10),
    "classifier__boosting_type": hp.choice("boosting_type", ['gbdt', 'dart', 'goss']),
    "classifier__feature_fraction": hp.uniform("feature_fraction", 0.6, 1.0),
    "classifier__bagging_fraction": hp.uniform("bagging_fraction", 0.6, 1.0),
    "classifier__bagging_freq": hp.quniform("bagging_freq", 0, 5, 1)
}
```

### Catboost

```{python}
#| eval: false

# Grid Searchìš©
params = {
    "classifier__n_estimators": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜
    "classifier__learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥ 
    "classifier__depth": [3, 4, 5, 6, 7, 8],  # íŠ¸ë¦¬ ê¹Šì´ (max_depth ëŒ€ì‹  depth ì‚¬ìš©)
    "classifier__l2_leaf_reg": [1, 3, 5, 10, 20],  # L2 ì •ê·œí™” íŒŒë¼ë¯¸í„°
    "classifier__border_count": [32, 64, 128, 255],  # ìˆ˜ì¹˜í˜• í”¼ì²˜ì˜ ë¶„í• ì  ê°œìˆ˜
    "classifier__bagging_temperature": [0, 0.5, 1, 2, 5],  # ë°°ê¹… ì˜¨ë„ (0: ë¹„í™œì„±í™”)
    "classifier__random_strength": [1, 2, 5, 10],  # íŠ¸ë¦¬ êµ¬ì¡°ì˜ ë¬´ì‘ìœ„ì„±
    "classifier__od_type": ['IncToDec', 'Iter'],  # ì¡°ê¸° ì¢…ë£Œ íƒ€ì…
    "classifier__od_wait": [10, 20, 50],  # ì¡°ê¸° ì¢…ë£Œ ëŒ€ê¸° ë¼ìš´ë“œ
    "classifier__bootstrap_type": ['Bayesian', 'Bernoulli', 'MVS', 'Poisson'],  # ë¶€íŠ¸ìŠ¤íŠ¸ë© íƒ€ì…
    "classifier__subsample": [0.6, 0.7, 0.8, 0.9, 1.0],  # ìƒ˜í”Œë§ ë¹„ìœ¨ (Bernoulliì¼ ë•Œë§Œ)
    "classifier__rsm": [0.5, 0.7, 0.9, 1.0],  # ë¬´ì‘ìœ„ ì„œë¸ŒìŠ¤í˜ì´ìŠ¤ ë°©ë²• (í”¼ì²˜ ìƒ˜í”Œë§)
    "classifier__leaf_estimation_iterations": [1, 3, 5, 10],  # ë¦¬í”„ ê°’ ì¶”ì • ë°˜ë³µ íšŸìˆ˜
    "classifier__grow_policy": ['SymmetricTree', 'Depthwise', 'Lossguide'],  # íŠ¸ë¦¬ ì„±ì¥ ì •ì±…
    "classifier__min_data_in_leaf": [1, 5, 10, 20],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    "classifier__max_leaves": [16, 31, 64, 127]  # ìµœëŒ€ ë¦¬í”„ ìˆ˜ (Lossguideì¼ ë•Œë§Œ)
}

# Bayesian Optimizationìš© (hyperopt)
params_bayes = {
    "classifier__n_estimators": hp.quniform("n_estimators", 50, 500, 50),
    "classifier__learning_rate": hp.loguniform("learning_rate", np.log(0.01), np.log(0.3)),
    "classifier__depth": hp.quniform("depth", 3, 8, 1),
    "classifier__l2_leaf_reg": hp.quniform("l2_leaf_reg", 1, 20, 1),
    "classifier__border_count": hp.choice("border_count", [32, 64, 128, 255]),
    "classifier__bagging_temperature": hp.uniform("bagging_temperature", 0, 5),
    "classifier__random_strength": hp.quniform("random_strength", 1, 10, 1),
    "classifier__od_type": hp.choice("od_type", ['IncToDec', 'Iter']),
    "classifier__od_wait": hp.quniform("od_wait", 10, 50, 10),
    "classifier__bootstrap_type": hp.choice("bootstrap_type", ['Bayesian', 'Bernoulli', 'MVS']),
    "classifier__subsample": hp.uniform("subsample", 0.6, 1.0),
    "classifier__rsm": hp.uniform("rsm", 0.5, 1.0),
    "classifier__leaf_estimation_iterations": hp.quniform("leaf_estimation_iterations", 1, 10, 1),
    "classifier__grow_policy": hp.choice("grow_policy", ['SymmetricTree', 'Depthwise', 'Lossguide']),
    "classifier__min_data_in_leaf": hp.quniform("min_data_in_leaf", 1, 20, 1),
    "classifier__max_leaves": hp.quniform("max_leaves", 16, 127, 1)
}

# ì£¼ìš” íŠ¹ì§•:
# - GPU ì§€ì› (gpu_device_id=-1ë¡œ ì„¤ì •í•˜ë©´ GPU ì‚¬ìš©)
# - ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬ (cat_features íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •)
# - ë‚´ì¥ëœ êµì°¨ê²€ì¦ ë° ì¡°ê¸° ì¢…ë£Œ
# - í…ìŠ¤íŠ¸ ë° ì„ë² ë”© í”¼ì²˜ ì§€ì›
```
