[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Inboxes\n\n\n\n\n\n\n분류되지 않은 노트 (1)\n    \n\n\n\nTitle\nDate\nCategories\n\n\n\n\n인간 관계론 - 데일 카네기\n2025-02-02\n독서, 인간 관계\n\n\n\n\n    \n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n현재 진행중인 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    학부 3학년 1학기\n                    on-going\n                \n                \n                    Started: 2024-12-21\n                    \n                     Ends: 2025-06-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                3학년 1학기 학부 할 일 총 정리\n            \n        \n        \n\n\n\n\n\n\n\n\nAreas\n\n\n\n\n\n관리 / 책임 영역\n\n\n\n\n    \n    \n        \n            Deep Learning\n        \n        \n        \n            42 Seoul\n        \n        \n        \n            Kaggle\n        \n        \n        \n            선형대수\n        \n        \n        \n            Machine Learning\n        \n        \n        \n            AirFlow\n        \n        \n\n\n\n\n\n\n\n\nResources\n\n\n\n\n\n진행 전인 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    TOFEL 준비\n                    before-start\n                \n                \n                    Started: None\n                    \n                     Ended: None\n                    \n                    Calculating...\n                \n                \n                    English\n                \n                준비해 봅시다\n            \n        \n        \n\n\n\n관심 분야\n\n\n\n\n    \n    \n        \n            Blog\n        \n        \n        \n            인생\n        \n        \n        \n            Terraform\n        \n        \n        \n            Problem Solving\n        \n        \n        \n            Smart Contract\n        \n        \n        \n            금융\n        \n        \n        \n            Quantum Programming\n        \n        \n\n\n\n\n\n\n\n\nArchives\n\n\n\n\n\n완료된 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    ADP 필기 준비\n                    completed\n                \n                \n                    Started: 2025-02-02\n                    \n                     Ended: 2025-02-22\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석\n                \n                과연 2번째 도전은 성공할 것인가\n            \n        \n        \n        \n            \n                \n                    2학년 2학기 학부 정리\n                    completed\n                \n                \n                    Started: 2024-09-02\n                    \n                     Ended: 2024-12-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                2학년 2학기 학부 개념 정리\n            \n        \n        \n        \n            \n                \n                    AWS SAA 준비\n                    completed\n                \n                \n                    Started: 2024-04-15\n                    \n                     Ended: 2024-05-22\n                    \n                    Calculating...\n                \n                \n                    자격증 cloud\n                \n                AWS SAA를 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    ADP 실기 준비\n                    failed\n                \n                \n                    Started: 2024-12-21\n                    \n                     Ended: 2025-02-05\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석 python\n                \n                ADP 실기를 준비해 봅시다.\n            \n        \n        \n\n\n\n보관중인 자료\n\n\n\n\n    \n    \n        \n            vault\n        \n        \n        \n            k8s"
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/scholarships/1.html",
    "href": "posts/01_projects/bs_3_1/notes/scholarships/1.html",
    "title": "자기 소개서",
    "section": "",
    "text": "자기소개 및 가치관 (500자 이내)\n\n저는 데이터 분석과 IT 인프라 설계 분야에 깊은 관심을 가지고 있는 산업공학과 학생입니다. 산업공학을 전공하며 시스템 최적화와 데이터 기반 의사결정에 대한 이론을 배우며 데이터 분석 및 IT 인프라 설계 분야에 관심을 가지게 되었고, 이를 실무에 적용할 수 있는 지식을 학습하고자 42서울 교육기관에서 2년 동안 IT 관련 학습을 진행했습니다. 또한 이 기간 동안 AWS와 ADsP(Advanced Data Analytics Semi-Professional) 자격증을 취득하며 클라우드 컴퓨팅과 데이터 분석에 대한 기초 역량을 쌓았습니다.\n저는 효율적이고 신뢰할 수 있는 시스템을 구축하는 것을 가장 중요한 가치로 삼고 있습니다. 이러한 시스템은 데이터 손실과 보안 위협을 방지할 뿐만 아니라, 장기적인 성장의 토대가 되기 때문입니다. 현재는 데이터와 블록체인 기술을 활용하여 복잡한 문제를 단순화하고, 효율적인 해결책을 찾는 데 큰 관심을 가지고 있습니다. 앞으로도 지속적인 학습과 경험을 통해 해당 분야에서 전문성을 키워가고자 합니다.\n\n졸업 후 IT 및 블록체인 분야에 관련해서 이루고자 하는 꿈과 선정 사유 (500자 이내)\n\n저는 데이터 분석, IT 인프라, 블록체인 기술을 융합하여 현실의 복잡한 문제들을 해결하고 혁신적인 가치를 창출하는 데 기여하고 싶습니다. 전공 수업과 프로젝트를 통해 데이터가 지닌 잠재력을 배워가면서, 동시에 데이터의 신뢰성과 보안이라는 중요한 과제에 대해서도 깊이 고민하게 되었습니다. 특히 42서울에서의 학습 경험을 통해, 안전하고 효율적인 데이터 활용을 위해서는 IT 인프라와 블록체인 기술의 역할이 매우 중요하다는 것을 깨달았습니다. 이러한 경험들을 바탕으로 IT 인프라와 블록체인 기술에 더욱 관심을 가지게 되었고, 관련 기술 서적과 온라인 자료를 통해 꾸준히 학습하며 이해의 폭을 넓혀가고 있습니다. 앞으로도 끊임없이 배우고 성장하여 데이터의 가치를 안전하게 실현할 수 있는 시스템을 만드는 데 기여하고 싶습니다.\n\n목표 달성을 위한 그간의 성과 및 계획 (500자 이내)\n\n저의 주요 성과로는 42서울에서의 프로젝트 경험과 AWS, ADsP 자격증 취득을 들 수 있습니다. 42서울에서 진행한 Solidity 기반 이더리움 스마트 컨트랙트 설계 및 배포 프로젝트를 통해 블록체인의 핵심 원리와 실제 활용 방안을 학습했습니다. 또한 Vagrant, Kubernetes(K8s), ArgoCD, GitLab helm 배포 프로젝트를 수행하며 온프레미스 환경에서의 인프라 설계와 개발 환경 관리 역량을 키웠고, 이를 통해 클라우드와 온프레미스 환경의 IT 인프라 운영에 대한 실질적인 이해도를 높일 수 있었습니다. 향후 계획으로는 학부 과정에 충실히 임하면서 데이터사이언스 대학원 진학을 위한 준비를 체계적으로 진행하고자 합니다. 대학원에서는 빅데이터 처리, 머신러닝, 딥러닝 등 데이터 분석의 핵심 기술을 심도 있게 학습하고자 합니다. 이와 병행하여 온라인 강좌 수강과 실전 프로젝트 수행을 통해 IT 인프라 및 블록체인 분야의 역량을 지속적으로 강화하고, 각종 공모전 참여를 통해 실력을 검증받고자 합니다. 궁극적으로는 이러한 기술들을 융합하여 데이터의 신뢰성과 보안을 보장하고, 효율적인 시스템을 설계하는 전문가로 성장하고 싶습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Scholarships",
      "자기 소개서"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-흐름도",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-흐름도",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "프로세스 흐름도",
    "text": "프로세스 흐름도\n\n블랙박스적 관점: 정보 안보여줌\n공학적 관점: 정보가 정적임. 얻을 수 있는 정보가 제한적\n\n→ 생산자 관점에서 둘 다 도움 안됨.\n→ input이 어떤 순서를 거쳐 output이 되는지 알아야 함\n→ 프로세스 흐름도\n\n\n\n프로세스 흐름도\n\n\n\n작업(네모)\n\n리소스에 의하여 수행 된다.\n하위 작업에도 capacity가 있다\n\n화살표\n재고/버퍼(세모)\n\ninput이 output이 되기 위해 반드시 필요한건 아님\n처리능력으로 표시 / 해석하지 않음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "프로세스 처리 능력 및 활용률",
    "text": "프로세스 처리 능력 및 활용률\n\nprocess capacity: 흐름률의 upper bound(유량).\n병목(bottleneck): 제일 낮은 처리능력의 자원\n전체 프로세스의 처리 능력 = 병목의 처리능력 (단 작업이 일렬로 수행될 때)\nproduct mix:\n\n다양한 제품이 input으로 들어와 처리능력이 달라짐\nbottleneck을 계산하기는 어려움\n\n비율이 매번 달라질 수도 있어서\n작업이 일렬로만 수행되지 않아서\n\n\n실제 생산한 양(흐름률)은 capacity에서만 결정되지 않는다.\n\n수요(market + 계절 / 안전 재고 같은 내부적 수요)\n원자재 투입량\n\n흐름률 = min(시간 당 수요, 프로세스 처리 능력)\n공급능력: 투입량, 처리 능력\n\n\n수요 / 공급 제약적 상황\n\n\n수요(가) 제약적: 수요 &lt; 공급\n\nbottleneck 활용률 &lt; 100%\nflow rate == Demand rate\n\n공급(이) 제약적: 수요 &gt; 공급\n\n투입 제약적\n처리능력 제약적\n\nbottleneck 활용률 == 100%\nflow rate = capacity\n\n\n\n\n\n활용률\n\n\n실제 생산하는 양을 capacity로 나눈 것\n\n\\(\\frac{흐름률}{처리능력}\\)\n활용률을 100% 달성하려면 쉬지 않고 프로세스가 돌아가야하지만 현실적으로 쉽지 않다.\n\n수요가 공급보다 적을 수 있다.\n투입물이 충분하지 않다.\n몇몇 공정의 사용이 공장이나 수리로 제한될 수 있다.\n불확실성\n\n\n병목을 제외한 다른 하위 작업은 활용률이 떨어질 수 있다.\n\n모든 프로세스가 병목인 것이 가장 이상적인 상황 (과도하게 높은 공급능력)\n\n공급 제약적 상황에서 수요가 얼마나 많은지 알 수 없음.\n→ implied utilization\n\n\n\nimplied utilization\n\n\\(U = \\frac{R}{Capacity}\\)\n\\(IU = \\frac{Demand or workload}{Capacity} (≤100% or &gt; 100%)\\)\nif min(demmand, capacity, input) = demand then U = IU\n이 외에도 잠재적 수요 못따라가는 작업도 알 수 있음\n\nIU 100 넘는거 개선 필요\n\n또, 작업이 sequential하게 진행되지 않을 때 병목현상을 확인할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "여러 종류의 흐름 단위",
    "text": "여러 종류의 흐름 단위\ninput 당 뭐가 다르면 다른 단위로 치환",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#footnotes",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "각주",
    "text": "각주\n\n\n왜 전체 프로세스 능력이 안 더해지고 min임?\n같은 속도로 진행되야해서일듯↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#조립공정의-분석",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#조립공정의-분석",
    "title": "인건비 추정과 감축",
    "section": "조립공정의 분석",
    "text": "조립공정의 분석\n\n처리 능력: \\(\\frac{자원의 수}{처리시간}\\)\nbottleneck은 처리능력이 제일 낮은 자원\nX개를 생산하는데 걸리는 시간\n\n가동중인 생산 시스템: \\(\\frac{X}{R}\\)\n비어있는 생산 시스템: 비어있는 시스템을 흘러 가는데 걸리는 시간 + \\(\\frac{X - 1}{R}\\)\n\n\n\n비어있는 시스템을 흘러가는데 걸리는 시간\n\nWorker-paced process: 모든 작업의 처리시간의 합\nMachine-paced process(컨베이어 벨트): 프로세스상의 단계 수 * 병목공정의 처리시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "title": "인건비 추정과 감축",
    "section": "노동량과 유휴시간",
    "text": "노동량과 유휴시간\n\n이상적인 노동비: 작업자의 처리시간의 합 * 시간당 평균 임금\n\n유휴시간을 고려하지 않았을 때\n\n직접 노동 인건비= \\(\\frac{단위시간당 총 임금}{단위시간당 흐름률}\\)\n\n실제 노동시간 + 유휴시간(idle time)\n흐름률이 높아지면 유휴시간이 줄어들면서 직접 노동 인건비가 줄어든다.\n\n\n\n유휴시간 종류\n\nbottleneck에 맞추기 위한 유휴시간\n수요에 맞추기 위해 발생하는 유휴시간\n\n이런 경우 작업시간을 줄이는 방법을 생각할 수 있다.\n하지만 flexible하게 맞추기는 어려울 것이다.\n\n\n\n\nCycle time(주기 시간)\n\n프로세스에서 산출되는 연속된 두 제품 간의 시간간격\n프로세스가 얼마나 빨리 생산하는지를 나타내는 지표\nflow rate의 역수\n1인 작업자의 유휴시간 = cycle time - 1인 작업자의 작업시간\n\n\n\n평균 노동 활용률\n\n제품 생산에만 들어가는 노동의 양과 실제 인건비 지불의 기준이 되는 노동의 양(노동량 + 유휴시간)을 비교\n\\(\\frac{노동량}{노동량 + 모든 작업자의 유휴시간 총합}\\)\n\\(\\frac{1}{노동자 수}(활용률의 합)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#line-balancing",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#line-balancing",
    "title": "인건비 추정과 감축",
    "section": "Line Balancing",
    "text": "Line Balancing\n\nbottleneck에 맞추기 위한 유휴시간으로 이상적인 노동비를 산출할 수 없음. → line balancing을 맞춰줌\n프로세스 내부적인 수요(요구되는 노동량)와 공급(작업자의 처리 능력)을 맞추는 것\n\n\n대량생산으로의 확장\n\n라인의 병렬적 배치\n프로세스 단계별 작업자 추가\n과업의 분화 및 전문화\n\n전문화될수록 라인 밸런싱이 어려워지고 평균 노동 활용률이 낮아짐(작업이 평탄하지 않음)\n→ 전문화 정도를 감소시켜 라인 밸런싱을 쉬워지게 한다.\n\n작업 셀: 한 명이 모든 과업을 수행함. 한 명의 작업시간이 노동량에 해당하고, 노동 활용률은 100%",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#footnotes",
    "title": "인건비 추정과 감축",
    "section": "각주",
    "text": "각주\n\n\n왜 달라?↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "href": "posts/01_projects/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "제품 설계 및 개발",
    "text": "제품 설계 및 개발\n\n설계의 중요성\n\n총 제품 비용 중 설계가 치지하는 비중은 적지만, 설계의 영향을 받는 비중이 높다.\n\n\n\n제품 설계 프로세스\n\n\n\n제품 설계 프로세스\n\n\n\n아이디어 선정: 소비자의 니즈, 경쟁사 제품 등 벤치마킹(reverse engineering)\n제품 선정: 시장 분석, 경제성 분석, 기술 분석\n\n\n\n제조 고려 설계\n\n제조 과정 단순화 및 비용 절감을 고려해서 설계해야 한다.\n\n부품 개수 최소화\n모듈화 표준화\n조립, 재활용, 분해 고려\n\nsubtract manufacturing보단 additive manufacturing(적층제조, DFAM)을 고려\n\n\n\n표준화 모듈화\n\n표준화: 부품 호환성 및 운영 효율성\n모듈화: 표준화된 부품의 집합 &lt;-&gt; integral\n\n\n\n지연전략\n\n차별화 지연 전략: 수요를 알기 전까지 많은 종류 생산은 지연",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "href": "posts/01_projects/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "생산 프로세스의 유형",
    "text": "생산 프로세스의 유형\n\n주문충족 방식에 따른 분류\n\nmake to stock: 수요가 발생하기 전에 생산. 수요가 예측이 쉬울 경우 적합 (push)\nassemble / configure / build to order: 제품 구성요소를 재고로 보유. 고객의 요구에 따라 조립하여 생산. 지연 전략에 맞닿아 있다 (pull)\nmake to order: 주문에 따른 생산. 설계가 완료된 걸 다른 옵션으로 제공. 옵션이 많거나 고가 제품에 적합 (push-pull)\nengineer to order: 고객의 요구에 따라 설계 및 생산. 일회성 프로젝트에 적합 (pull)\n\n위로 갈 수록 제공 시간은 짧아지고, 아래로 갈 수록 유연성이 높아진다.\n\n\n생산 흐름에 따른 분류\n\n프로젝트 프로세스: 일회성 생산. 흐름이라고 할 수는 없다.\n개별작업 프로세스(job shop): 공정별 배치. 높은 유연성, 낮은 규모\n배치 프로세스: job shop과 라인 프로세스의 중간 형태. batch 수가 맞춰지기 전까지 대기 / 유휴시간 존재.\n라인 프로세스: 제품별 배치. 낮은 유연성, 대규모\n연속 흐름 프로세스: 멈춤, 수정, 변경 최소화\n\n\n\n다양한 유형의 프로세스와 설비배치를 혼합 적용하는 것이 일반적\n\n\n\n다품종소량생산, 개인맞춤생산 시대 도래\n\n\nreconfigurable manufacturing system: 다양한 제품을 생산할 수 있는 유연한 생산 시스템\n\n\n\ncell manufacturing\n\nline process랑 job shop이 혼합된거\n비슷한 작업이 필요한 부품들을 하나의 그룹으로 묶어서 전용 셀에서 생산",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/04.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/04.html#footnotes",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "각주",
    "text": "각주\n\n\n라인 프로세스랑 연속흐름 프로세스는 뭐가 다르지 → 연속 흐름은 계속 흘러서 멈추기 힘듦↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/00.html",
    "href": "posts/01_projects/bs_3_1/notes/product/00.html",
    "title": "Intro",
    "section": "",
    "text": "과목 목표: 전통적인 생산 시스템 관리 방법론을 학습\n\n생산: 유, 무형의 제품을 만드는 것\n\n제조업, 서비스업 등\n\n시스템: 투입물을 산출물로 만드는 구성 요소와 프로세스의 총체적 집합\n관리: 목표를 달성하기 위한 체계적인 의사결정\n\n목표: 비용, 품질, 납품, 유연성\n\n네 가지 모두 고려해야하고, 이 사이에는 trade-off가 존재.\n\n\n\n중간 시험범위: lec2 ~ lec9\n\n질문:\n\n\n재고 유지의 다섯 가지 이유 - 수송중 vs 안전\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#general",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#general",
    "title": "Simplex 표 계산",
    "section": "General",
    "text": "General\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\n\n# Convert all elements to Fraction\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 설정\nobj = [-5, -10, 0, 0, 0, 0]\nA = [\n    [3, 1, 1, 0, 0, 40],\n    [1, 1, 0, 1, 0, 20],\n    [5, 3, 0, 0, 1, 90],\n]\nbasic = np.array([3, 4, 5]) - 1  # x5, x6\nnon_basic = np.array([1, 2]) - 1  # x1, x2, x3, x4\n\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        min_rc_idx = None\n        min_rc = Fraction(0)\n        for j in range(len(obj) - 1):  # RHS 제외\n            if obj[j] &lt; min_rc:\n                min_rc = obj[j]\n                min_rc_idx = j\n        \n        # 종료 조건: 음수 계수가 없으면 최적\n        if min_rc_idx is None or min_rc &gt;= 0:\n            print(\"Optimal solution reached.\")\n            solution = {f\"x{i+1}\": Fraction(0) for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((float('inf'), i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == float('inf'):\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [x / pivot for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [A[i][j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [obj[j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   RHS |\n+====+=====+======+======+======+======+======+=======+\n| Z  |   1 |   -5 |  -10 |    0 |    0 |    0 |     0 |\n+----+-----+------+------+------+------+------+-------+\n| x3 |   0 |    3 |    1 |    1 |    0 |    0 |    40 |\n+----+-----+------+------+------+------+------+-------+\n| x4 |   0 |    1 |    1 |    0 |    1 |    0 |    20 |\n+----+-----+------+------+------+------+------+-------+\n| x5 |   0 |    5 |    3 |    0 |    0 |    1 |    90 |\n+----+-----+------+------+------+------+------+-------+\nEntering variable: x2\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   RHS |\n+====+=====+======+======+======+======+======+=======+\n| Z  |   1 |    5 |    0 |    0 |   10 |    0 |   200 |\n+----+-----+------+------+------+------+------+-------+\n| x3 |   0 |    2 |    0 |    1 |   -1 |    0 |    20 |\n+----+-----+------+------+------+------+------+-------+\n| x2 |   0 |    1 |    1 |    0 |    1 |    0 |    20 |\n+----+-----+------+------+------+------+------+-------+\n| x5 |   0 |    2 |    0 |    0 |   -3 |    1 |    30 |\n+----+-----+------+------+------+------+------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 0\nx2 = 20\nx3 = 20\nx4 = 0\nx5 = 30\nObjective Value = 200",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#big-m-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#big-m-method",
    "title": "Simplex 표 계산",
    "section": "Big M Method",
    "text": "Big M Method\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\nfrom sympy import symbols, simplify, oo\n\nM = symbols('M')\n\n# 초기 설정\nobj = [20, 10, 0, M, 0, M, 0]\nA = [\n    [5, 1, -1, 1, 0, 0, 6],\n    [2, 2, 0, 0, -1, 1, 8],\n]\nbasic = np.array([4, 6]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3, 5]) - 1  # x1, x2, x3, x4\n\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef adjust_obj_for_big_m():\n    global obj\n    print(\"\\nAdjusting Objective Function for Big M Method\")\n    for i in range(len(basic)):\n        basic_var_idx = basic[i]\n        if obj[basic_var_idx] != 0:  # 인공변수일 경우(M이 포함된 경우)\n            factor = obj[basic_var_idx]\n            obj = [simplify(obj[j] - factor * A[i][j]) for j in range(len(obj))]\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    adjust_obj_for_big_m()\n    print_table()\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        eval_obj = [x.evalf(subs={M: 1e6}) if x.has(M) else float(x) for x in obj[:-1]]\n        min_rc_idx = min(range(len(obj)-1), key=lambda j: eval_obj[j])\n        \n        if eval_obj[min_rc_idx] &gt;= 0:\n            print(\"Optimal solution reached.\")\n            # 최적 해 출력\n            solution = {f\"x{i+1}\": 0 for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((oo, i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == oo:\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [simplify(x / pivot) for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [simplify(A[i][j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [simplify(obj[j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nAdjusting Objective Function for Big M Method\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\nEntering variable: x1\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n|    |   Z |   x1 | x2        | x3        | x4        | x5   |   x6 | RHS          |\n+====+=====+======+===========+===========+===========+======+======+==============+\n| Z  |   1 |    0 | 6 - 8*M/5 | 4 - 2*M/5 | 7*M/5 - 4 | M    |    0 | -28*M/5 - 24 |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x1 |   0 |    1 | 1/5       | -1/5      | 1/5       | 0    |    0 | 6/5          |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x6 |   0 |    0 | 8/5       | 2/5       | -2/5      | -1   |    1 | 28/5         |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\nEntering variable: x2\nLeaving variable: x6\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+---------+------+----------+-------+\n|    |   Z |   x1 |   x2 | x3   | x4      | x5   | x6       | RHS   |\n+====+=====+======+======+======+=========+======+==========+=======+\n| Z  |   1 |    0 |    0 | 5/2  | M - 5/2 | 15/4 | M - 15/4 | -45   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x1 |   0 |    1 |    0 | -1/4 | 1/4     | 1/8  | -1/8     | 1/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x2 |   0 |    0 |    1 | 1/4  | -1/4    | -5/8 | 5/8      | 7/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 1/2\nx2 = 7/2\nx3 = 0\nx4 = 0\nx5 = 0\nx6 = 0\nObjective Value = -45",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#grubi",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#grubi",
    "title": "Simplex 표 계산",
    "section": "Grubi",
    "text": "Grubi\n\nfrom gurobipy import *\n\nmodel = Model(\"ex4.4-6\")\nmodel.setParam(GRB.Param.OutputFlag, 0)\n\nx1 = model.addVar(vtype=GRB.CONTINUOUS, name=\"x1\")\nx2 = model.addVar(vtype=GRB.CONTINUOUS, name=\"x2\")\n\nmodel.setObjective(2 * x1 + 3 * x2, GRB.MAXIMIZE)\n\nmodel.addConstr(3 * x1 - 9 * x2 &lt;= 20)\nmodel.addConstr(3 * x1 &lt;= 40)\nmodel.addConstr(9 * x2 &lt;= 40)\n\nmodel.optimize()\n\nfor var in model.getVars():\n    print(f\"{var.varName}: {var.x}\")\nprint(\"Obj: \", model.objVal)\n\nx1: 13.333333333333334\nx2: 4.444444444444445\nObj:  40.0",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#business-analytics",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#business-analytics",
    "title": "Intro",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nData Analysis\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\n\nOperations Research\n\nPrescriptive Analytics: What should we do? (Optimization)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#process-of-or-study",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#process-of-or-study",
    "title": "Intro",
    "section": "Process of OR Study",
    "text": "Process of OR Study\n\n\n\n\n\nflowchart LR\n  A(Collect data) --&gt; B(Define the problem)\n  B --&gt; C{Data are sufficient?}\n  C --&gt;|No| A\n  C --&gt;|Yes| D(Formulate a model)\n  D --&gt; E(Solve the model)\n  E --&gt; F{Model is good?}\n  F --&gt;|Yes| G(Interpret results make suggestions)\n  F --&gt;|No| D",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#lp-model-표준형",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#lp-model-표준형",
    "title": "Intro",
    "section": "LP Model (표준형)",
    "text": "LP Model (표준형)\n\n제한된 자원을 경쟁하는 활동들에게 가능한 최적으로 분배하거나 이와 비슷한 수학적 구조를 가진 문제를 다루는 방법\n\n\\[\\begin{aligned}\nmax & \\sum_{i=1}^{n} c_i x_i \\\\\ns.t. & \\sum_{i=1}^{n} a_{ij} x_i \\leq b_j, j ≤ m \\\\\n& x_1, x_2, ..., x_n ≥ 0\n\\end{aligned}\\]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#lp의-가정",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#lp의-가정",
    "title": "Intro",
    "section": "LP의 가정",
    "text": "LP의 가정\n\n선형계획은 현실을 단순화한 모델로, 아래의 네 가지 가정이 완벽히 맞지 않을 수 있음.\n작은 불일치는 허용 가능하며, 민감도 분석으로 보완.\n심각한 위반 시 대안 모델(비선형계획, 정수계획 등)을 사용하나, 선형계획의 강력한 알고리즘이 유리하므로 초기 분석에 활용 후 필요 시 복잡한 모델로 전환.\n\n\n비례성(Proportionality)\n\n정의: 목적함수와 제약식에서 활동 수준(예: xx)에 대한 기여도가 선형(비례적)으로 표현됨.\n위반 사례:\n\n초기 투자비용(고정비용)이 있어 \\(Z=3x_1−1\\)이 되는 경우, 비례성이 깨짐.\n규모의 경제로 한계 이익이 증가하면 비례성이 위반됨.\n한계 이익이 감소(예: 마케팅 비용 증가)하면 역시 비례성이 깨짐.\n\n대안: 비례성이 깨지면 비선형계획(12장)이나 혼합정수계획(11장)을 고려.\n\n가합성(Additivity)\n\n정의: 목적함수와 제약식의 값이 각 활동의 개별 기여도의 합으로 표현됨. 즉, 변수 간 교차곱이 없음.\n위반 사례:\n\n제품 간 보완적 상호작용(예: 공동 광고 효과)으로 \\(Z=3x_1+5x_2+x_1x_2\\)가 됨.\n경쟁적 상호작용(예: 설비 공유로 비효율 발생)으로 \\(Z=3x_1+5x_2−x_1x_2\\)가 됨.\n\n대안: 가합성이 위반되면 비선형계획(12장)으로 전환.\n\n가분성(Divisibility)\n\n정의: 의사결정 변수가 실수 값을 가질 수 있음. 즉, 활동 수준이 정수로 제한되지 않음.\n위반 사례: 변수가 정수로 제한되면(예: 배치 단위가 1, 2, 3만 가능) 가분성이 깨짐.\n대안: 정수계획(11장) 사용.\n\n확실성(Certainty)\n\n정의: 모델의 매개변수(예: \\(c_j, a_{ij}, b_i\\))가 알려진 상수로 고정. 해당 상수는 미래 예측에 기반하므로 불확실성이 존재.\n대응: 불확실성이 크면 민감도 분석(6.7절)으로 최적해의 변화를 확인하거나, 확률변수를 도입한 모델(23장) 사용.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/07.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/07.html",
    "title": "pandas",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndata = [[1.4, np.nan],\n        [7.1, -4.5],\n        [np.nan, np.nan],\n        [0.75, -1.3]]\ndf = pd.DataFrame(data, columns=['one', 'two'], index=['a', 'b', 'c', 'd'])\ndf\n\n\n\n\n\n\n\n\none\ntwo\n\n\n\n\na\n1.40\nNaN\n\n\nb\n7.10\n-4.5\n\n\nc\nNaN\nNaN\n\n\nd\n0.75\n-1.3\n\n\n\n\n\n\n\n\ndf.sum(axis=0)\n\none    9.25\ntwo   -5.80\ndtype: float64\n\n\n\ndf.sum(axis=1)\n\na    1.40\nb    2.60\nc    0.00\nd   -0.55\ndtype: float64\n\n\n\ndf.mean(axis=1, skipna=False)\n\na      NaN\nb    1.300\nc      NaN\nd   -0.275\ndtype: float64\n\n\n\ndf['one'].sum()\n\n9.25\n\n\n\ndf.loc['b'].sum().round(2)\n\n2.6\n\n\n\ndf2 = pd.DataFrame(np.random.randn(6, 4), columns=['A', 'B', 'C', 'D'], index=pd.date_range(\"2025-03-24\", periods=6))\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2025-03-24\n-2.067944\n0.005654\n-0.247531\n0.415550\n\n\n2025-03-25\n-0.060965\n2.243269\n0.130425\n-1.306482\n\n\n2025-03-26\n0.694764\n-1.074375\n1.032794\n-0.562044\n\n\n2025-03-27\n0.095192\n-0.074051\n-0.285623\n0.045712\n\n\n2025-03-28\n-2.189070\n0.637167\n1.609518\n0.632818\n\n\n2025-03-29\n-0.941798\n-0.413012\n-0.463396\n0.260044\n\n\n\n\n\n\n\n\ndf2['A'].corr(df2['B'])\n\n-0.13561373135141022\n\n\n\ndf2['B'].cov(df2['C'])\n\n0.005500285499809432\n\n\n\ndf2['A'].isin(['alpha', 'beta'])\n\n2025-03-24    False\n2025-03-25    False\n2025-03-26    False\n2025-03-27    False\n2025-03-28    False\n2025-03-29    False\nFreq: D, Name: A, dtype: bool\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "pandas"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html",
    "title": "pandas",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndata = {\n    'name': ['John', 'Anna', 'Peter', 'Linda'],\n    'location': ['New York', 'Paris', 'Berlin', 'London'],\n    'age': [24, 13, 53, 33]\n}\ndf = pd.DataFrame(data)\n\n\ndf.loc[(df['age'] &gt; 20) & (df['age'] &lt; 50), 'location']\n\n0    New York\n3      London\nName: location, dtype: object\n\n\n\ndf = pd.DataFrame(np.random.randn(6, 4))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1.204059\n-0.335996\n-0.818220\n0.861070\n\n\n1\n0.868450\n-0.164946\n0.917664\n-1.945286\n\n\n2\n0.291923\n-0.328159\n1.166051\n-0.479075\n\n\n3\n0.766692\n0.123853\n0.487815\n-0.161050\n\n\n4\n-1.695214\n-0.001611\n-0.631717\n-0.148206\n\n\n5\n-0.354958\n0.263346\n-0.822940\n0.523039\n\n\n\n\n\n\n\n\ndf.columns = ['A', 'B', 'C', 'D']\ndf.index = pd.date_range('20250101', periods=6)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2025-01-01\n1.204059\n-0.335996\n-0.818220\n0.861070\n\n\n2025-01-02\n0.868450\n-0.164946\n0.917664\n-1.945286\n\n\n2025-01-03\n0.291923\n-0.328159\n1.166051\n-0.479075\n\n\n2025-01-04\n0.766692\n0.123853\n0.487815\n-0.161050\n\n\n2025-01-05\n-1.695214\n-0.001611\n-0.631717\n-0.148206\n\n\n2025-01-06\n-0.354958\n0.263346\n-0.822940\n0.523039\n\n\n\n\n\n\n\n\ndf['F'] = [1.0, np.nan, 3.5, 6.1, np.nan, 7.0]\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2025-01-01\n1.204059\n-0.335996\n-0.818220\n0.861070\n1.0\n\n\n2025-01-02\n0.868450\n-0.164946\n0.917664\n-1.945286\nNaN\n\n\n2025-01-03\n0.291923\n-0.328159\n1.166051\n-0.479075\n3.5\n\n\n2025-01-04\n0.766692\n0.123853\n0.487815\n-0.161050\n6.1\n\n\n2025-01-05\n-1.695214\n-0.001611\n-0.631717\n-0.148206\nNaN\n\n\n2025-01-06\n-0.354958\n0.263346\n-0.822940\n0.523039\n7.0\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2025-01-01\n1.204059\n-0.335996\n-0.818220\n0.861070\n1.0\n\n\n2025-01-03\n0.291923\n-0.328159\n1.166051\n-0.479075\n3.5\n\n\n2025-01-04\n0.766692\n0.123853\n0.487815\n-0.161050\n6.1\n\n\n2025-01-06\n-0.354958\n0.263346\n-0.822940\n0.523039\n7.0\n\n\n\n\n\n\n\n\ndf.dropna(axis=1, subset=['2025-01-02'])\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2025-01-01\n1.204059\n-0.335996\n-0.818220\n0.861070\n\n\n2025-01-02\n0.868450\n-0.164946\n0.917664\n-1.945286\n\n\n2025-01-03\n0.291923\n-0.328159\n1.166051\n-0.479075\n\n\n2025-01-04\n0.766692\n0.123853\n0.487815\n-0.161050\n\n\n2025-01-05\n-1.695214\n-0.001611\n-0.631717\n-0.148206\n\n\n2025-01-06\n-0.354958\n0.263346\n-0.822940\n0.523039\n\n\n\n\n\n\n\n\ndf['F'].dropna()\n\n2025-01-01    1.0\n2025-01-03    3.5\n2025-01-04    6.1\n2025-01-06    7.0\nName: F, dtype: float64\n\n\n\ndf.loc[df.isnull()['F'], :]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2025-01-02\n0.868450\n-0.164946\n0.917664\n-1.945286\nNaN\n\n\n2025-01-05\n-1.695214\n-0.001611\n-0.631717\n-0.148206\nNaN\n\n\n\n\n\n\n\n\ndf.drop([pd.to_datetime('2025-01-02'), pd.to_datetime('2025-01-04')])\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nF\n\n\n\n\n2025-01-01\n1.204059\n-0.335996\n-0.818220\n0.861070\n1.0\n\n\n2025-01-03\n0.291923\n-0.328159\n1.166051\n-0.479075\n3.5\n\n\n2025-01-05\n-1.695214\n-0.001611\n-0.631717\n-0.148206\nNaN\n\n\n2025-01-06\n-0.354958\n0.263346\n-0.822940\n0.523039\n7.0\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "pandas"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/08.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/08.html",
    "title": "preprocessing",
    "section": "",
    "text": "import chardet\n\nwith open(\"_data/class/2012년_1당사자_법규위반별_주야별_교통사고.csv\", \"rb\") as f:\n    print(chardet.detect(f.read()))\n\n{'encoding': 'EUC-KR', 'confidence': 0.99, 'language': 'Korean'}\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"_data/class/2012년_1당사자_법규위반별_주야별_교통사고.csv\", encoding=\"EUC-KR\"\n)\ndf\n\n\n\n\n\n\n\n\n법규위반\n주야\n발생건수\n사망자수\n부상자수\n중상\n경상\n부상신고\n\n\n\n\n0\n과속\n주\n159\n34\n334\n140\n178\n16\n\n\n1\n과속\n야\n218\n73\n348\n200\n139\n9\n\n\n2\n교차로 통행방법 위반\n주\n8817\n82\n14031\n3915\n9530\n586\n\n\n3\n교차로 통행방법 위반\n야\n5904\n29\n9728\n2401\n6884\n443\n\n\n4\n기타\n주\n9388\n141\n14070\n4271\n9217\n582\n\n\n5\n기타\n야\n6073\n56\n9218\n2348\n6457\n413\n\n\n6\n보행자 보호의무 위반\n주\n3772\n80\n3914\n2063\n1729\n122\n\n\n7\n보행자 보호의무 위반\n야\n3334\n94\n3535\n1802\n1626\n107\n\n\n8\n신호위반\n주\n12552\n210\n20396\n6816\n12765\n815\n\n\n9\n신호위반\n야\n12755\n179\n21724\n7177\n13749\n798\n\n\n10\n안전거리 미확보\n주\n12408\n57\n22392\n4509\n16882\n1001\n\n\n11\n안전거리 미확보\n야\n9867\n40\n17422\n3025\n13644\n753\n\n\n12\n안전운전 의무 불이행\n주\n62608\n1740\n91076\n28081\n58472\n4523\n\n\n13\n안전운전 의무 불이행\n야\n62783\n2132\n92942\n26451\n62209\n4282\n\n\n14\n중앙선 침범\n주\n6825\n243\n12427\n4658\n7339\n430\n\n\n15\n중앙선 침범\n야\n6193\n202\n11008\n3846\n6770\n392\n\n\n\n\n\n\n\n\ndf = pd.read_csv(\"_data/class/vote.csv\")\ndf\n\n\n\n\n\n\n\n\ngender\nregion\nedu\nincome\nage\nscore_gov\nscore_progress\nscore_intention\nvote\nparties\n\n\n\n\n0\n1\n4\n3\n3\n3\n2\n2\n4.0\n1\n2\n\n\n1\n1\n5\n2\n3\n3\n2\n4\n3.0\n0\n3\n\n\n2\n1\n3\n1\n2\n4\n1\n3\n2.8\n1\n4\n\n\n3\n2\n1\n2\n1\n3\n5\n4\n2.6\n1\n1\n\n\n4\n1\n1\n1\n2\n4\n4\n3\n2.4\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n206\n1\n4\n1\n4\n4\n3\n3\n1.8\n1\n2\n\n\n207\n1\n1\n2\n1\n2\n3\n4\n2.6\n1\n4\n\n\n208\n1\n2\n2\n1\n2\n3\n3\n2.6\n1\n2\n\n\n209\n1\n1\n2\n3\n4\n3\n2\n4.0\n1\n4\n\n\n210\n2\n1\n2\n2\n2\n3\n3\n3.8\n1\n2\n\n\n\n\n211 rows × 10 columns\n\n\n\n\nx = df.iloc[:, :8]\nx\n\n\n\n\n\n\n\n\ngender\nregion\nedu\nincome\nage\nscore_gov\nscore_progress\nscore_intention\n\n\n\n\n0\n1\n4\n3\n3\n3\n2\n2\n4.0\n\n\n1\n1\n5\n2\n3\n3\n2\n4\n3.0\n\n\n2\n1\n3\n1\n2\n4\n1\n3\n2.8\n\n\n3\n2\n1\n2\n1\n3\n5\n4\n2.6\n\n\n4\n1\n1\n1\n2\n4\n4\n3\n2.4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n206\n1\n4\n1\n4\n4\n3\n3\n1.8\n\n\n207\n1\n1\n2\n1\n2\n3\n4\n2.6\n\n\n208\n1\n2\n2\n1\n2\n3\n3\n2.6\n\n\n209\n1\n1\n2\n3\n4\n3\n2\n4.0\n\n\n210\n2\n1\n2\n2\n2\n3\n3\n3.8\n\n\n\n\n211 rows × 8 columns\n\n\n\n\ny = df[\"vote\"]\ny\n\n0      1\n1      0\n2      1\n3      1\n4      1\n      ..\n206    1\n207    1\n208    1\n209    1\n210    1\nName: vote, Length: 211, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"_data/class/Car-selling price.csv\")\ndf\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\nSell Price($)\n\n\n\n\n0\n69000\n6\n18000\n\n\n1\n35000\n3\n34000\n\n\n2\n57000\n5\n26100\n\n\n3\n22500\n2\n40000\n\n\n4\n46000\n4\n31500\n\n\n5\n59000\n5\n26750\n\n\n6\n52000\n5\n32000\n\n\n7\n72000\n6\n19300\n\n\n8\n91000\n8\n12000\n\n\n9\n67000\n6\n22000\n\n\n10\n83000\n7\n18700\n\n\n11\n79000\n7\n19500\n\n\n12\n59000\n5\n26000\n\n\n13\n58780\n4\n27500\n\n\n14\n82450\n7\n19400\n\n\n15\n25400\n3\n35000\n\n\n16\n28000\n2\n35500\n\n\n17\n69000\n5\n19700\n\n\n18\n87600\n8\n12800\n\n\n19\n52000\n5\n18200\n\n\n\n\n\n\n\n\nplt.title(\"Scatter Diagram\")\nplt.xlabel(\"Mileage\")\nplt.ylabel(\"Sell Price($)\")\nplt.scatter(df[\"Mileage\"], df[\"Sell Price($)\"])\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nx = df[[\"Mileage\", \"Age(yrs)\"]]\ny = df[\"Sell Price($)\"]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\n\nfrom sklearn.preprocessing import Binarizer\nimport numpy as np\n\ndata = np.array([[1.1], [2.2], [3.3], [4.4], [5.5]])\nscaler = Binarizer(threshold=3.0)\nscaler.fit(data)\nresult = scaler.transform(data)\nresult\n\narray([[0.],\n       [0.],\n       [1.],\n       [1.],\n       [1.]])\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(data)\nresult = scaler.transform(data)\nresult\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nimport numpy as np\nfrom sklearn.preprocessing import Normalizer\n\ndata = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0], [5.0, 6.0]])\n\nscaler = Normalizer(norm='l2')\nX_transformed = scaler.fit_transform(data.T).T\nX_transformed\n\narray([[0.13483997, 0.21081851],\n       [0.26967994, 0.31622777],\n       [0.40451992, 0.42163702],\n       [0.53935989, 0.52704628],\n       [0.67419986, 0.63245553]])\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmax_scaler = MinMaxScaler(feature_range=(0, 1))\nminmax_scaler.fit_transform(data)\n\narray([[0.  , 0.  ],\n       [0.25, 0.25],\n       [0.5 , 0.5 ],\n       [0.75, 0.75],\n       [1.  , 1.  ]])\n\n\n\ndata = pd.read_csv('_data/class/homeprices.csv')\ndf = pd.DataFrame(data)\n\ndummies = pd.get_dummies(df.town)\nmerged = pd.concat([df, dummies], axis='columns')\nmerged\n\n\n\n\n\n\n\n\ntown\narea\nprice\nmonroe township\nrobinsville\nwest windsor\n\n\n\n\n0\nmonroe township\n2600\n550000\nTrue\nFalse\nFalse\n\n\n1\nmonroe township\n3000\n565000\nTrue\nFalse\nFalse\n\n\n2\nmonroe township\n3200\n610000\nTrue\nFalse\nFalse\n\n\n3\nmonroe township\n3600\n680000\nTrue\nFalse\nFalse\n\n\n4\nmonroe township\n4000\n725000\nTrue\nFalse\nFalse\n\n\n5\nwest windsor\n2600\n585000\nFalse\nFalse\nTrue\n\n\n6\nwest windsor\n2800\n615000\nFalse\nFalse\nTrue\n\n\n7\nwest windsor\n3300\n650000\nFalse\nFalse\nTrue\n\n\n8\nwest windsor\n3600\n710000\nFalse\nFalse\nTrue\n\n\n9\nrobinsville\n2600\n575000\nFalse\nTrue\nFalse\n\n\n10\nrobinsville\n2900\n600000\nFalse\nTrue\nFalse\n\n\n11\nrobinsville\n3100\n620000\nFalse\nTrue\nFalse\n\n\n12\nrobinsville\n3600\n695000\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\nfinal = merged.drop(['town', 'west windsor'], axis='columns')\nfinal\n\n\n\n\n\n\n\n\narea\nprice\nmonroe township\nrobinsville\n\n\n\n\n0\n2600\n550000\nTrue\nFalse\n\n\n1\n3000\n565000\nTrue\nFalse\n\n\n2\n3200\n610000\nTrue\nFalse\n\n\n3\n3600\n680000\nTrue\nFalse\n\n\n4\n4000\n725000\nTrue\nFalse\n\n\n5\n2600\n585000\nFalse\nFalse\n\n\n6\n2800\n615000\nFalse\nFalse\n\n\n7\n3300\n650000\nFalse\nFalse\n\n\n8\n3600\n710000\nFalse\nFalse\n\n\n9\n2600\n575000\nFalse\nTrue\n\n\n10\n2900\n600000\nFalse\nTrue\n\n\n11\n3100\n620000\nFalse\nTrue\n\n\n12\n3600\n695000\nFalse\nTrue\n\n\n\n\n\n\n\n\nX = final.drop('price', axis='columns')\nX\n\n\n\n\n\n\n\n\narea\nmonroe township\nrobinsville\n\n\n\n\n0\n2600\nTrue\nFalse\n\n\n1\n3000\nTrue\nFalse\n\n\n2\n3200\nTrue\nFalse\n\n\n3\n3600\nTrue\nFalse\n\n\n4\n4000\nTrue\nFalse\n\n\n5\n2600\nFalse\nFalse\n\n\n6\n2800\nFalse\nFalse\n\n\n7\n3300\nFalse\nFalse\n\n\n8\n3600\nFalse\nFalse\n\n\n9\n2600\nFalse\nTrue\n\n\n10\n2900\nFalse\nTrue\n\n\n11\n3100\nFalse\nTrue\n\n\n12\n3600\nFalse\nTrue\n\n\n\n\n\n\n\n\ny = final.price\ny\n\n0     550000\n1     565000\n2     610000\n3     680000\n4     725000\n5     585000\n6     615000\n7     650000\n8     710000\n9     575000\n10    600000\n11    620000\n12    695000\nName: price, dtype: int64\n\n\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\nmodel.predict([[2800, 0, 1]])\n\n/home/cryscham123/.local/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\narray([590775.63964739])\n\n\n\nmodel.score(X, y)\n\n0.9573929037221873\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndfle = df\ndfle.town = le.fit_transform(dfle.town)\ndfle\n\n\n\n\n\n\n\n\ntown\narea\nprice\n\n\n\n\n0\n0\n2600\n550000\n\n\n1\n0\n3000\n565000\n\n\n2\n0\n3200\n610000\n\n\n3\n0\n3600\n680000\n\n\n4\n0\n4000\n725000\n\n\n5\n2\n2600\n585000\n\n\n6\n2\n2800\n615000\n\n\n7\n2\n3300\n650000\n\n\n8\n2\n3600\n710000\n\n\n9\n1\n2600\n575000\n\n\n10\n1\n2900\n600000\n\n\n11\n1\n3100\n620000\n\n\n12\n1\n3600\n695000\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html",
    "title": "Homework",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv(\"_data/class/carprices.csv\")\n\ndataset\n\n\n\n\n\n\n\n\nCar Model\nMileage\nSell Price($)\nAge(yrs)\n\n\n\n\n0\nBMW X5\n69000\n18000\n6\n\n\n1\nBMW X5\n35000\n34000\n3\n\n\n2\nBMW X5\n57000\n26100\n5\n\n\n3\nBMW X5\n22500\n40000\n2\n\n\n4\nBMW X5\n46000\n31500\n4\n\n\n5\nAudi A5\n59000\n29400\n5\n\n\n6\nAudi A5\n52000\n32000\n5\n\n\n7\nAudi A5\n72000\n19300\n6\n\n\n8\nAudi A5\n91000\n12000\n8\n\n\n9\nMercedez Benz C class\n67000\n22000\n6\n\n\n10\nMercedez Benz C class\n83000\n20000\n7\n\n\n11\nMercedez Benz C class\n79000\n21000\n7\n\n\n12\nMercedez Benz C class\n59000\n33000\n5\nscatter plot을 이용해 선형성을 확인하겠습니다.\n연속형 변수인 Mileage와 Sell Price($)에 대해 확인하겠습니다.\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(dataset.iloc[:, 1], dataset.iloc[:, 2], alpha=0.7)\naxes[0].set_xlabel(\"Mileage\")\naxes[0].set_ylabel(\"Sell Price($)\")\naxes[0].set_title(\"Mileage vs Sell Price($)\")\n\naxes[1].scatter(dataset.iloc[:, 3], dataset.iloc[:, 2], alpha=0.7)\naxes[1].set_xlabel(\"Age(yrs)\")\naxes[1].set_ylabel(\"Price($)\")\naxes[1].set_title(\"Age(yrs) vs Price($)\")\n\nplt.show()\n선형성이 보입니다.\n두 변수 모두 linear regression을 적용할 수 있어 보입니다.\nx = dataset.drop('Sell Price($)', axis=1).values\ny = dataset.iloc[:, 2].values\n독립 변수와 종속 변수를 저장해줍니다.\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\n범주형 변수 Car Model에 대해 one hot encoding을 적용하였습니다.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nX_train\n\narray([[0.0, 1.0, 0.0, 46000, 4],\n       [1.0, 0.0, 0.0, 91000, 8],\n       [0.0, 0.0, 1.0, 59000, 5],\n       [0.0, 1.0, 0.0, 35000, 3],\n       [0.0, 1.0, 0.0, 22500, 2],\n       [0.0, 0.0, 1.0, 67000, 6],\n       [1.0, 0.0, 0.0, 52000, 5],\n       [0.0, 0.0, 1.0, 79000, 7],\n       [1.0, 0.0, 0.0, 72000, 6],\n       [0.0, 1.0, 0.0, 57000, 5]], dtype=object)\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\nX_train\n\narray([[0.0, 1.0, 0.0, -0.6199306919491117, -0.6470588235294114],\n       [1.0, 0.0, 0.0, 1.695163178400268, 1.7058823529411762],\n       [0.0, 0.0, 1.0, 0.048874203929598015, -0.05882352941176448],\n       [0.0, 1.0, 0.0, -1.1858425269234045, -1.2352941176470584],\n       [0.0, 1.0, 0.0, -1.82892415757601, -1.8235294117647052],\n       [0.0, 0.0, 1.0, 0.46044644754726555, 0.5294117647058825],\n       [1.0, 0.0, 0.0, -0.31125150923586103, -0.05882352941176448],\n       [0.0, 0.0, 1.0, 1.0778048129737667, 1.1176470588235294],\n       [1.0, 0.0, 0.0, 0.7176790998083077, 0.5294117647058825],\n       [0.0, 1.0, 0.0, -0.05401885697481886, -0.05882352941176448]],\n      dtype=object)\ntrain set과 test set을 나눈 뒤 연속형 변수에 대해 feature scailing을 진행해줍니다.\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()\n모델을 학습시킵니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section",
    "title": "Homework",
    "section": "1",
    "text": "1\n\ntarget = ct.transform([['Mercedez Benz C class', 45000, 4]])\ntarget[:, 3:] = sc.transform(target[:, 3:])\ny_pred = regressor.predict(target)\nprint(y_pred)\n\n[36216.12684278]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-1",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-1",
    "title": "Homework",
    "section": "2",
    "text": "2\n\ntarget = ct.transform([['BMW X5', 86000, 7]])\ntarget[:, 3:] = sc.transform(target[:, 3:])\ny_pred = regressor.predict(target)\nprint(y_pred)\n\n[11152.07829935]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-2",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-2",
    "title": "Homework",
    "section": "3",
    "text": "3\n\nprint(regressor.score(X_test, y_test))\n\n0.8409834346492212",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#통계적-추론",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#통계적-추론",
    "title": "통계적 추정",
    "section": "통계적 추론",
    "text": "통계적 추론\n모집단에서 추출된 표본의 통계량으로부터 모수를 추론하는 것\n\n추정\n\n점추정\n구간추정\n\n가설 검정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#추정",
    "title": "통계적 추정",
    "section": "추정",
    "text": "추정\n\n불편성\n\n\\(E(\\hat{\\theta}) = θ\\)\nbias = \\(E(\\hat{\\theta}) - \\theta\\)\n\n보통 sample size가 커질수록 bias는 0에 수렴\n\n\\(\\bar{X}, X_n\\)은 μ의 불편추정량이다.\n\n\n\n최소분산\n\n\\(Var(\\bar{X})\\)가 \\(Var(X_n)\\)보다 분산이 작아서 더 좋은 추정량\n\\(MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2\\)\n\n큰 오차에 더 큰 페널티를 주기 위해 제곱\n\n\n\n\n대표적인 불편추정량\n\n전부 중심극한의정리를 적용할 수 있다.\n\n\n모평균\n모비율\n모평균 차이 (독립이라는 가정 필요)\n모비율 차이 (독립이라는 가정 필요)\n\n\n이때, 이들의 평균과 표준편차는 모집단의 분포와 관계없이 일정하다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#구간-추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#구간-추정",
    "title": "통계적 추정",
    "section": "구간 추정",
    "text": "구간 추정\n\nα: 유의수준\n1 - α: 신뢰수준\n(θ_L, θ_U) = (1 - α) × 100% 신뢰구간\n\n\n(\\(θ_L, θ_U\\)) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다\n구간이 충분히 좁아야 한다\n\n표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.\n고로 신뢰구간이 대칭임\n\n\n\n신뢰 구간의 확률적인 의미\n\n샘플링을 무한히 반복했을 때, 이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함한다. → 구간이 확률 변수이다.\n\n\n\n표본의 크기 결정\n특정 오차 아래로 하는 표본의 수 구하는 법\n\n그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.\n모비율을 모를 때는 일단 0.5로 보수적으로 놓고 계산",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "title": "통계적 추정",
    "section": "소표본 신뢰구간",
    "text": "소표본 신뢰구간\n\n표본이 작다. → 크면 정규분포\n모집단 정규분포를 따른다. → 비모수 검정\nσ 모름 → 알면 그냥 정규 분포\n\\(σ_1 = σ_2\\)\n\n→ t분포\n\n정규분포에 비해 신뢰구간은 더 길어짐",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#모분산-추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#모분산-추정",
    "title": "통계적 추정",
    "section": "모분산 추정",
    "text": "모분산 추정\n\n카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음\n\n그냥 쉽게 구하기 위해 \\((x^2_{α/2}, x^2_{1-α/2})\\)를 사용\n\n모분산의 신뢰구간: \\((\\frac{(n-1)s^2}{x^2_{(1-\\alpha)/2}(n-1)}, \\frac{(n-1)s^2}{x^2_{\\alpha/2}(n-1)})\\)\n표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html",
    "href": "posts/04_archives/bs_2_2/index.html",
    "title": "2학년 2학기 학부 정리",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-09-02\n        종료일: 2024-12-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#details",
    "href": "posts/04_archives/bs_2_2/index.html#details",
    "title": "2학년 2학기 학부 정리",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 2학년 2학기 수강 과목들에 대한 개념 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#tasks",
    "href": "posts/04_archives/bs_2_2/index.html#tasks",
    "title": "2학년 2학기 학부 정리",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#related-posts",
    "href": "posts/04_archives/bs_2_2/index.html#related-posts",
    "title": "2학년 2학기 학부 정리",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "title": "Database Administration",
    "section": "Concurrency control",
    "text": "Concurrency control\nEnsuring that one user’s work does not inappropriately influence another user’s work\n\nStrict concurrency control requires locking the database, 다른 사용자의 동시 사용 허가 x\nLower concurrency control allows more throughput\n\n\nTransactions\nUsers submit Transactions(LUWs)\n\nAtmomic Transaction: 데이터베이스에서 일련의 작업들이 모두 성공적으로 수행되거나, 그렇지 않을 경우 작업이 전혀 수행되지 않아 데이터베이스가 변경되지 않는 상태를 유지하는 트랜잭션\n→ Before committed, all LUWs must be successfully completed, or rollback\nConcurrent Transactions: 여러 트랜잭션이 동시에 실행되는 것\n\nLost update problem: 두 트랜잭션이 동시에 같은 데이터를 수정할 때, 하나의 트렌잭션이 다른 트랜잭션의 변경을 덮어쓰는 문제\nInconsistent read problem: 한 트랜잭션이 데이터를 읽는 도중 다른 트랜잭션이 데이터를 수정하는 문제\n\nDirty read: commit 되기 이전에 수정된 데이터를 읽는 것. 만약 rollback이 될 경우 문제가 발생.\nNonrepeatable read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 값이 다른 경우\nPhantom read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 새로운 row가 추가된 경우\n\nResource locking\n\nImplicit locks: DBMS가 자동으로 수행하는 lock\nExplicit locks\nLOCK TABLES table_name READ -- or WRITE\nUNLOCK TABLES\nExclusive locks: 다른 트랜잭션에서 읽기/쓰기 불가\nShared locks: 다른 트랜잭션에서 읽기 가능, 쓰기 불가\nrock granularity: row-level vs table-level vs database-level\n\n\nSerializable Transactions: 가장 강력한 격리 수준 보장\n\nTwo-pase locking(2PL): growing phase와 shrinking phase로 나뉨\n\nACID Transaction\n\nAtomic: 성공한 transaction만 저장되어야 한다\nConsistent: 현재의 transaction이 마무리 되기 전 까지 record를 저장할 수 없다\n→ 트랜잭션의 살향 결과로 데이터베이스 상태가 모순되지 않음\nIsolated\n\nread uncommitted: 다른 트랜잭션에서 commit되지 않은 데이터도 읽을 수 있음\nread committed: 다른 트랜잭션이 commit된 데이터만 읽을 수 있음\nrepeatable read: 한 트랜잭션에서 하나의 스냅션만 사용\nserializable: 가장 강력한 격리 수준 보장\n\nDurable: 트랜잭션이 성공적으로 완료되면, 그 결과는 영구적으로 저장되어야 한다\n\n\n\n\n\nDeadlock / deadly embrace\n두 개 이상의 트랜잭션이 서로 unlock을 무한히 기다리는 상태\n\n\nlock\n\noptimistic locking\n\nassumption: No conflict will occur\nif no conflict occurs, the transaction is committed else it is rolled back and repeated\n\npessimistic locking\n\nassumption: Conflict will occur\nlock the data before the transaction starts\n\n\n\n\nCursor\nA cursor is a pointer into a set of rows that are the result set from an SQL SELECT statement\nDECLARE cursor_name CURSOR FOR SELECT column_name FROM table_name",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "title": "Database Administration",
    "section": "Backup and recovery",
    "text": "Backup and recovery\n\nRecovery\n\nvia Reprocessing\nvia Rollback and Rollforward\n\nlog file transaction을 undo할 때, before-images가 존재해함. (rollback) transaction을 redo할 때, after-images가 존재해함.(rollforward)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#security",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#security",
    "title": "Database Administration",
    "section": "Security",
    "text": "Security\nonly authenticated users perform authorized activities\n\nAuthentication: User ID와 password를 사용하여 사용자를 인증\nAuthorization: user groups(roles): dbcreator, public, … sql  GRANT SELECT, INSERT, UPDATE, DELETE ON table_name TO user_name",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#database-performance",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#database-performance",
    "title": "Database Administration",
    "section": "Database Performance",
    "text": "Database Performance\n\nindex\ndisk mirroring: 데이터 복제 말씀하신 듯\nRAID\nSANs\nDistributed database: service cluster partitioned replicated\n\n\nDBA Responsibilities\n\nuser reported errors를 모아서 system이 잘 돌아갈 수 있게 해야함\ndatabase 설정을 잘 관리해야함\n문서화 잘 해야함\ncloud로 db 관리(service level agreement)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization",
    "title": "Database Normalization",
    "section": "Normalization",
    "text": "Normalization\n\nprocess of organizing a database to reduce redundancy problem and improve data integrity",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "title": "Database Normalization",
    "section": "Functional Dependency",
    "text": "Functional Dependency\n\n하나의 atrribute가 다른 attribute의 value를 결정하는지 여부를 판단\nwell formed인지 판별할 수 있는 기준\nA(Determinant) -&gt; B(dependent): A가 결정되면 B도 결정된다면 B는 A에 함수적 종속\nEvery determinant must be a Candidate Key",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "title": "Database Normalization",
    "section": "Normalization Process",
    "text": "Normalization Process\n\nBCFNF: Boyce-Codd Normal Form =&gt; Each relation has only one theme\n\n\nIdentify all the Candidate Keys.\nIdentify all the Functional Dependencies.\nExamine the determinants of the functional dependencies\n\nplace the columns of the functional dependency in a new relation of their own\nmake the determinant of the functianl dependency the primary key of the new relation\nLeabe a copy of the determinant as a foreign key in the original relation\ncreate a referential integrity constraint between the original and new relation\n\nRepeat the process until every determinant of every relation is a candidate key",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#entity",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#entity",
    "title": "The Relational Model",
    "section": "entity",
    "text": "entity\na formal name for a thing that is being tracked one theme or topic (just single table)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#relation",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#relation",
    "title": "The Relational Model",
    "section": "Relation",
    "text": "Relation\n\na two-dimensional table that has specific charateristics\nCell of the table hold single value\nAll entries in a column are of the same kind\nNo two rows in a table are identical",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "title": "The Relational Model",
    "section": "domain & cartesian product",
    "text": "domain & cartesian product\n\ndomain: set of possible values for a column\ncartesian product: set of all possible combinations of rows from two tables",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "title": "The Relational Model",
    "section": "Presenting Relation Structures",
    "text": "Presenting Relation Structures\nRELATION_NAME(PrimaryKey, ForeignKey, ColumnName, …)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#key",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#key",
    "title": "The Relational Model",
    "section": "key",
    "text": "key\n\nidentify a row\nUnique Key(Primary Key)\nNonUnique Key(Foreign Key)\nComposite Key: Primary key가 두개 이상. Surrogate Key로 대체되곤 함.\nCandidate Key: unique한 columns\nSurrogate Key: 자동으로 할당되는 일련번호\nIDENTITY (start, increment)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "title": "The Relational Model",
    "section": "Referential Integrity Constraint",
    "text": "Referential Integrity Constraint\n\n모든 foriegn key는 존재하는 primary key와 매칭되야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#null-values",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#null-values",
    "title": "The Relational Model",
    "section": "Null values",
    "text": "Null values\n\nrequired, allow nulls 설정으로 null값을 허용할지 결정",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "title": "SQL",
    "section": "DDL (Data Definition Language)",
    "text": "DDL (Data Definition Language)\n\nCREATE (database, tables, views, indexes)\nALTER: modify columns / constraints\nDROP (database, tables, views, indexes)\nTRUNCATE: delete table data while keeping structure.\nMS Access에서는 지원하지 않음 =&gt; DELETE FROM table\n\nCREATE TABLE student (\n    id INT NOT NULL,\n    CourseID INT NOT NULL,\n    Name VARCHAR(100) UNIQUE, # unique는 자동으로 index 생성\n    Age INT,\n    CONSTRAINT STUDENT_PK PRIMARY KEY (id),\n    CONSTRAINT \n    COURSE_FK FOREIGN KEY (CourseID) \n    REFERENCES Course(CourseID) \n    ON UPDATE CASACADE \n    ON DELETE NO ACTION\n);\nALTER TABLE student ADD COLUMN major VARCHAR(100);\nALTER TABLE student ADD CONSTRAINT STUDENT_FK FOREIGN KEY (CourseID) REFERENCES Course(CourseID) ON DELETE CASCADE;\nALTER TABLE student ADD CONSTRAINT AGE_CHECK CHECK (Age &gt; 0);\nALTER TABLE student DROP CONSTRAINT AGE_CHECK;\nDROP TABLE student;\nTRUNCATE TABLE student;\n\nCREATE VIEW [view name] AS SELECT * FROM student;\n\nDML (Data Manipulation Language)\nINSERT INTO student VALUES (1, 'Alice', 20);\nUPDATE student SET age = 21, Name = 'babo' WHERE id = 1;\nDELETE FROM student WHERE id = 1;\n\n\nDQL (Data Query Language)\nA query create temporarily a new table.\nthis allows a query to create a new relation and feed information to another query as a subquery\nSELECT * FROM student;\nSELECT name \nFROM student \nWHERE age &gt; 20\nORDER BY name DESC, age ASC;\nSELECT DISTINCT name FROM student;\nSELECT name, age FROM student WHERE Age &gt; (SELECT AVG(Age) FROM student);\n\n\nJOIN\n\ninner join(equijoin)\n\nexplicit join: FROM table1 INNER JOIN table2 ON table1.id = table2.id\n(MS Access에서는 INNER를 명시해야됨)\nimplicit join: FROM table1, table2 WHERE table1.id = table2.id\n\nouter join\n\nleft outer join: FROM table1 LEFT JOIN table2 ON table1.id = table2.id\nright outer join: FROM table1 RIGHT JOIN table2 ON table1.id = table2.id",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "SQL"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "title": "표본의 분포",
    "section": "좋은 추정량이 되기 위한 조건",
    "text": "좋은 추정량이 되기 위한 조건\n\n불편성 (Unbiasedness) - 기본 조건\n추정량의 기대값이 추정하려는 모수와 같아야 함\n\\(E(\\hat{X}) = μ\\)\n\\(E(X_1) = μ\\)\n최소분산 (Minimum Variance)\n추정량의 분산이 가능한 작아야 함.\n표본의 갯수를 늘릴수록 분산이 줄어들어서 더 좋은 추정량이 됨\n\\(Var(\\hat{X}) = \\frac{σ^2}{n}\\)\n\\(Var(X_1) = \\sigma^2\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "title": "표본의 분포",
    "section": "표본평균의 분포",
    "text": "표본평균의 분포",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "title": "표본의 분포",
    "section": "표본분산의 분포",
    "text": "표본분산의 분포\n정규분포로 부터 추출된 표본의 \\(\\sum_{i=1}^{n} Z^2\\)은 자유도가 n인 카이제곱분포를 따름\n정규분포로 부터 추출된 표본의 \\(\\frac{(n-1)s^2}{\\sigma^2}\\)은 자유도가 n-1인 카이제곱분포를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "title": "표본의 분포",
    "section": "평균",
    "text": "평균\n\\(\\frac{\\hat{X} - μ}{s/\\sqrt{n}}\\) t-분포를 따름\nT분포: 표준 정규분포 Z, 자유도가 n인 카이제곱분포가 서로 독립일 때 \\(T=\\frac{Z}{\\sqrt{Y/n}}\\)\nT분포는 정규분포와 비슷하지만, 표본의 크기가 작을 때 정규분포보다 두꺼운 꼬리를 가짐\nT분포가 값이 더 작고, 신뢰도가 감소함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "title": "표본의 분포",
    "section": "분산",
    "text": "분산\n확률변수 U와 V가 자유도가 n1, n2인 카이제곱분포를 따르고 서로 독립이면, \\(F=\\frac{U/n1}{V/n2}\\)는 F분포를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "title": "확률변수의 기댓값",
    "section": "확률변수의 기댓값",
    "text": "확률변수의 기댓값\n\n\\(μ = E(x)\\)로 가정. (모집단)\ncovariance는 선형관계를 보여준다.\nx와 y는 독립이다 -&gt; cov(x, y) = 0",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "title": "확률변수의 기댓값",
    "section": "Moment Generationg Functions",
    "text": "Moment Generationg Functions\n\n평균과 분산만으로 확률분포를 설명하기에는 부족하다.\n\nmoment: \\(μ_k′ = E(X^k), k ∈ ℤ+\\)\nVar(x) = \\(μ_2′ - μ_1′^2\\)\nE(x) = \\(μ_1′\\)\n모든 k에 대해 검증 불가 =&gt; mgf(moment generating function)\nmgf: \\(M_X(t) = E(e^{tx})\\)\n\n\n\\(M_X(t) = 1 + tμ_1′ + \\frac{t^2}{2!}μ_2′ + \\frac{t^3}{3!}μ_3′ + ...\\)\n\\(M′(t) = μ_1′ + \\frac{2t}{2!}μ_2′ + \\frac{3t^2}{3!}μ_3′ + ...\\)\n\\(M′(0) = μ_1′\\)\n\\(M′′(0) = μ_2′\\)\n\\(M^{(k)}(0) = μ_k′\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "title": "확률과 통계의 정의",
    "section": "통계학의 정의",
    "text": "통계학의 정의\n\n불확실한 상황에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법 체계\n모집단으로부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 함\n\n\n\n모집단: 통계분석의 대상이 되는 모든 개체들의 집합\n표본: 모집단으로부터 일정한 규칙에 의해 추출한 부분집합",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "title": "확률과 통계의 정의",
    "section": "확률의 개념",
    "text": "확률의 개념\n\n모집단에서 특정 사건(event)의 상대도수의 극한\n\n\nLaw of Large Numbers\n무수히 많은 시행이 반복되면 상대도수에 의해 계산되는 확률(통계적 확률)이 이론적 확률로 수렴한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "title": "확률과 통계의 정의",
    "section": "Sample Space and Events",
    "text": "Sample Space and Events\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n\n\n\n\nevent는 여러 원소를 가질 수 있다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "title": "확률과 통계의 정의",
    "section": "확률의 정의",
    "text": "확률의 정의\n\n고전적 확률: 모든 simple event가 동일한 확률을 가질 때 P(A)는 sample space가 n개의 원소로 이루어져 있을 때 k개의 원소를 가지는 event A의 확률\n통계적 확률: simple event가 동일한 확률을 가지지 않아도 된다. 표본의 수가 무한대로 갈 때, 표본의 확률이 수렴하는 값\n\n\n확률의 성질\n\n모든x에 대하여 P(x) &gt;= 0\nP(sample space) = 1\nA와 B가 배반사건이면 P(A or B) = P(A) + P(B)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "title": "확률과 통계의 정의",
    "section": "조건부 확률",
    "text": "조건부 확률\n\nEvent B가 발생했을 때 Event A의 확률 \\[P(A|B) = \\frac{P(A∩B)}{P(B)}\\]\n결합확률 (joint probability): P(A∩B)\n주변확률 (marginal probability): P(A), P(B), …\n\n\nMultiplication Law\n\\[P(A∩B) = P(A|B)P(B)\\]",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "title": "확률과 통계의 정의",
    "section": "Independent Events",
    "text": "Independent Events\n\n두 사건 A와 B가 독립일 때, P(A|B) = P(A), P(B|A) = P(B)\nsample space는 임의의 event와 독립이다.\n공집합은 임의의 event와 독립이다. (P(∅∩A) = P(∅) * P(A) = 0 * P(A) = 0 = P(∅))",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "title": "확률과 통계의 정의",
    "section": "베이즈 정리",
    "text": "베이즈 정리\n\n\nsample space를 상호 배반인 {B1, B2, …, Bn}으로 분할 (partition)\n\\(P(A) = P(A∩B_1) + P(A∩B_2) + ... + P(A∩B_n)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "title": "정규 분포",
    "section": "정규 분포의 합",
    "text": "정규 분포의 합\n두 분포의 합이 같은 분포가 되는 경우는 흔치 않다 (uniform distribution도 같지 않다)\n두 정규분포의 합은 정규분포가 된다\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "title": "정규 분포",
    "section": "Chi-square 분포",
    "text": "Chi-square 분포\nα = ν/2, θ = 2인 감마분포\n\\(Z \\sim N(0,1)\\)일 때, \\(Z^2 \\sim χ^2(1)\\)\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "title": "Auditory Haptic",
    "section": "귀의 구조",
    "text": "귀의 구조",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "title": "Auditory Haptic",
    "section": "Sound Waves",
    "text": "Sound Waves\n\n소리는 압력이 변하는 것\n소리는 vibrating object에 의해 발생한다\n연못에 돌을 던지면 생기는 wave와 비슷함\n어떤 분자든 움직이고 압력을 만들 수 있는건 전달 가능함\n물속에서도 소리가 전달됨. (밀도가 높아서 더 빨리 전달됨)\n고체(층간소음, 철로), gas\n진공에서는 매질이 없어서 소리가 안들림(우주 공간) &lt;-&gt; 빛은 매질이 없어도 이동됨\n파동이 전기신호로 바뀌어서 들림\n\n\n물리적 특성\n\n\namplitude: 진폭의 크기 -&gt; volume\nwavelength: 진폭의 넓이 -&gt; pitch, 1초 안에 몇 번 진동하는지(주파수 10Hz = 10번 진동)\n\n\n\n사람이 느끼는 perception\n\nPitch(소리의 높낮이)\n사람이 들을 수 있는 주파수는 20Hz ~ 15kHz\n어릴 때는 고주파를 잘 들음 (고주파에 고막이 반응을 못해서)\n사람은 고주파에 반응을 잘 못함\n절대 음감이랑 관련\nTimbre(음색)\n음악에서는 악기마다 다른 음색이 있음\n음색은 여러 주파수의 하모니(complex set of resonance공명)로 결정됨\nAmplitude and loudness\n소리의 물리적 강도가 2배 증가할 때 우리가 느끼는 소리의 크기(loudness)가 배로 느껴짐(찾아보니까 2배는 아니긴 함) 160db까지 들을 수 있음 130db부터는 고통스러움\nSpatialisation\n소리는 어느 방향에서 소리가 나도 들을 수 있음 (omnidirectional).\n시각은 볼 수 있는 방향만 볼 수 있음\n\n\n\nSound intensity (dB)\n\n데시벨(dB)은 기준점에서 로그 스케일만큼 증가. (선형적 x)\nthreshold: 주변의 소음에 비해 소리가 들리는 정도. 주파수에 따라 다른 특성을 가짐.\n\n\n\n85 dB에 장시간 노출되면 청력 손상\n\n신경의 손상: 장시간 센 자극에 hair cell이 손상됨\nconduction damage: 소리의 세기가 너무 커서 고막이나 뼈에서 손상이 생김",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "title": "Auditory Haptic",
    "section": "Masking Effect",
    "text": "Masking Effect\n\n청각에서만 주로 나타남.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "title": "Auditory Haptic",
    "section": "Equal Loudness Contour",
    "text": "Equal Loudness Contour\n\n곡선은 사람들이 소리가 같은 크기라고 느끼는 지점을 나타냄\n인간 청력의 threshold가 주파수마다 다르다.\n저주파수와 고주파수는 중간 주파수에 비해 같은 강도에서 상대적으로 작게 들립니다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "title": "Auditory Haptic",
    "section": "Locating Sounds",
    "text": "Locating Sounds\n왼쪽 귀와 오른쪽 귀에서 들리는 소리의 차이\n\ndifference in phase(위상): 소리의 파장이 오목한 phase, 볼록한 phase 차이\ndifference in loudness: 가까운게 더 크게 들림\ndifference in onset: 가까운게 더 빨리 도달함\n\n\n여기까지가 소리의 mechanical한 특성이고, 이후는 이 소리를 인간이 어떻게 perception하는지에 관한 내용",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "title": "Auditory Haptic",
    "section": "Hearing Without Awareness",
    "text": "Hearing Without Awareness\n\nCocktail Party Effect: 주변 소음에서도 특정 소리를 들을 수 있음\nEx) 친구 이름을 듣고 반응하는 것, 한국인들이 한국말을 잘 듣는것\nDichotic Listening: 두 귀에 다른 소리를 들려주고 정보를 인식했는지 확인\nignored 귀에서 여전히 정보를 인식할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "title": "Auditory Haptic",
    "section": "Alarms",
    "text": "Alarms\n\nOverview\nomnidirectional한 특성때문에 visual alarm에 비해 자주 사용된다\nEx) 소방차 사이렌 소리\n\n주변 소음에 비해 충분히 db이 커야함 주변 소음과의 차이가 15dB minimal, 30dB required\n하지만 소리가 너무 크면 청각에 손상을 일으킬 수 있음\n안전상의 이유로 소리는 85 ~ 90dB 이하로 유지되어야 함\nmasking 위협때문에 여러 주파수를 혼합해서 냄\n다른 signal과 헷갈리지 않아야함\nEx) 병원의 환자실에 여러 장비가 있는데 장비마다 알람이 구분이 안되면 안됨.\nInformative and distinctive\n각각의 physical dimension(pitch(4), duration(4), amplitude(4))은 4개를 넘게 쓰지 마라\nEx) 컴퓨터 메인보드, 장비 고장 시 비프음 기준이 있음\nEx) 자동차\n\nstereotypic: 어디서 소리가 오는지\n\npitch\n\n\n\n\nNon-speech Alarm\n\nlanguage independent\n글로벌하게 가고 싶다면\nNSA가 유용하다는 증거\n클릭을할 때 딸깍 소리가 나면 실수가 덜 함\n비디오 게이머들은 소리가 없으면 게임을 못함\n일시적이고 부수적인 상태 정보 전달에 효과적\n예시) 게임에서:\n\nHP가 부족할 때 주기적인 경고음\n\n아이템 획득 시 짧은 효과음\n\n배경에서 지속적으로 재생되는 상태 알림음\n\nstereo sound로 방향을 알려줄 수 있음\n비쥬얼로는 3d 표현하기 어려움\n\n\n\nVoice Alarm\n\n자연스러운 방법으로 기기와 통신할 수 있음\nSymbolic alarm에 비해 더 많은 정보를 전달할 수 있음\nSymbolic alarm은 학습을 해야한다는 단점이 있음\nNon-Speech에 비한 한계\n\n소리가 섞이면 헷갈림\n\nmore susceptible to frequency-specific masking\n사람의 voice는 정해진 주파수가 있다.\n그 주파수에 소리가 섞이면 소리를 못들을 수 있음\n다국어 환경을 고려해야함\n\nSound Transmission Problem\n말하고자 하는 바가 전달이 잘 안될 수 있음\nEx) 파일럿 안내. 라디오에서 사용할 수 있는 대역폭이 제한되있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "title": "Auditory Haptic",
    "section": "Voice Recognition",
    "text": "Voice Recognition\nSound Transmission Problem을 해결하기 위해 사용됨\n\nArticulation index: pure bottom-up(signal의 특성에 의존) approach\nsignal이 얼마나 명확하게 잘 들리는지 평가\n1.0: 주변소음에 상관없이 잘 들리는 상태\n0.0: 주변소음에 묻혀서 소리가 들리지 않는 상태\nSpeech intelligibility measure\npoor signal quality is compensated by top-down processing =&gt; 어떻게 top-down processing을 잘 할까?를 테스트 해 보았다.\n전달하는 정보의 양을 제한, 문장의 형태로 전달하는게 좋음\n긍정이나 부정이 잘 나타나는 단어를 사용(Ok는 애매함)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "title": "Auditory Haptic",
    "section": "Tactile Perception",
    "text": "Tactile Perception\n\nTouch is complex\nOnly bi-directional communication channel: 접촉하거나 움직이거나 한 다음에 반응을 얻는 등, input과 output이 동시에 일어남\n환경에 대한 정보를 포괄해서 전달함\n온도, 표면의 거칠기, 등등\nfeedback을 제공함\n수용체가 피부 변형을 감지함\n민감도는 단위 면적당 촉점이 얼마나 분포되어 있는지에 따라 결정됨",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "title": "Auditory Haptic",
    "section": "Tactile information",
    "text": "Tactile information\n\n운동 감각을 통해 받아들이는 것 (force feedback)\n\n\n\n큰 물체는 움직이기 어렵게 함 =&gt; 더 세밀하게 조종 가능\n\n게임에서 많이 사용됨\n\n\n촉감을 통해 받아들이는 것 (vibration feedback)\n\n\n\nusing vibration for information transfer\nsimilar physical characteristics to auditory signal\n\namplitude, frequency, duration, wave pattern\n\nused for navigation aid\n중요한 정보는 시각, 나머지 feedback은 촉각, 청각으로 받아들임.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "title": "Human Information Processing Model",
    "section": "인간 정보 처리 과정",
    "text": "인간 정보 처리 과정\n\n\n1. 감각 처리 (Sensory Processing)\n\n주요 감각: 시각, 청각, 운동 감각(proprioception)\n운동 감각: 생리적 신호, 중력과 가속도에 따른 몸의 위치 감각\nSTSS (Short Term Sensory Store)\n\n각 감각기관별 정보 저장 공간\n주의 집중 불필요, 정보 그대로 저장\n빠르게 소멸 (예: 시각 - Iconic Memory 200-300ms, 청각 - Echoic Memory 2-8s)\n\n\n\n\n2. 지각 (Perception)\n\n정보 해석 과정\nTop-Down Processing: 과거 기억으로 정보 해석 (Long Term Memory 활용)\nBottom-Up Processing: 새로운/익숙하지 않은 정보 해석\nPreattentive Processing: 주의 집중 없이 자동적 정보 해석 (예: Stroop Effect)\n\n\n\n3. 주의 자원 (Attention Resources)\n\n전반적인 정보 처리 과정에 영향\nSearch Light Metaphor: 주의 집중의 비유적 설명\n주의 실패 유형:\n\nSelective Attention: 잘못된 곳에 집중\nFocused Attention: 주의 집중 부족\nDivided Attention: 다중 작업 시 주의 분산\nSustained Attention: 장시간 주의 유지 실패\n\n\n\n\n4. 장기 기억 (Long Term Memory)\n\n학습을 통한 정보 저장\n유형:\n\nDeclarative Memory (What): 사실적 지식 (Episodic, Semantic)\nProcedural Memory (How): 절차적 지식\n\n기억 실패: Encoding, Storage, Retrieval 단계에서 발생 가능\n\n\n\n5. 인지 (Cognition)\n\nWorking Memory:\n\n단기적 정보 처리 및 조작\n제한된 용량 (7±2 items)\nLong Term Memory와 Perception으로부터 정보 획득\n\n\n\n\n6. 반응 선택 (Response Selection)\n\n의사결정 이론:\n\nSignal Detection Theory\nExpected Value\nBayesian Decision Theory\nMulti-attribute Theory\n\nInformation Processing\n\nAttention and working memory\nHeuristics and biases: 인간은 합리적이지 않음\n\nNaturalistic Decision Making: Recognition-Primed Decision Model (직감)\n\n\n\n7. 반응 실행 (Response Execution)\n\n\n8. 시스템 환경 (System Environment)\n\nclosed-loop 형태의 피드백 제공\ndelay가 발생시 성능 저하",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Human Information Processing Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "title": "Display",
    "section": "Display Purpose",
    "text": "Display Purpose\n\n사람의 인지와 시스템의 실제 정보 사이의 커뮤니케이션을 위한 중간 다리 역할.\n시스템이 무엇을 하는 중이고, 무엇을 해야 하고, 어떻게 작동하는지 오퍼레이터에게 전달하기 위한 목적(mental model을 만들기 위함)\n설계된 sensory input을 통해서 파악하게 해야함\n다른 sensory input과 구별이 되야함.\n사용자가 이해할 수 있어야함(Compatible)\n\nConceptual Compatibility\nex) 플로피 디스크 심볼은 저장 용도로 사용됨\nmovement compatibility(pictorial realism): 실제와 유사한 모양을 사용하면 이해하기 쉬움\nex) 엘리베이터가 위 아래로 움직이니까 스케일을 linear로 맞춤",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "title": "Display",
    "section": "Display Rules",
    "text": "Display Rules\n\nFour Cardinal Rules\n\n꼭 필요한 정보만 제공해라\n필요한 수준의 정확도만 제공하라 (ex. 소숫점 3자리까지만. 굳이 다 보여주지 않아도 됨)\n가장 direct, simple, understandable, and usable하게 정보를 제공하라\nex) 지하철 디스플레이에서 열차가 언제 도착하는지 알려줘야 하는데 이상한걸 보여줘서 멘탈 워크로드가 높아진다.\n고장이나 작업 실패의 경우 명확히 어디서 문제가 발생했는지 바로 알아차리게 제공하라\n첫번째 원칙을 위반할 수도 있다(alarm flooding)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "title": "Display",
    "section": "Types of Displays",
    "text": "Types of Displays\n\nAuditory\n\nDetectable\nDiscrmination\nMeaningful\nMain problem: hearing ability depends on environments / background noise (auditory spatial coding 인지하기 힘듦)\n\nTactual(Haptic)\n\nDetectable: 손처럼 민감한 부분은 가능하지만, 둔감한 부분은 어렵다\nDiscrimination: nomal job이랑 구분되어야 한다\nMeaningful: tactual display에서는 어려운 부분. convention이 없음\nMain problem: 잘 안쓰이고, 손 이외에는 사용하기 어렵다\n\nOlfactory Displays - smell\n\nDetectable\nDiscrimination\nMeaningful\nMain problem: 냄새에 대한 민감도가 사람마다 다르다. regenerate 하기가 어렵다. 후각이 금방 마비된다. 전쟁에서 후각을 이용한 의사전달을 시도하기도 함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "title": "Display",
    "section": "Visual Displays",
    "text": "Visual Displays\n\nAppropriate if:\n\nNoisy environment\n한 자리에 머무는 경우(traditional. 옛날엔 들고 다니기 어려웠음)\nmessage가 길거나 복잡한 경우, spatial coding이 필요한 경우\n\nGuiding principles for design:\n\n눈에 잘 띄어야함(배경과 전경의 차이가 구분되어야함)\nLegible, 쉽게 보고 읽을 수 있어야함(ex. 엠뷸런스의 글씨)\nUnderstandable\nmain problem: 시각이 overload됨. 정보가 너무 많음\n\n\n\nDynamic Information\n변화하는 정보를 보여주는 것에는 4가지 원칙이 있다.\n\nSituation awareness\n가까운 미래에 무슨 일이 일어날 지 예측할 수 있어야함\n상황 인식 3단계:\n\nPerception: 무엇이 일어나고 있는지 인지(check readings)\nComprehension: 그것이 무엇을 의미하는지 이해\nProjection: 미래에 무슨 일이 일어날지 예측\n\n\n\nQuantitative readings\n정확한 값을 보여주는 용도로 사용됨\n\n고정 스케일의 움직이는 초점 (generally best)\n움직이는 스케일의 고정 초점\ndigital display (변동성이 큰 경우 그냥 숫자만 보여주는것보다 스케일, 초점을 사용하는게 더 효과적임)\nDesign of Analog Scales\n\n일반적으로 fixed scale, moving pointer가 좋다\n숫자의 증가는, linear 스케일에 움직이는 포인터가 자연스럽다.\nex) 온도계, 엘리베이터\n같은 작업을 하는 여러개의 pointer, scale indicator를 섞어 쓰지 말아라.\ncontrol, display가 혼합된경우 control로 pointer로 움직여라\n작은 변화 감지가 중요한 경우는 moving pointer가 더 좋다\n범위가 너무 큰 경우는 moving scale이 더 좋다\n\n\n\n\nQualitative readings\n대략적인 값, 트렌드, 변화의 비율, 변화의 방향을 보여주는 용도로 주로 사용됨.\n\ncontinuous data converted to range\n의미를 강조하고 싶을 때 color를 보조 도구로써 보여주기도 함\nShape coding\nex) 교통 표지판 8각형은 stop을 의미\nZone coding\nex) 신호등의 위치가 고정되어 있음\n\nRedundancy gain: 시각 청각 촉각, 혹은 칼라코드, 위치코드 같이 여러가지 정보(multi-modal)를 제공하면 정확히 해석할 수 있다.\n\n\nCheck readings\n시스템의 상태를 확인할 수 있어야함\n\nqualitaative reading의 특별한 case\n정상인 상태는 명확히 보여줘야함\n정상적인 것은 align해서, 비정상적인 것은 삐뚤어지게 설계해서 pre-attentive processing을 유도하라\n시각 정보를 보완하기 위해 청각 시그널을 제공하라",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "title": "Display",
    "section": "Signal and Warning Lights",
    "text": "Signal and Warning Lights\n실질적인, 잠재적인 위험 상황을 알리는 용도\n일반적으로 하나의 라이트만 사용함\n\nsteady-state light: 지속적인 싱태를 나타냄\nflashing light: 위급 상황 (flash 비율은 3-10 per seconds)\n배경에 비해 최소 두 배 이상 밝아야함\n유효 시야 30도 안쪽에 배치해야함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "title": "Sensor System (Visual)",
    "section": "눈의 구조",
    "text": "눈의 구조\n\n결막 (conjunctiva): 눈을 보호\n각막 (cornea): 빛을 굴절\n홍채(iris): 빛의 양을 조절\n동공 (pupil): 빛이 들어오는 곳\n수정체(lens): 빛을 집중하는 역할\n공막 (sclera): 눈을 보호\n유리체 (vitreous humor): 눈을 유지\n망막(retina): 빛을 감지\n망막의 세포\n\nNerve cell: 빛을 감지\nPhotoreceptor: 빛을 감지\n\n간상세포(cone): 세부적인 정보, 색상 인식, photopic conditions. fovea에 몰려있음. 짧은 파장의 색에 더 민감함\n막대세포(rod): 어두운 곳에서 활동, 주변 시야 빛을 받으면 rhodopsin이 분해됨, scotopic conditions. 긴 파장의 색에 더 민감함.\n\nChoroid: 영양 공급\n\n시신경(optic nerve): 망막에서 뇌로 정보 전달\n맹점(optic disk)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "title": "Sensor System (Visual)",
    "section": "light adaption",
    "text": "light adaption\n\n눈이 어두운 곳에서 밝은 곳으로 이동할 때, 시간이 걸림\n명순응동안 rod sensitivity가 감소하고 cone sensitivity가 증가\n어두운 곳에서 밝은 곳으로 이동할 때, 눈이 눈부실 수 있음\n암순응동안 cone sensitivity가 감소하고 rod sensitivity가 증가",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "title": "Sensor System (Visual)",
    "section": "color vision",
    "text": "color vision\n\ncone cell의 photo-pigment(RGB 64:32:2)로 색상을 인식\n망막 중앙에는 파란색이 없음\nsharpness는 brightness와 color difference에 영향을 받음\n사람은 7백만가지 색상을 인식할 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "title": "Sensor System (Visual)",
    "section": "design with color",
    "text": "design with color\n\nMono-chromatic: 단색\nAnalogous: 비슷한 색\nComplementary: 반대 색\n\n\nbefore design\n\n굳이 흑백을 안쓰고 color를 사용해야하는 이유가 있는지\ncolor가 텍스트나 object에 적합한지\ncolor가 이해나 관습에 도움이 되는지\n노안 / 색맹 고려",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "title": "Sensor System (Visual)",
    "section": "depth perception",
    "text": "depth perception\n\ndepth judgment\n\nobject-centered cues\n\n\nlinear perspective: 두 평행선이 좁을 수록 더 멀리 있는 것으로 인식\ninterposition(occlusion): 물체가 다른 물체를 가리면 가려진 물체가 더 멀리 있는 것으로 인식\nheight in the plane: 물체가 높이 있을수록 더 멀리 있는 것으로 인식\nlight and shadow: 빛과 그림자로 물체의 거리를 인식\nrelative size: 물체가 작을수록 더 멀리 있는 것으로 인식\ntexture gradient: 물체가 멀어질수록 세부적인 텍스처가 사라짐\nbrightness: 물체가 밝을수록 더 가까이 있는 것으로 인식\naerial perspective: 물체가 먼발에서 가까워질수록 색이 흐려짐\nmotion parallax: 물체가 빠르게 움직일수록 더 가까이 있는 것으로 인식 fixation point\n\n\nobserver-centered cues\n\n\nbinocular disparity: 두 눈의 시각적 차이\nconvergence: 눈이 물체를 바라볼 때 발생하는 각도\naccommodation: 눈의 렌즈가 물체를 바라볼 때 발생하는 조절. 가까운 물체일수록 렌즈가 더 둥글어짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html",
    "href": "posts/04_archives/aws_saa/index.html",
    "title": "AWS SAA 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-04-15\n        종료일: 2024-05-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증cloud",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#details",
    "href": "posts/04_archives/aws_saa/index.html#details",
    "title": "AWS SAA 준비",
    "section": "Details",
    "text": "Details\nAWS Solution Architect Associate 자격증을 취득하였습니다.\n자격증 링크",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#tasks",
    "href": "posts/04_archives/aws_saa/index.html#tasks",
    "title": "AWS SAA 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#참고-자료",
    "href": "posts/04_archives/aws_saa/index.html#참고-자료",
    "title": "AWS SAA 준비",
    "section": "참고 자료",
    "text": "참고 자료\n\nAWS Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#related-posts",
    "href": "posts/04_archives/aws_saa/index.html#related-posts",
    "title": "AWS SAA 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "title": "김형훈의 학습 블로그",
    "section": "AWS FSx",
    "text": "AWS FSx\n\nAmazon FSx for Windows File Server: fully managed Windows file system\nAmazon FSx for Lustre: fully managed Lustre file system, seamlessly integrated with S3 (can read and write data directly to S3)\nAmazon FSx for NetApp ONTAP: fully managed NetApp ONTAP file system, point-in-time snapshots, data deduplication, and data compression\nAmazon FSx for OpenZFS: fully managed OpenZFS file system, point-in-time snapshots, data deduplication, and data compression\n\n\nFile System Deployment Options\n\nScratch File System: temporary storage for data processing. no replication.\nPersistent File System: long-term storage for data processing. replicate data across multiple Availability Zones.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Storage Gateway",
    "text": "AWS Storage Gateway\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.\nS3 File Gateway: store and retrieve objects in Amazon S3 using file protocols (NFS, SMB), cache data locally, not glacier.\nFSx File Gateway: store and retrieve objects in Amazon FSx using file protocols (NFS, SMB) for window file server.\nVolume Gateway: store and retrieve objects in Amazon S3, EBS Snapshot using iSCSI protocol.\n\ncached volume: cache frequently accessed data locally.\nstored volume: entire dataset stored locally, asynchronously backed up to S3.\n\nTape Gateway: store and retrieve objects in Amazon S3 using virtual tape library (VTL) interface",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "title": "김형훈의 학습 블로그",
    "section": "AWS DataSync",
    "text": "AWS DataSync\n\nAWS DataSync is a data transfer service that makes it easy for you to automate moving data between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server.\nFile permissions and metadata are preserved during transfer.\nif not aws to aws, need agent to transfer data.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/19_DR.html",
    "href": "posts/04_archives/aws_saa/notes/19_DR.html",
    "title": "Disaster Recovery(DR)",
    "section": "",
    "text": "Disaster Recovery(DR)\n\nRPO: Recovery Point Objective\n\nthe maximum acceptable amount of data loss measured in time\n\nRTO: Recovery Time Objective\n\nthe maximum acceptable amount of time to recover the system  ## DR Strategies\n\nBackup and Restore: Simple but RPO and RTO are high\nPilot Light: Minimal version of the environment is always running\nWarm Standby: A scaled-down version of a fully functional environment is always running\nHot-site / Multi-Site Approach: Fully functional environment is always running\n\n\n\nDatabase Migration Service(DMS)\n\nmigrate data from one database to another\nsource is available during migration\nHomogeneous Migration: same database engine\nHeterogeneous Migration: different database engine. must use Schema Conversion Tool(SCT)\nContinuous Data Replication using CDC\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Disaster Recovery(DR)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/06_route53.html",
    "href": "posts/04_archives/aws_saa/notes/06_route53.html",
    "title": "Route53",
    "section": "",
    "text": "Domain/subdomain\nRecord type (A, AAAA, CNAME, NS)\n\nA: IPv4\nAAAA: IPv6\nCNAME: Canonical name. cant be used for root domain\nNS: Name server (another DNS server)\nalias: Route53 specific. can be used for root domain. free. health check. no TTL, can’t be used for ec2 instance\n\nValue\nRouting policy\n\nSimple: one record with multiple values, choose randomly by client, no health check, if alias then specify only one\nWeighted: split traffic based on weight\nLatency based: split traffic based on latency\nFailover: primary and secondary\nGeolocation\nMultivalue answer: multiple values, health check, choose randomly by client\nGeo-proximity\n\nTTL\n\n\n\n\nEndpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/06_route53.html#health-check",
    "href": "posts/04_archives/aws_saa/notes/06_route53.html#health-check",
    "title": "Route53",
    "section": "",
    "text": "Endpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/03_ebs.html",
    "href": "posts/04_archives/aws_saa/notes/03_ebs.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/03_ebs.html#what-is-ebs",
    "href": "posts/04_archives/aws_saa/notes/03_ebs.html#what-is-ebs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#aws-lambda",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#aws-lambda",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#dynamodb",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#dynamodb",
    "title": "김형훈의 학습 블로그",
    "section": "DynamoDB",
    "text": "DynamoDB\n\nAmazon DynamoDB is a fully managed, serverless, key-value and document database that delivers single-digit millisecond performance at any scale.\nstandard table: high availability, durability, and performance\nIA table: infrequently accessed data\nmax item size: 400KB\nprovisioned mode, on-demand mode\n\n\nDynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement - from milliseconds to microseconds - even at millions of requests per second.\nmicroseconds latency\nno need to modify application \n\n\n\nDynamoDB Streams\n\nDynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables.\ncan trigger lambda function, SQS, Kinesis\n24 hours retention period\nlimit: 5 active streams per table\n\n\n\nGlobal Tables\n\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution.\nautomatic replication\nmust enable DynamoDB Streams\n\n\n\nbackup and restore\n\nPoint-in-Time Recovery\n\nPoint-in-time recovery helps protect your DynamoDB tables from accidental write or delete operations.\nrestore to any point in time within 35 days\nthe recovery process creates a new table\n\non-demand backup\n\nrestore to any point\nthe recovery process creates a new table",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#api-gateway",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#api-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "API Gateway",
    "text": "API Gateway\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nEdge-optimized API: global, use CloudFront\nRegional API: regional, use API Gateway\nPrivate API: VPC endpoint",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#step-functions",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#step-functions",
    "title": "김형훈의 학습 블로그",
    "section": "Step Functions",
    "text": "Step Functions\n\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#amazon-cognito",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#amazon-cognito",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Cognito",
    "text": "Amazon Cognito\n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.\nUser Pools: user directory\nIdentity Pools: federated identity, temporary access AWS resources",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sqs",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sqs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sns",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sns",
    "title": "김형훈의 학습 블로그",
    "section": "AWS SNS",
    "text": "AWS SNS\n\npublish/subscribe messaging service\nSNS FIFO (only SQS can subscribe)\nmessage filtering\n\n\nFanout\nSNS + multiple SQS",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Kinesis",
    "text": "Amazon Kinesis\n\nKinesis Data Streams\n\nreal-time data streaming service\ndata retention: default 24 hours, maximum 365 days\nonce data inserted, cannot be deleted\nprovisioned mode, on-demand mode\nVPC endpoint available \n\n\n\nKinensis Data Firehose\n\ndata transformation, compression, encryption\nbatch data delivery\nserverless  \n\n\n\nKinesis Data Analytics\n\n\nKinesis Video Streams",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html",
    "title": "Amazon Redshift",
    "section": "",
    "text": "SQLqueries -S3as data source - supportsCSV,JSON,Parquet,ORCdata formats - 5$ per TB scanned ## Performance Improvement - usecolumnardata formats (less scan) =&gt;Parquet,ORCby usingAWS Glue- compress data =&gt;GZIP,Snappy,LZO-partition datasetsin S3 for easy querying on virtual columns (path) -Use larger files` (&gt; 128MB) to minimize overhead",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#federated-query",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#federated-query",
    "title": "Amazon Redshift",
    "section": "Federated Query",
    "text": "Federated Query\nallows you to query data in relational, non-relational, object, … in a single query on AWS or on-premises",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#cluster",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#cluster",
    "title": "Amazon Redshift",
    "section": "cluster",
    "text": "cluster\n\nleader node\ncompute node",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "title": "Amazon Redshift",
    "section": "snapshots and DR",
    "text": "snapshots and DR\n\nMulti-AZ for some cluster\nsnapshots are point-in-time backups in S3\nchange is saved\nautomate snapshot, manual snapshote",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "title": "Amazon Redshift",
    "section": "loading data into redshift",
    "text": "loading data into redshift\n\nAmazon Kinesis Data Firehose\nAmazon S3 copy\nEC2 instance JDBC driver",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "title": "Amazon Redshift",
    "section": "Redshift Spectrum",
    "text": "Redshift Spectrum\n: query data directly in S3 without loading it into Redshift",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/07_S3.html",
    "href": "posts/04_archives/aws_saa/notes/07_S3.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/07_S3.html#aws-s3",
    "href": "posts/04_archives/aws_saa/notes/07_S3.html#aws-s3",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/02_ec2.html",
    "href": "posts/04_archives/aws_saa/notes/02_ec2.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/02_ec2.html#what-is-ec2",
    "href": "posts/04_archives/aws_saa/notes/02_ec2.html#what-is-ec2",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/14_machine_learning.html",
    "href": "posts/04_archives/aws_saa/notes/14_machine_learning.html",
    "title": "Amazon Rekognition",
    "section": "",
    "text": "Amazon Rekognition\n\nfind objects, people, text, scenes in image and video analysis ## content moderation\ndetect explicit and suggestive content\na2i for human review\n\n\n\nAmazon Transcribe\n: speech-to-text service\n\n\nAmazon Polly\n: text-to-speech service\n\n\nAmazon Translate\n: language translation service\n\n\nAmazon Comprehend\n: natural language processing service\n\n\nAmazon Lex\n: chatbot service\n\n\nAmazon SageMaker\n: machine learning service\n\n\nAmazon Forecast\n: time series forecasting service\n\n\nkendra\n: document search service\n\n\nAmazon Personalize\n: personalized recommendation service\n\n\nTextract\n: OCR service\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon Rekognition"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html",
    "href": "posts/04_archives/adp_실기/notes/01.html",
    "title": "pandas data 구조",
    "section": "",
    "text": "pandas: numpy를 라벨링한거",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#before",
    "href": "posts/04_archives/adp_실기/notes/01.html#before",
    "title": "pandas data 구조",
    "section": "Before",
    "text": "Before\n데이터를 호출하고, 데이터 내용과 요약 / 통계 정보를 확인해야함\n칼럼명이 칼럼 타입을 변경해야할 때도 있음\n\nPandas 사용 준비\n\n라이브러리 설치\n라이브러리 호출\n\n\nimport pandas as pd\n\npd.set_option('display.max_rows', 10)\n\n\n\nDataFrame 선언\n\nimport numpy as np\ndataset = np.array([['kor', 70], ['math', 80]])\n# declare df 1\ndf = pd.DataFrame(dataset, columns=['class', 'score'])\n# declare df 2\ndf = pd.DataFrame([['kor', 70], ['math', 80]], columns=['class', 'score'])\n# declare df 3\ndf = pd.DataFrame({'class': ['kor', 'math'], 'score': [70, 80]})\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\n\nDataFrame 읽고 저장\n\n# filepath = '../book/data/data.csv'\n# data = pd.read_csv(filepath, na_values='NA', encoding='utf8')\n# data.to_csv('result.csv', header=True, index=True, encoding='utf8')\n\n\n\nDataFrame 출력\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.8 KB\n\n\n\niris.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\nsepal length와 petal width의 값의 차이가 크다.\n전처리 과정에서 변수 정규화 수행의 근거가 된다.\n\n\nindex / column 명 변경\n\ndf.index\n\nRangeIndex(start=0, stop=2, step=1)\n\n\n\nlist(df.index)\n\n[0, 1]\n\n\n\ndf.index = ['A', 'B']\ndf.index\n\nIndex(['A', 'B'], dtype='object')\n\n\n\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\nA\nkor\n70\n\n\nB\nmath\n80\n\n\n\n\n\n\n\n\ndf.set_index('class', drop=True, append=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nscore\n\n\nclass\n\n\n\n\n\nkor\n70\n\n\nmath\n80\n\n\n\n\n\n\n\n\ndf.reset_index(drop=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\niris.columns\n\nIndex(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)'],\n      dtype='object')\n\n\n\niris.columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\niris\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.columns = iris.columns.str.replace(' ', '_')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\n데이터 타입 변경\n사용 가능한 타입\n\nint\nfloat\nbool\ndatetime\ncategory\nobject\n\n\niris.dtypes\n\nsepal_length    float64\nsepal_width     float64\npetal_length    float64\npetal_width     float64\ndtype: object\n\n\n\niris['sepal_length'] = iris['sepal_length'].astype('int')\niris[['sepal_width', 'petal_length']] = \\\niris[['sepal_width', 'petal_length']].astype('int')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n4\n5\n3\n1\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6\n3\n5\n2.3\n\n\n146\n6\n2\n5\n1.9\n\n\n147\n6\n3\n5\n2.0\n\n\n148\n6\n3\n5\n2.3\n\n\n149\n5\n3\n5\n1.8\n\n\n\n\n150 rows × 4 columns",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "href": "posts/04_archives/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "title": "pandas data 구조",
    "section": "row / coumn 선택 추가 삭제",
    "text": "row / coumn 선택 추가 삭제\n\nrow 선택\n\niris[0:4]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n\n\n\n\n\n\n\ncolumn 선택\nSeries 형식으로 출력\n\niris['sepal_length']\n\n0      5\n1      4\n2      4\n3      4\n4      5\n      ..\n145    6\n146    6\n147    6\n148    6\n149    5\nName: sepal_length, Length: 150, dtype: int64\n\n\nDataFrame 형식으로 출력\n\niris[['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n...\n...\n...\n\n\n145\n6\n3\n\n\n146\n6\n2\n\n\n147\n6\n3\n\n\n148\n6\n3\n\n\n149\n5\n3\n\n\n\n\n150 rows × 2 columns\n\n\n\n\n\ncolumn, row 선택\n\niris.loc[0:4, ['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n\n\n\n\n\n\niris.iloc[0:4, [1, 2]]\n\n\n\n\n\n\n\n\nsepal_width\npetal_length\n\n\n\n\n0\n3\n1\n\n\n1\n3\n1\n\n\n2\n3\n1\n\n\n3\n3\n1\n\n\n\n\n\n\n\n\n\nrow 추가\n\n# 방법 1: concat 사용\n# df = pd.concat([df, pd.DataFrame([{'class': 'eng', 'score': 90}])], ignore_index=True)\n\n# 방법 2: loc 사용 \ndf.loc[len(df)] = {'class': 'eng', 'score': 90}\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n2\neng\n90\n\n\n\n\n\n\n\n\n\ncolumn 추가\n\ndf['yo'] = df['score'] + 10\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n2\neng\n90\n100\n\n\n\n\n\n\n\n\n\nrow 삭제\n\ndf.drop(2, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n\n\n\n\n\n\n\ncolumn 삭제\n\ndf.drop(columns=['yo'], inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#조건-선택",
    "href": "posts/04_archives/adp_실기/notes/01.html#조건-선택",
    "title": "pandas data 구조",
    "section": "조건 선택",
    "text": "조건 선택\n\niris[(iris['sepal_length'] &gt; 5) & (iris['sepal_width'] &lt; 3)]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n54\n6\n2\n4\n1.5\n\n\n58\n6\n2\n4\n1.3\n\n\n62\n6\n2\n4\n1.0\n\n\n63\n6\n2\n4\n1.4\n\n\n68\n6\n2\n4\n1.5\n\n\n...\n...\n...\n...\n...\n\n\n130\n7\n2\n6\n1.9\n\n\n132\n6\n2\n5\n2.2\n\n\n133\n6\n2\n5\n1.5\n\n\n134\n6\n2\n5\n1.4\n\n\n146\n6\n2\n5\n1.9\n\n\n\n\n29 rows × 4 columns\n\n\n\n\ndf.loc[df['score'] &gt; 70, '합격'] = 'Pass'\ndf.loc[df['합격'] != 'Pass', '합격'] = 'Fail'\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\n\n\n\n\n0\nkor\n70\nFail\n\n\n1\nmath\n80\nPass\n\n\n\n\n\n\n\n\nimport numpy as np\n\ncondition_list = [(df['score'] &gt;= 70), \n                  (df['score'] &lt; 70) & (df['score'] &gt;= 60),\n                  (df['score'] &lt; 60)]\ngrade_list = ['A', 'B', 'C']\ndf['grade'] = np.select(condition_list, grade_list, default='F')\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n결측치 탐색\n\ndf.isna().sum()\n\nclass    0\nscore    0\n합격       0\ngrade    0\ndtype: int64\n\n\n\ndf.notna().sum(1) # 행 기준\n\n0    4\n1    4\ndtype: int64\n\n\n\n\n결측치 제거\n\n# dropna(axis=0, how='any' or 'all', thresh=None, subset=None, inplace=False)\ndf.dropna()\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n\n결측치 대체\n\n# fillna(value=None, method=None ('pad', 'ffill', 'backfill', 'bfill'), axis=None, inplace=False, limit=None)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-전처리의-의미",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-전처리의-의미",
    "title": "데이터 전처리",
    "section": "데이터 전처리의 의미",
    "text": "데이터 전처리의 의미\n\n데이터 클리닝\n데이터 통합\n데이터 변환\n데이터 축소\n불균형 데이터 처리\n데이터 분할",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#이상치-확인-및-정제",
    "href": "posts/04_archives/adp_실기/notes/03.html#이상치-확인-및-정제",
    "title": "데이터 전처리",
    "section": "이상치 확인 및 정제",
    "text": "이상치 확인 및 정제\n\n이상치 확인\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.core.common import random_state\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine['class'] = wine_load.target\nwine['class'] = wine['class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nplt.boxplot(wine['color_intensity'], whis=1.5)\nplt.title('Boxplot of color_intensity')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef outliers_iqr(dt, col):\n    q1, q3 = np.percentile(dt[col], [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return dt[(dt[col] &lt; lower_bound) | (dt[col] &gt; upper_bound)]\n\noutliers = outliers_iqr(wine, 'color_intensity')\noutliers\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nclass\n\n\n\n\n151\n12.79\n2.67\n2.48\n22.0\n112.0\n1.48\n1.36\n0.24\n1.26\n10.80\n0.48\n1.47\n480.0\nclass_2\n\n\n158\n14.34\n1.68\n2.70\n25.0\n98.0\n2.80\n1.31\n0.53\n2.70\n13.00\n0.57\n1.96\n660.0\nclass_2\n\n\n159\n13.48\n1.67\n2.64\n22.5\n89.0\n2.60\n1.10\n0.52\n2.29\n11.75\n0.57\n1.78\n620.0\nclass_2\n\n\n166\n13.45\n3.70\n2.60\n23.0\n111.0\n1.70\n0.92\n0.43\n1.46\n10.68\n0.85\n1.56\n695.0\nclass_2\n\n\n\n\n\n\n\n\n\n이상치 정제\n\n이상치 제거\n\n\ndrop_outliers = wine.drop(index=outliers.index)\n\nprint(\"Original:\", wine.shape)\nprint(\"Drop outliers:\", drop_outliers.shape)\n\nOriginal: (178, 14)\nDrop outliers: (174, 14)\n\n\n\n이상치 대체\n\n이상치를 NULL로 만든 후, 결측치와 함께 대체\n\nwine.loc[outliers.index, 'color_intensity'] = np.NaN\n\nwine['color_intensity'].fillna(wine['color_intensity'].mean(), inplace=True)\nwine.loc[outliers.index, 'color_intensity']\n\n/tmp/ipykernel_13054/3568685677.py:3: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\n151    4.908678\n158    4.908678\n159    4.908678\n166    4.908678\nName: color_intensity, dtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#범주형-데이터-처리",
    "href": "posts/04_archives/adp_실기/notes/03.html#범주형-데이터-처리",
    "title": "데이터 전처리",
    "section": "범주형 데이터 처리",
    "text": "범주형 데이터 처리\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris['Class'] = load_iris().target\niris['Class'] = iris['Class'].map({0: 'Setosa', \n                                   1:'Versicolour', \n                                   2: 'Virginica'})",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-분할",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-분할",
    "title": "데이터 전처리",
    "section": "데이터 분할",
    "text": "데이터 분할\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(iris.drop(\n  columns='Class'), iris['Class'], test_size=0.2, random_state=1004)\nprint('X_train: ', X_train.shape, 'X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape, 'y_test: ', y_test.shape)\n\nX_train:  (120, 4) X_test:  (30, 4)\ny_train:  (120,) y_test:  (30,)\n\n\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n87\n6.3\n2.3\n4.4\n1.3\n\n\n67\n5.8\n2.7\n4.1\n1.0\n\n\n131\n7.9\n3.8\n6.4\n2.0\n\n\n\n\n\n\n\n\ny_train.head(3)\n\n87     Versicolour\n67     Versicolour\n131      Virginica\nName: Class, dtype: object\n\n\n\niris['Class'].value_counts()\n\nClass\nSetosa         50\nVersicolour    50\nVirginica      50\nName: count, dtype: int64\n\n\n\ny_train.value_counts()\n\nClass\nVersicolour    41\nSetosa         40\nVirginica      39\nName: count, dtype: int64",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-스케일링",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-스케일링",
    "title": "데이터 전처리",
    "section": "데이터 스케일링",
    "text": "데이터 스케일링\n\nStandard Scaler\n\n평균이 0, 분산이 1이 되도록 변환\n이상치에 민감하다.\n회귀분석보다는 분류분석에 적합\n\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\nStdScaler = StandardScaler()\n\nStdScaler.fit(X_train)\nX_train_sc = StdScaler.transform(X_train)\nX_test_sc = StdScaler.transform(X_test)\n\n\n\nMin-Max Scaler\n\n0 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nMinMaxScaler = MinMaxScaler()\n\nMinMaxScaler.fit(X_train)\nX_train_sc = MinMaxScaler.transform(X_train)\n\nX_test_sc = MinMaxScaler.transform(X_test)\n\n\n\nMax Abs Scaler\n\n-1 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nMaxAbsScaler = MaxAbsScaler()\n\nMaxAbsScaler.fit(X_train)\nX_train_sc = MaxAbsScaler.transform(X_train)\n\nX_test_sc = MaxAbsScaler.transform(X_test)\n\n\n\nRobust Scaler\n\n중앙값을 0으로 설정하고, IQR을 사용하여 잉상치 영향을 최소화함\n\n\nfrom sklearn.preprocessing import RobustScaler\n\nRobustScaler = RobustScaler()\n\nRobustScaler.fit(X_train)\nX_train_sc = RobustScaler.transform(X_train)\n\nX_test_sc = RobustScaler.transform(X_test)\n\n\n\n다시 완본으로 변경\n\nscaler.inverse_transform()\n\n\npd.DataFrame(X_train_sc).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.384615\n-1.4\n0.028369\n0.000000\n\n\n1\n0.000000\n-0.6\n-0.056738\n-0.200000\n\n\n2\n1.615385\n1.6\n0.595745\n0.466667\n\n\n\n\n\n\n\n\nX_original = RobustScaler.inverse_transform(X_train_sc)\n\npd.DataFrame(X_original).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n6.3\n2.3\n4.4\n1.3\n\n\n1\n5.8\n2.7\n4.1\n1.0\n\n\n2\n7.9\n3.8\n6.4\n2.0",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#차원-축소",
    "href": "posts/04_archives/adp_실기/notes/03.html#차원-축소",
    "title": "데이터 전처리",
    "section": "차원 축소",
    "text": "차원 축소\n\nfeatures = []\nx = iris.drop(columns='Class')\n\nx = StandardScaler().fit_transform(x)\n\npd.DataFrame(x).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\npca_fit = pca.fit(x)\n\nprint(pca.singular_values_)\nprint(pca.explained_variance_ratio_.cumsum())\n\n[20.92306556 11.7091661   4.69185798  1.76273239]\n[0.72962445 0.95813207 0.99482129 1.        ]\n\n\n\nplt.title('Scree Plot')\nplt.plot(pca.explained_variance_ratio_, 'o-')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "title": "데이터 전처리",
    "section": "데이터 불균형 문제 처리",
    "text": "데이터 불균형 문제 처리",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html",
    "href": "posts/04_archives/k8s/index.html",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#details",
    "href": "posts/04_archives/k8s/index.html#details",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#tasks",
    "href": "posts/04_archives/k8s/index.html#tasks",
    "title": "k8s",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#참고-자료",
    "href": "posts/04_archives/k8s/index.html#참고-자료",
    "title": "k8s",
    "section": "참고 자료",
    "text": "참고 자료\n\nCKA Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#related-posts",
    "href": "posts/04_archives/k8s/index.html#related-posts",
    "title": "k8s",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html",
    "href": "posts/04_archives/k8s/notes/4_security.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#authentication",
    "href": "posts/04_archives/k8s/notes/4_security.html#authentication",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#authorization",
    "href": "posts/04_archives/k8s/notes/4_security.html#authorization",
    "title": "김형훈의 학습 블로그",
    "section": "Authorization",
    "text": "Authorization\n\nAPI groups\n\nk8s API is divided into groups\ncore group is the default group\ngroup has its own set of resources and verbs \n\n\n\nRBAC\n\ncreate Role object (namespace scoped resources)\ncreate RoleBinding object\n\nor\n\ncreate ClusterRole object (cluster scoped resources)\ncreate ClusterRoleBinding object\n\n\n\nservice account\n\ncreate ServiceAccount object\nthen it create token\nthen create secret object with the token\nthen secret object is linked to the service account\nand the token is automatically mounted to the pod\n\n=&gt; but this is not secure, and scalable =&gt; TokenRequest API is used",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#image-security",
    "href": "posts/04_archives/k8s/notes/4_security.html#image-security",
    "title": "김형훈의 학습 블로그",
    "section": "image security",
    "text": "image security\nif you use private image registry, you need to create secret object 1. create docker-registry type secret 2. add imagePullSecrets field in the pod spec",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html",
    "href": "posts/04_archives/k8s/notes/6_network.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html#core-dns",
    "href": "posts/04_archives/k8s/notes/6_network.html#core-dns",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html#network-plugin",
    "href": "posts/04_archives/k8s/notes/6_network.html#network-plugin",
    "title": "김형훈의 학습 블로그",
    "section": "network plugin",
    "text": "network plugin\n\nbridge type network\n - all container runtime solutions use same bridge script - and you can use third party plugins like flannel, calico, weave, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html#metrics-server",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html#metrics-server",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html#cadvisor",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html#cadvisor",
    "title": "김형훈의 학습 블로그",
    "section": "cAdvisor",
    "text": "cAdvisor\n\ncontainer advisor\nsub-component of kubelet\ncollects, aggregates, processes, and exports information about running containers",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- master nodes: manage the worker nodes and the pods in the cluster - etcd: key-value store for all cluster data - kube-scheduler: schedules pods to worker nodes - kube-controller-manager: runs controller processes - replication controller: ensures that the correct number of pods are running - node controller: monitors the nodes - worker nodes: host the pods that are the components of the application - kubelet: communicates with the master node - kube-proxy: forwards requests to the correct pod\n\n\n  - initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.\n\n\n\n\nkey-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.\n\n\n\n\n\n\n\nkube-api-server\n\n\n\n\n\n\n\n\n\n\n\n\nmust be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#etcd",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#etcd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "key-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#kube-api-server",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#kube-api-server",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "kube-api-server",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#kubelet",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#kubelet",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "must be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/notes/0_overview.html#how-vault-encrypt-data",
    "href": "posts/04_archives/vault/notes/0_overview.html#how-vault-encrypt-data",
    "title": "Overview",
    "section": "how vault encrypt data",
    "text": "how vault encrypt data",
    "crumbs": [
      "PARA",
      "Archives",
      "vault",
      "Notes",
      "Overview"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#etl",
    "href": "posts/04_archives/adp_필기/notes/03.html#etl",
    "title": "2 - 데이터 처리 프로세스",
    "section": "ETL",
    "text": "ETL\n\n1. ETL 개요\n\nETL(Extract, Transformation, Load): 데이터 이동 및 변환 절차\nbatch ETL, real-time ETL으로 나뉨\n\n\n\n\nETL 작업 단계\n\n\n\nInterface: 다양한 소스로부터 데이터 휙득을 위한 인터페이스(OLEDB, ODBC, FTP)\nStaging: 정기적으로 데이터 원천으로 부터 저장. 아직은 정규화 x\nProfiling: staging table의 데이터 특성을 식별하고, 품질 측정\nCleansing: profiling된 데이터를 보정\nIntegration: 데이터 충돌을 해소하고, 데이터를 통합. 아마 여기서 정규화가 이루어질듯(왜 책에 설명 똑바로 안해놓지)\nExport: 운영보고서 생성, 데이터웨어하우스 / 데이터마트에 적재하기 위한 최적화(denormalization) 진행\n\n\n\n2. ODS 구성\n\n통합된 데이터를 저정하는 중간 저장소\n실시간, 거의 실시간으로 데이터 적재\n\n\n\n3. 데이터 웨어하우스\n\nODS를 통해 정제 / 통합된 데이터를 분석 및 보고서 생성을 위해 저장\n\n특징\n\n주제중심성\n영속성/비휘발성\n통합성\n시계열성\n\n모델링 기법\n\n스타 스키마(조인 스키마)\n\n제 3정규형의 fact 테이블과 제 2정규형의 차원 테이블로 구성\n복잡성이 낮지만, 데이터 무결성이 떨어짐\n\n\n\n\n스노우플레이크 스키마\n\n스타 스키마의 차원 테이블을 제 3정규형으로 정규화한 상태\n데이터 무결성이 높지만, 복잡성이 높음\n\n\n\n\n\n\n\n\n\n제 1 정규형: 반복되는 record나 다치 attribute를 포함하지 않음 제 2 정규형: 부분 종속성(primary key의 일부가 다른 일부를 종속함)이 없음 제 3 정규형: 이행적 종속성(primary key가 아닌 attribute의 종속성)이 없음\n\n\n\n\n\n4. ODS vs DW",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#cdcchange-data-capture",
    "href": "posts/04_archives/adp_필기/notes/03.html#cdcchange-data-capture",
    "title": "2 - 데이터 처리 프로세스",
    "section": "CDC(change data capture)",
    "text": "CDC(change data capture)\n\n1. CDC 개념 및 특징\n\n데이터 변경을 감지하고, 변경된 데이터를 추출하는 기술\n하드웨어 계층부터 어플리케이션 계층까지 다양한 수준에서 적용 가능\n\n\n\n2. CDC 구현 기법\n\nTime Stamp on Rows\nVersion Numbers on Rows: 참조테이블을 같이 사용하는게 일반적이라고 한다.\nStatus on Rows: time stamp, version number 보완 용도로, 사람이 레코드 반영 여부를 직접 판단할 수 있게 적용할 수 있음\nTime/Version/Status on Rows\nTriggers on Tables: message queue로 변경 발생시 subscribe 된 대상에 publish하는 방식. 시스템 관리 복잡도가 높아짐\nEvent Programming: 어플리케이션에 데이터 변경 식별 기능을 추가\nLog Scanner on Database: 데이터 스키마 변경 불필요, 어플리케이션 영향 최소화, 지연시간 최소화\n\n\n\n3. CDC 구현 방식\n\nPush: 데이터 원천에서 변경 식별(agent)\nPull: 대상 시스템에서 원천을 주기적으로 모니터링",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#eai",
    "href": "posts/04_archives/adp_필기/notes/03.html#eai",
    "title": "2 - 데이터 처리 프로세스",
    "section": "EAI",
    "text": "EAI\n\n1. EAI의 개념 및 특징\n\n기업 내 혹은 기업 간 정보시스템을 연계하여 동기화.\nETL은 batch 처리 중심, EAI는 실시간 혹은 근접 실시간 처리 중심\n\n\n\n2. 데이터 연계 방식\n\n\nETL/CDC는 운영 데이터와 분석을 위한 데이터베이스가 구분되지만, EAI는 그냥 통합\n\n\n\n3. EAI 구성요소\n\nAdapter: 시스템 간 데이터 변환\nBroker: 데이터 전송\nBus: 데이터 전송 경로 설정\nTransformer: 데이터 형식 변환\n\n\n\n4. EAI 구현 유형\n\nMediation: Publish/Subscribe 방식\nFederaion: Request/Reply 방식\n\n\n\n5. EAI 활용 효과\n\n협력사, 파트너, 고객과의 상호 협력 프로세스 연계\n그룹 및 지주 회사 계열사들 간 상호 관련 데이터 동기화 등을 위한 데이터 표준화 기반 제공\n\n\n\n6. EAI vs ESB\n\n추가적인 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "href": "posts/04_archives/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "데이터 통합 및 연계 기법",
    "text": "데이터 통합 및 연계 기법\n\n\n빅데이터는 시각화도 하고, NoSQL 같은 환경에서도 사용한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "href": "posts/04_archives/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "대용량의 비정형 데이터 처리 방법",
    "text": "대용량의 비정형 데이터 처리 방법\n\n2. 대규모 분산 병렬 처리\n\n하둡:\n\nMapReduce와 HDFS를 기반으로 한 분산 병렬 처리 프레임워크\n비공유 분산 아키텍쳐\n선형적인 성능과 용량 확장\nMapReduce failover\n\n\n\nHadoop ecosystem\n\n\n\n3. 데이터 연동\n대규모 연산을 데이터베이스에서 처리하기 어렵기 때문에, 하둡으로 복사해와서 MapReduce 연산 후, 결과를 다시 데이터베이스에 기록하기 위해 스쿱 사용\n\nSqoop\n\nJDBC를 지원하는 RDBMS, Hbase와 Hadoop 간 데이터 전송(Import, Export)\nSQL 질의로 데이터 추출\nMapReduce 사용\n\n\n\n\n4. 데이터 질의 기술\n\nHive: SQL과 유사한 HiveQL 질의, batch 처리\nSQL on Hadoop: SQL 질의, 실시간 처리\n\napache Drill, Stinger, Shark, Tajo, Impala, HAWQ, Presto",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-과제-발굴",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-과제-발굴",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 과제 발굴",
    "text": "분석 과제 발굴\n\n풀어야 할 다양한 문제를 데이터 분석 문제로 변환 후, 프로젝트를 수행할 수 있는 과제 정의서 형태로 도출\n\n\n\n\n도출을 위한 접근 방법\n\n\n최적의 의사결정은 두 접근 방식이 상호 보완 관계에 있을 때 가능하다.\n\n\n1. 하향식 접근법\n\n사물을 why 관점에서 보는 방식\n\n\n\n문제 탐색: 문제를 해결함으로써 발생하는 가치에 중점\n\n비즈니스 모델기반\n분석 기회 발굴의 범위 확장\n외부참조 모델 기반\n분석 유즈 케이스\n\n문제 정의: 식별된 비즈니스 문제를 데이터의 문제로 변환\n해결방안 탐색: 분석 역량과, 분석 기법 및 시스템 존재 여부를 고려한다.\n타당성 검토\n\n경제적 타당성: 비용대비 편익 분석 관점의 접근\n데이터 및 기술적 타당성\n\n\n\n\n2. 상향식 접근법\n\n사물을 what 관점에서 보는 방식\n\n\n비지도 학습\n지도 학습\n\n\n프로토타이핑 접근법",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 기획 방향성 도출",
    "text": "분석 기획 방향성 도출\n\n1. 분석기획의 특징\n\n과제를 발굴, 정의하고 의도했던 결과를 도출할 수 있도록 적절하게 관리할 수 있는 방안을 사전에 계획하는 일련의 작업 (말 그대로 기획)\n\n\n\n3. 목표 시점 별 분석 기획 방안\n\n\n\n4. 분석 기획시 고려사항\n\n가용 데이터\n적절한 활용방안과 유즈케이스\n장애요소들에 대한 사전계획 수립",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-방법론",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-방법론",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 방법론",
    "text": "분석 방법론\n\n방법론은 절차, 방법, 도구와 기법, 템플릿과 산출물로 구성된다.\n\n\n\n\n방법론 절차의 구성 요소\n\n\n\n폭포수 모델\n프로토타입 모델\n나선형 모델\n\n\n1. KDD 분석 방법론\n\n비즈니스 도메인에 대한 이해, 프로젝트 목표 설정\n데이터셋 선택\n데이터 전처리: 잡음, 이상치, 결측치 처리. 추가로 요구되는 데이터 셋이 필요한 경우, 데이터 선택 프로세스로 돌아감\n데이터 변환: 데이터 차원 축소, 학습용 데이터, 시험용 데이터 분리\n데이터 마이닝\n데이터 마이닝 결과 평가\n\n\n\n2. CRISP-DM 분석 방법론\n\n\n\nCRISP-DM 4레벨 구조\n\n\nGeneric Tasks 예시: 데이터 정제\nSpecialized Tass 예시: 범주형 데이터 정제, 연속형 데이터 정제\n\n\n\nCRISP-DM 6Phase\n\n\n\n업무 이해\n데이터 이해: 데이터셋 선택, 데이터 전처리\n데이터 준비: 데이터 변환\n모델링: 모델 평가\n평가: 모델 적용성 평가\n전개\n\n\n\n3. 빅데이터 분석 방법론\n\n\n\n빅데이터 분석 방법론의 5단계\n\n\n\n분석 기획\n\n비즈니스 이해 및 범위 설정\n프로젝트 정의 및 계획 수립 → SOW\n프로젝트 위험 계획 수립 → 회피, 전이, 완화, 수용\n\n데이터 준비\n\n필요 데이터 정의\n데이터 스토어 설계\n데이터 수집 및 정합성 점검\n\n데이터 분석\n\n분석 데이터 준비\n텍스트 분석\n탐색적 분석\n모델링 → 훈련용, 테스트용 데이터 분리\n모델 평가\n\n시스템 구현\n평가 및 전개",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 프로젝트 관리 방안",
    "text": "분석 프로젝트 관리 방안\n\n1. 분석과제 관리를 위한 5가지 주요 영역\n\nData Size\nData Complexity\nSpeed: 분석 모델의 성능 및 속도를 고려해야한다.\nAnalytic Complexity: 분석 모델의 정확도를 높이면서 해석이 가능하도록 최적 모델을 찾아야 한다.\nAccurancy & Precision: 정확도, 정밀도\n\n\n\n3. 분석 프로젝트 관리방안\n\n범위\n시간\n원가\n품질\n통합\n조달\n자원\n리스크\n의사소통\n이해관계자",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 데이터 저장 기술",
    "text": "분산 데이터 저장 기술\n\n1. 분산 파일 시스템\n\nGFS(Google File System): 구글의 분산 파일 시스템\n\nchunk: 64MB\n트리 구조가 아닌, 해시 테이블 구조로 관리\nPOSIX 인터페이스 지원하지 않음\n단일 마스터 노드가 메모리상에서 메타데이터 관리\n마스터 노드에 대한 로그를 기록하고, 마스터의 상태를 섀도우 마스터 노드에 복제\n하나의 파일에 대한 primary node를 정하고, 다른 노드에 복제본 분산 저장\n낮은 응답 지연시간보다 높은 처리율 중시\nMaster Node, Chunk Node, Client 구성\n\nHDFS(Hadoop Distributed File System): 아파치 하둡의 분산 파일 시스템\n\nGFS의 clone project\nPOSIX 인터페이스 지원하지 않음\nblock: 128MB\nNameNode가 메타데이터 관리\n낮은 응답 지연시간보다 높은 처리율 중시\nNameNode, DataNode, 보조 네임 노드, job tracker, task tracker 구성\n\nLustre: 고성능 컴퓨팅을 위한 분산 파일 시스템\n\nPOSIX 인터페이스 지원\nchunk가 아닌 striping 방식 데이터 저장\nClient Filesystem, Metadata Server, 객체 저장 서버로 구성\n\n\n\n\n2. 데이터베이스 클러스터\n\n\n\n\n\n\n\n무공유 디스크\n\n각 노드가 완전히 분리된 데이터를 가짐\nOracle RAC를 제외한 대부분의 클러스터가 채택\n노드 확장에 제한이 없음\n\n공유 디스크\n\nSAN과 같은 네트워크로 모든 노드가 디스크 공유\n노드 확장시 디스크 병목현상 고려 필요\n\n\n\n\n\n\nOrace RAC 데이터베이스 서버: 확장성보다는 고가용성이 중요한 서비스에 적합\nIBM DB2 ICE(integrated cluster environment)\n마이크로소프트 SQL Server: 전역 스키마가 없어서 모든 노드에 질의를 해야함. active-stanby 구성\nMySQL:\n\n클러스터에 참여하는 노드는 최대 255, 그 중 데이터 노드는 최대 48개까지 가능\n운영중에 노드를 추가 삭제 불가\n\n\n\n\n3. NoSQL\n\nGoogle BigTable:\n\n공유 디스크 방식\nRow Key 순으로 정렬 되어 있고, Row 내부적으로는 Column Key 순으로 정렬\nColumn Key, Value, Timestamp로 구성\nChubby를 이용해 마스터 노드 관리\n\nHBase\nAmazon SimpleDB\n\nschema가 없고, Domain(table), Item(record), Attribute(column), Value으로 구성\n\n마이크로소프트 SSDS: Container(table), Entity(record), Property(column)로 구성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 컴퓨팅 기술",
    "text": "분산 컴퓨팅 기술\n\n1. MapReduce\n\n대용량 데이터를 분산 처리할 수 있는 모델\n보통 64MB를 기준으로 데이터 분할\n하나의 블록당 하나의 Map Task, 사용자가 지정한 갯수만큼의 Reduce Task 생성\nCount 작업에 적합하고, Sort 작업에는 적합하지 않음\n\n\nGoogle MapReduce\nHadoop MapReduce\n\n절차: 1. Split 1. Map 1. Combine 1. Partition 1. Shuffle 1. Sort 1. Reduce\n\n\n2. 병렬 쿼리 시스템\n\nGoogle Sawzall: MapReduce에 대한 이해가 없어도 쉽게 사용 가능\nApache Pig\nApache Hive\n\n\n\n3. SQL on Hadoop",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#클라우드-인프라-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#클라우드-인프라-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "클라우드 인프라 기술",
    "text": "클라우드 인프라 기술\n\n2. CPU 가상화\n\n하이퍼바이저: 하드웨어 리소스를 가상화하여 여러 개의 가상 머신을 생성하는 소프트웨어\n\nbare-metal hypervisor: 하드웨어와 host 운영체제 사이에 hypervisor가 존재  \nhosted hypervisor: host 운영체제와 guest 운영체제 사이에 hypervisor가 존재\n\nContainer\n\n\n\n3. 메모리 가상화\n\nVMKernnel: hypervisor 내에 Show Page Table을 두고, 각 VM의 Guest OS의 Page Table을 관리\nMemory Ballooning: Guest OS의 메모리를 빼앗아서 다른 VM에 할당\nTransparent Page Sharing: 같은 내용의 메모리 페이지는 VM들이 공유\nMemory Overcommitment: VM에 할당된 메모리보다 더 많은 메모리를 할당할 수 있음\n\n\n\n4. I/O 가상화\n\n가상 이더넷: 가상 머신 간의 네트워크 통신을 위한 가상 네트워크. LAN 세그먼트를 가상화\n공유 이더넷 어댑터: 하나의 물리적 네트워크 어댑터를 여러 VM이 공유. 병목현상 발생 가능\n가상 디스크 어댑터",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-이해",
    "href": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-이해",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 이해",
    "text": "빅데이터의 이해\n\n1. 빅데이터의 정의\n\n\n좁은 범위\n\n데이터 자체의 특성에 초점을 맞춘 정의\n3V(다양성, 속도, 규모)를 강조\n\n중간 범위\n\n데이터 자체뿐 아니라 처리, 분석 방법도 포함하는 정의\n\n넓은 관점\n\n인재, 조직 변화까지 포함한 정의\n\n\n\n∴ 기존 방식으로는 얻을 수 없는 통찰 및 가치 창출\n\n\n2. 출현 배경과 변화\n\n산업계: 고객 데이터가 축적되며 새로운 가치 활용\n학계: 거대 데이터 활용 분야가 늘어나며 통계 도구들이 발전\n기술발전: 관련기술의 발전\n\n\n\n3. 빅데이터의 기능\n\n산업혁명의 석탄, 철: 산업 전반에 혁명적 변화를 가져옴\n21세기의 원유: 생산성을 향상시키고, 기존에 없던 새로운 범주의 산업을 만들어낼 것으로 전망\n렌즈: 데이터가 산업에 영향을 미침\n플랫폼\n\n\n\n4. 빅데이터가 만들어 내는 본질적인 변화\n\n사전처리 → 사후처리\n표본조사 → 전수조사\n질 → 양\n인과관계 → 상관관계",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "href": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 가치와 영향",
    "text": "빅데이터의 가치와 영향\n\n1. 빅데이터의 가치\n빅데이터는 아래와 같은 이유로 가치 선정이 어렵다.\n\n데이터 활용방식: 데이터를 언제 어디서 누가 사용할지 미리 예측하기 어려움\n새로운 가치 창출: 기존에 없던 가치를 창출하기 때문에 가치를 예측하기 어려움\n분석 기술 발전: 현재 가치가 없더라도, 추후 기술이 발전하면 가치가 생길 수 있음\n\n\n\n2. 빅데이터의 영향\n빅데이터는 다양한 주체(기업, 정부, 개인)에 영향을 미친다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#비즈니스-모델",
    "href": "posts/04_archives/adp_필기/notes/01.html#비즈니스-모델",
    "title": "1 - 데이터의 가치와 미래",
    "section": "비즈니스 모델",
    "text": "비즈니스 모델\n\n1. 빅데이터 활용 사례\n여러가지 활용 사례가 있다.\n\n\n2. 빅데이터 활용 기본 테크닉\n\n연관규칙학습: 범주형 데이터의 변인들간의 규칙을 발견. 비지도 학습 (ex. 장바구니 분석)\n유형(군집)분석: 데이터를 분류하거나 군집화. 비지도 학습 (not 분류분석)\n유전자 알고리즘: 최적해를 찾는 알고리즘\n기계학습: 훈련한 데이터로 예측\n회귀분석: 연속형 데이터의 독립변수와 종속변수의 관계를 수학적으로 모델링해서 예측\n감정분석: 비정형 데이터 분석\n소셜네트워크분석(사회관계망분석): 비정형 데이터 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "href": "posts/04_archives/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "title": "1 - 데이터의 가치와 미래",
    "section": "위기 요인과 통제 방안",
    "text": "위기 요인과 통제 방안\n\n1. 빅데이터 시대의 위기 요인과 통제 방안\n\n사생활 침해: 동의에서 책임으로\n책임 원칙 훼손: 결과 기반 책임 원칙 고수\n데이터 오용: 알고리즘 접근 허용, 알고리즈미스트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#미래의-빅데이터",
    "href": "posts/04_archives/adp_필기/notes/01.html#미래의-빅데이터",
    "title": "1 - 데이터의 가치와 미래",
    "section": "미래의 빅데이터",
    "text": "미래의 빅데이터\n\n1. 빅데이터 활용의 3요소\n\n데이터: 모든것의 데이터화\n기술: 인공지능\n인력: 데이터 사이언티스트, 알고리즈미스트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화의-정의",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화의-정의",
    "title": "5 - 시각화 디자인",
    "section": "시각화의 정의",
    "text": "시각화의 정의\n\n1. 데이터 시각화의 중요성\n데이터 시각화의 목적은 데이터 분석과 의사소통\n\n\n2. 시각 이해와 시각화\n\n\n데이터: 디자인의 대상이 될 수 없음\n정보\n\n데이터가 의미를 전달하기위한 형태를 가짐\n자기조직화 되지 않은 일반적인 의미를 가지고 있고, 생산자와 사용자의 관점에 따라 다르게 전달될 수 있다.\n\n지식: 다른 영역의 정보가 자기조직화된 형태\n지혜: 지식이 내면화되어 개인적 맥락에 포함된 형태. 명시적으로 상대에게 전달하기 어려움\n\n\n\n\n정보 인터랙션 디자인(사진좀 보이게 올려둬라 좀..)\n\n\n\n\n3. 시각화 분류와 구분\n\n\n데이터 시각화\n정보 시각화\n정보 디자인 \n\n데이터 시각화, 정보 시각화, 인포그래픽도 정보 디자인의 범위에 속한다고 볼 수 있다.\n대표적인 예시로 나폴레옹 행군 다이어그램, 나이팅게일 폴라 지역 다이어그램이 있다.\n\n인포그래픽(뉴스 그래픽): 중요한 정보를 한 장의 그래픽으로 표현한 것. 원 데이터는 취급 안함.\n\n정보형 메세지: 객관적인 정보를 전달하는데 목적을 둠. 대표적인 예시: 워싱턴 지하철 지도\n설득형 메세지: 대충 포스터 생각하면 됨",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화-프로세스",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화-프로세스",
    "title": "5 - 시각화 디자인",
    "section": "시각화 프로세스",
    "text": "시각화 프로세스\n\n1. 정보 디자인 프로세스\n\n데이터 수집\n모든 것을 읽기\n내리티브 찾기\n문제의 정의\n계층 구조 만들기\n와이어프레임 그리기\n포맷 선택하기\n시각 접근 방법 결정하기\n정제와 테스트\n세상에 선보이기\n\n\n\n2. 빅데이터 시각화 프로세스\n\n\n\n시각화 프로세스\n\n\n\n\n\n방법론\n\n\n\n\n\n에드워드 터프티 시각 정보 디자인 7원칙",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화-방법",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화-방법",
    "title": "5 - 시각화 디자인",
    "section": "시각화 방법",
    "text": "시각화 방법\n\n2. 정보 구조화\n\n데이터 수집 및 탐색\n데이터 분류: 확장자 맞게 분류\n데이터 배열: LATCH\n\nLocation\nAlphabet\nTime\nCategory\nHierarchy: 정보의 변화에 따라 데이터의 값이나 중요도 순서로 정렬\n\n데이터 재배열(관계 맺기):\n\n\n\n3. 정보 시각화\n\n\n\n\n좋은 그래프 디자인\n\n\n\n범례 만들지 말고, 직접 그려 넣은거\n테두리, 보조선 없는거\n굵은 글씨 대신 글자를 흐리게\n색깔은 최대한 적게 사용\n\n\n\n4. 정보 시각 표현\n\n자크 베르탱의 그래픽 7요소\n\n위치: 가장 중요한거는 좌측 상단에 배치\n크기\n모양\n색\n명도\n기울기\n질감\n\n타이포그래피\n\n산세리프: 돌기가 없음. 제목에 적합\n세리프: 돌기가 있음. 본문에 적합\n\n아이소타이프",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터와-정보",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터와-정보",
    "title": "1 - 데이터 이해",
    "section": "데이터와 정보",
    "text": "데이터와 정보\n\n1. 데이터\n\n객관적 사실을 나타내는 존재적 특성과, 추론 예측 전망 추정을 위한 근거가 되는 당위적 특성을 모두 포함하는 개념\n단위: 바이트(byte), 킬로바이트(KB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB), 페타바이트(PB), 엑사바이트(EB), 제타바이트(ZB), 요타바이트(YB)\n유형:\n\n정성적 데이터: 비정형 데이터, 주관적 내용, 통계분석이 어려움\n정량적 데이터: 정형 데이터, 객관적 내용, 통계분석이 용이함\n\n지식 경영의 핵심 이슈인 암묵지와 형식지를 연결하는 역할을 함\n\n\n\n\n\n\n\n\n정형 데이터: 표 형태로 정리된 데이터\n반정형 데이터: HTML, XML, JSON 등의 형태(스키마, 메타데이터)가 있고, 연산이 불가능한 데이터\n비정형 데이터: 형태가 없고, 연산이 불가능한 데이터\n\n\n\n\n\n\n\n\n\n\n\n암묵지:\n\n학습과 경험을 통해 개인에게 체화되어 잇지만 겉으로 드러나지 않는 지식\n개인에게 축적된 내면화된 지식 → 조직의 지식으로 공통화\n\n형식지:\n\n문서나 메뉴얼처럼 형상화된 지식\n언어, 기호, 숫자로 표출화된 지식 → 개인의 지식으로 연결화\n\n\n∴ 내면화 → 공통화 → 표출화 → 연결화 → 내면화\n\n\n\n\n\n2. 데이터와 정보의 관계\n\n데이터(data): 그 자체로는 의미가 중요하지 않은 객관적인 사실\nex) A마트는 100원, B마트는 200원에 휴지를 판다.\n정보(information): 데이터를 가공하여 의미를 부여한 결과물\nex) A마트가 100원에 판 휴지는 B마트보다 100원 싸다.\n지식(knowledge): 정보를 구조화하여 유의미한 정보를 분류하고 개인적인 경험을 결합시켜 고유의 지식으로 내재화된 것\nex) 가격이 더 저렴한 A마트에 가서 휴지를 사야겠다.\n지혜(wisdom): 지식의 축적과 아이디어가 결합된 창의적인 결과물\nex) A마트의 다른 물건도 B마트보다 저렴할 것이다.\n\n\n\n\nDIKW 피라미드",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스 정의와 특징",
    "text": "데이터베이스 정의와 특징\n\n1. 데이터베이스의 정의\n기존에는 정형 데이터 관리의 의미로 사용되다가, 빅데이터의 출현으로 비정형 데이터까지 포함하는 개념으로 확장됨\n\n\n2. 데이터베이스의 일반적인 특징\n\n통합된 데이터: 동일한 내용의 데이터가 중복되어 있지 않다.\n저장된 데이터\n공용 데이터\n변화되는 데이터: 데이터베이스에는 항상 현재의 정확한 데이터를 유지한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터베이스의-활용",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터베이스의-활용",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스의 활용",
    "text": "데이터베이스의 활용\n\n1. 1980년대 기업 내부 데이터베이스\n\nOLTP(On-Line Transaction Processing)\n\n데이터베이스의 데이터를 실시간으로 갱신하는 프로세싱.\n구조가 복잡하고, 현재의 단기간 데이터.\n갱신이 동적이고, 엑세스 빈도가 높다.\n질의가 단순하고, 주기적이다.\n\nOLAP(On-Line Analytical Processing)\n\n데이터 조회, 분석 위주.\n구조가 단순하고, 과거의 장기간 요약 데이터.\n갱신이 정적이고, 엑세스 빈도가 보통이다.\n질의가 복잡하다.\n\n\n\n\n2. 2000년대 기업 내부 데이터베이스\n\nCRM(Customer Relationship Management): 고객 관리 시스템\nSCM(Supply Chain Management): 공급망 관리 시스템\n\n\n\n3. 각 분야별 내부 데이터베이스\n\n제조부문\n\nERP(Enterprise Resource Planning): 기업 내부 자료를 하나의 통합 시스템으로 재구축\nBI(Business Intelligence): 기업의 수많은 데이터를 정리, 분석해 의사결정에 활용하는 프로세스\nCRM\nRTE(Real-Time Enterprise): ERP, SCM, CRM 등의 부문별 전산화 시스템을 하나로 통합\n\n금융부문\n\nEAI(Enterprise Application Integration)\nEDW(Enterprise Data Warehouse): BPR, CRM, BSC 등의다양한 분석 시스템을 위한 원천\n\n유통부문\n\nKMS(Knowledge Management System)\nRFID(Radio Frequency Identification): 주파수를 이용해 ID를 식별\n\n\n\n\n4. 사회기반구조로서의 데이터베이스\n\nEDI(Electronic Data Interchange): 전자상거래를 위한 표준화된 데이터 포맷\nVAN(Value Added Network): EDI를 위한 통신망 (카드 결제 시, 가맹점과 카드사 사이에서 승인 요청 및 결과 전달을 중계함.)\nCALS(Commerce At Light Speed): 제품의 설계, 생산, 유통, 판매 등의 모든 과정을 통합한 경영정보시스템",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/09.html#데이터-변경-및-요약",
    "href": "posts/04_archives/adp_필기/notes/09.html#데이터-변경-및-요약",
    "title": "4 - 데이터 마트",
    "section": "데이터 변경 및 요약",
    "text": "데이터 변경 및 요약\n\n요약 변수: 전체적 특성을 대표하여 aggregate한 변수. 제활용성이 높다.\n파생 변수: 기존 데이터를 변환, 조합, 계산하여 새롭게 만든 변수. 주관이 개입될 수 있다.\n\n\n\n\n요약 변수 예시\n\n\n\n1. reshape 패키지 활용",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 데이터 마트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#데이터-마이닝-개요",
    "href": "posts/04_archives/adp_필기/notes/11.html#데이터-마이닝-개요",
    "title": "4 - 정형 데이터 마이닝",
    "section": "데이터 마이닝 개요",
    "text": "데이터 마이닝 개요\n\n데이터 마이닝 분석 방법\n\n지도 학습\n\n의사결정 나무\n인공신경망\n회귀 분석\n사례기반 추론\nk-최근접 이웃\n\n비지도 학습\n\nOLAP\n연관성 규칙\n군집 분석\nSOM\n\n\n\n\n데이터 마이닝 추진 단계\n\n목표 설정\n데이터 준비\n가공\n기법 적용\n검증\n\n\n\n데이터 분할\n\n구축용(추정용, 훈련용): 50%\n검정용: 30%\n시험용: 20%\nfold-out\nk-fold\nleave-one-out\n\n\n\n성과 분석\n\n정분류율\n오분류율\n민감도(재현율): 실제 True인데 True라고 예측한 비율\n특이도: 실제 False인데 False라고 예측한 비율\n정밀도: True라고 예측했는데 True인 비율\nF1-score: \\(\\frac{정밀도 * 재현율}{정밀도 + 재현율}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#앙상블-기법",
    "href": "posts/04_archives/adp_필기/notes/11.html#앙상블-기법",
    "title": "4 - 정형 데이터 마이닝",
    "section": "앙상블 기법",
    "text": "앙상블 기법\n\n여러 개의 분석 모델을 결합하여 하나의 모델을 구축하는 기법\n\n\n배깅\n\n여러 부트스트랩(복원 추출된 샘플)에 대해 동일한 모델을 독립적으로 학습시키고, 결과를 투표하여 최종 결과를 결정\n\n\n\n부스팅\n\n부트스트랩을 순차적으로 학습시키며, 이전 모델의 오차를 보완하는 방식\nGradient Boosting\nXGBoost\nLightGBM",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#분류-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#분류-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "분류 분석",
    "text": "분류 분석\n\n지도학습. 데이터의 범주형 속성 값을 예측\n\n\n의사결정 나무\n\n이상치에 민감하지 않다.\n대용량 데이터에 대해 적합하다.\n과적합 문제 발생 가능성\n\n\n성장\n\n분리\n\n이산형 변수\n\n카이제곱량\n지니지수: \\(1 - \\sum_{i=1}^{n} p_i^2\\). 낮춰주는 변수 선택\n엔트로피: \\(-\\sum_{i=1}^{n} p_i \\log_2 p_i\\). 낮춰주는 변수 선택\n\n범주형 변수\n\n분산\nF 통계량\n\n\n정지 기준: 의사경정 나무의 높이, 리프 노드의 최소 갯수\n\n가지치기\n타당성 평가\n예측\n\n\n\n인공신경망 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#som",
    "href": "posts/04_archives/adp_필기/notes/11.html#som",
    "title": "4 - 정형 데이터 마이닝",
    "section": "SOM",
    "text": "SOM\n\n고차원의 데이터를 이해하기 쉬운 저차원의 데이터로 변환\n구성\n\n입력층\n경쟁층",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#군집-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#군집-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "군집 분석",
    "text": "군집 분석\n\n사전정보가 없는 상태에서 관측값들의 거리 또는 유사성을 이용하여 군집을 형성하는 분석 방법\n군집분석은 알고리즘에 따라 결과가 매번 다르고, 명확한 정답이나 정답을 찾기 위한 p-value가 없다.\noutlier에 민감하다.\n\n\n계층적 군집 분석\n\n군집의 갯수를 모를 때, 우선적으로 갯수를 정하기 위해 사용\n가까운 개체끼리 차례로 묶거나 멀리 떨어진 개체를 분리해 가는 방식\n한 번 분류된 개체는 재분류되지 않음\n계층적 군집분석 단계\n\nDistance Measure 결정\n\n연속형 변수\n\n유클리디안 거리\n맨하탄 거리\n민코우스키 거리: 유클리디안 거리(L2)와 맨하탄 거리(L1)의 일반화된 공식.\n표준화 거리: 표준편차로 표준화된 길이의 유클리디안 거리\n마할라노비스 거리: 공분산으로 표준화된 길이의 유클리디안 거리\n체비셰프 거리: x 좌표 차이와 y 좌표 차이 중 최댓 값\n캔버라 거리: 두 벡터의 각 차이의 비율\n\n범주형 변수\n\n자카드 거리: \\(1 - \\frac{A \\cap B}{A \\cup B}\\)\n코사인 거리: \\(1 - \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)\n\n\nClustering Algorithm 결정\n\n합병에 의한 방법: 가장 가까운 거리를 가진 두 군집을 합침\n\n단일 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최솟값 사용\n완전 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최댓값 사용\n평균 연결법: 군집의 개체들 사이의 모든 거리 조합의 평균 사용\n와드 연결법: ESS(군집 내 제곱합)의 증가량이 최소가 되는 두 군집을 합침\n\n분할에 의한 방법\n\n다이아나 연결법\n\n\n군집의 갯수 결정: 1, 2번 단계에서 나온 dendrogram을 보고 알아서 결정\n분석의 타당성 검토\n\n\n\n\n비계층적(분할적) 군집 분석\n\n군집의 갯수를 알고 있을 때 사용\n판정기준을 최적화 시키는 방법으로 군집을 나눔\n한 번 분류된 개체도 재분류될 수 있음\nk-means\n\nk개의 군집을 사전에 설정\n군집의 초기 시작 포인트를 설정\n각 군집의 중심을 계산하여, 개체들을 다시 가장 가까운 군집에 재할당\n3 반복\n\n혼합분포군집\n\nk-means와 비슷하지만, 군집의 형태가 원형이 아닐 때도 사용 가능\n\nPAM\n\nk-means와 비슷하지만, 중심을 평균이 아닌 중앙값으로 설정\n연속형이 아닌 여러 종류의 변수가 혼합된 경우에도 사용할 수 있음\n\n\n\n\n타당성 지표\n\nsilhouette\nDunn index",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#연관-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#연관-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "연관 분석",
    "text": "연관 분석\n\n지지도: \\(\\frac{A \\cap B}{전체}\\)\n신뢰도: \\(\\frac{지지도}{A}\\)\n향상도: \\(\\frac{신뢰도}{B}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#what-is-airflow",
    "href": "posts/02_areas/air_flow/notes/00.html#what-is-airflow",
    "title": "Getting Started",
    "section": "What is Airflow",
    "text": "What is Airflow\n\nopen source platform to pragramatically author, schedule and monitor workflows\nNot a data processing framework\nNot a Real time streaming solution (only for batch processing)\nNot a data storage system\nand simple linear workflow might overkill",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#why-airflow",
    "href": "posts/02_areas/air_flow/notes/00.html#why-airflow",
    "title": "Getting Started",
    "section": "Why Airflow",
    "text": "Why Airflow\n\nautomation\nvisibility\nflexibility and scalability\nextensibility",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#core-components",
    "href": "posts/02_areas/air_flow/notes/00.html#core-components",
    "title": "Getting Started",
    "section": "Core Components",
    "text": "Core Components\n\nWebserver: provides UI\nScheduler: triggers tasks. ensure that task runs in correct time and order\nmeta database: memmory, communication between components\ntrigger: daemon that listens to external events and triggers tasks\nexecuter: traffic controller that decide how tasks are executed (sequential or parallel, local or remote)\nqueue\nworker",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#core-concepts",
    "href": "posts/02_areas/air_flow/notes/00.html#core-concepts",
    "title": "Getting Started",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nDAG\n\nDirected Acyclic Graph\ncollection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies\nno cycles in dependencies graph\n\n\n\nOperator\n\ndefines a single task in a workflow\ne.g. BashOperator, PythonOperator, EmailOperator, etc.\n\n\n\nTask / Task Instance\n\nspecific instance of an operator\nwhen operator assigned to a DAG, it becomes a task\n\n\n\nWorkflow\n\nentire process defined by DAG\nDAG = workflow",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#arcitecture",
    "href": "posts/02_areas/air_flow/notes/00.html#arcitecture",
    "title": "Getting Started",
    "section": "Arcitecture",
    "text": "Arcitecture",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html",
    "href": "posts/02_areas/machine_learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#details",
    "href": "posts/02_areas/machine_learning/index.html#details",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#tasks",
    "href": "posts/02_areas/machine_learning/index.html#tasks",
    "title": "Machine Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#참고-자료",
    "href": "posts/02_areas/machine_learning/index.html#참고-자료",
    "title": "Machine Learning",
    "section": "참고 자료",
    "text": "참고 자료\n\n이 책\nudemy machine learning 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#related-posts",
    "href": "posts/02_areas/machine_learning/index.html#related-posts",
    "title": "Machine Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/17.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/17.html#preprocessing",
    "title": "Eclat",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/17.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/17.html#modeling",
    "title": "Eclat",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'escalope', 'mushroom cream sauce'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'herb & pepper', 'ground beef'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'tomato sauce', 'ground beef'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'olive oil', 'light cream'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'olive oil', 'whole wheat pasta'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'pasta', 'shrimp'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    return list(zip(lhs, rhs, supports))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n2\npasta\nescalope\n0.005866\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n6\nlight cream\nolive oil\n0.003200\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n8\npasta\nshrimp\n0.005066\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Support')\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n2\npasta\nescalope\n0.005866\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n8\npasta\nshrimp\n0.005066\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n6\nlight cream\nolive oil\n0.003200",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#load-library-and-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#load-library-and-data",
    "title": "data preprocessing",
    "section": "Load Library and data",
    "text": "Load Library and data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/00-data.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nx\n\narray([['France', 44.0, 72000.0],\n       ['Spain', 27.0, 48000.0],\n       ['Germany', 30.0, 54000.0],\n       ['Spain', 38.0, 61000.0],\n       ['Germany', 40.0, nan],\n       ['France', 35.0, 58000.0],\n       ['Spain', nan, 52000.0],\n       ['France', 48.0, 79000.0],\n       ['Germany', 50.0, 83000.0],\n       ['France', 37.0, 67000.0]], dtype=object)\n\n\n\ny\n\narray(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n      dtype=object)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#taking-care-of-missing-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#taking-care-of-missing-data",
    "title": "data preprocessing",
    "section": "Taking care of Missing data",
    "text": "Taking care of Missing data\n\ndelete\nreplace\n\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x[:, 1:3])\nx[:, 1:3] = imputer.transform(x[:, 1:3])\nprint(x)\n\n[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#encoding-cagegorical-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#encoding-cagegorical-data",
    "title": "data preprocessing",
    "section": "Encoding Cagegorical data",
    "text": "Encoding Cagegorical data\n\n단순히 categorical 변수를 1, 2, 3으로 변형하면 순서가 고려된 것으로 간주될 수 있다.\n그래서 [0, 0, 1], [1, 0, 1] 이런 식으로 one hot encoding을 진행한다.\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x)\n\n[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)\n\n[0 1 0 0 1 1 0 1 0 1]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "href": "posts/02_areas/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "title": "data preprocessing",
    "section": "Split dataset into training set and test set",
    "text": "Split dataset into training set and test set\n\nfeature scaling 이전에 진행되어야함. (test set은 모델이 모르는 정보가 되야하기 때문)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#feature-scaling",
    "href": "posts/02_areas/machine_learning/notes/01.html#feature-scaling",
    "title": "data preprocessing",
    "section": "feature scaling",
    "text": "feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n[[0.0 1.0 0.0 -1.2231822690784795 -1.074632541818236]\n [1.0 0.0 0.0 0.628120624661922 0.5410562913562817]\n [0.0 1.0 0.0 1.4215361505506654 1.5284216894073759]\n [0.0 1.0 0.0 0.09917694073609294 -0.19697441021726317]\n [1.0 0.0 0.0 -0.2975308222082788 0.09225383769669344]\n [0.0 0.0 1.0 -0.16529490122682156 -0.4463091066948125]\n [0.0 0.0 1.0 -1.6198900320228513 -1.6131954862097422]\n [1.0 0.0 0.0 1.157064308587751 1.1693797264797055]]\n\n\n\nprint(X_test)\n\n[[0.0 0.0 1.0 -0.06244474046346582 -1.2541535232820715]\n [1.0 0.0 0.0 -0.5620026641711934 -0.7155905788905655]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "href": "posts/02_areas/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "title": "Naive Bayes",
    "section": "독립성 가정이 필요한 수학적 이유",
    "text": "독립성 가정이 필요한 수학적 이유\nNaive Bayes는 베이즈 정리를 기반으로 합니다. 클래스 \\(C\\)와 특성 벡터 \\(X = (x_1, x_2, ..., x_n)\\)이 있을 때, 베이즈 정리는 다음과 같습니다:\n\\[P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\\]\n여기서 \\(P(C|X)\\)는 특성 \\(X\\)가 주어졌을 때 클래스 \\(C\\)일 확률입니다. 문제는 \\(P(X|C)\\)를 계산하기가 어렵다는 점입니다. 특성이 많을수록 가능한 \\(X\\) 조합의 수가 기하급수적으로 증가하기 때문입니다.\n이 문제를 해결하기 위해 Naive Bayes는 모든 특성이 서로 조건부 독립이라고 가정합니다. 즉:\n\\[P(x_i|C, x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i|C)\\]\n이 독립성 가정을 통해 \\(P(X|C)\\)를 다음과 같이 단순화할 수 있습니다:\n\\[P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C) = \\prod_{i=1}^{n} P(x_i|C)\\]\n이렇게 특성 간 독립성을 가정함으로써 복잡한 결합 확률을 개별 특성의 확률들의 곱으로 계산할 수 있게 되어 계산이 매우 단순해집니다. 이것이 바로 Naive Bayes에서 “naive(순진한)” 독립성 가정이 반드시 필요한 이유입니다.\n이제 Naive Bayes에서 독립성 가정이 필요한 이유가 수학적으로 명확하게 설명되었습니다. 이 설명을 통해 알 수 있듯이:\n\nNaive Bayes는 베이즈 정리를 사용하여 P(C|X)를 계산합니다.\n문제는 P(X|C)를 계산하는 것이 복잡하다는 점입니다.\n독립성 가정을 통해 P(X|C)를 개별 특성들의 조건부 확률 곱으로 단순화할 수 있습니다.\n이 단순화가 없다면, 특성의 조합이 많아질수록 계산이 기하급수적으로 복잡해집니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/11.html#preprocessing",
    "title": "Naive Bayes",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/11.html#modeling---linear",
    "title": "Naive Bayes",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GaussianNB?Documentation for GaussianNBiFittedGaussianNB()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#predict",
    "href": "posts/02_areas/machine_learning/notes/11.html#predict",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[61  4]\n [10 25]]\n\n\n0.86",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/11.html#predict-1",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[61  4]\n [10 25]]\n\n\n0.86",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/18.html",
    "href": "posts/02_areas/machine_learning/notes/18.html",
    "title": "Upper Confidence Bound",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Upper Confidence Bound"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/04.html#preprocessing",
    "title": "Polynorminal Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#linear-regression-model",
    "href": "posts/02_areas/machine_learning/notes/04.html#linear-regression-model",
    "title": "Polynorminal Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(x, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#polynorminal-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#polynorminal-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Polynorminal Linear Regression",
    "text": "Polynorminal Linear Regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=4)\nx_poly = poly.fit_transform(x)\nregressor2 = LinearRegression()\nregressor2.fit(x_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#visualize-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#visualize-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Linear Regression",
    "text": "Visualize Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor.predict(x), color='blue')\nplt.title('Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Poly Linear Regression",
    "text": "Visualize Poly Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor2.predict(x_poly), color='blue')\nplt.title('Poly Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/13.html#preprocessing",
    "title": "Random Forest",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/13.html#modeling---linear",
    "title": "Random Forest",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(criterion='entropy', n_estimators=10)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#predict",
    "href": "posts/02_areas/machine_learning/notes/13.html#predict",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[54  4]\n [ 6 36]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/13.html#predict-1",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[54  4]\n [ 6 36]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/08.html#preprocessing",
    "title": "Logistic Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/08.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(x_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#predict",
    "href": "posts/02_areas/machine_learning/notes/08.html#predict",
    "title": "Logistic Regression",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[52  9]\n [12 27]]\n\n\n0.79",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/02.html#preprocessing",
    "title": "Simple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/02-data.csv')\n\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#train",
    "href": "posts/02_areas/machine_learning/notes/02.html#train",
    "title": "Simple Linear Regression",
    "section": "train",
    "text": "train\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#predict",
    "href": "posts/02_areas/machine_learning/notes/02.html#predict",
    "title": "Simple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/02.html#visualize",
    "title": "Simple Linear Regression",
    "section": "visualize",
    "text": "visualize\n\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(X_test, y_test, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#evaluate",
    "href": "posts/02_areas/machine_learning/notes/02.html#evaluate",
    "title": "Simple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9261621443754907",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#k-means-algorithm",
    "href": "posts/02_areas/machine_learning/notes/14.html#k-means-algorithm",
    "title": "k-means clustering",
    "section": "K-means++ algorithm",
    "text": "K-means++ algorithm\n\n시작점을 잘 선택하여 수렴 속도를 높이는 알고리즘\n초기 중심점을 선택할 때, 멀리 떨어진 중심점을 선택하도록 함\n\n첫 번째 중심점을 랜덤하게 선택\n나머지 중심점을 선택할 때, 각 데이터 포인트와 가장 먼 중심점을 선택\nk개의 중심점을 선택할 때까지 반복",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/14.html#preprocessing",
    "title": "k-means clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/14.html#modeling",
    "title": "k-means clustering",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n  cluster = KMeans(n_clusters=i, init='k-means++')\n  cluster.fit(x)\n  wcss.append(cluster.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Cluster')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster = KMeans(n_clusters=5, init='k-means++')\ny_kmeans = cluster.fit_predict(x)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/14.html#visualize",
    "title": "k-means clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], c='cyan', label='Cluster 5')\nplt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], s=100, c='black', label='Centroids')\nplt.legend()\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html",
    "href": "posts/02_areas/선형대수/index.html",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#details",
    "href": "posts/02_areas/선형대수/index.html#details",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#tasks",
    "href": "posts/02_areas/선형대수/index.html#tasks",
    "title": "선형대수",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#참고-자료",
    "href": "posts/02_areas/선형대수/index.html#참고-자료",
    "title": "선형대수",
    "section": "참고 자료",
    "text": "참고 자료\n\nKhan Academy 강의\n3Blue1Brown 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#related-posts",
    "href": "posts/02_areas/선형대수/index.html#related-posts",
    "title": "선형대수",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/06.html#linear-independence",
    "href": "posts/02_areas/선형대수/notes/06.html#linear-independence",
    "title": "linear independence",
    "section": "Linear independence",
    "text": "Linear independence\n\nDefinition\n\nDependence: one of the vectors in the set can be written as a linear combination of the others.\nIndependence: ⫬ dependence\n\n\n\nTheorem\nS = \\({v_1, v_2, ..., v_n}\\)\n\\(S\\) is linearly dependent ⟺ ∃(\\(c_i\\) is not 0) \\(c_1v_1 + c_2v_2 + ... + c_nv_n = 0\\) is \\(c_1 = c_2 = ... = c_n = 0\\).\n\nif \\(c_1 = c_2 = ... = c_n = 0\\), then \\(S\\) is linearly independent.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear independence"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/05.html#선형결합",
    "href": "posts/02_areas/선형대수/notes/05.html#선형결합",
    "title": "선형결합과 생성",
    "section": "선형결합",
    "text": "선형결합\n벡터들의 상수배 합으로 만들 수 있는 벡터의 집합",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "선형결합과 생성"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#what-is-vector",
    "href": "posts/02_areas/선형대수/notes/04.html#what-is-vector",
    "title": "벡터와 공간",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있고, 2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\n\nvector의 수학적 표현\nvector는 ordered list인 tuple 형태로 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\ndomain과 dimension에 따라 vector는 다음과 같이 표현할 수 있다.\n\\[\n\\vec{v} ∈ R^2\n\\]\n\n1차원: \\(R^1\\)\n2차원: \\(R^2\\)\n3차원: \\(R^3\\)\nn차원: \\(R^n\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-합",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-합",
    "title": "벡터와 공간",
    "section": "vector의 합",
    "text": "vector의 합\nvector의 합은 각 성분별로 더한 결과를 반환한다.\n\n기하학적 의미\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-scalar-곱",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-scalar-곱",
    "title": "벡터와 공간",
    "section": "vector의 scalar 곱",
    "text": "vector의 scalar 곱\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-차",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-차",
    "title": "벡터와 공간",
    "section": "vector의 차",
    "text": "vector의 차\nvector의 차는 각 성분별로 뺀 결과를 반환한다.\n기하학적으로는 두 벡터의 끝점을 연결한 벡터가 된다.\n\\(\\vec{x} - \\vec{y}\\)는 y에서 x를 연결한 벡터가 된다.\n\\(\\vec{y} - \\vec{x}\\)는 x에서 y를 연결한 벡터가 된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#단위-벡터",
    "href": "posts/02_areas/선형대수/notes/04.html#단위-벡터",
    "title": "벡터와 공간",
    "section": "단위 벡터",
    "text": "단위 벡터\n\\[\n\\vec{v} = \\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\n위의 벡터를 단위 벡터의 합으로 만들면 다음과 같다.\n\\[\n\\hat{i} = \\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix},\n\\hat{j} = \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\\[\n\\vec{v} = 3\\hat{i} + 4\\hat{j}\n\\]\n\n\n\n\n\n\nScalar 배를 한 기저 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#what-is-linear-algebra",
    "href": "posts/02_areas/선형대수/notes/00.html#what-is-linear-algebra",
    "title": "what is linear algebra",
    "section": "what is linear algebra",
    "text": "what is linear algebra\n선형 방정식을 matrix와 vector로 표현해서 다루는 수학\n\\(ax^2 + bx + c = 0\\) (x)\n\\(ax_1 + bx_2 + c = 0\\) (0)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#what-is-vector",
    "href": "posts/02_areas/선형대수/notes/00.html#what-is-vector",
    "title": "what is linear algebra",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있다.\n2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\nvector는 수학적으로, 아래와 같이 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#example",
    "href": "posts/02_areas/선형대수/notes/00.html#example",
    "title": "what is linear algebra",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 연립 1차 방정식을 matrix와 vector로 표현해보자\n\\[\n\\underset{A}{\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}}\n\\underset{x}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} =\n\\begin{bmatrix}\n1x + 2y \\\\\n2x + 5y\n\\end{bmatrix} =\n\\underset{b}{\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "href": "posts/02_areas/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "title": "2-기초(2)",
    "section": "행렬의 곱을 바라보는 관점",
    "text": "행렬의 곱을 바라보는 관점\n\n내적으로 바라보기\n\n\\[\nA = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\quad (a_x = \\text{column vector})\n\\]\n\\[\nAB = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & b_3\n\\end{bmatrix} =\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3 \\\\\na_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\\na_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3\n\\end{bmatrix}\n\\]\n\nrank-1 matrix의 합\n\n\\[\nAB = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\nb_3^T\n\\end{bmatrix} =\na_1^Tb_1 + a_2^Tb_2 + a_3^Tb_3\n\\]\n\nColumn space로 바라보기\n\n\\[\nAx = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} = a_1x_1 + a_2x_2 + a_3x_3\n\\]\n\nRow space로 바라보기\n\n\\[\nx^TA = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix} = x_1a_1^T + x_2a_2^T + x_3a_3^T\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#span과-column-space",
    "href": "posts/02_areas/선형대수/notes/02.html#span과-column-space",
    "title": "2-기초(2)",
    "section": "span과 column space",
    "text": "span과 column space\n\ncolumn space: column vector들이 span하는 영역\nspan: linear combination으로 만들어지는 모든 벡터들의 집합\nlinear combination: vector들을 scalar 배 하고 더한 것\nlinear independent: span하는 vector들이 서로 독립적인 경우\n수학적 정의: \\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\) 일 때 \\(a_1 = a_2 = \\cdots = a_n = 0\\) 인 경우\nbasis: 어떤 공간을 이루는 필수적인 구성요소 (linear independent, span)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#항등행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#항등행렬",
    "title": "2-기초(2)",
    "section": "항등행렬",
    "text": "항등행렬\n\\(AI = IA = A\\)를 만족하는 행렬 \\(I\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#역행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#역행렬",
    "title": "2-기초(2)",
    "section": "역행렬",
    "text": "역행렬\n\\(Ax = b\\)를 만족하는 \\(x\\)를 찾는 것은 \\(A^{-1}Ax = A^{-1}b\\)를 만족하는 \\(x\\)를 찾는 것과 같다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#대각-행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#대각-행렬",
    "title": "2-기초(2)",
    "section": "대각 행렬",
    "text": "대각 행렬\ndiagonal을 제외한 모든 요소가 0인 행렬 (square, rectangular 모두 가능)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#orthogonal-행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#orthogonal-행렬",
    "title": "2-기초(2)",
    "section": "Orthogonal 행렬",
    "text": "Orthogonal 행렬\n행렬의 모든 column들이 orthonormal vector인 경우\n\\(Q^{-1} = Q^T\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#행렬의-rank",
    "href": "posts/02_areas/선형대수/notes/02.html#행렬의-rank",
    "title": "2-기초(2)",
    "section": "행렬의 rank",
    "text": "행렬의 rank\nrank: 행렬이 가지는 independent한 column의 개수 → column space의 차원\nrank(A) = rank(A^T)\n\nfull-column rank: 해가 없거나 한 개 존재\nfull-row rank: 해가 무한하다\nfull rank: 해가 한 개 있다.\nrank-deficient: b가 column space에 속하지 않는 경우 해가 없고, 그렇지 않으면 해가 무한하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#null-space",
    "href": "posts/02_areas/선형대수/notes/02.html#null-space",
    "title": "2-기초(2)",
    "section": "Null space",
    "text": "Null space\n\\(Ax = 0\\)을 만족하는 모든 \\(x\\)의 집합\nA가 m x n 행렬이라면, dim(N(A)) = n - rank(A)\nnull space와 row space는 orthogonal하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html",
    "href": "posts/02_areas/kaggle/index.html",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#details",
    "href": "posts/02_areas/kaggle/index.html#details",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#tasks",
    "href": "posts/02_areas/kaggle/index.html#tasks",
    "title": "Kaggle",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#참고-자료",
    "href": "posts/02_areas/kaggle/index.html#참고-자료",
    "title": "Kaggle",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#related-posts",
    "href": "posts/02_areas/kaggle/index.html#related-posts",
    "title": "Kaggle",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html",
    "href": "posts/02_areas/42_seoul/index.html",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#details",
    "href": "posts/02_areas/42_seoul/index.html#details",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#tasks",
    "href": "posts/02_areas/42_seoul/index.html#tasks",
    "title": "42 Seoul",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#related-posts",
    "href": "posts/02_areas/42_seoul/index.html#related-posts",
    "title": "42 Seoul",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#intro",
    "href": "posts/02_areas/42_seoul/notes/01.html#intro",
    "title": "ft_transcendence - github action",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul 공통과정 6서클 과제\n\n\n42 Seoul 공통과정의 마지막 과제입니다. 이 프로젝트는 개발자가 선호하는 라이브러리와 프레임워크를 자유롭게 선택하여 구현할 수 있다는 점이 특징입니다.\n대형 협업 과제인 만큼, 과제에 명시되어있지 않지만 협업을 위한 툴도 공부해서 다양하게 적용해볼 수 있는 좋은 과제인것 같습니다. 저같은 경우에는 coursera, udemy 강의를 통해 agile 협업 방식과 github에서의 적용 방법에 대해 공부를 했고, 프로젝트 진행에 있어서 꽤 도움이 됐던걸로 기억합니다. 사실 프로젝트를 진행하다보니, agile 방식을 온전히 다 적용하기엔 적합하지 않다고 판단했지만, Kanban Board로 프로젝트를 관리하는 것 같은 부분은 꽤 유용하게 활용할 수 있었습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "title": "ft_transcendence - github action",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n해당 과제는 실시간 Pong 게임 매칭 웹사이트를 만드는게 목표입니다. 저는 이번 프로젝트에서 github action 설정, User Management Backend 설계와 42 API를 이용한 OAuth 인증, JWT 구현, Game History를 Block Chain으로 저장하는 파트를 담당했습니다.\n참고한 자료는 다음과 같습니다:\n\nGoogle Agile Project 관리\nGithub Action Docs\nGithub CLI Docs\nDjango udemy 강좌\nDjango Rest Framework Docs\nDjango Simple JWT\nJWT Token 탈취 대응 시나리오\nmicro service에서 JWT 활용 방법\nRefresh Token을 사용해야 하는 이유\nCookie에서의 same site 옵션\nBitcoin 백서\nSolidity Udemy 강의\nSolidity Docs\nnomad coder 블록체인 시리즈\n블록체인 강의\n\n\n\n\n\n\n\n이 포스팅에서는 github action setting, jwt, block chain 부분만 다루겠습니다.\n전체 코드는 비공개 되어있는 상태입니다.\n\n\n\n\n\nGithub Action Setting\ngithub를 이용해서 agile 방법론을 적용할 수 있도록 의도했고, 자동화와 template을 이용해 통일성 있는 구조를 유지하려고 했습니다.\n1. 회의를 통해 진행해야 하는 작업을 Kanban board에 정리한다.\n\n\n\nGithub Kanban Board\n\n\n각각의 column에는 다음과 같은 내용이 들어갑니다.\n\nDiscussion: 논의가 필요한 작업. 개개인이 자유롭게 올릴 수 있습니다\nBacklog: Discussion에 있는 내용 중 구현하기로 회의에서 정한 작업\nReady: Back log에 있는 작업 중 이번 Sprint에서 구현할 작업들\nIn Progress: Ready에 있는 작업 중 누군가가 작업중인 것\nDone: master branch에 merge가 완료된 작업\n\n자세한 내용은 meeting 부분을 참고해 주세요.\n참고로 Disccusion에 작업을 올리는 방법은 template에 맞게 issue를 올리면 됩니다.\n\n\n\nDiscussion template\n\n\n아래와 같이 설정 파일을 만들어서 ‘.github/ISSUE_TEMPLATE/’ 폴더 안에 저장하면 issue create 시 자동으로 template이 뜨게 할 수 있습니다.\nname: New discussion\ndescription: new discussion\ntitle: \"[DISCUSSION]\"\nlabels: [\"enhancement\"]\nprojects: [\"org_name/5\"]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        해당 기능과 관련된 request가 이미 존재하는지 확인해주세요.\n  - type: textarea\n    id: story\n    attributes:\n      label: Story\n      description: 해당 기능에 대한 설명이나 필요한 배경을 작성해주세요.\n      placeholder: 자유로운 양식으로 작성해주세요.\n    validations:\n      required: true\n2. Kanban board를 보고 개인이 능동적으로 고유 브랜치에 작업을 진행한다.\n\n\n\n빨간 밑줄 부분을 설정해줍니다.\n\n\nKanban board의 Ready section에 있는 작업을 클릭해서 들어간 후, assignees를 본인으로 선택해서 작업하면 됩니다. task completion criteria라는 내용이 보이는데, 이는 회의를 통해 결정하는 것으로, 나중에 작업이 완료되고 pull request 시, 평가자가 작업에 완성도에 대해 판단할 수 있는 기준으로 제공됩니다.\n자동화 코드는 아래와 같이 구현했습니다.\nname: Create branch\non:\n  issues:\n    types: [ assigned ]\n  pull_request:\n    types: [ opened, closed ]\njobs:\n  create_issue_branch_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n\n      # gh 명령어를 이용해 project의 상태를 In progress로 수정해줍니다.\n      - name: Project in-progress\n        if: github.event.action == 'assigned'\n        run: |\n          PROJECT_ID=$(gh project view 5 --owner organization-for-practice --format=json --jq '.id')\n          ITEM_ID=$(gh project item-list 5 --owner organization-for-practice --format=json --jq \".items[] | select(.content.number == ${NUMBER}) | .id\")\n          FIELD_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].id')\n          SINGLE_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].options[] | select(.name == \"In progress\") | .id')\n          gh project item-edit --id ${ITEM_ID} --field-id ${FIELD_ID} --single-select-option-id ${SINGLE_ID} --project-id ${PROJECT_ID}\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUMBER: ${{ github.event.issue.number }}\n\n      # assign한 작업에 대한 branch를 새로 만들어줍니다.\n      - name: Create Issue Branch\n        uses: robvanderleek/create-issue-branch@main\n        env:\n          GITHUB_TOKEN: ${{ steps.generate_token.outputs.token }}\n위의 코드는 assign한 작업을 Ready column에서 In progress column으로 옮겨주고, 자동으로 작업할 branch를 만들어줍니다.\nbranch 자동 생성은 이 workflow를 사용하였고, 적용 시 아래와 같이 브랜치가 생성됩니다.\nautoLinkIssue: true\nautoCloseIssue: true\nbranchName: tiny\ncommentMessage: |\n  \\\"${branchName}\\\" branch 생성 완료.\n  해당 branch를 통해서 main에 pull request 올려주세요.\nbranches:\n  - label: 'task list'\n    prefix: feature/${issue.title[12,27],}/\n    copyIssueAssigneeToPR: true\n  - label: 'bug'\n    prefix: hot_fix/${issue.title[6,21],}/\n    copyIssueAssigneeToPR: true\n  - label: '*'\n    skip: true\n위의 config 파일을 작성해주면 아래와 같이 브랜치가 생성됩니다.\n\n\n\n자동 생성된 branch\n\n\n이름도 자동으로 생성되게 해서 convention을 지켜야 한다는 부담을 줄여줬습니다.\n3. 작업이 완료되면, 모든 조건을 충족하는지 확인한 후, master에 merge 한다.\n\n\n\npull request 화면\n\n\n작업이 완료됬다고 판단되면 위 화면과 같이 pull request를 생성하고, Reviewer를 설정해주면 됩니다.\n\n\n\ntask completion criteria\n\n\n그러면 이전에 설정했던 기준들이 자동으로 불러와지고, 모든 항목에 체크가 완료되어야 merge를 할 수 있게 설정했습니다. 구현 코드는 아래와 같습니다.\nname: Master merge rutine\non:\n  pull_request_target:\n    types: [ opened, synchronize ]\n    branches:\n      - master\nenv:\n  PR_NUM: ${{ github.event.pull_request.number }}\n  GH_REPO: ${{ github.repository }}\njobs:\n  get_checklist:\n    runs-on: ubuntu-latest\n    if: github.event.action == 'opened'\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n      - name: Get issue\n        id: issue_num\n        env:\n          BRANCH: ${{ github.event.pull_request.head.ref }}\n        run: |\n          echo $BRANCH | grep -o 'feature\\/.*\\/i[0-9]\\+' || echo $BRANCH | grep -o 'hot_fix\\/.*\\/i[0-9]\\+'\n          TMP=$(echo $BRANCH | grep -o 'i[0-9]\\+')\n          echo \"NUMBER=${TMP#i}\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Get issue body\n        id: issue_body\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUM: ${{ steps.issue_num.outputs.number }}\n        run: |\n          echo \"CONTENTS&lt;&lt;EOF\" &gt;&gt; $GITHUB_OUTPUT\n          gh issue view ${NUM} --json body --jq '.body' &gt;&gt; $GITHUB_OUTPUT\n          echo \"EOF\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Update checklist\n        run: |\n          gh pr comment $PR_NUM --body \"${BODY}\"\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          BODY: \"${{ steps.issue_body.outputs.contents }}\"\nmerge가 완료된 branch는 자동으로 삭제가 되도록 설정을 해주었습니다.\n이제 아래는 실제 프로젝트를 진행할 때 만들었던 rule들입니다.\n\n1. work flow\ngithub flow로 진행됩니다.\n\n\n\ngithub flow\n\n\n\n매 작업은 master branch의 HEAD를 기반으로 이루어집니다.\npr을 올리지 않는 개인 작업용 local branch는 자유롭게 생성해주세요.\nmaster에 직접적인 push는 관리자를 제외하고는 불가능합니다.\nmaster에 대한 merge는 squash merge로 진행됩니다.\n그 외의 merge는 rebase로 진행해주세요.\n\n\n\n2. work\n\nkanban board의 'Ready' 섹션에서 하나를 정해서 새로운 기능에 대한 작업을 진행해주세요.\n선택한 작업은 assignees에 자신의 팀원을 등록 후, Start Date를 해당 날짜로 설정해주세요.\nassignees 등록이 완료되면 자동으로 target branch가 생성됩니다.\n해당 branch에 팀원들이 필요한 기능들을 자유로운 방식으로 구현한 후, master branch에 merge 해주세요.\n단, 해당 branch에 대한 merge는 rebase로 진행해주세요.\nhot_fix issue나, new feature request issue는 discussion의 필요성이 있을 경우에 등록해주세요.\n작업 중, 현재 작업하는 범위 외에서 추가적인 기능이 필요할 경우 관련 issue에 comment를 남기거나, reopen 해주세요.\n\n\n\n3. commit message convention\n아래의 명령어를 입력해주세요\ngit config commit.template .github/COMMIT_MESSAGE_TEMPLATE\n이후, -m 옵션 없이 ’git commit’으로 message를 입력해주세요.\n\n\nCOMMIT_MESSAGE_TEMPLATE\n\n# commit message template\n# ▼ &lt;Title&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;body&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;footer&gt; 작성\n\n\n# About Convention\n#   &lt;Title&gt;\n#       - 필수로 입력해주세요\n#       - 형식: &lt;type&gt;: &lt;short summary&gt;\n#\n#       &lt;type&gt;\n#           - config: 설정 관련 파일 작성 또는 변경\n#           - docs: 문서 변경사항\n#           - feat: 새로운 기능\n#           - fix: 버그 수정\n#           - refactor: 기능 추가나 버그 수정이 아닌 변경 사항\n#           - remove: 코드나 파일 제거\n#           - style: 스타일 작성 또는 수정\n#           - test: 누락된 테스트 추가 또는 기존 테스트 수정\n#           - core: 기능 구현 외 시스템 관련 작업\n#\n#       &lt;short summary&gt;\n#           - 변경 사항에 대한 간단한 설명\n#           - 첫글자 소문자, 현재 시제, 명령문으로 마지막에 .(마침표) 없이 작성\n#\n#   &lt;body&gt;\n#       - 선택적으로 입력 해주세요\n#       - 현재 시제, 명령문으로 작성\n#       - 변경 사항의 동기(왜)를 설명\n#       - 변경 효과를 설명하기 위해 이전 동작과 현재 동작의 비교를 포함할 수 있음\n#\n#   &lt;footer&gt;\n#       - 선택적으로 입력 해주세요\n#       - 해당 commit과 관련된 task의 issue 번호들을 적어주세요\n#       - 'bug'나 'task list' label이 붙은 issue는 제외해주세요\n#       - ex) closes #&lt;issue 번호&gt; closes #&lt;issue 번호&gt; ...\n\n\n\n\n\n\n\ncommit message template은 이 사이트를 참고해서 만들었습니다.\n\n\n\n\n\n4. pull request\n\npull request는 500줄의 코드를 넘어가지 않게 작성 바랍니다.\n모든 check list를 통과한 request만 master에 merge 가능합니다.\nreviewers에는 해당 작업과 관련된 domain의 팀원을 선택해주세요. 최소 1명 이상의 동료에게 평가를 받은 request만 merge 가능합니다.\n\n\n\n5. meeting\n\ndaily meeting\n\n매일 정해진 시간에 팀원들은 각각 다음과 같은 사안에 대해 논의합니다.\n\n개인이 어제 작업한 내용\n개인이 오늘 작업할 내용\n개인이 현재 도움이 필요한 내용\n\n이후, 새로운 내용이 추가된 ('Disccusion' 섹션에 있는) issue 중 다음과 같은 내용에 대해 논의합니다.\n\n해당 issue가 유효한가\n추가적으로 필요하거나 필요 없는 내용\n해당 issue의 priority (매우 급함 / 급함 / 안 급함)\n해당 issue의 estimate (작업하는데 필요한 노력의 정량적인 수치)\n\n추가적으로, project의 'Back log' 항목에서 'Ready' 항목으로 추가해야 할 작업에 대해 논의하거나 'Ready' 항목에서 'Back log' 항목으로 제외할 작업에 대해 논의할 수 있습니다.\n\nsprint planning / retrospective\n\n2주에 한번 진행.\n이전 sprint에 대한 평가와 이후 sprint를 위한 계획을 세웁니다.\n\nplanning\n\nProject의 'Back log' 항목 중 본격적으로 작업을 진행할 항목을 정합니다.\ndaily meeting 시간을 조정할 수 있습니다.\n\nretrospective\n\n이전 sprint의 문제점에 대해 서로 의논해봅니다.\n\n\n\n\n\n\n\n\n\n프로젝트를 하다보니, 생각보다 진행 속도가 빨라서 2주에 한번 진행하는 sprint는 유명무실해져버렸습니다. 실제로는 daily meeting만 진행을 했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#outro",
    "href": "posts/02_areas/42_seoul/notes/01.html#outro",
    "title": "ft_transcendence - github action",
    "section": "outro",
    "text": "outro\n내용이 너무 길어져서 2편에 계속 포스팅 하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#intro",
    "href": "posts/02_areas/42_seoul/notes/09.html#intro",
    "title": "cloud-1 코드 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n개념 설명에 이어서 진행하도록 하겠습니다.\n\n\n\n\n\n\n전체 코드는 github repo에서 확인하실 수 있습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "title": "cloud-1 코드 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\npre requirements\n이 프로젝트를 진행하기 위해 필요한 것들은 다음과 같습니다.\n\nAWS IAM 계정\nPacker\nTerraform\nAnsible\njq\nboto3\n\n\n\nbuild\n최종 build는 (42 seoul 사람에게 익숙한) makefile을 사용했습니다.\n\n\n\n\n\n\n제가 아직 로컬에서 돌려볼만한 다른 build 툴을 배우지 않아서 makefile을 사용하긴 했지만, 사실 c언어도 아니고..이 과제 구현에서 이 tool이 그렇게 어울리진 않은거 같긴 합니다.\n\n\n\n\n\n.env\n\n# only 1 line variable is allowed\n\nAWS_REGION=\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nSERVER_INSTANCE_COUNT=\n\n# public subnet에 접근할 수 있는 ip address를 지정해줍니다.\nSSH_IP=\n\n# public subnet에 접근할 때 사용할 ssh key path를 지정해줍니다.\nSSH_PUBLIC_KEY_PATH=\nSSH_PRIVATE_KEY_PATH=\n\n# docker compose setting\nMYSQL_USER=\nMYSQL_PASSWORD=\nMYSQL_ROOT_PASSWORD=\nDATABASE_NAME=\nSITE_TITLE=\nADMIN_NAME=\nADMIN_PASSWORD=\nADMIN_EMAIL=\nUSER_NAME=\nUSER_PASSWORD=\nUSER_EMAIL=\n\n\n\nMakefile\n\n# .env의 내용들을 makefile의 변수로 load 해줍니다.\n\ninclude .env\nexport\n\n먼저 필요한 변수들을 모두 .env에 저장해 한번에 관리할 수 있게 구현했습니다. 저장된 .env 내용은 makefile에서 위의 명령어로 불러와 build 명령어 실행시 사용할 수 있게 했습니다.\nmakefile이 .env 파일을 읽을 때 한 줄씩 읽기 때문에, 위의 방식으로 구현하면 여러 줄에 걸친 환경변수는 사용하기 어려울 수 있습니다. (그럴땐 그냥 makefile 말고 다른 tool을 쓰면 됩니다)\n\n\nMakefile\n\n.PHONY: provision deploy all destroy re build_ami\n\nall: build_ami provision deploy\n\nbuild_ami: packer\n    packer init $(PACKER_PATH)/database.pkr.hcl\n    @PKR_VAR_AWS_REGION=$(AWS_REGION) \\\n    PKR_VAR_MYSQL_USER=$(MYSQL_USER) \\\n    PKR_VAR_MYSQL_PASSWORD=$(MYSQL_PASSWORD) \\\n    PKR_VAR_DATABASE_NAME=$(DATABASE_NAME) \\\n    PKR_VAR_MYSQL_ROOT_PASSWORD=$(MYSQL_ROOT_PASSWORD) \\\n    packer build $(PACKER_PATH)/database.pkr.hcl\n\nprovision: build_ami terraform\n    terraform -chdir=$(PROVISION_PATH) init\n    @TF_VAR_AWS_REGION=$(AWS_REGION) \\\n    TF_VAR_SERVER_INSTANCE_COUNT=$(SERVER_INSTANCE_COUNT) \\\n    TF_VAR_SSH_IP=$(SSH_IP) \\\n    TF_VAR_SSH_PUBLIC_KEY_PATH=$(SSH_PUBLIC_KEY_PATH) \\\n    terraform -chdir=$(PROVISION_PATH) apply -auto-approve\n\ndeploy: ansible\n    @DB_PRIVATE_IP=\"$(shell terraform -chdir=$(PROVISION_PATH) output -json db_private_ip | jq -r '.[]' | tr '\\n' ' ')\" \\\n    ANSIBLE_HOST_KEY_CHECKING=False \\\n    ANSIBLE_REMOTE_USER=ubuntu \\\n    AWS_DEFAULT_REGION=$(AWS_REGION) \\\n    ANSIBLE_PYTHON_INTERPRETER=auto_silent \\\n    ansible-playbook \\\n    -i $(DEPLOY_PATH)/inventories \\\n    --private-key=$(SSH_PRIVATE_KEY_PATH) \\\n    $(DEPLOY_PATH)/server.yml \n\nbuild 과정은 ami 생성, provision, ansible deploy 순서로 진행됩니다.\n각 과정에 필요한 변수들은 명령어 수행 시 환경변수로 제공해줍니다. 대표적으로 ansible의 경우, provision 이후 생성된 database ec2의 private ip를 전달하고 있습니다.\n\n\nPacker 코드\n이 프로젝트에서는 데이터베이스 서버를 Private subnet에 위치시키고, Public subnet의 EC2만 이 데이터베이스에 접근할 수 있도록 설계했습니다. Private subnet에 있는 서버는 SSH 접근이 제한되기 때문에 Ansible로 직접 설정하기는 어렵습니다. 이런 경우 Packer로 미리 설정된 AMI를 생성하는 방법을 생각해볼 수 있습니다.\n구현한 Packer 파일 구조는 아래와 같습니다.\npacker/\n├── database.pkr.hcl\n└── ansible/\n    ├── _requirements/                      # docker compose setting files\n    ├── roles/setting_docker/tasks\n    │   └── main.yml\n    └── database.yml                        # playbook\n먼저 기본 이미지로 Ubuntu 20.04를 사용하도록 작성했습니다.\n\n\ndatabase.pkr.hcl\n\nsource \"amazon-ebs\" \"database\" {\n  region  = var.AWS_REGION\n  profile = \"default\"\n\n  ami_name      = \"hyunghki-database-${formatdate(\"YYYYMMDDhhmmss\", timestamp())}\"\n  instance_type = \"t2.micro\"\n  source_ami_filter {\n    filters = {\n      name                = \"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"\n      root-device-type    = \"ebs\"\n      virtualization-type = \"hvm\"\n    }\n    most_recent = true\n    owners      = [\"099720109477\"]\n  }\n  ssh_username = \"ubuntu\"\n}\n\nPacker는 기본적으로 이미지 생성을 위한 최소한의 기능만 제공하지만, 다양한 플러그인을 지원합니다. 여기서는 Ansible 플러그인을 사용하여 데이터베이스 서버 설정을 자동화했습니다.\n\n\ndatabase.pkr.hcl\n\nbuild {\n  sources = [\"source.amazon-ebs.database\"]\n\n  provisioner \"ansible\" {\n    playbook_file = \"${path.root}/ansible/database.yml\"\n    user = \"ubuntu\"\n    ansible_env_vars = [\n      \"ANSIBLE_HOST_KEY_CHECKING=False\",\n      \"MYSQL_USER=${var.MYSQL_USER}\",\n      \"MYSQL_PASSWORD=${var.MYSQL_PASSWORD}\",\n      \"DATABASE_NAME=${var.DATABASE_NAME}\",\n      \"MYSQL_ROOT_PASSWORD=${var.MYSQL_ROOT_PASSWORD}\",\n      \"ANSIBLE_PYTHON_INTERPRETER=auto_silent\"\n    ]\n  }\n}\n\n\n\nansible/database.yml\n\n- hosts: all\n  gather_facts: false\n  become: true\n  roles:\n    # docker compose를 machine에 설치해줍니다.\n    - role: setting_docker\n\n  tasks:\n    # docker compose에 필요한 파일들을 옮겨줍니다.\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    # 적절한 환경변수와 함께 docker compose 명령어를 실행합니다.\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        MYSQL_ROOT_PASSWORD: \"{{ lookup('env', 'MYSQL_ROOT_PASSWORD') }}\"\n\n이렇게 Ansible과 Packer를 조합하면 멱등성이 보장되는 안정적인 서버 이미지를 생성할 수 있습니다.\n참고로 packer에서 ansible plugin을 사용할 때 taget host를 ami가 build되는 임시 EC2로 간주하기 때문에, inventory는 사용하지 않습니다. 자세한 내용은 ansible part를 참고해주세요.\n\n\nTerraform 코드\n이제 본격적으로 provision을 해보겠습니다. 잠시 전체적인 구조를 다시 한번 보겠습니다.\n\n\n\n구현 aws 구조\n\n\n필요한 리소스는 VPC, subnet, security group, ec2 입니다.\nserver ec2와 database ec2는 환경변수 SERVER_INSTANCE_COUNT에 지정된 갯수 만큼 생성됩니다. database ec2는 이전 단계에서 생성한 ami를 사용해줍니다.\npublic, private subnet의 갯수는 임의로 생성했습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── main/\n│   ├── main.tf\n│   ├── data.tf\n│   ├── output.tf\n│   └── variables.tf\n└── modules/network/\n    ├── main.tf\n    ├── output.tf\n    └── variables.tf\nmain.tf에서는 aws_instance를 생성하고, 그 외 VPC, subnet과 같은 리소스는 network module로 분리해서 생성했습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_vpc\" \"main_vpc\" {\n  cidr_block           = \"10.0.0.0/16\"\n  instance_tenancy     = \"default\"\n  enable_dns_hostnames = \"true\"\n}\n\nresource \"aws_subnet\" \"public-1\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.1.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\nresource \"aws_subnet\" \"public-2\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.2.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}c\"\n}\n\nresource \"aws_subnet\" \"private\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.3.0/24\"\n  map_public_ip_on_launch = \"false\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\n먼저 VPC와 subnet을 생성합니다.\ncidr_block은 private ip 중에서 겹치지 않는 범위로 지정해줍니다.\n\n\n\n\n\n\nPrivate IP ranges\n\n\n\n\nClass A: 10.0.0.0–10.255.255.255\nClass B: 172.16.0.0–172.31.255.255\nClass C: 192.168.0.0–192.168.255.255\n\n\n\nPublic subnet이 인터넷과 통신하기 위해서는 Internet Gateway와 Route Table이 필요합니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_internet_gateway\" \"gate_way\" {\n  vpc_id = aws_vpc.main_vpc.id\n}\n\nresource \"aws_route_table\" \"public_route_table\" {\n  vpc_id = aws_vpc.main_vpc.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.gate_way.id\n  }\n}\n\nresource \"aws_route_table_association\" \"public-1\" {\n  subnet_id      = aws_subnet.public-1.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\nresource \"aws_route_table_association\" \"public-2\" {\n  subnet_id      = aws_subnet.public-2.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\n모든 외부 트래픽을 Internet Gateway로 보내도록 Route Table을 설정하고, 이를 두 개의 Public subnet에 연결했습니다.\n참고로 VPC 내부 통신은 자동으로 라우팅됩니다. 같은 VPC 안에 있는 리소스들은 VPC의 기본 라우팅 테이블을 통해 서로 통신할 수 있기 때문에 내부 통신을 위한 route table은 따로 생성하지 않았습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_security_group\" \"server_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"server_sg\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = var.SSH_CIDR_BLOCKS\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_security_group\" \"database_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"efs_sg\"\n\n  ingress {\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.server_sg.id]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n마지막으로 security group입니다.\nserver ec2의 ssh 접근은 환경변수를 통해 ansible을 실행하는 머신의 ip에서만 접근 가능하도록 설정해줬습니다.\ndatabase ec2는 server ec2만 접근할 수 있도록 설정했습니다.\n\n\nmain/main.tf\n\n# 사용자가 지정한 경로의 ssh key를 사용해 ec2에 접근 가능하도록 설정했습니다.\nresource \"aws_key_pair\" \"my_labtop\" {\n  key_name   = \"my_labtop\"\n  public_key = file(var.SSH_PUBLIC_KEY_PATH)\n}\n\nmodule \"network\" {\n  source = \"../modules/network\"\n\n  AWS_REGION           = var.AWS_REGION\n  SSH_CIDR_BLOCKS      = [\"${var.SSH_IP}/32\"]\n}\n\nresource \"aws_instance\" \"server\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.latest_ubuntu.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.server_sg_id]\n  # subnet은 2개를 번걸아가면서 사용하도록 설정했습니다.\n  subnet_id              = module.network.public_subnets[count.index % 2]\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"serverNode\"\n  }\n}\n\nresource \"aws_instance\" \"database\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.database_ami.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.database_sg_id]\n  subnet_id              = module.network.private_subnets\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"dbNode\"\n  }\n}\n\n최종적으로 main.tf에서 network module을 불러와서 필요한 리소스를 생성한 후, server와 database ec2를 생성했습니다.\n\n\nmain/data.tf\n\ndata \"aws_ami\" \"latest_ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"]\n}\n\ndata \"aws_ami\" \"database_ami\" {\n  most_recent = true\n  owners = [\"self\"]\n  filter {\n    name = \"name\"\n    values = [\"hyunghki-database-*\"]\n  }\n  filter {\n    name = \"root-device-type\"\n    values = [\"ebs\"]\n  }\n  filter {\n    name = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nserver ec2는 기본 ubuntu 20.04 이미지를 사용하고, database ec2는 이전에 생성한 ami를 사용했습니다.\n\n\nansible 코드\n이제 필요한 설정을 진행하겠습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── _requirements/                      # docker compose setting files\n├── inventories/\n│   └── aws_ec2.yml\n├── roles/setting_docker/tasks\n│   └── main.yml\n└── server.yml\n먼저 용어를 알아야 합니다.\n\nInventory (인벤토리)\n인벤토리는 Ansible이 관리할 호스트(서버)의 목록입니다. 호스트를 그룹으로 묶어 관리할 수 있습니다.\nPlaybook (플레이북)\n플레이북은 Ansible에서 작업을 정의하는 YAML 파일입니다. 플레이북은 하나 이상의 플레이로 구성되며, 각 플레이는 특정 호스트 그룹에 대해 수행할 작업(task)을 정의합니다.\nRole (롤)\n롤은 Ansible에서 재사용 가능한 구성 단위입니다. 플레이북을 모듈화하고 구조화하여 재사용성을 높이는 데 사용됩니다.\n\nInventory에서 server 그룹을 정의한 후, playbook으로 docker compose 환경을 설정하겠습니다.\n\n\naws_ec2.yml\n\nplugin: aws_ec2\nkeyed_groups:\n  - key: tags\ncompose:\n  ansible_host: public_ip_address\nleading_separator: False\nfilters:\n  instance-state-name: running\n\nAWS EC2 동적 인벤토리 설정입니다. Terraform으로 생성한 EC2 인스턴스들을 자동으로 관리할 수 있습니다.\n\n\nserver.yml\n\n- hosts: \"Name_serverNode\"\n  gather_facts: false\n  become: true\n  roles:\n    - role: setting_docker\n  tasks:\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    - name: Split array values from DB_PRIVATE_IP\n      set_fact:\n        target: \"{{ lookup('env', 'DB_PRIVATE_IP') | split(' ') }}\"\n\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        DOMAIN_NAME: \"{{ ansible_host }}\"\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        SITE_TITLE: \"{{ lookup('env', 'SITE_TITLE') }}\"\n        ADMIN_NAME: \"{{ lookup('env', 'ADMIN_NAME') }}\"\n        ADMIN_PASSWORD: \"{{ lookup('env', 'ADMIN_PASSWORD') }}\"\n        ADMIN_EMAIL: \"{{ lookup('env', 'ADMIN_EMAIL') }}\"\n        USER_NAME: \"{{ lookup('env', 'USER_NAME') }}\"\n        USER_PASSWORD: \"{{ lookup('env', 'USER_PASSWORD') }}\"\n        USER_EMAIL: \"{{ lookup('env', 'USER_EMAIL') }}\"\n        DB_PRIVATE_IP: \"{{ target[ansible_play_hosts.index(inventory_hostname)] }}\"\n\n    - name: all done message\n      debug:\n        msg: \"https://{{ ansible_host }}\"\n\n’Name’이 ’serverNode’인 인스턴스들만 선택하여 설정을 진행하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#실행",
    "href": "posts/02_areas/42_seoul/notes/09.html#실행",
    "title": "cloud-1 코드 설명",
    "section": "실행",
    "text": "실행\n먼저 .env 파일에 환경변수를 설정해줍니다.\nip 정보도 알아낸 후, SSH_IP에 설정해줍니다.\n\n\n\nnaver에 내 ip 검색\n\n\n\n\n.env\n\n# only 1 line variable is allowed\nAWS_REGION=ap-northeast-2\nAWS_ACCESS_KEY_ID=********************\nAWS_SECRET_ACCESS_KEY=********************\nSERVER_INSTANCE_COUNT=2\nSSH_IP=121.135.181.56\nSSH_PUBLIC_KEY_PATH=~/.ssh/id_rsa.pub\nSSH_PRIVATE_KEY_PATH=~/.ssh/id_rsa\nMYSQL_USER=dudu\nMYSQL_PASSWORD=secret\nMYSQL_ROOT_PASSWORD=secret\nDATABASE_NAME=cloud\nSITE_TITLE='hyunghki blog'\nADMIN_NAME=admin\nADMIN_PASSWORD=secret\nADMIN_EMAIL=admin@example.com\nUSER_NAME=user\nUSER_PASSWORD=secret\nUSER_EMAIL=user@example.com\n\n그후 make 명령어를 입력하면 자동으로 build가 진행됩니다.\n\n\n\n명령어 실행 결과\n\n\nbuild가 완료되면 완료 메세지의 ip로 접속해줍니다.\n\n\n\n접속 페이지\n\n\nwordpress 접속 페이지가 잘 뜨는 것을 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#결과",
    "href": "posts/02_areas/42_seoul/notes/09.html#결과",
    "title": "cloud-1 코드 설명",
    "section": "결과",
    "text": "결과\n\n\n\n최종 점수\n\n\n\n\n\n최종 평가",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#outro",
    "href": "posts/02_areas/42_seoul/notes/09.html#outro",
    "title": "cloud-1 코드 설명",
    "section": "outro",
    "text": "outro\n솔직히 일반적으로 사용되는 cloud 구조를 적용한건 아니긴 하지만, 과제에 맞춰서 진행하기 위해 고민하는 과정에서 다양한 구조를 적용해봤는데, 그 과정이 나름 학습에 도움이 된거 같습니다. 이 분야에 공부를 꽤 했고, 그 내용들을 다양하게 고민하며 적용해보고 싶다면 이 프로젝트가 괜찮은 선택지가 될 수도 있어 보입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html",
    "href": "posts/02_areas/deep_learning/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#details",
    "href": "posts/02_areas/deep_learning/index.html#details",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#tasks",
    "href": "posts/02_areas/deep_learning/index.html#tasks",
    "title": "Deep Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#참고-자료",
    "href": "posts/02_areas/deep_learning/index.html#참고-자료",
    "title": "Deep Learning",
    "section": "참고 자료",
    "text": "참고 자료\n\n밑바닥 부터 시작하는 딥러닝",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#related-posts",
    "href": "posts/02_areas/deep_learning/index.html#related-posts",
    "title": "Deep Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#신경망이란",
    "href": "posts/02_areas/deep_learning/notes/01.html#신경망이란",
    "title": "신경망",
    "section": "신경망이란",
    "text": "신경망이란\n\n퍼셉트론에서 가중치를 자동으로 학습하는 방법이다.\n입력층, 은닉층, 출력층으로 구성된다.\n\n앞에서 살펴본 퍼셉트론 함수를 다시 살펴보자.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때, \\(y = h(b + w_1x_1 + w_2x_2)\\)로 표현하면 다음과 같이 표현할 수 있다.\n\\[\nh(x) = \\begin{cases} 0 & (x ≤ 0) \\\\ 1 & (x &gt; 0) \\end{cases}\n\\]\n이때 \\(h(x)\\)는 활성화 함수(activation function)라고 한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#활성화-함수",
    "href": "posts/02_areas/deep_learning/notes/01.html#활성화-함수",
    "title": "신경망",
    "section": "활성화 함수",
    "text": "활성화 함수\n\n계단 함수(step function): 앞서 살펴본 함수\n\n\nimport numpy as np\n\ndef step_function(x):\n    return np.array(x &gt; 0, dtype=int)\n\nprint(step_function(np.array([-1.0, 1.0, 2.0])))\n\n[0 1 1]\n\n\n\nimport matplotlib.pyplot as plt\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = step_function(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n시그모이드 함수: \\(h(x) = \\frac{1}{1 + \\exp(-x)}\\)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = sigmoid(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n계단 함수와의 차이점\n\n시그모이드가 더 부드러움\n\n계단 함수와의 공통점\n\n입력이 작을 때는 0에 가깝고, 입력이 커지면 1에 가까워짐\n입력이 아무리 작거나 커도 출력은 0에서 1 사이\n비선형 함수 (선형 함수는 은닉층 업싱도 똑같이 구현할 수 있기 때문에 신경망에서 활성화 함수는 반드시 비선형 함수여야 함)\n\nReLU 함수: \\(h(x) = \\begin{cases} x & (x &gt; 0) \\\\ 0 & (x ≤ 0) \\end{cases}\\)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\nSoftMax 함수: \\(y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\\)\n\n\ndef softmax(a):\n    exp_a = np.exp(a)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsoftmax 함수는 값이 기하급수적으로 증가하기 때문에 쉽게 overflow가 발생할 수 있음.\n따라서 다음과 같이 개선이 필요함\n\ndef softmax(a):\n    c = np.max(a)\n    exp_a = np.exp(a - c)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsofrmax 함수 출력의 총합은 1이고, 개별 출력은 0에서 1 사이이다.\n따라서 softmax 함수의 출력을 확률로 해석할 수 있다.\n추론 단계에서 출력층의 softmax 함수는 생략하는 것이 일반적이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#층-신경망-구성",
    "href": "posts/02_areas/deep_learning/notes/01.html#층-신경망-구성",
    "title": "신경망",
    "section": "3층 신경망 구성",
    "text": "3층 신경망 구성\n입력층에서 1층으로 신호 전달\n\nX = np.array([1.0, 0.5])\nW1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\nB1 = np.array([0.1, 0.2, 0.3])\n\nA1 = np.dot(X, W1) + B1\nZ1 = sigmoid(A1)\n\nprint(A1)\nprint(Z1)\n\n[0.3 0.7 1.1]\n[0.57444252 0.66818777 0.75026011]\n\n\n1층에서 2층으로 신호 전달\n\nW2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\nB2 = np.array([0.1, 0.2])\n\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n\n\n2층에서 출력층으로 신호 전달\n\ndef identity_function(x):\n    return x\n\nW3 = np.array([[0.1, 0.3], [0.2, 0.4]])\nB3 = np.array([0.1, 0.2])\n\nA3 = np.dot(Z2, W3) + B3\nY = identity_function(A3)\n\n출력층의 활성화 함수는 보통 풀고자 하는 문제의 성질에 맞게 정함\n\n회귀: 항등 함수\n2클래스 분류: 시그모이드 함수\n다중 클래스 분류: 소프트맥스 함수",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-sdk",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-sdk",
    "title": "Qiskit",
    "section": "Qiskit SDK",
    "text": "Qiskit SDK\nQiskit SDK(패키지 이름: qiskit)는 확장된(정적, 동적, 스케줄된) 양자 회로, 연산자, 프리미티브 수준에서 양자 컴퓨터를 다루기 위한 오픈소스 소프트웨어 개발 키트입니다. 이 라이브러리는 Qiskit의 핵심 구성 요소로, 양자 계산을 위한 가장 광범위한 도구 모음을 제공하며, 다른 많은 구성 요소가 여기에 연결됩니다.\nQiskit SDK의 가장 유용한 기능은 다음과 같습니다:\n\n회로 구축 도구(qiskit.circuit): 레지스터, 회로, 명령, 게이트, 매개변수, 제어 흐름 객체를 초기화하고 조작하기 위한 도구.\n회로 라이브러리(qiskit.circuit.library): 회로, 명령, 게이트의 방대한 범위 - 회로 기반 양자 계산의 핵심 구성 요소.\n양자 정보 라이브러리(qiskit.quantum_info): 샘플링 노이즈 없이 정확한 계산을 통해 양자 상태, 연산자, 채널을 다루는 툴킷. 입력 관측 가능 항목을 지정하고 프리미티브 쿼리의 출력 충실도를 분석하는 데 사용.\n트랜스파일러(qiskit.transpiler): 특정 장치 토폴로지에 맞게 양자 회로를 변환 및 적응시키고, 실제 양자 처리 장치(QPU)에서 실행을 최적화.\n프리미티브(qiskit.primitives): Sampler와 Estimator 프리미티브의 기본 정의와 참조 구현을 포함하는 모듈로, 다양한 양자 하드웨어 제공자가 이를 기반으로 자체 구현을 파생할 수 있음. Qiskit Runtime 프리미티브에 대한 자세한 내용은 문서에서 확인 가능.\n\n\n설치\nQiskit SDK 설치에 대한 자세한 소개는 설치 페이지를 확인하세요. 지금 설치할 준비가 되었다면 다음 명령어를 실행하세요:\npip install qiskit",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "href": "posts/03_resources/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "title": "Qiskit",
    "section": "벤치마킹과 Benchpress 패키지",
    "text": "벤치마킹과 Benchpress 패키지\n벤치마킹은 개발 워크플로우의 여러 단계에서 양자 소프트웨어의 상대적 성능을 비교하는 데 중요합니다. 예를 들어, 양자 소프트웨어 벤치마킹 테스트는 회로 구축, 조작, 트랜스파일링의 속도와 품질을 평가할 수 있습니다. IBM Quantum은 가능한 한 성능이 뛰어난 SDK를 제공하기 위해 노력하며, 이를 위해 Qiskit SDK는 주요 대학, 국립 연구소, IBM 연구원들이 개발한 1,000개 이상의 테스트를 통해 벤치마킹됩니다. 이러한 테스트에 사용되는 벤치마킹 스위트는 Benchpress라는 이름으로 오픈소스 패키지로 제공됩니다. 이제 Benchpress 패키지를 사용해 양자 SDK 성능을 직접 분석할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-runtime",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-runtime",
    "title": "Qiskit",
    "section": "Qiskit Runtime",
    "text": "Qiskit Runtime\nQiskit Runtime은 IBM Quantum® 하드웨어에서 양자 계산을 실행하기 위한 클라우드 기반 서비스입니다. qiskit-ibm-runtime 패키지는 이 서비스의 클라이언트로, Qiskit IBM Provider의 후속 버전입니다. Qiskit Runtime 서비스는 양자 계산을 간소화하고 IBM Quantum 하드웨어에 최적화된 Qiskit 프리미티브 구현을 제공합니다. Qiskit Runtime 프리미티브를 시작하려면 문서를 방문하세요.\nQiskit Runtime은 추가적인 고전 및 양자 컴퓨팅 자원을 활용하도록 설계되었으며, 오류 억제(error suppression)와 오류 완화(error mitigation) 같은 기술을 사용해 양자 회로 실행에서 더 높은 품질의 결과를 반환합니다. 예로는 오류 억제를 위한 동적 디커플링(dynamical decoupling), 오류 완화를 위한 판독 완화(readout mitigation)와 제로 노이즈 외삽(ZNE)이 있습니다. 이러한 옵션 설정 방법은 오류 완화 설정 페이지에서 확인할 수 있습니다.\nQiskit Runtime은 IBM 하드웨어에서 양자 프로그램을 실행하기 위해 세 가지 실행 모드(Job, Session, Batch)를 제공하며, 각각은 서로 다른 사용 사례와 양자 작업 큐에 대한 영향을 가집니다: - Job: 지정된 샷 수로 실행되는 단일 프리미티브 쿼리. - Session: 양자 컴퓨터에서 반복 작업 부하를 여러 작업으로 효율적으로 실행. - Batch: 모든 작업을 한 번에 제출해 병렬 처리. 참고: Open Plan 사용자는 세션 작업을 제출할 수 없습니다.\nQiskit Runtime을 빠르게 설치하려면 다음 명령어를 실행하세요:\npip install qiskit-ibm-runtime\n개발 환경 설정에 대한 자세한 내용은 설치 페이지에서 확인할 수 있습니다.\n\nQiskit Runtime은 오픈소스인가요?\n간단히 답하면, 전부는 아닙니다. IBM Quantum 장치에서 양자 프로그램을 실행하는 기술적 세부 사항(오류 완화 및 억제 포함)을 처리하는 Qiskit Runtime 서비스 소프트웨어는 오픈소스가 아닙니다. 그러나 Qiskit Runtime 클라이언트(사용자가 Qiskit Runtime 서비스에 접근하는 인터페이스), 서버 측에서 실행되는 Qiskit SDK, 오류 완화에 사용되는 일부 소프트웨어는 오픈소스입니다. Qiskit 오픈소스 활동에 참여하려면 GitHub 조직(github.com/Qiskit 및 github.com/Qiskit-Extensions)을 방문하세요.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-serverless",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-serverless",
    "title": "Qiskit",
    "section": "Qiskit Serverless",
    "text": "Qiskit Serverless\n유틸리티 규모의 양자 응용 프로그램을 만들려면 일반적으로 다양한 컴퓨팅 자원 요구 사항이 필요합니다. Qiskit Serverless(qiskit-ibm-catalog.QiskitServerless)는 양자-고전 자원 전반에 걸쳐 작업 부하를 실행하기 위한 간단한 인터페이스를 제공합니다. 여기에는 IBM Quantum Platform에 프로그램 배포, 원격 작업 부하 실행, 멀티 클라우드 및 양자 중심 슈퍼컴퓨팅 사용 사례를 위한 쉬운 자원 관리가 포함됩니다. 자세한 내용은 Qiskit Serverless 문서에서 확인할 수 있습니다: - 고전 작업(전처리, 후처리 등) 병렬화. - 노트북이 꺼져 있어도 클라우드에서 장기 작업 유지. - 클라우드에 재사용 가능한 프로그램 배포.\nQiskit Serverless를 바로 사용하려면 다음 명령어로 설치하세요:\npip install qiskit_serverless",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-functions",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-functions",
    "title": "Qiskit",
    "section": "Qiskit Functions",
    "text": "Qiskit Functions\nQiskit Functions(qiskit-ibm-catalog.QiskitFunctionsCatalog)는 알고리즘 발견과 응용 프로토타이핑을 가속화하도록 설계된 추상화된 서비스입니다. Qiskit Functions Catalog를 탐색해보세요: - Circuit Functions: 트랜스파일링, 오류 억제, 오류 완화, 후처리 기술을 포함하며, 추상 회로와 원하는 측정 관측 가능 항목을 입력으로 받는 서비스. 사용자는 하드웨어 성능 관리를 신경 쓰지 않고 새로운 알고리즘과 응용을 탐구 가능. - Application Functions: 고전에서 양자로의 매핑, 하드웨어 최적화, 하드웨어 실행, 후처리를 포함한 전체 양자 워크플로우를 제공. 사용자는 산업별 친숙한 입력과 출력으로 응용 프로토타이핑 가능.\nPremium Plan 회원은 IBM 제공 함수에 즉시 접근하거나 파트너가 제공하는 함수를 파트너로부터 직접 라이선스 구매 가능합니다. 카탈로그는 다음 명령어로 설치 가능:\npip install qiskit-ibm-catalog",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "title": "Qiskit",
    "section": "Qiskit Transpiler as a Service",
    "text": "Qiskit Transpiler as a Service\nQiskit Transpiler Service(패키지 이름: qiskit-ibm-transpiler)는 IBM Quantum Premium Plan 사용자에게 클라우드에서 원격 트랜스파일링 기능을 제공하는 새로운 실험적 서비스입니다. 로컬 Qiskit SDK 트랜스파일러 기능 외에도, 이 서비스를 통해 트랜스파일링 작업은 IBM Quantum 클라우드 자원과 AI 기반 트랜스파일러 패스를 활용할 수 있습니다. 클라우드 기반 트랜스파일링을 Qiskit 워크플로우에 통합하는 방법은 문서에서 확인하세요.\n트랜스파일러 서비스는 다음 명령어로 설치 가능:\npip install qiskit-ibm-transpiler",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-addons",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-addons",
    "title": "Qiskit",
    "section": "Qiskit Addons",
    "text": "Qiskit Addons\nQiskit Addons는 유틸리티 규모 알고리즘 발견을 위한 연구 기능 모음입니다. 이러한 기능은 Qiskit의 고성능 기반 위에 양자 알고리즘 생성 및 실행 도구를 구축합니다. Addons는 워크플로우에 연결되어 새로운 양자 알고리즘을 확장하거나 설계하는 모듈형 소프트웨어 구성 요소입니다. 사용 가능한 Qiskit Addons 세트와 시작 방법은 문서에서 확인하세요.\n관심 있는 연구 기능에 따라 여러 Addons가 있으며, 각기 pip로 설치 가능: - Sample-based Quantum Diagonalization (SQD): pip install qiskit-addon-sqd - Approximate Quantum Compilation (AQC): pip install qiskit-addon-aqc-tensor[quimb-jax] - Operator Backpropagation (OBP): pip install qiskit-addon-obp - Multi-product Formulas (MPF): pip install qiskit-addon-mpf",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-생태계",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-생태계",
    "title": "Qiskit",
    "section": "Qiskit 생태계",
    "text": "Qiskit 생태계\nQiskit 외에도 “Qiskit” 이름을 사용하는 많은 오픈소스 프로젝트가 있으며, 이는 Qiskit 자체의 일부는 아니지만 Qiskit과 인터페이스를 이루며 핵심 Qiskit 워크플로우를 보완하는 유용한 추가 기능을 제공합니다. 일부 프로젝트는 IBM Quantum 팀이 유지 관리하며, 다른 일부는 더 넓은 오픈소스 커뮤니티에서 지원됩니다. Qiskit SDK는 모듈화되고 확장 가능한 방식으로 설계되어 개발자들이 이를 확장하는 프로젝트를 쉽게 만들 수 있습니다.\nQiskit 생태계의 인기 있는 프로젝트: - Qiskit Aer(qiskit-aer): 현실적인 노이즈 모델을 포함한 양자 컴퓨팅 시뮬레이터 패키지. 여러 시뮬레이션 방법으로 노이즈 유무에 따라 양자 회로 실행 가능. IBM Quantum 유지 관리. - qBraid SDK(qbraid): 양자 소프트웨어 및 하드웨어 제공자를 위한 플랫폼 독립적 양자 런타임 프레임워크로, 프로그램 사양 정의부터 작업 제출, 후처리 및 결과 시각화까지 양자 작업의 전체 수명 주기 관리를 간소화. qBraid 유지 관리. - mthree: M3(Matrix-free Measurement Mitigation) 측정 완화 기술을 구현하는 패키지로, 차원 축소 후 직접 LU 분해 또는 O(1) 단계로 수렴하는 전처리 반복 방법을 사용해 수정된 측정 확률을 계산하며 병렬 처리 가능. IBM Quantum 유지 관리.\nQiskit 생태계 페이지에서 프로젝트 카탈로그와 자신의 프로젝트를 추천하는 방법에 대한 정보를 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html",
    "href": "posts/03_resources/금융/index.html",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#details",
    "href": "posts/03_resources/금융/index.html#details",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#tasks",
    "href": "posts/03_resources/금융/index.html#tasks",
    "title": "금융",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#related-posts",
    "href": "posts/03_resources/금융/index.html#related-posts",
    "title": "금융",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html",
    "href": "posts/03_resources/smart_contract/index.html",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#details",
    "href": "posts/03_resources/smart_contract/index.html#details",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#tasks",
    "href": "posts/03_resources/smart_contract/index.html#tasks",
    "title": "Smart Contract",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#참고-자료",
    "href": "posts/03_resources/smart_contract/index.html#참고-자료",
    "title": "Smart Contract",
    "section": "참고 자료",
    "text": "참고 자료\n\nblock chain 강의 사이트",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#related-posts",
    "href": "posts/03_resources/smart_contract/index.html#related-posts",
    "title": "Smart Contract",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html",
    "href": "posts/03_resources/problem_solve/index.html",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#details",
    "href": "posts/03_resources/problem_solve/index.html#details",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#tasks",
    "href": "posts/03_resources/problem_solve/index.html#tasks",
    "title": "Problem Solving",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#related-posts",
    "href": "posts/03_resources/problem_solve/index.html#related-posts",
    "title": "Problem Solving",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "title": "Terraform Cloud",
    "section": "What is Terraform Cloud?",
    "text": "What is Terraform Cloud?\n\nTerraform Open-Source를 확장해주는 서비스\n\n\n\n\nTerraform Open-Source의 한계\n\n\n\n기존의 terraform을 대규모 팀 단위에서 사용하기엔 무리가 있음 → TFC\non-premise 환경을 위한 Terraform Enterpise 서비스도 존재함.\nTACOS: Terraform Automation & Collaboration Software",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#what-is-organization",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#what-is-organization",
    "title": "Terraform Cloud",
    "section": "What is Organization?",
    "text": "What is Organization?\n\nworkspaces, policies, terraform modules를 공유하는 공간\n\n\n\n\nOrganization level에서 모든 setting이 이루어짐\n\n\n\n하나의 조직을 운용하는 것이 일반적이나, 조직 구조에 따라 여러 조직을 생성해서 운용할 수 있다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "title": "Terraform Cloud",
    "section": "Authenticating to TFC",
    "text": "Authenticating to TFC\n\nweb interface\nCLI\n\n\nToken\n\nUser Tokens\nTeam Tokens: CI/CD pipeline에 주로 사용됨\nOrganization Tokens",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html",
    "href": "posts/03_resources/인생/index.html",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#details",
    "href": "posts/03_resources/인생/index.html#details",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#tasks",
    "href": "posts/03_resources/인생/index.html#tasks",
    "title": "인생",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#related-posts",
    "href": "posts/03_resources/인생/index.html#related-posts",
    "title": "인생",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html",
    "href": "posts/03_resources/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#details",
    "href": "posts/03_resources/blog/index.html#details",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#tasks",
    "href": "posts/03_resources/blog/index.html#tasks",
    "title": "Blog",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    PARA 구조에 맞게 블로그 구조 변경\n                \n                \n            \n\n            \n            \n                \n                    \n                    게시글에 관련 게시글, 관련 directory 추가\n                \n                별로 마음에 들진 않지만 일단 완성\n            \n\n            \n            \n                \n                    \n                    Hugo 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    google analytics 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    about me 페이지 작성\n                \n                \n            \n\n            \n            \n                \n                    \n                    link 미리보기 기능 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    task 리스트 캘린더 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    종합 task 캘린더 추가",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#참고-자료",
    "href": "posts/03_resources/blog/index.html#참고-자료",
    "title": "Blog",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#related-posts",
    "href": "posts/03_resources/blog/index.html#related-posts",
    "title": "Blog",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/0.html#overview",
    "href": "posts/03_resources/blog/notes/0.html#overview",
    "title": "PARA Blog 제작",
    "section": "Overview",
    "text": "Overview\n유튜브에서 ‘제2의 두뇌’ 관련 영상을 보고 이 구조로 제 학습 블로그에 적용하면 좋겠다는 생각이 들었습니다. Quarto로 만들어진 제 블로그에 이 구조를 적용하는 것이 생각보다 쉽지 않긴 했지만, 나름 해볼만 했습니다.\n사실 이 글을 작성하는 시점에는 이미 블로그 리뉴얼이 어느 정도 완료된 상태입니다. 코드가 최적화되지 않아 따로 제작 과정을 상세히 공유하지는 않으려 합니다만, 제 GitHub 레포에서 전체 코드를 확인하실 수 있습니다. 이전 블로그는 여기에서 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/0.html#이후-목표",
    "href": "posts/03_resources/blog/notes/0.html#이후-목표",
    "title": "PARA Blog 제작",
    "section": "이후 목표",
    "text": "이후 목표\n블로그를 완성하고 보니 Quarto의 필요성에 대해 다시 한번 생각해보게 되었습니다. 데이터 분석을 공부하는 입장에서 Quarto는 분명 대체 불가능한 장점들이 있지만, 웹사이트 구조를 구축하는 데에는 일정 부분 한계가 있어 보입니다.\nDocument를 읽어보던 중 Quarto와 Hugo를 통합하는 방법이 있다는 것을 알게 되었습니다. 이를 통해 Hugo로 블로그의 기본 구조를 만들고, R과 Python 코드 실행 환경으로 Quarto를 활용하는 방안을 고려하고 있습니다.\n이번이 jekyll, framer 블로그에 이어서 세번째로 만드는 블로그입니다. 저는 웹 개발보다는 다른 분야에 집중하고 싶기 때문에, 다음 리뉴얼을 마지막으로 블로그 구조 개선을 마무리해보려 합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "all.html",
    "href": "all.html",
    "title": "전체 게시글",
    "section": "",
    "text": "정렬\n       디폴트\n         \n          날짜 - 날짜(오름차순)\n        \n         \n          날짜 - 날짜(내림차순)\n        \n         \n          제목\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n제목\n\n\n날짜\n\n\n분류\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n청소년기의 심리·정서적 요인을 통한 진로 의사결정 성향 예측\n\n\n2025-05-23\n\n\n보고서, 데이터 마이닝\n\n\n\n\nPCA 분석\n\n\n2025-05-23\n\n\ndata mining\n\n\n\n\npreprocessing\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\nanalysis\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\n총괄생산계획\n\n\n2025-05-19\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 6\n\n\n2025-05-18\n\n\n보고서, OR\n\n\n\n\nRegression Analysis\n\n\n2025-05-13\n\n\n확률과 통계\n\n\n\n\n예측\n\n\n2025-05-12\n\n\n생산시스템관리\n\n\n\n\n데이터마이닝 팀 과제 주제\n\n\n2025-05-10\n\n\n보고서, 데이터 마이닝\n\n\n\n\n데이터마이닝 1차 팀과제 script\n\n\n2025-05-10\n\n\n보고서, 데이터 마이닝\n\n\n\n\n의사결정 트리\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\nSupport vector machine\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\n수송문제와 할당 문제들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n선형계획을 위한 다른 알고리즘들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n프로젝트 관리\n\n\n2025-04-30\n\n\n생산시스템관리\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 산술 손실\n\n\n2025-04-28\n\n\n생산시스템관리\n\n\n\n\nassociation rule mining\n\n\n2025-04-28\n\n\ndata mining\n\n\n\n\n컴퓨팅적 사고 1차 발표 구현 raw script\n\n\n2025-04-26\n\n\n보고서\n\n\n\n\nOR 과제 - 2\n\n\n2025-04-19\n\n\n보고서, OR\n\n\n\n\n쌍대이론과 민감도 분석 (part 6)\n\n\n2025-04-19\n\n\nOR, 학부 정리\n\n\n\n\nSimplex Method (part 5)\n\n\n2025-04-18\n\n\nOR, 학부 정리\n\n\n\n\nANOVA\n\n\n2025-04-15\n\n\n확률과 통계\n\n\n\n\nclassification with trees\n\n\n2025-04-14\n\n\ndata mining\n\n\n\n\n컴퓨팅적사고 발표 ppt\n\n\n2025-04-13\n\n\n보고서\n\n\n\n\nDataminig 1차 발표 ppt\n\n\n2025-04-13\n\n\ndata mining, 보고서\n\n\n\n\nOR 과제 - 4\n\n\n2025-04-10\n\n\n보고서, OR\n\n\n\n\nHomework - 2\n\n\n2025-04-10\n\n\ndata mining\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 대기시간 문제\n\n\n2025-04-09\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 3\n\n\n2025-04-06\n\n\n보고서, OR\n\n\n\n\nCoding pipeline\n\n\n2025-04-05\n\n\nAir flow\n\n\n\n\n시험 범위\n\n\n2025-04-03\n\n\nData Structure, 학부 정리\n\n\n\n\nSimplex 표 계산\n\n\n2025-04-03\n\n\nOR, 학부 정리\n\n\n\n\n배치 생산 및 경제적 주문량 모형\n\n\n2025-04-02\n\n\n생산시스템관리\n\n\n\n\nData Transformation\n\n\n2025-04-02\n\n\ndata mining\n\n\n\n\nHomework\n\n\n2025-03-31\n\n\ndata mining\n\n\n\n\n인건비 추정과 감축\n\n\n2025-03-31\n\n\n생산시스템관리\n\n\n\n\n통계적 가설검정\n\n\n2025-03-27\n\n\n확률과 통계\n\n\n\n\n제품 설계 기법 및 기업 프로세스 유형\n\n\n2025-03-26\n\n\n생산시스템관리\n\n\n\n\npreprocessing\n\n\n2025-03-26\n\n\ndata mining\n\n\n\n\nGetting Started\n\n\n2025-03-26\n\n\nAir flow\n\n\n\n\n공급 프로세스의 이해: 프로세스 처리능력 평가\n\n\n2025-03-19\n\n\n생산시스템관리\n\n\n\n\n신경망\n\n\n2025-03-18\n\n\ndeep learning\n\n\n\n\n퍼셉트론\n\n\n2025-03-17\n\n\ndeep learning\n\n\n\n\ntitanic\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nUpper Confidence Bound\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nNull space and Column space\n\n\n2025-03-16\n\n\n선형 대수\n\n\n\n\nEclat\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nApriori\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\n안녕하세요. 데이터 분석 전문가(진)입니다.\n\n\n2025-03-14\n\n\nadp, 후기\n\n\n\n\nOR 과제 - 1\n\n\n2025-03-13\n\n\n보고서, OR\n\n\n\n\n통계적 추정\n\n\n2025-03-13\n\n\n확률과 통계\n\n\n\n\n봉사\n\n\n2025-03-13\n\n\n봉사\n\n\n\n\n조직을 프로세스 관점에서 바라보기\n\n\n2025-03-12\n\n\n생산시스템관리\n\n\n\n\n가감법으로 연립방정식을 풀기 위한 행렬\n\n\n2025-03-11\n\n\n선형 대수\n\n\n\n\nintro\n\n\n2025-03-10\n\n\n학부 개념 정리\n\n\n\n\nMatching Supply with Demand\n\n\n2025-03-10\n\n\n생산시스템관리\n\n\n\n\nvector dot product, cross product\n\n\n2025-03-09\n\n\n선형 대수\n\n\n\n\nk-means clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nhierarchical clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nLinear Programming Algorithm\n\n\n2025-03-08\n\n\nOperational Research\n\n\n\n\nIntro\n\n\n2025-03-07\n\n\nOR, 학부 정리\n\n\n\n\n확률과 통계 1 정리\n\n\n2025-03-06\n\n\n확률과 통계\n\n\n\n\n나의 단점에 관한 고찰\n\n\n2025-03-06\n\n\n인생\n\n\n\n\nQuantum Programming\n\n\n2025-03-06\n\n\nQuantum Programming\n\n\n\n\nQiskit\n\n\n2025-03-06\n\n\nQuantum Programming\n\n\n\n\nRandom Forest\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nNaive Bayes\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nIntro\n\n\n2025-03-05\n\n\n생산시스템관리\n\n\n\n\nDecision Tree Classification\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nSubspaces and the basis\n\n\n2025-03-03\n\n\n선형 대수\n\n\n\n\nlinear independence\n\n\n2025-03-02\n\n\n선형 대수\n\n\n\n\nSupport Vector Machine\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nK Nearest Neighbors\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nrandom forest\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nSupport Vector Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nLogistic Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nDecision Tree Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nPolynorminal Linear Regression\n\n\n2025-02-27\n\n\nmachine learning\n\n\n\n\noverview\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\ndata preprocessing\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nSimple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nMultiple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\n머신 러닝\n\n\n2025-02-25\n\n\n데이터 분석\n\n\n\n\n시험을 보고 왔습니다.\n\n\n2025-02-22\n\n\nadp, 후기\n\n\n\n\nwhat is a blockchain?\n\n\n2025-02-22\n\n\n블록 체인\n\n\n\n\n4 - 정형 데이터 마이닝\n\n\n2025-02-20\n\n\nadp\n\n\n\n\n4 - 비정형 데이터 마이닝\n\n\n2025-02-18\n\n\nadp\n\n\n\n\ninception-of-things part 1\n\n\n2025-02-17\n\n\nvagrant, k8s, argoCD, gitlab, 42 seoul\n\n\n\n\n4 - 통계분석\n\n\n2025-02-16\n\n\nadp\n\n\n\n\n4 - 데이터 마트\n\n\n2025-02-15\n\n\nadp\n\n\n\n\nTerraform Cloud\n\n\n2025-02-11\n\n\nterraform, terraform cloud, devops, IaC\n\n\n\n\n5 - 시각화 인사이트 프로세스\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n5 - 시각화 디자인\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n3 - 분석 마스터 플랜\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n3 - 데이터 분석 기획의 이해\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n2 - 데이터 처리 프로세스\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n2 - 데이터 처리 기술\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n성적 장학금\n\n\n2025-02-05\n\n\n장학금\n\n\n\n\n1 - 데이터의 가치와 미래\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 데이터 이해\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n인간 관계론 - 데일 카네기\n\n\n2025-02-02\n\n\n독서, 인간 관계\n\n\n\n\n선형결합과 생성\n\n\n2025-01-31\n\n\n선형 대수\n\n\n\n\n벡터와 공간\n\n\n2025-01-30\n\n\n선형 대수\n\n\n\n\ncloud-1 코드 설명\n\n\n2025-01-30\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\ncloud-1 개념 설명\n\n\n2025-01-28\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\n3-몰라\n\n\n2025-01-22\n\n\n선형 대수\n\n\n\n\n자기 소개서\n\n\n2025-01-17\n\n\n자기 소개서\n\n\n\n\nft_transcendence - github action\n\n\n2025-01-17\n\n\nagile, github action, 42 seoul\n\n\n\n\n2-기초(2)\n\n\n2025-01-11\n\n\n선형 대수\n\n\n\n\n2-기초(1)\n\n\n2025-01-10\n\n\n선형 대수\n\n\n\n\nSecond Brain - 티아고 포르테\n\n\n2025-01-09\n\n\n학습, 독서\n\n\n\n\nwhat is linear algebra\n\n\n2025-01-07\n\n\n선형 대수\n\n\n\n\n돈의 심리학 - 모건 하우절\n\n\n2025-01-02\n\n\n금융, 독서\n\n\n\n\n데이터 전처리\n\n\n2025-01-02\n\n\n데이터 분석\n\n\n\n\nEDA와 시각화\n\n\n2024-12-30\n\n\n데이터 분석\n\n\n\n\nPARA Blog 제작\n\n\n2024-12-26\n\n\n블로그\n\n\n\n\npandas data 구조\n\n\n2024-12-20\n\n\n데이터 분석\n\n\n\n\n맥도날드 키오스크 UI 개선 보고서\n\n\n2024-11-27\n\n\n보고서, 인간 공학\n\n\n\n\n숭실대학교 학생식당 식자제 SCM 설계\n\n\n2024-11-26\n\n\n보고서, database\n\n\n\n\nControl\n\n\n2024-11-21\n\n\n인간 공학\n\n\n\n\n표본의 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n중심 극한 정리\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n정규 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\nDisplay\n\n\n2024-11-14\n\n\n인간 공학\n\n\n\n\n연속형 확률분포\n\n\n2024-11-05\n\n\n확률과 통계\n\n\n\n\nAttention\n\n\n2024-11-05\n\n\n인간 공학\n\n\n\n\nDatabase Design\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nDatabase Administration\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nASP.NET\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\n4조 기말과제 제안서\n\n\n2024-10-30\n\n\n보고서, database\n\n\n\n\n이산형 확률분포\n\n\n2024-10-28\n\n\n확률과 통계\n\n\n\n\n데이터베이스설계및활용 개인과제 #2\n\n\n2024-10-27\n\n\n보고서, database\n\n\n\n\n확률변수의 기댓값\n\n\n2024-10-16\n\n\n확률과 통계\n\n\n\n\nSignal Detection Theory\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nAuditory Haptic\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nData Modeling and the Entity-Relationship Model\n\n\n2024-10-14\n\n\ndatabase\n\n\n\n\nSQL\n\n\n2024-09-27\n\n\ndatabase\n\n\n\n\nSensor System (Visual)\n\n\n2024-09-24\n\n\n인간 공학\n\n\n\n\nDatabase Normalization\n\n\n2024-09-24\n\n\ndatabase\n\n\n\n\nThe Relational Model\n\n\n2024-09-17\n\n\ndatabase\n\n\n\n\nHuman Information Processing Model\n\n\n2024-09-17\n\n\n인간 공학\n\n\n\n\nResearch Method in Human Factors\n\n\n2024-09-10\n\n\n인간 공학\n\n\n\n\n확률변수와 확률분포\n\n\n2024-09-03\n\n\n확률과 통계\n\n\n\n\nIntroduction to Human Factors\n\n\n2024-09-03\n\n\n인간 공학\n\n\n\n\nAn Overview of Database\n\n\n2024-09-03\n\n\ndatabase\n\n\n\n\n확률과 통계의 정의\n\n\n2024-09-02\n\n\n확률과 통계\n\n\n\n\nmetrics server\n\n\n2024-05-15\n\n\n \n\n\n\n\nmanual scheduling\n\n\n2024-05-15\n\n\n \n\n\n\n\nk8s cluster architecture\n\n\n2024-05-15\n\n\n \n\n\n\n\nfail tolerance\n\n\n2024-05-15\n\n\n \n\n\n\n\ncore DNS\n\n\n2024-05-15\n\n\n \n\n\n\n\nPersistant volume\n\n\n2024-05-15\n\n\n \n\n\n\n\nHA in master node\n\n\n2024-05-15\n\n\n \n\n\n\n\nAuthentication\n\n\n2024-05-15\n\n\n \n\n\n\n\nwhat is ebs\n\n\n2024-04-30\n\n\n \n\n\n\n\nwhat is EC2\n\n\n2024-04-30\n\n\n \n\n\n\n\ndatabase choice in aws\n\n\n2024-04-30\n\n\n \n\n\n\n\naws global infrastructure\n\n\n2024-04-30\n\n\n \n\n\n\n\nVPC\n\n\n2024-04-30\n\n\n \n\n\n\n\nRoute53\n\n\n2024-04-30\n\n\n \n\n\n\n\nPerformance Improvement\n\n\n2024-04-30\n\n\n \n\n\n\n\nOverview\n\n\n2024-04-30\n\n\nvault, devops\n\n\n\n\nKMS(Key Management Service)\n\n\n2024-04-30\n\n\n \n\n\n\n\nELB\n\n\n2024-04-30\n\n\n \n\n\n\n\nDisaster Recovery(DR)\n\n\n2024-04-30\n\n\n \n\n\n\n\nDefine IAM\n\n\n2024-04-30\n\n\n \n\n\n\n\nCloudFront\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon Rekognition\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon RDS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon CloudWatch\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Snow Family\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS SQS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS S3\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Organization\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Lambda\n\n\n2024-04-30\n\n\n \n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람을-대하는-기본-기술",
    "href": "posts/00_inboxes/notes/01.html#사람을-대하는-기본-기술",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람을 대하는 기본 기술",
    "text": "사람을 대하는 기본 기술\n\n꿀을 얻으려면 벌집을 걷어차지 마라\n남을 비난하고 원망하며 불평하는 것은 어떤 바보라도 할 수 있다. 실제로 바보들은 그렇게 한다. 하지만 남을 이해하고 용서하려면 인격과 자제력이 필요하다.\n사람을 비난하는 대신 그들을 이해하려고 노력해 보자. 그들이 왜 그런 행동을 하는지 곰곰이 생각해 보자. 그편이 비난하는 것보다 훨씬 이롭고 흥미롭다.\n\n\n사람을 대하는 핵심 비결\n누군가에게 어떤 일을 하게 만드는 방법은 상대방이 그 일을 하고 싶게 만드는 것 뿐이다. 강제적인 방법들은 반드시 역효과를 일으킨다.\n타인에게 어떤 일을 하게 하려면 그 사람이 원하는 것을 주는 방법밖에 없다. 인간의 본성이 지닌 가장 깊은 충동이 바로 중요한 사람이 되고자 하는 욕망이다.\n이러한 갈망을 제대로 충족시켜 주는 사람은 다른 사람의 마음을 사로잡을 수 있다. 타인을 진실된 마음으로 칭찬하자. 이는 이기적이고 거짓인 아첨과는 다르다.\n\n\n이 일을 해내는 사람은 세상을 얻을 것이고, 그렇지 못한 사람은 외로운 길을 걸을 것이다.\n상대방에게 영향을 미치는  방법은 그사람이 원하는 것을 이야기하고, 이를 얻는 방법을 보여 주는 것이다.\n\n\n\n\n\n\n…이 부분 예시로 드는 것들이 조금 오버스럽다고 느껴진다.\n문화가 달라서 그런가? 아니면 번역 이슈인가?\n무슨 말을 하는진 알겠는데, 몇몇 부분은 별로 공감이 안 된다.",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "href": "posts/00_inboxes/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들에게 호감을 얻는 6가지 방법",
    "text": "사람들에게 호감을 얻는 6가지 방법\n\n이렇게 하면 어디서든 환영받을 것이다.\n다른 사람에게 관심이 없는 사람은 인생에서 가장 큰 어려움을 겪고, 다른 사람에게 가장 큰 상처를 준다. 상대방에게 진심으로 관심을 가져라\n\n\n좋은 첫인상을 남기는 간단한 방법\n미소를 지어라\n\n\n이렇게 하지 않으면 문제가 생길 것이다.\n사람의 이름을 기억하라\n\n\n좋은 대화 상대가 되는 쉬운 방법\n상대의 이야기를 경청하고, 상대가 자신에 관해 이야기하도록 격려하라\n\n\n사람들의 관심을 얻는 방법\n상대방의 관심사에 대해 이야기하라\n\n\n사람들에게 즉시 호감을 얻는 방법\n모든 사람은 자신이 상대보다 우월하다고 생각한다. 항상 상대방이 자신을 중요하다고 느끼게 하라",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "href": "posts/00_inboxes/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들의 마음을 사로잡는 12가지 방법",
    "text": "사람들의 마음을 사로잡는 12가지 방법\n\n논쟁으로는 이길 수 없다\n자기 의사에 반하여 설득당한 사람은 여전히 자기 생각을 바꾸지 않는 법이다. 논쟁에서 최선의 결과를 얻는 유일한 방법은 논쟁을 피하는 것뿐이다.\n\n\n적을 만드는 확실한 방법과 이를 피하는 방법\n되도록 남들보다 지혜로운 사람이 되거라. 하지만 남들에게 그렇다고 말하지 않도록 해라.\n상대가 틀린 말을 해도 굳이 지적하지 마라\n\n\n틀렸다면, 인정하라\n조금 잘못했는데, 상대가 비난할거 같으면 오바해서 자기 잘못을 시인하라\n\n\n이성에 호소하는 확실한 방법\n우호적인 방식으로 시작하라\n\n\n소크라테스의 비결\n상대방이 동의할 수 밖에 없는 질문을 유도하라.\n\n\n불만을 잠재우는 안전밸브\n여기부터 읽어야함",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#second-brain의-핵심-기능",
    "href": "posts/03_resources/blog/notes/1.html#second-brain의-핵심-기능",
    "title": "Second Brain - 티아고 포르테",
    "section": "Second Brain의 핵심 기능",
    "text": "Second Brain의 핵심 기능\n\n아이디어를 구체화한다\n머릿속에서 아이디어를 분리하여 구체적인 형태로 만들어야 한다.\n아이디어 사이의 연관성을 새롭게 밝혀낸다.\n다양한 자료를 한곳에 보관하면 자료간 연결 작업이 촉진되며, 생각지 못한 연관성을 찾아낼 가능성을 높일 수 있다.\n시간을 두고 아이디어를 발전시킨다.\n사람들은 줄곧 아이디어를 떠올릴 때 최신 정보에 중요성을 더 부여하는 경향이 있다.\n몇년 간 축적된 아이디어를 마음껏 이용할 수 있다면 더 좋을 것이다.\n나만의 독특한 관점을 정교하게 다듬는다.\n작가의 벽에 부딪히는 것은 적절한 단어를 떠올릴 수 없다는 것이 아니라, 글을 쓸 탄약이 부족하다는 것이다.\n자신의 견해를 지지할 수 있는 자료를 지속적으로 모아야한다.\n\n\n머리는 아이디어를 생각하는 곳이지 보관하는 곳이어선 안된다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "href": "posts/03_resources/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "title": "Second Brain - 티아고 포르테",
    "section": "중요한 것을 기억하는 4 단계 (CODE)",
    "text": "중요한 것을 기억하는 4 단계 (CODE)\n\nCapture: 공명하는 내용을 수집하라\n당신과 마음에 닿는 내용을 분별하여 보관하고 나머지는 버려라\nOrganazie: 실행을 목표로 정리하라\n실행을 염두에 두고 정리하라.\nDistill: 핵심을 찾아 추출하라\n메모의 요점을 정리하라.\n메모를 저장한 이유, 생각하던 내용, 무엇이 당신의 관심을 끌었는지에 대한 설명\nExpress: 작업한 결과물을 표현하라\n개인적이고 구체적이며 검증된 정보는 실제로 사용할 때에 비로소 지식이 된다.\n당신이 아는 내용을 다른 사람과 공유하기 전까지는 그저 이론에 불과하다.\n\n\nCapture\n미래에 어떻게 될지 전혀 모르는데 무엇을 저장할지 어떻게 결정할 수 있을까? 어떤 정보가 보관할 가치가 있는지 정확히 알아내도록 통찰력을 키우기 위해 리처드 파인만의 좋아하는 12가지 문제 방법을 제시한다. 자신에게 흥미를 불러일으키는 열린 질문들을 자유롭게 적어보자. 그후 해당 질문들을 학습의 방향을 제시하는 북극성으로 삼아 활용한다.\n\n\n\n\n\n\n직접 적어본 질문들\n\n\n\n\n쇠퇴하지 않는 사람이 되기 위해 꾸준히 해야하는 활동에는 어떤게 있을까?\n운에 좌절하지 않기 위해 어떤걸 준비해야 할까?\n학점을 잘 받으려면 어떻게 공부해야 할까?\n소중한 인연은 무엇인가?\n무엇을 위해 발전해야 하는가?\n시간이 지나도 가치있는건 무엇일까?\n공명하는 지식이 매번 진실일까?\n돈을 잘 벌려면 어떻게 해야할까?\n\n\n\n그런 다음, 해당 주제와 관련된 자료에서 아래의 기준에 해당하는 내용들을 선별한다. 수집하는 자료는 외부에 존재하는 자료뿐만 아니라 자료를 수집하면서 얻은 내면 세계의 아이디어 역시 그 대상이될 수 있다.\n\n영감을 불러일으키는가\n나와 내 일에 유용한가\n개인적인 정보인가\n가족이나 친구들과 나눈 문자 메세지들도 수집의 대상이 될 수 있다.\n놀랄 만한 사실인가\n기존의 알고있는 자료만 수집하면 확증편향의 위험이 있다.\nsecond brain은 이미 알고 있는 내용을 또 확인하는 방법이 되어서는 안 된다.\n\n다음과 같은 유형들은 보관하기에 적합하지 않다.\n\n민감한 정보\n포토샵 파일이나 비디오 영상처럼 전용 앱이 필요한 경우\n대용량 파일\n공동 편집이 필요한 경우\n\n\n\nOrganize\n수집한 자료를 정리할 때, 종류별로 나누지 않고, 얼마나 실행 가능한지에 따라 정리할 수 있다. 주제와 하위 항목으로 연달아 이루어진 복잡한 계층 체계에 따라 메모를 정리하는 대신, 이것은 어떤 프로젝트에 가장 도움이 될까?라는 간단한 질문 하나에만 답하면 된다.\n\nPARA\n\nProject: 일이나 생활에서 현재 진행 중이며 단기간 노력이 필요한 일\n시작과 끝이 존재. 완성, 승인, 착수, 발표처럼 구체적이고 확실한 결과가 있어야 한다.\nArea: 오랫동안 관리하고 싶고 장기적으로 책임지는 일\n정해진 종료 날짜와 최종 목표가 없음.\nResource: 향후 도움이 될 수 있는 주제 혹은 관심사\n현재 진행하는 프로젝트 혹은 영역과 관련 없는 자료, 당분간 실행할 수 없는 메모나 파일 등을 보관할 수 있다.\nArchive: 전에는 위의 세 가지 유형에 속했지만, 지금은 비활성화된 항목\n완료하거나 취소된 프로젝트, 이제는 관리하지 않는 책임 영역, 흥미를 잃은 자원 등을 보관할 수 있다.\n\nPARA 정리 방식은 부엌 정리 방식과 유사하다. 부엌에 있는 물건들은 전부 식사를 준비하도록 설계되고 정리된다. 각각의 상위 폴더들을 비유하면, archive는 냉동고, resource는 식료품 저장고, 영역은 냉장고, 프로젝트는 불 위에서 끓고 있는 냄비나 팬과 같다.\n부엌을 음식 종류에 따라 정리하면 얼마나 터무니없을지 상상해보라. 신선한 과일과 말린 과일, 과일 주스와 냉동 과일은 모두 과일로 만들었다는 이유로 같은 장소에 보관될 것이다. 그런데 이것이 바로 대부분의 사람들이 파일과 메모를 정리하는 방식이다. 책을 읽으며 메모했다는 이유만으로 책 메모는 책 메모끼리, 다른 사람의 말을 인용했다는 이유만으로 인용문은 인용문끼리 보관한다.\n\n\n\nDistill\n메모는 단순한 수집을 넘어 실제 활용이 가능한 형태로 정제되어야 한다. 이를 위해 다음과 같은 단계별 요약 과정을 거친다.\n\n메모 수집: 먼저 빠르게 수집, 정리 이후 정제는 나중에 진행\n굵게 처리: 중요한 문장이나 구절을 표시\n하이라이트 처리: 굵게 처리된 내용 중 핵심을 강조\n핵심 요약: 최종적으로 메모의 핵심을 추출\n\n이 과정에서 주의할 점들\n\n과다 하이라이트 처리: 이전 단계 내용의 10-20% 정도만 선별\n목적 없는 하이라이트 처리: 무작정 시작하지 않고, 메모를 어떻게 사용할지 알게 될 때까지 기다린 후, 필요에 따라 하이라이트 한다.\n어려운 방식의 하이라이트 처리: 본인의 직관에 맞게 흥미로운 구절들을 하이라이트 한다.\n\n\n\n\n\n\n\n메모의 생존 여부는 ’얼마나 쉽게 찾을 수 있는가’에 달려있다.\n\n\n\n\n\nExpress\n맡은일을 중간 단계로 나누어서 최대한 빠르게 결과물을 도출하라 도출한 작업물들을 중간단계로써 다른 프로젝트에 사용할 때, 도움을 얻을 수 있다. 도출한 결과물들을 다른사람들과 공유를 해서 피드백을 받아라",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#창조력을-완성하는-과정",
    "href": "posts/03_resources/blog/notes/1.html#창조력을-완성하는-과정",
    "title": "Second Brain - 티아고 포르테",
    "section": "창조력을 완성하는 과정",
    "text": "창조력을 완성하는 과정\n언제든 참신한 아이디어를 떠올릴 수 있다고 기대해서는 안된다. 혁신과 문제 해결은 흥미로운 아이디어를 체계적으로 불러일으켜 우리가 인식하게 하는 일상에 달려 있다.\n세컨드브레인은 창의적인 과정들을 아이디어 수집, 정리, 핵심 추출, 조립의 단계로 표준화하여 우리의 뇌 활동을 돕는다.\n\n창의적인 프로젝트를 완료할 때 도움이 되는 전략\n\n아이디어 군도: 프로젝트 수행에 필요한 모든 문서를 모은다. 그리고 해당 문서들을 연결하라.\n헤밍웨이 다리: 현재 진행중인 프로젝트에서 다음과 같은 사항들을 메모에 기록하라.\n\n다음 단계에는 어떤 이야기를 쓸 지\n현재 상황\n잊어버리기 쉬운 세부 사항\n다음 작업 시간의 목표\n\n범위 조금씩 축소하기: 프로젝트의 복잡한 문제가 드러나면 과감하게 범위를 축소하라\n\n\n\n효율적인 실행을 위한 세 가지 습관\n\n\n\n\n\n\n정리정돈은 타고난 특성이 아닌 습관이다.\n\n\n\n\n체크리스트 습관\n\n수집: 프로젝트에 대한 내 생각을 수집하라\n\n이 프로젝트에 대해 이미 알고 있는 것은 무엇인가?\n알아내야 하지만 아직 모르는 것은 무엇인가?\n목표나 목적은 무엇인가?\n통찰력을 얻으려면 누구와 대화해야 하는가?\n아이디어를 얻으려면 어떤 것을 읽거나 들어야 하는가?\n\n검토: 관련 메모가 있을 만한 폴더나 태그를 검토하라\n검색: 모든 폴더에서 관련 용어를 검색하라\n이동: 관련 메모를 프로젝트 폴더로 이동하거나 태그를 설정하라\n작성: 수집한 메모로 개요를 작성하고 프로젝트를 계획하라\n\n\n\n리뷰 습관\n\n주간 리뷰\n일주일 동안 작업한 모든 메체의 메모를 검토하라 그리고 이번 주 할 과제를 정하라\n월간 리뷰\n솔직히 이런건 잘 안할거 같다.\n\n\n\n알아차리는 습관",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/notes/02.html",
    "href": "posts/03_resources/인생/notes/02.html",
    "title": "나의 단점에 관한 고찰",
    "section": "",
    "text": "나는 단점이 없다고 생각했다. 그래서 자기소개서 같은 곳에 장점, 단점을 적는 칸이 있으면 아래같이 유머 아닌 유머같은 답을 적곤 했다.\n\n\n\n장점\n단점이 없다.\n\n\n단점\n\n\n\n\n최근 드는 생각인데, 내 단점은 사람을 대할 때 꽤나 간사한 면이 있다는 것이다.\n사회적으로나, 능력적으로나, 누구든 나보다 잘나다고 생각되는 사람 앞에서 나는 수줍고 소심한 사람이 된다. 뭐.. 사실 이정도는 누구나 그런 면이 있을 수 있다 생각한다. 하지만 나보다 열등하다고 생각되는 사람 앞에서 나는 꽤 강압적이고 무례한 사람이 된다. 흔히 말하는 ’강약약강’이라는 말이 아마 나를 잘 설명해주는 것 같다.\n사실 어쩌면 이런 강압적인 모습이 나의 본성이 아닐까 하는 생각이 든다. 수줍고 소심한 모습은 아마 사회화된 또 다른 나의 모습이 아닐까. 왜냐하면 나는 나의 부모님에게서 수줍고 소심한 모습을 본 적이 없기 때문이다.\n그렇다면 나는 이 단점들을 극복해야 하나? 극복한다면 어떤 모습이 바람직한 나의 모습일까? 쓸모없는 사회화된 모습을 덜어야 할까? 아니면 추한 본성을 덜어야 할까? 사실 지금의 나도 살아가는데 그렇게 큰 불편함은 없긴 하다. 조금씩 밸런스를 맞춰가며 살아가야지.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "인생",
      "Notes",
      "나의 단점에 관한 고찰"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html",
    "href": "posts/03_resources/tofel_준비/index.html",
    "title": "TOFEL 준비",
    "section": "",
    "text": "BEFORE-START\n    \n    \n        시작일: None\n        종료일: None\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        English",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#details",
    "href": "posts/03_resources/tofel_준비/index.html#details",
    "title": "TOFEL 준비",
    "section": "Details",
    "text": "Details\nTOFEL을 준비해 봅시다.",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#tasks",
    "href": "posts/03_resources/tofel_준비/index.html#tasks",
    "title": "TOFEL 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#related-posts",
    "href": "posts/03_resources/tofel_준비/index.html#related-posts",
    "title": "TOFEL 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html",
    "href": "posts/03_resources/terraform/index.html",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#details",
    "href": "posts/03_resources/terraform/index.html#details",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#tasks",
    "href": "posts/03_resources/terraform/index.html#tasks",
    "title": "Terraform",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#참고-자료",
    "href": "posts/03_resources/terraform/index.html#참고-자료",
    "title": "Terraform",
    "section": "참고 자료",
    "text": "참고 자료\n\nKodeKloud - Terraform cloud",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#related-posts",
    "href": "posts/03_resources/terraform/index.html#related-posts",
    "title": "Terraform",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/notes/block_chain_basic/00.html",
    "href": "posts/03_resources/smart_contract/notes/block_chain_basic/00.html",
    "title": "what is a blockchain?",
    "section": "",
    "text": "oracle(회사 아님): a trusted third party that provides data to the blockchain\nchain link: a decentralized oracle network that connects smart contracts to external data sourcesa\nsmart contract: trust minimized agreements, unbrakable promises\nmetamask는 nimonics를 이용해 private key를 생성. 다계정을 만들 때는 nimonics + &lt;index&gt;를 이용해 계정 생성\nprivate key는 transaction을 sign할 때 사용. public key는 transaction을 verify할 때 사용\nverify된 transaction은 miner에 의해 블록에 추가됨\ngas price: Base Fee + Priority Fee\ntransaction fee: 실제로 지불하는 금액. Gas Prics * used gas (&lt; gas limit). transaction fee - burnt fee만큼 즉, priority fee * used gas만큼 miner에게 지급됨\ngas fee: transaction을 처리하는데 필요한 비용. gas fee가 높을수록 빨리 처리됨\n\nBase fee: network congestion에 따라 변동. Base fee * used gas 만큼 소각됨\nMax fee: 사용자가 지불할 수 있는 최대 gas price\nMax Priority: 사용자가 지불할 수 있는 최대 fee + tip\n\nconsensus algorithm: 블록체인 네트워크의 모든 노드가 동의하는 방식 (nakamoto consensus: proof of work + longest chain)\n\nChain selection:\n\nlongest chain: 가장 긴 체인을 선택\n\nsybil resistance: 한 사람이 여러 개의 가짜 계정을 만들고 시스템을 조작하는 Sybil 공격을 방어하는 능력\n\nproof of works: hash를 0으로 만드는 nonce를 찾음. 제일 먼저 찾은 사람이 블록을 추가할 수 있음 (transaction fee + block reward(네트워크에서 새로 발행하는 코인. 갈수록 줄어듦))\nproof of stake\n\n\nL1: base layer of blockchain ecosystem\nL2: application built outside of the L1 and hooks back into the L1\nroll up: L2에서 발생한 transaction을 L1에 기록하는 방식\n\noptimistic roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되기 전까지는 롤백 가능\nzk roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되면 롤백 불가능\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract",
      "Notes",
      "Block Chain Basic",
      "what is a blockchain?"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#금융-행동의-개인차",
    "href": "posts/03_resources/금융/notes/00.html#금융-행동의-개인차",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "1. 금융 행동의 개인차",
    "text": "1. 금융 행동의 개인차\n금융 시장에서 개인의 행동 차이는 단순히 정보의 우위나 지적 능력의 차이가 아닌, 개인의 경험과 가치관에서 비롯된다. 우리는 각자의 경험을 바탕으로 나름의 합리적인 의사결정을 내린다. 따라서 겉보기에 비합리적으로 보이는 행동도 개인의 맥락에서는 충분히 이해될 수 있다. 돈 문제에 있어서 누구나 미친짓을 한다. 거의 모두가 이 게임이 처음이기 때문이다. 하지만 실제로 미친사람은 없다. 누구나 자신만의 경험에 근거해서 합리적으로 보이는 의사결정을 내릴 뿐이다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#운과-리스크의-역할",
    "href": "posts/03_resources/금융/notes/00.html#운과-리스크의-역할",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "2. 운과 리스크의 역할",
    "text": "2. 운과 리스크의 역할\n금융 시장에서의 결과는 우리의 행동만으로 결정되지 않는다. 운의 영향력을 인정하고, 리스크를 적절히 관리하는 것이 중요하다. 이를 위해 우리는 다음과 같은 질문들을 스스로에게 던져야 한다\n\n추가적인 수익이 정말 필요한가?\n타인과의 비교가 판단을 흐리고 있지는 않은가?\n’충분함’의 기준은 무엇인가?\n돈보다 우선시해야 할 가치는 무엇인가?\n\n어느 정도가 충분한지 깨닫고 리스크를 멈출줄 알아야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#지속가능한-투자의-원칙",
    "href": "posts/03_resources/금융/notes/00.html#지속가능한-투자의-원칙",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "3. 지속가능한 투자의 원칙",
    "text": "3. 지속가능한 투자의 원칙\n일회성 수익보다는 지속가능한 수익이 더 가치있다. 투자에는 두 가지 다른 기술이 필요하다\n\n수익 창출: 리스크 감수, 낙관적 사고, 적극적 태도\n자산 보존: 신중함, 위험 관리, 절제\n\n최고의 수익률은 일회성이어서 반복할 수 없는 경향이 있다. 꽤 괜찮은 수익률을 오랫동안 반복할 수 있는게 훌륭한 투자다. 성공적인 투자자는 대중이 비이성적일 때도 침착함을 유지할 수 있는 사람이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#돈과-시간의-관계",
    "href": "posts/03_resources/금융/notes/00.html#돈과-시간의-관계",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "4. 돈과 시간의 관계",
    "text": "4. 돈과 시간의 관계\n돈의 진정한 가치는 그것이 우리에게 주는 시간의 자유에 있다. 돈이 주는 가장 큰 배당금은 시간이다 단순히 부자(rich)가 되는 것과 진정한 부(wealthy)를 이루는 것은 다르다. 진정한 부자들은 겉으로 보이는 치장(rich)에 돈을 쓰기 보다는 부를 축적(wealthy)하여 자유를 얻는다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "href": "posts/03_resources/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "5. 금융시장의 불변요소와 가변요소",
    "text": "5. 금융시장의 불변요소와 가변요소\n금융 시장에서 인간의 기본적인 행동 패턴은 크게 변하지 않는다. 탐욕, 공포, 스트레스 상황에서의 반응 등은 시대가 바뀌어도 유사하다. 반면, 시장 트렌드, 산업 구조, 투자 방식 등은 끊임없이 진화한다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#리스크-관리의-중요성",
    "href": "posts/03_resources/금융/notes/00.html#리스크-관리의-중요성",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "6. 리스크 관리의 중요성",
    "text": "6. 리스크 관리의 중요성\n\n파산 위험이 있는 리스크는 절대 감수하지 않는다\n계획이 실패했을 때를 대비한 백업 플랜이 필수적이다\n시장의 변동성은 피해야 할 벌금이 아닌, 수수료로 인식해야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#현실적인-목표-설정",
    "href": "posts/03_resources/금융/notes/00.html#현실적인-목표-설정",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "7. 현실적인 목표 설정",
    "text": "7. 현실적인 목표 설정\n\n이상적인 목표와 현실적인 스트레스 상황은 큰 차이가 있다\n과거의 비현실적 목표는 과감히 버려야 한다\n내가 지금과 다른 사람일 때 세웠던 목표는 생명 유지 장치를 달고 시간을 질질 끌 게 아니라 가차 없이 버리는 편이 낫다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#시장의-본질-이해",
    "href": "posts/03_resources/금융/notes/00.html#시장의-본질-이해",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "8. 시장의 본질 이해",
    "text": "8. 시장의 본질 이해\n\n극단적 상황은 오래 지속되지 않는다\n투자 성공의 대가를 이해하고 지불할 준비가 필요하다\n시장을 완벽히 통제할 수 있다는 환상을 버려야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍이란?",
    "text": "양자 프로그래밍이란?\n양자 프로그래밍은 양자 컴퓨터의 힘을 활용해 알고리즘과 소프트웨어를 개발하는 것으로, 중첩(superposition), 얽힘(entanglement), 양자 병렬성(quantum parallelism)과 같은 양자 역학 원리를 사용합니다. 이는 양자 회로를 설계하고, 양자 게이트를 적용하며, 큰 수를 인수분해하는 쇼어(Shor) 알고리즘이나 데이터베이스 검색을 위한 그로버(Grover) 검색 알고리즘과 같은 양자 알고리즘을 구현하는 작업을 포함합니다.\n양자 프로그래밍은 아직 초기 단계에 있지만 암호학, AI, 최적화, 과학적 시뮬레이션 등에서 잠재적인 응용 가능성을 가지고 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 언어",
    "text": "양자 프로그래밍 언어\n양자 프로그래밍 언어는 정의상 양자 컴퓨터용 프로그램을 작성하기 위해 설계된 언어입니다. 양자 프로그래밍 언어를 고전 프로그래밍 언어와 구분 짓는 요소는 양자 시스템의 원리(큐비트, 얽힘, 중첩 법칙 등)에 기반해 양자 알고리즘을 평가하는 방식입니다.\n양자 컴퓨팅에 널리 사용되는 프로그래밍 언어로는 Qiskit, Cirq, Q# 등이 있으며, 이들은 고전 컴퓨팅보다 훨씬 빠르게 복잡한 문제를 해결할 수 있는 양자 알고리즘 개발을 가능하게 합니다. 특히 암호학, 최적화, 머신러닝 분야에서 두각을 나타냅니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 vs 고전 프로그래밍",
    "text": "양자 프로그래밍 vs 고전 프로그래밍\n양자 프로그래밍과 고전 프로그래밍 사이에는 근본적인 차이가 있습니다. 각각의 논리, 언어, 응용 분야가 다르며, 이는 양자 컴퓨팅과 고전 컴퓨팅의 차이와 비슷합니다.\n\n고전 프로그래밍\n고전 프로그래밍은 이진 논리에 기반하며, 정보는 비트(0과 1)로 표현되고 계산은 결정론적 단계를 따릅니다. 프로그램은 CPU나 GPU와 같은 고전 하드웨어에서 실행되며, AND, OR, NOT 같은 부울 논리 게이트를 사용해 순차적이거나 병렬적으로 연산을 수행합니다. Python, C++, Java 같은 전통적인 프로그래밍 언어를 사용하며, 주어진 입력에 대해 출력은 항상 예측 가능합니다.\n고전 컴퓨터는 웹 개발부터 과학적 시뮬레이션까지 일상적인 대부분의 응용 프로그램을 처리합니다. 하지만 암호학이나 복잡한 최적화와 같이 대규모 계산이 필요한 문제에서는 한계를 보입니다.\n\n\n양자 프로그래밍\n양자 프로그래밍은 양자 역학 원리에 기반하며, 중첩 상태에 존재하고 얽힐 수 있는 큐비트를 사용해 훨씬 빠른 계산을 수행합니다. 고전 프로그램과 달리 양자 프로그램은 확률적(probabilistic)입니다. 즉, 출력은 큐비트를 반복적으로 측정해 얻어지며, 이 과정에서 큐비트는 확정된 상태로 붕괴합니다.\n양자 프로그래밍은 Qiskit(Python 기반), Quipper(Haskell 기반), Cirq 같은 특수 양자 언어를 필요로 하며, IBM Quantum이나 Google Sycamore 같은 양자 프로세서에서 작동합니다. 양자 회로는 Hadamard, CNOT, Pauli-X 등의 양자 게이트를 사용하며, 암호학, 최적화, 양자 시뮬레이션과 같은 분야에서 전례 없는 능력을 제공합니다. 다만 기술은 아직 개발 중입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "href": "posts/03_resources/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "title": "Quantum Programming",
    "section": "집에서 양자 프로그래밍: 가능할까?",
    "text": "집에서 양자 프로그래밍: 가능할까?\n과거에는 양자 프로그래밍이 복잡성과 양자 컴퓨팅 하드웨어의 접근성 문제로 인해 대부분의 개인에게 불가능해 보였을 수 있습니다. 하지만 BlueQubit의 등장으로 양자 개발은 열정가와 초보자 모두에게 현실이 되었습니다.\nBlueQubit은 누구나 언제 어디서나 양자 컴퓨팅의 힘을 경험할 수 있게 하는 고급스럽고 사용자 친화적인 플랫폼입니다. BlueQubit이 양자 컴퓨팅 입문자에게 최고의 선택인 이유 중 하나는 사용 편의성입니다. 더 나은 사용자 경험을 제공하는 데 초점을 맞춘 이 플랫폼은 기술적 세부 사항에 깊이 들어가지 않아도 양자 컴퓨터의 능력을 활용할 수 있게 합니다.\nCirq와 Qiskit 같은 오픈소스 라이브러리와 매끄럽게 통합되어 사용자는 집에서도 양자 프로그램을 실행할 수 있습니다. 이 기능은 인프라 투자 없이 양자 컴퓨팅의 잠재력을 탐구하고자 하는 개발자와 연구자에게 무한한 가능성을 열어줍니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "title": "Quantum Programming",
    "section": "양자 컴퓨팅 언어의 유형",
    "text": "양자 컴퓨팅 언어의 유형\n양자 컴퓨팅 언어는 양자 알고리즘을 프로그래밍하고 실행하는 데 각기 다른 역할을 하며 다양한 형태로 존재합니다. 여기에는 고급 양자 프로그래밍 언어, 저수준 명령어 세트, 소프트웨어 개발 키트가 포함됩니다.\n\n양자 프로그래밍 언어\n양자 프로그래밍 언어는 양자 알고리즘을 표현하고 큐비트, 양자 게이트, 측정을 제어하기 위해 설계되었습니다. 양자 프로그램 작성을 위한 고수준 추상화를 제공합니다. 고전 언어와 달리 중첩, 얽힘, 양자 병렬성과 같은 양자 특유의 연산을 지원합니다.\n예로는 Qiskit(Python 기반), Quipper(Haskell 기반), Silq(고수준 양자 언어), Q#(Microsoft의 양자 언어)가 있습니다. 이 언어들은 연구자와 개발자가 양자 응용 프로그램을 구축하고 고전 코드와 통합해 하이브리드 양자-고전 계산을 가능하게 합니다.\n\n\n양자 명령어 세트\n양자 명령어 세트는 양자 하드웨어를 직접 제어하는 저수준 명령을 정의합니다. 이는 고전 컴퓨팅의 어셈블리 언어와 비슷합니다. Hadamard, CNOT, 위상 게이트 같은 양자 연산을 위한 게이트 수준 명령을 제공하며, 서로 다른 양자 하드웨어 아키텍처에서 효율적인 실행을 보장합니다.\n예로는 OpenQASM(IBM), Quil(Rigetti), Blackbird(Xanadu)가 있습니다. 이들은 양자 알고리즘과 물리적 큐비트 간의 인터페이스 역할을 합니다.\n\n\n양자 소프트웨어 개발 키트\n양자 SDK는 양자 프로그램을 개발하고, 테스트하고, 실행하기 위한 도구, 라이브러리, 시뮬레이터를 제공합니다. 고수준 프로그래밍 언어와 양자 하드웨어 간의 간극을 메웁니다. 대표적인 SDK로는 Qiskit(IBM), Cirq(Google), PennyLane(Xanadu), Braket(AWS)이 있습니다. 이 SDK들은 양자 회로 시뮬레이션, 실제 양자 장치에서 알고리즘 실행, 기존 응용 프로그램에 양자 컴퓨팅 통합을 가능하게 해 연구와 실용적 채택을 가속화합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "href": "posts/03_resources/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "title": "Quantum Programming",
    "section": "인기 있는 양자 프로그래밍 언어와 라이브러리",
    "text": "인기 있는 양자 프로그래밍 언어와 라이브러리\n양자 시스템의 힘을 활용하기 위해 다양한 프로그래밍 언어와 라이브러리가 개발되었습니다. 이들은 양자 회로를 생성, 조작, 실행하도록 특별히 설계되었으며 고전 프로그래밍 언어와는 다릅니다. 다음은 익숙해질 만한 최고의 양자 프로그래밍 언어 목록입니다:\n\nQiskit\nQiskit은 IBM에서 만든 오픈소스 양자 컴퓨팅 프레임워크입니다. 양자 회로 설계 및 실행을 위한 사용하기 쉬운 인터페이스와 양자 시스템 시뮬레이션 및 양자 알고리즘 최적화 도구를 제공합니다. 널리 채택된 도구로, 초보자와 숙련된 개발자 모두에게 최고의 양자 프로그래밍 언어 중 하나입니다.\n\n\nCirq\nCirq는 Google Quantum AI에서 개발한 인기 있는 양자 프로그래밍 라이브러리입니다. 개발자가 시뮬레이터와 실제 양자 하드웨어에서 양자 회로를 생성, 편집, 실행할 수 있게 합니다. 사용자 친화적인 인터페이스와 강력한 기능으로 양자 프로그래밍을 탐구하려는 이들에게 최고의 선택입니다.\n\n\nPyQuil\nPyQuil은 Rigetti Computing에서 만든 독창적인 양자 명령어 언어로, 양자 프로그래밍에 독특한 접근 방식을 제공합니다. 양자 알고리즘 생성 과정을 단순화하도록 설계된 PyQuil은 Rigetti의 양자 프로세서 및 시뮬레이터와의 호환성을 유지하며 양자 응용 프로그램 개발을 간소화합니다.\n\n\nQ\nMicrosoft에서 개발한 Q#은 양자 프로그래밍을 위해 특화된 도메인별 언어입니다. Quantum Development Kit(QDK)와 통합되어 개발자가 양자 알고리즘을 고전 및 양자 하드웨어에서 작성, 테스트, 디버깅하기 쉽게 합니다. 고수준 문법과 풍부한 라이브러리로 Q#은 양자 응용 프로그램 생성을 단순화합니다.\n\n\nQasm과 OpenQasm\nQasm(Quantum Assembly Language)과 그 오픈소스 버전인 OpenQasm은 양자 회로를 위한 중급 표현입니다. 이 언어들은 양자 명령을 위한 표준 형식을 제공하여 다양한 플랫폼에서 양자 회로를 설계하고 시뮬레이션하기 쉽게 합니다. 특히 OpenQasm은 모듈성과 확장성을 지원해 복잡한 양자 프로그램을 효율적으로 작성할 수 있게 합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#마무리",
    "href": "posts/03_resources/quantum_programming/notes/00.html#마무리",
    "title": "Quantum Programming",
    "section": "마무리",
    "text": "마무리\n양자 프로그래밍은 산업을 변화시킬 엄청난 잠재력을 가진 흥미로운 분야입니다. 쇼어 알고리즘과 그로버 알고리즘 같은 핵심 알고리즘을 이해하고, Qiskit, Cirq, PyQuil, Q#, OpenQasm과 같은 인기 언어와 라이브러리를 사용하면 초보자도 자신 있게 양자 세계에 입문할 수 있습니다.\n양자 컴퓨팅 회사인 BlueQubit은 사용자 친화적인 인터페이스, 강력한 양자 시뮬레이터, 실제 양자 하드웨어 접근성을 제공하여 개발자가 양자 컴퓨팅의 힘을 활용하고 혁신을 이끌어내기에 이상적인 선택입니다. 지금 가입하고 프로그래밍을 시작하세요.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#자주-묻는-질문",
    "href": "posts/03_resources/quantum_programming/notes/00.html#자주-묻는-질문",
    "title": "Quantum Programming",
    "section": "자주 묻는 질문",
    "text": "자주 묻는 질문\n\n양자 컴퓨팅을 위한 C 언어란 무엇인가요?\nC 자체는 양자 컴퓨팅에 일반적으로 사용되지 않지만, QCOR(Quantum Computing ORchestration)은 C++의 확장으로 양자 프로그래밍과 고전 컴퓨팅을 통합합니다. 이 언어는 양자 하드웨어와 시뮬레이터와 함께 작동하도록 설계되어 개발자가 하이브리드 양자-고전 알고리즘을 효율적으로 작성할 수 있게 합니다. 그러나 오늘날 대부분의 양자 프로그래밍은 Qiskit(Python), Cirq(Python), Q#(Microsoft의 양자 언어)와 같은 고수준 언어에 의존합니다. 이는 사용 편의성과 양자 특유의 기능을 제공하기 때문입니다.\n\n\nPython은 양자 컴퓨팅에 사용되나요?\n네, Python은 Qiskit, Cirq, PennyLane과 같은 강력한 양자 컴퓨터 프로그래밍 라이브러리 덕분에 양자 컴퓨팅에 널리 사용됩니다. 이 라이브러리들은 직관적인 API, 양자 회로 시뮬레이터, 실제 양자 하드웨어에서 프로그램을 실행할 수 있는 도구를 제공합니다. Python의 유연성과 단순함은 양자 연구에 이상적이며, 양자 알고리즘을 구축, 테스트, 배포하면서 고전 계산과 통합하기 쉽게 합니다. IBM Quantum Experience와 Amazon Braket 같은 많은 양자 컴퓨팅 플랫폼도 Python 기반 프레임워크를 지원합니다.\n\n\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 무엇인가요?\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 사용 사례와 하드웨어 호환성에 따라 다릅니다. Qiskit(Python 기반)은 사용자 친화적인 인터페이스와 IBM Quantum의 강력한 지원으로 초보자와 연구자에게 널리 사용됩니다. Cirq(역시 Python 기반)는 Google의 양자 하드웨어에 최적화되어 있으며, Q#(Microsoft)는 고전 통합과 함께 양자 알고리즘 개발에 설계되었습니다.\n기타 주목할 만한 양자 컴퓨팅 프로그래밍 언어로는 Silq(고수준 양자 프로그래밍), Quipper(Haskell 기반), OpenQASM(어셈블리 스타일 양자 언어)이 있습니다. Python 기반 프레임워크가 이 분야를 지배하고 있으므로 Qiskit과 Cirq가 가장 인기 있는 선택입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html",
    "href": "posts/03_resources/quantum_programming/index.html",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#details",
    "href": "posts/03_resources/quantum_programming/index.html#details",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#tasks",
    "href": "posts/03_resources/quantum_programming/index.html#tasks",
    "title": "Quantum Programming",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#참고-자료",
    "href": "posts/03_resources/quantum_programming/index.html#참고-자료",
    "title": "Quantum Programming",
    "section": "참고 자료",
    "text": "참고 자료\n\n인프런 양자 프로그래밍 강의",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#related-posts",
    "href": "posts/03_resources/quantum_programming/index.html#related-posts",
    "title": "Quantum Programming",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론이란",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론이란",
    "title": "퍼셉트론",
    "section": "퍼셉트론이란",
    "text": "퍼셉트론이란\n다수의 신호를 입력으로 받아 하나의 신호를 출력하는 것\n\\[\ny = \\begin{cases} 0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\ 1 & (w_1x_1 + w_2x_2 &gt; \\theta) \\end{cases}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#논리-회로",
    "href": "posts/02_areas/deep_learning/notes/00.html#논리-회로",
    "title": "퍼셉트론",
    "section": "논리 회로",
    "text": "논리 회로\n파라미터 \\((w_1, w_2, θ)\\)의 값을 조정하여 AND, OR, NAND 게이트를 구현할 수 있다.\n머신 러닝의 목적은, 기계가 알아서 파라미터의 값을 적절히 조정하는 것이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론-구현",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론-구현",
    "title": "퍼셉트론",
    "section": "퍼셉트론 구현",
    "text": "퍼셉트론 구현\n\nAND 게이트\n\ndef AND(x1, x2):\n    w1, w2, theta = 0.5, 0.5, 0.7 # parameter\n    return (x1*w1 + x2*w2 &gt; theta)\n\n\nprint(AND(0, 0))\nprint(AND(1, 0))\nprint(AND(0, 1))\nprint(AND(1, 1))\n\nFalse\nFalse\nFalse\nTrue\n\n\n여기서 θ를 \\(-b\\)로 치환하고 식을 다시 정리하면 다음과 같다.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때 \\(w_1\\)과 \\(w_2\\)(가중치)는 각각의 입력신호가 결과에 주는 영향력을 조절하고, \\(b\\)(편향)은 뉴런이 얼마나 쉽게 활성화되는지를 조정한다. (가중치 합이 -b를 초과할 때만 뉴런이 활성화된다.)\n이제 재구성한 식과, numpy를 이용하여 NAND와 OR 게이트를 구현해보자.\n\n\nNAND 게이트\n\nimport numpy as np\n\ndef NAND(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([-0.5, -0.5])\n  b = 0.7\n  return (b + np.sum(x * w) &gt; 0)\n\n\ndef OR(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([0.5, 0.5])\n  b = -0.2\n  return (b + np.sum(x * w) &gt; 0)\n\n세 게이트의 차이는 오직 파라미터의 값이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론의-한계",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론의-한계",
    "title": "퍼셉트론",
    "section": "퍼셉트론의 한계",
    "text": "퍼셉트론의 한계\nAND, NAND, OR 게이트는 만들 수 있지만, XOR 게이트는 만들 수 없다. 다른 게이트들과 다르게 선형적으로 구분이 안되기 때문이다.\n하지만 AND NAND OR 게이트를 다음과 같이 배치하면 XOR 게이트를 만들 수 있다.\n\n\n\n\n\nflowchart LR\n    x1((x1)) --&gt; OR\n    x2((x2)) --&gt; OR\n    x1 --&gt; NAND\n    x2 --&gt; NAND\n    OR[OR 게이트] --&gt; AND\n    NAND[NAND 게이트] --&gt; AND\n    AND[AND 게이트] --&gt; output((XOR 출력))\n\n\n\n\n\n\n(mermaid로는 이렇게 그리는게 최선이다.)\n이와 같이 여러 퍼셉트론을 연결한 형태를 다층 퍼센트론이라고 한다.\n\nXOR 게이트\n\ndef XOR(x1, x2):\n  s1 = OR(x1, x2)\n  s2 = NAND(x1, x2)\n  return AND(s1, s2)\n\n\nprint(XOR(0, 0))\nprint(XOR(0, 1))\nprint(XOR(1, 0))\nprint(XOR(1, 1))\n\nFalse\nTrue\nTrue\nFalse",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#intro",
    "href": "posts/02_areas/42_seoul/notes/08.html#intro",
    "title": "cloud-1 개념 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n다음 학기 시작 전까지 개념공부만 하면서 시간을 보내려고 하니까 프로젝트가 하고 싶어졌습니다. 원래는 python 과제를 하려고 했는데, 이전에 cloud 과제를 진행하다가 말았던게 기억나서 이어서 해보면 괜찮겠다 생각했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "title": "cloud-1 개념 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nAWS SAA Udemy 강의\nansible terraform Udemy 강의\n\n이 강의들도 본 지 1년이 다되어가긴 하지만..과제할 때 사용한 제 배경지식이 여기서 나온거니까요. 과제를 진행하실 분들은 한번 수강해보시면 도움이 될 것 같습니다.\n\n\n\n\n\n\n이 포스팅에서 docker와 nginx, wordpress, mysql 구조에 대한 설명은 생략하겠습니다.\n전체 코드는 github repo에서 확인하실 수 있습니다.\n\n\n\n\n\nWhat is IaC?\n이 프로젝트의 목표는 IaC(Infrastructure as Code) tool을 이용하여 wordpress 사이트를 cloud에 자동으로 배포하는 것입니다.\nIaC는 인프라 구성을 코드로 관리하는 방식으로, 수동으로 리소스를 생성하고 설정하는 방식에 비해 버전 관리가 간편하고, 동일한 환경을 쉽게 재현하거나, 코드 리뷰 등의 방식으로 휴먼 에러를 줄이는 데 용이하게 사용할 수 있습니다.\n이번 프로젝트에서는 Packer, Terraform, Ansible 세 가지 IaC tool을 조합해 사용했습니다\n\n\nPacker: 인프라 생성 전, 상세 설정이 되어있는 image를 build할 수 있는 tool 입니다.\nTerraform: cloud 인프라를 생성하는 tool입니다. packer에서 생성한 ami를 사용할 수 있습니다.\nAnsible: 서버 내부의 상세 설정을 자동화합니다. 일반적인 bash script와는 다르게 멱등성 있는 설정이 가능하다는 점이 큰 장점입니다. 이때, 서버는 python이 설치되어 있어야 하고, ssh로 접근 가능해야 합니다.\n\n위의 이미지 처럼, packer로 필요한 설정이 완료된 image를 생성한 뒤, 그 이미지를 기반으로 cloud infra를 terraform으로 생성하고, 생성된 infra의 상세 설정을 ansible을 이용해서 구현해줄 것입니다.\nPacker와 Ansible은 서버 설정 자동화라는 동일한 기능을 수행하는 도구입니다. 두 도구는 각각 다양한 특징과 장단점이 있지만, 이 과제에서 알아야 하는 차이점은 아래와 같습니다.\nPacker는 임시 EC2 인스턴스를 생성하여 그 위에서 필요한 설정을 완료한 후, 해당 인스턴스를 AMI로 변환하는 방식으로 동작합니다. 이렇게 생성된 AMI는 이후 실제 인프라 구축 시 그대로 사용할 수 있습니다. 따라서 최종 목적지 서버가 SSH 접근이 제한되는 환경이더라도, 미리 필요한 모든 설정이 완료된 이미지를 사용할 수 있다는 장점이 있습니다.\n반면에 Ansible은 SSH 접근이 가능한 서버에서만 동작하지만, Packer와 달리 인프라 구축 후에 얻을 수 있는 정보(예: EC2의 IP)를 활용할 수 있습니다.\n이러한 특성을 고려하여 이 프로젝트에서는 두 도구를 상황에 맞게 조합하여 사용했습니다.\n\n\n전체적인 구조\n\n\n\n구현 aws 구조\n\n\nPublic subnet의 EC2들에 대한 ssh 접근은 관리용 컴퓨터에서만(terraform, ansible 코드가 실행되는 컴퓨터) 접근이 가능하도록 제한했고, MySQL의 데이터는 Private subnet의 EC2에 저장한 뒤 Public subnet의 EC2만 접근할 수 있도록 설정했습니다. Public subnet의 EC2는 사용자가 원하는 갯수를 설정할 수 있고, 그 갯수에 맞춰서 private subnet의 dbms EC2가 생성되도록 설계했습니다.\n실제 프로덕션 환경이라면 위와 같은 구조로는 설계하지 않습니다. 일단 EC2 머신들을 Auto Scaling Group으로 묶고, 그 앞에 Network Load Balancer를 두어 단일 엔드포인트로 관리하는 것이 좋습니다. 또한 Database는 AWS RDS를 이용하고, WordPress의 파일 시스템은 EFS나 S3를 활용해 Stateless하게 구현하는게 좋습니다.\n\n\n\n조금 더 일반적인 구조(물론 docker compose는 잘 안쓸것 같긴 합니다)\n\n\n제 구현에서는 각 서버가 독립적인 상태와 엔드포인트를 가지고 있습니다.\n그렇게 한 이유는 일단 aws free tier 서비스만으로 과제를 구현하려고 했던게 제일 크고요..(NLB는 사용할 수 없었습니다.) 나머지는 과제 제약사항 때문인데,\n\n\n\n과제 제약사항\n\n\n모든 프로세스는 컨테이너 안에서 동작해야 한다는 제약때문에, aws RDS는 사용할 수 없었습니다. 그리고 database는 public internet에서 접근할 수 없다고 해서, db는 private subnet의 ec2에서 돌아가게 설계했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#outro",
    "href": "posts/02_areas/42_seoul/notes/08.html#outro",
    "title": "cloud-1 개념 설명",
    "section": "outro",
    "text": "outro\n여기서 구현된 infra 구조는 사실 별로 근본있는 구조는 아니니까, 이것보다는 IaC 툴을 얼마나 편리하게 사용할 수 있는지에 초점을 맞춰서 봐주시길 바라고 있습니다.\n이어서 코드에 대한 설명은 다음 게시글에 포스팅하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#intro",
    "href": "posts/02_areas/42_seoul/notes/04.html#intro",
    "title": "inception-of-things part 1",
    "section": "Intro",
    "text": "Intro\n\n\n\n42 seoul outer 과제\n\n\n42 Seoul의 공통 과정을 마무리하면, 원하는 분야를 선택하여 심화 과제를 수행할 수 있습니다. 그중에서도 ’Inception-of-Things’는 인프라 관련 심화 과제로, 가장 많은 경험치를 얻을 수 있는 과제입니다.\n얼핏 보면 매우 어려운 과제처럼 느껴질 수 있지만, 개념을 확실히 이해하고 공부한다면 누구나 빠르게 완료할 수 있다고 생각합니다. 저의 경우, CKA 자격증 취득을 목표로 k8s를 공부하던 중 우연히 팀원을 구하게 되어 이 과제를 수행하게 되었습니다. 배경지식이 어느 정도 있는 상태에서 진행하다 보니, 크게 어렵지 않게 잘 마무리할 수 있었던 것 같습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#개요",
    "href": "posts/02_areas/42_seoul/notes/04.html#개요",
    "title": "inception-of-things part 1",
    "section": "개요",
    "text": "개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nCKA Udemy 강의\nArgoCD Udemy 강의\ngitlab helm 베포 Docs\nVagrant Docs\n\n\n\n\n\n\n\n전체 코드는 비공개 되어있는 상태입니다",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#개념-설명",
    "href": "posts/02_areas/42_seoul/notes/04.html#개념-설명",
    "title": "inception-of-things part 1",
    "section": "개념 설명",
    "text": "개념 설명\ncluster는 노드(컴퓨터)들의 논리적인 집합을 의미합니다. 일반적으로, 하나의 컴퓨터로 처리하기 어려운 방대한 양의 작업을 처리하기 위해 도입을 합니다.\n클러스터는 특정한 목적을 가지고 있고, 그 안의 노드들을 각자 맡은 역할을 수행합니다. (보통 클러스터 내부의 노드들을 관리하는 master, 작업을 수행하는 worker로 구분할 수 있습니다.) 이때, k8s는 분산된 노드(컴퓨터)들을 하나의 클러스터로 묶어주고, 관리해주는 도구로써 사용할 수 있습니다.\n\n\n\n\n\n\n노트\n\n\n\n컴퓨팅 능력을 확장할 목적으로 수직적 확장과 수평적 확장을 고려할 수 있습니다.\n수직적 확장은 cpu나 memmory 성능을 높여서 단일 노드의 성능을 향상시키는 것을 의미하고, 수평적 확장은 작업을 분산시킬 수 있는 여러 노드를 추가하는 것을 의미합니다.\n클러스터링은 수평적으로 확장된 컴퓨팅 리소스들을 그룹화 해주는 것을 의미합니다.\n\n\n한 가지 주의해야 하는 것은, k8s 자체는 노드를 생성(provision)해주는 도구가 아니라는 것입니다. 즉, provision 단계는 k8s clustering 이전에 진행되어야 합니다.\n\n\n\nPart 1 구조\n\n\nPart 1에서는 vagrant tool을 이용해서 master, agent 역할을 하는 두 대의 가상 머신을 local에서 provision하고, k3s를 이용해서 clustering 하는 것을 요구합니다. 참고로 k3s는 k8s의 경량화 버전입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#코드-설명",
    "href": "posts/02_areas/42_seoul/notes/04.html#코드-설명",
    "title": "inception-of-things part 1",
    "section": "코드 설명",
    "text": "코드 설명\n파일 구조는 아래와 같습니다.\np1/\n├── scripts/\n│   ├── agent.yml\n│   └── server.yml\n└── Vagrantfile\nvagrant는 local에서 가상 머신을 생성하고, provision을 할 수 있는 도구입니다. 사용자가 원하는 스펙을 Vagrantfile 이름의 파일에 정의하면, vagrant up 명령어를 통해 간단하게 가상머신을 생성할 수 있습니다.\n과제 요구사항에 맞게 spec을 정의해줍니다.\n\n\nVagrantfile\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"bento/ubuntu-24.04\"\n  config.vm.box_version = \"202404.26.0\"\n\n  config.vm.define \"hyunghkiS\" do |control|\n    control.vm.hostname = \"hyunghkiS\"\n    control.vm.network \"private_network\", ip: \"192.168.56.110\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiS\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/server.sh\"\n  end\n  config.vm.define \"hyunghkiSW\" do |control|\n    control.vm.hostname = \"hyunghkiSW\"\n    control.vm.network \"private_network\", ip: \"192.168.56.111\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiSW\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/agent.sh\"\n  end\nend\n\n저 just for evaluation 부분은 아마 과제 명세서에 ifconfig 명령어를 입력해보는 부분 때문에 추가한 것 같습니다. (사실 이 글을 쓰는 시점은 과제를 수행하고 1년이 지난 시점이라 기억이 가물가물 합니다.)\n\n\nserver.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s - server --node-ip 192.168.56.110\nK3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)\necho $K3S_TOKEN &gt; /vagrant/k3s_token # vagrant 공유 폴더에 master token 정보를 저장해주었습니다.\n\n\n\nagent.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\nK3S_TOKEN=$(cat /vagrant/k3s_token) # vagrant 공유 폴더에 저장된 master token 정보를 읽어옵니다.\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.56.110:6443 K3S_TOKEN=$K3S_TOKEN sh -s - --node-ip 192.168.56.111\n\nk3s 공식 문서를 참고해서 master와 agent를 clustering 해주는 스크립트를 작성해주었습니다. 각각 노드 안에서 로직이 실행되어, 하나는 master로, 하나는 agent로 역할을 수행하게 됩니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#outro",
    "href": "posts/02_areas/42_seoul/notes/04.html#outro",
    "title": "inception-of-things part 1",
    "section": "Outro",
    "text": "Outro\n오랜만에 해당 과제의 로직을 다시 보니까 기억이 잘 안납니다.\n남은 부분은 천천히 포스팅하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/notes/titanic/00.html#data-이해",
    "href": "posts/02_areas/kaggle/notes/titanic/00.html#data-이해",
    "title": "titanic",
    "section": "Data 이해",
    "text": "Data 이해\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('_data/train.csv')\ntest = pd.read_csv('_data/test.csv')\n\n\ntrain.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 177개\n\n\ntest.describe()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n418.000000\n418.000000\n332.000000\n418.000000\n418.000000\n417.000000\n\n\nmean\n1100.500000\n2.265550\n30.272590\n0.447368\n0.392344\n35.627188\n\n\nstd\n120.810458\n0.841838\n14.181209\n0.896760\n0.981429\n55.907576\n\n\nmin\n892.000000\n1.000000\n0.170000\n0.000000\n0.000000\n0.000000\n\n\n25%\n996.250000\n1.000000\n21.000000\n0.000000\n0.000000\n7.895800\n\n\n50%\n1100.500000\n3.000000\n27.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1204.750000\n3.000000\n39.000000\n1.000000\n0.000000\n31.500000\n\n\nmax\n1309.000000\n3.000000\n76.000000\n8.000000\n9.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 86개\nFare 결측치 1개: 이 정도는 그냥 삭제해도 될듯\n\n\nplt.figure(figsize=(15, 10))\n\n# Age 분포 확인\nplt.subplot(2, 2, 1)\nsns.boxplot(x='Survived', y='Age', data=train)\nplt.title('Age 분포 (생존 여부별)')\n\n# Fare 분포 확인\nplt.subplot(2, 2, 2)\nsns.boxplot(x='Survived', y='Fare', data=train)\nplt.title('Fare 분포 (생존 여부별)')\n\n# Pclass에 따른 Age 분포\nplt.subplot(2, 2, 3)\nsns.boxplot(x='Pclass', y='Age', data=train)\nplt.title('Age 분포 (객실 등급별)')\n\n# Pclass에 따른 Fare 분포\nplt.subplot(2, 2, 4)\nsns.boxplot(x='Pclass', y='Fare', data=train)\nplt.title('Fare 분포 (객실 등급별)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain_x = train.drop('Survived', axis=1).values\ntrain_y = train['Survived'].values\ntest_x = test.values",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle",
      "Notes",
      "Titanic",
      "titanic"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#dot-product",
    "href": "posts/02_areas/선형대수/notes/08.html#dot-product",
    "title": "vector dot product, cross product",
    "section": "Dot Product",
    "text": "Dot Product\n\n\\(v ⋅ w = \\sum_{i=1}^{n} v_i w_i\\)\n\n\nProperties\n\n\\(v ⋅ w = w ⋅ v\\)\n\\(v ⋅ (w + u) = v ⋅ w + v ⋅ u\\)\n\\(v ⋅ (c w) = c (v ⋅ w)\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#length-of-vector",
    "href": "posts/02_areas/선형대수/notes/08.html#length-of-vector",
    "title": "vector dot product, cross product",
    "section": "Length of vector",
    "text": "Length of vector\n\n\\(||v|| = \\sqrt{v ⋅ v}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#some-properties",
    "href": "posts/02_areas/선형대수/notes/08.html#some-properties",
    "title": "vector dot product, cross product",
    "section": "Some properties",
    "text": "Some properties\nfor non-zero vectors \\(v\\) and \\(w\\):\n\ncauchy-schwarz inequality\n\n\\(|v ⋅ w| ≤ ||v|| ||w||\\)\n\\(|v ⋅ w| = ||v|| ||w||\\) ⟺ \\(v = cw\\)\n\n\n\nTriangle inequality\n\n\\(||v + w|| ≤ ||v|| + ||w||\\)\n\n\n\nAngle between vectors\n\n\\(cosθ = \\frac{v ⋅ w}{||v|| ||w||}\\)\nif \\(v\\) and \\(w\\) are orthogonal, then \\(v ⋅ w = 0\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#cross-product",
    "href": "posts/02_areas/선형대수/notes/08.html#cross-product",
    "title": "vector dot product, cross product",
    "section": "Cross Product",
    "text": "Cross Product\n\nonly for 3D vectors\nget a vector that is orthogonal to both \\(v\\) and \\(w\\)\n\\(v × w = (v_2 w_3 - v_3 w_2, v_3 w_1 - v_1 w_3, v_1 w_2 - v_2 w_1)\\)\n\\(sinθ = \\frac{||v × w||}{||v|| ||w||}\\)\n\\(v × w = 0\\) ⟺ \\(v\\) and \\(w\\) are parallel\nv와 w로 이루어진 평행사변형의 넓이는 \\(||v × w||\\)이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#triple-productlagrange-identity",
    "href": "posts/02_areas/선형대수/notes/08.html#triple-productlagrange-identity",
    "title": "vector dot product, cross product",
    "section": "Triple product(lagrange identity)",
    "text": "Triple product(lagrange identity)\n\n\\(a x (b x c) = b(a ⋅ c) - c(a ⋅ b)\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "href": "posts/02_areas/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "title": "vector dot product, cross product",
    "section": "3차원의 평면에서 직선의 방정식",
    "text": "3차원의 평면에서 직선의 방정식\n\n평면에 대한 법선벡터 \\((a, b, c)\\)와 평면 위의 한 점 \\((x_p, y_p, z_p)\\)이 주어졌을 때, 평면의 방정식은 다음과 같다.\n\\(ax + by + cz = D\\), \\(D = ax_p + by_p + cz_p\\)\n평행한 평면은 a, b, c의 계수가 같은 평면이다.\n\\(\\frac{Ax_0 + By_0 + Cz_0 + D}{\\sqrt{A^2 + B^2 + C^2}}\\)은 평면과 점 \\((x_0, y_0, z_0)\\) 사이의 거리이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/09.html#reduced-row-echelon-form",
    "href": "posts/02_areas/선형대수/notes/09.html#reduced-row-echelon-form",
    "title": "가감법으로 연립방정식을 풀기 위한 행렬",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\n각 행의 선행항을 1로 만들고, 그 열의 다른 항을 0으로 만드는 방법을 행렬로 표현한 것이다.\n이때 선행항의 변수를 pivot variable이라고 하고, 다른 변수들은 free variable이라고 한다.\n관행적으로 pivot entry는 우하향으로 이동하고, zeroed out 행은 맨 아래쪽에 위치한다.\n기약행렬을 이용해 연립방정식으로 풀 수 없는 식을 행렬의 선형 결합으로 표현할 수 있다.\n\n\n혹은 식이 유효하지 않다면, 해가 없는 경우이며, 이것은 평행한 조건이 존재할 경우 발생한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "가감법으로 연립방정식을 풀기 위한 행렬"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/07.html#subspaces",
    "href": "posts/02_areas/선형대수/notes/07.html#subspaces",
    "title": "Subspaces and the basis",
    "section": "Subspaces",
    "text": "Subspaces\n\n\\(S\\) is a subset of \\(V\\).\n\nS ⊆ V\nS is a vector space\n\ninclude zero vector\nclosed under addition\nclosed under scalar multiplication",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/07.html#basis",
    "href": "posts/02_areas/선형대수/notes/07.html#basis",
    "title": "Subspaces and the basis",
    "section": "Basis",
    "text": "Basis\n\nminimum set of vectors that spans the subset\n\\(S\\) is a basis of \\(V\\) ⟺\n\nelements of \\(S\\) are linearly independent\n\\(S\\) spans \\(V\\)\n\n특정 부분집합의 basis의 linear combination으로 표현되는 모든 벡터는 유일하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#metrix-vector-product",
    "href": "posts/02_areas/선형대수/notes/10.html#metrix-vector-product",
    "title": "Null space and Column space",
    "section": "Metrix vector product",
    "text": "Metrix vector product\n\nSee \\(A\\)’s column space as a set of vectors. → \\(A\\)’s column space is the set of all linear combinations of the columns of \\(A\\).\nSee \\(B\\)’s row space as a set of vectors.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#null-space",
    "href": "posts/02_areas/선형대수/notes/10.html#null-space",
    "title": "Null space and Column space",
    "section": "Null space",
    "text": "Null space\n\n\\(N = \\{x \\in \\mathbb{R}^n | Ax = 0\\}\\)\n\n\\(N\\) is Null space of \\(A\\).\n\n\n\nN(A) = N(rref(A))\nif N(A) = {0}, then column vector of \\(A\\) is linearly independent. → column vector of \\(A\\) is not a basis for C(A) → pivot variable의 합이 free variable을 만든다. (redundant column)\ndim(N(A)) = nullity of A = # of free variables in rref(A)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#column-space",
    "href": "posts/02_areas/선형대수/notes/10.html#column-space",
    "title": "Null space and Column space",
    "section": "Column Space",
    "text": "Column Space\n\nC(A) = span(columns of A)\nrref(A)의 pivot variable의 column vector가 C(A)의 basis이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#column-space의-평면의-방정식",
    "href": "posts/02_areas/선형대수/notes/10.html#column-space의-평면의-방정식",
    "title": "Null space and Column space",
    "section": "Column Space의 평면의 방정식",
    "text": "Column Space의 평면의 방정식\n\n\\({\\rightVectorBar{b} | A\\rightVectorBar{x} = \\rightVectorBar{b} ∧ \\rightVectorBar{x} ∈ R^n}\\)\n위를 만족하는 기약행렬을 만들어, 해가 존재하도록 방정식을 구성하면 평면의 방정식을 구할 수 있다.\n혹은 column space의 basis를 구하고, 이를 이용해 평면의 방정식을 구할 수 있다.\n\\(R^n\\)을 span하는 basis의 vector는 n개이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#vector",
    "href": "posts/02_areas/선형대수/notes/01.html#vector",
    "title": "2-기초(1)",
    "section": "Vector",
    "text": "Vector\nvector는 크기와 방향을 가지고 있다.\n\nExample\n\\[\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n크기: \\(\\sqrt{9 + 4} = \\sqrt{13}\\)\n방향: \\(tan^{-1}(\\frac{2}{3})\\)\n\n크기와 방향이 같으면 같은 벡터이다.\n\n\n덧셈\n벡터의 덧셈을 기하학적으로 알아보자\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.\n\n\nScalar 배\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]\n마찬가지로 좌표평면으로 나타내는건 귀찮아서 생략하겠다.\n\n\n\n\n\n\nScalar 배를 한 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#전치-transpose",
    "href": "posts/02_areas/선형대수/notes/01.html#전치-transpose",
    "title": "2-기초(1)",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n행렬 \\(A\\)의 요소 \\(a_{ij}\\)는 A의 Transpose인 \\(A^T\\)의 \\(a_{ji}\\)가 된다. 즉, 행렬 \\(A\\)를 전치하면 diagnal(대각선 요소)를 제외한 모든 요소가 대각선을 기준으로 서로 뒤바뀐다.\n\nSymmetrix matrix: \\(A = A^T\\)인 행렬, 즉 대각선을 기준으로 값이 전부 같은 행렬 Hermitian matrix: \\((A^*)^T = A^H(conjugate transpose) = A\\)를 만족하는 행렬\n\nVector의 경우에는 Column Vector의 경우, Transpose시 Row Vector로, Row Vector의 경우도 반대로 작용한다.\n\nProperties\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\(\\color{red}{(AB)^T = B^TA^T}\\)\n\\((A^TA)^T\\)와 \\((AA^T)^T\\)의 결과는 항상 자기 자신이 된다. → Symmetrix matrix\n\\(C(A)^T = CA^T\\)\n\\(det(A^T) = det(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#inner-product-projection",
    "href": "posts/02_areas/선형대수/notes/01.html#inner-product-projection",
    "title": "2-기초(1)",
    "section": "Inner Product & Projection",
    "text": "Inner Product & Projection\n\\[\n\\underset{a}{\\begin{bmatrix}\n1 \\\\\n3\n\\end{bmatrix}} *\n\\underset{b}{\\begin{bmatrix}\n5 \\\\\n1\n\\end{bmatrix}} = 1 * 5 + 3 * 1 = 8 = a^Tb = b^Ta\n\\]\n갑자기 등장한 \\(a^Tb\\)가 의미하는건 아래와 같다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n\n||a||는 a 벡터의 크기를 의미한다.\n\n위의 식을 그림으로 표현해보자\n\n\n\n\n\n\n\n\n\n내적은 초록색 화살표와 파란색 화살표의 곱으로 표현할 수 있다.\n이는 a 벡터가 b 벡터의 방향에 대해 얼마나 투영되었는지를 나타낸다.\n두 벡터의 방향이 일치할 때 내적의 값이 가장 크고, 수직일 때 0 (안 닮음을 의미), 반대 방향일 때 가장 작은 값이 된다.\n\n단위 벡터(크기가 1인 벡터) 계산\n위의 식으로 부터 다음의 추론 과정을 통해 단위 벡터를 계산할 수 있다.\n\\(a^Ta = ||a||^2\\)\n∴ \\(||a|| = \\sqrt{a^Ta}\\)\n∴ 단위 벡터는 \\(\\frac{a}{||a||}\\) = \\(\\frac{a}{\\sqrt{a^Ta}}\\)\n\n\n정사형 벡터의 좌표 계산\n벡터의 좌표는 방향과 크기의 곱으로 표현할 수 있다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n정사형 벡터의 크기는 \\(\\frac{a^Tb}{||b||} = \\frac{a^Tb}{\\sqrt{b^Tb}}\\)\n장사형 벡터의 방향은 b의 단위 벡터와 같다.\n즉, 정사형 벡터의 좌표는 \\(\\frac{a^Tb}{\\sqrt{b^Tb}} * \\frac{b}{\\sqrt{b^Tb}} = \\frac{a^Tb}{b^Tb}b\\)\n\\(a^T\\frac{b}{\\sqrt{b^Tb}}*\\frac{b}{\\sqrt{b^Tb}}\\)로도 구할 수 있다.\n\na와 수직으로 연결되는 정사형 벡터 \\(\\hat{x}\\)\n\\((a-b\\hat{x})^Tb\\hat{x} = 0\\)\n\\(a^Tb - b^Tbb\\hat{x} = 0\\)\n\\(\\hat{x} = \\frac{a^Tb}{b^Tb}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#norm",
    "href": "posts/02_areas/선형대수/notes/01.html#norm",
    "title": "2-기초(1)",
    "section": "Norm",
    "text": "Norm\n크기를 나타내는 것(0 포함, 양 음수 scalar)\n\n2-Norm (\\(l_2\\)-norm)\n벡터의 물리적인 길이.\n\\[\na = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\n\\(||a||_2 = \\sqrt{1^2+2^2+3^2} = (|1|^{\\color{red}{2}}+|2|^{\\color{red}{2}}+|3|^{\\color{red}{2}})^{\\color{red}{\\frac{1}{2}}}\\)\n2 제곱에, \\(\\frac{1}{2}\\)여서 2-norm이다.\n\n두 벡터 사이의 거리는 두 벡터의 차이의 2-norm이다.\n\n\n\n1-Norm (\\(l_1\\)-norm)\n1 제곱에 \\(\\frac{1}{1}\\)을 계산해주면 된다.\n\\(||a||_1 = (|1|^1+|2|^1+|3|^1)^{\\frac{1}{1}}\\)\n\n\np-Norm (\\(l_p\\)-norm)\n\\(||a||_p = (|x_1|^p+|x_2|^p+|x_3|^p+...)^{\\frac{1}{p}} = (\\underset{t}{\\Sigma} |x_t|^p)^{\\frac{1}{p}} \\quad (p ≥ 1)\\)\n\n\ninfinity-Norm\n\\(||a||_∞ = \\underset{t}{max}|x_t|\\)\n1-norm, 2-norm, infinity-norm의 값이 1이 되는 모든 벡터들을 좌표평면에 나타내면 다음과 같다.\n\n\n\n\n\n\n\n\n\n같은 벡터일 때, 1-norm ≥ 2-norm ≥ ∞-norm 순으로 크다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#가우스-조던-소거법",
    "href": "posts/02_areas/선형대수/notes/03.html#가우스-조던-소거법",
    "title": "3-몰라",
    "section": "가우스 조던 소거법",
    "text": "가우스 조던 소거법\n\n선형대수의 목표는 \\(Ax = b\\)에서 x를 찾는 것이다.\n\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n이 수식을 다시 살펴보자. 위의 수식은 아래와 같이 적용할 수 있다.\n\\[\\begin{aligned}\n2x + 4y \\quad  &= 8 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 열립방정식을 풀면 \\(y = 1\\)이라는 결과를 얻는다. 다시 \\(y=1\\)을 대입해서 \\(x=2\\)라는 값을 구할 수 있다.\n이제 이를 matrix와 vector로 풀어보자.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}\n\\]\n이를 확장행렬로 표현하면 다음과 같다\n\\[\n[A|b] = \\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n2 & 5 & | & 9\n\\end{bmatrix}\n\\]\n이제 가우스 조던 소거법을 적용해보자\n적용 순서는 다음과 같다.\n\n양 변에 0이 아닌 상수배를 해준다.\n상수배를 한 행을 다른행에 더하거나 뺀다.\n행끼리 자리 바꾼다.\n\n이에 맞춰서 위의 식을 풀이하면,\n\n두 번째 행에서 첫 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n\n첫 번째 행에서 두 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 0 & | & 2 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n따라서 \\(x = 2\\), \\(y = 1\\)이라는 해를 얻을 수 있다.\n즉 가우스조던 소거법은 왼쪽을 항등행렬로 만들고, 그 오른쪽에 있는 값이 답이되는 소거법이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#역행렬-구하기",
    "href": "posts/02_areas/선형대수/notes/03.html#역행렬-구하기",
    "title": "3-몰라",
    "section": "역행렬 구하기",
    "text": "역행렬 구하기\n역행렬을 구할 수 있다면 x의 값을 쉽게 구할 수 있다. (\\(x = A^{-1}b\\))\n가우스 조던 소거법을 이용해 역행렬을 구해보자.\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\nc & d & | & 0 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & \\frac{ad-bc}{a} & | & -\\frac{c}{a} & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & 0 & | & \\frac{ad}{ad-bc} & \\frac{-ab}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 0 & | & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n∴ A^{-1} = \\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\n\ninvertible\n역행렬이 존재할 경우 invertible하다고 한다.\n\nnon singular matrix\ndet(A) ≠ 0: ad - bc(determinant) = 0인 경우 역행렬이 존재하지 않는다.\nA가 full rank이다\nN(A) = 0",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#determinant",
    "href": "posts/02_areas/선형대수/notes/03.html#determinant",
    "title": "3-몰라",
    "section": "determinant",
    "text": "determinant\n정사각행렬의 element로 scalar 값을 만드는 함\n\n3 x 3 행렬의 det\n\\[\nA=\n\\begin{bmatrix}\na & b & c\\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n\\]\n\\(det(A) = a(ei - fh) - b(di-fg)+c(dh-eg)\\)\nLaplace expansion or cofactor expansion\n\n\nproperties\n\ndet(A) = 0 이면 A is singular\nA가 rank-deficient 이면 det(A) = 0\ndiagonal or triangular matrix, det(A) = 대각요소의 곱\n항등행렬의 det=1\ndet(cA) = \\(c^ndet(A)\\) (A = nxn)\n\\(det(A^T) = det(A)\\)\ndet(AB) = det(A)det(B)\n\\(\\color{red}{det(A^{-1}) = \\frac{1}{det(A)}}\\)\n\\(\\color{red}{det(A) = λ_1λ_2,...,λ_n}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#trace",
    "href": "posts/02_areas/선형대수/notes/03.html#trace",
    "title": "3-몰라",
    "section": "Trace",
    "text": "Trace\n정사각 행렬에 대해서만 정의되는 것, diagonal 전부 더함\n\\(tr(A) = \\sum_{i=1}^{n}a_{ii}\\)\n\ntr(A + B) = tr(A) + tr(B)\ntr(cA) = ctr(A)\n\\(tr(A^T) = tr(A)\\)\ntr(AB) = tr(BA)\n\\(tr(a^Tb) = tr(ba^T)\\)\ntr(ABCD) = tr(BCDA) = tr(CDAB) = tr(DABC) (cyclic property)\n\\(tr(A) = \\sum_{i=1}^{n}\\lambda_i\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#최소자승법",
    "href": "posts/02_areas/선형대수/notes/03.html#최소자승법",
    "title": "3-몰라",
    "section": "최소자승법",
    "text": "최소자승법",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/09.html#preprocessing",
    "title": "K Nearest Neighbors",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/09.html#modeling",
    "title": "K Nearest Neighbors",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#predict",
    "href": "posts/02_areas/machine_learning/notes/09.html#predict",
    "title": "K Nearest Neighbors",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[57  5]\n [ 5 33]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/16.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/16.html#preprocessing",
    "title": "Apriori",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])\ntransactions\n\n[['shrimp',\n  'almonds',\n  'avocado',\n  'vegetables mix',\n  'green grapes',\n  'whole weat flour',\n  'yams',\n  'cottage cheese',\n  'energy drink',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'honey',\n  'salad',\n  'mineral water',\n  'salmon',\n  'antioxydant juice',\n  'frozen smoothie',\n  'spinach',\n  'olive oil'],\n ['burgers',\n  'meatballs',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chutney',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'energy bar',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'light cream',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'chicken',\n  'honey',\n  'oil',\n  'cooking oil',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'black tea',\n  'salmon',\n  'eggs',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'honey',\n  'french fries',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'eggs',\n  'chocolate',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'ham',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'shrimp',\n  'pasta',\n  'avocado',\n  'honey',\n  'white wine',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soup',\n  'avocado',\n  'milk',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'energy bar',\n  'black tea',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomato sauce',\n  'light cream',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'avocado',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'strong cheese',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'hot dogs',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'avocado',\n  'french fries',\n  'hot dogs',\n  'brownies',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'cereals',\n  'clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'black tea',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'brownies',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'frozen vegetables',\n  'tomatoes',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'chicken',\n  'gums',\n  'soda',\n  'body spray',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'mineral water',\n  'cider',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'bug spray',\n  'shallot',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'avocado',\n  'cider',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'tomatoes',\n  'tomato sauce',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'milk',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'butter',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['asparagus',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'french fries',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'energy bar',\n  'french wine',\n  'eggs',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'almonds',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'soup',\n  'escalope',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ham',\n  'frozen vegetables',\n  'pepper',\n  'oil',\n  'extra dark chocolate',\n  'tea',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'barbecue sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'energy bar',\n  'almonds',\n  'eggs',\n  'corn',\n  'mayonnaise',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'chocolate',\n  'soup',\n  'almonds',\n  'eggs',\n  'hot dogs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'chocolate',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'energy bar',\n  'pet food',\n  'carrots',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'gums',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'soup',\n  'avocado',\n  'milk',\n  'olive oil',\n  'green grapes',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'soup',\n  'cake',\n  'cooking oil',\n  'chicken',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'escalope',\n  'mushroom cream sauce',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'oil',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'eggs',\n  'french fries',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'salmon',\n  'white wine',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'olive oil',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'antioxydant juice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['carrots',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'cooking oil',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'pancakes',\n  'chicken',\n  'chili',\n  'tea',\n  'french fries',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'chocolate',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cooking oil',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'light cream',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'cake',\n  'gums',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'olive oil',\n  'eggs',\n  'rice',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'nonfat milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'soup',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'barbecue sauce',\n  'carrots',\n  'chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'frozen vegetables',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'cake',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'chicken',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'spaghetti',\n  'eggs',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'strong cheese',\n  'light cream',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'yams',\n  'milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'ground beef',\n  'soup',\n  'avocado',\n  'milk',\n  'black tea',\n  'eggs',\n  'barbecue sauce',\n  'carrots',\n  'cookies',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'french fries',\n  'strawberries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'yams',\n  'chicken',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'honey',\n  'cake',\n  'rice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'light mayo',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mineral water',\n  'corn',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'french wine',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'whole wheat rice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'milk',\n  'green tea',\n  'french fries',\n  'cookies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'whole wheat rice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'meatballs',\n  'chicken',\n  'blueberries',\n  'cooking oil',\n  'champagne',\n  'yogurt cake',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'milk',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'olive oil',\n  'french wine',\n  'eggs',\n  'french fries',\n  'champagne',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'honey',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bacon',\n  'eggs',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'tomatoes',\n  'energy bar',\n  'french wine',\n  'antioxydant juice',\n  'french fries',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'spinach',\n  'soda',\n  'energy drink',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'energy bar',\n  'chicken',\n  'eggs',\n  'cake',\n  'french fries',\n  'body spray',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'fromage blanc',\n  'bacon',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'vegetables mix',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'cereals',\n  'brownies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pancakes',\n  'cooking oil',\n  'gluten free bar',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'soup',\n  'black tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'grated cheese',\n  'tomatoes',\n  'chocolate',\n  'fromage blanc',\n  'honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'milk',\n  'olive oil',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'soup',\n  'chutney',\n  'cereals',\n  'energy drink',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'soup',\n  'avocado',\n  'salmon',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'vegetables mix',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'shrimp',\n  'pasta',\n  'mineral water',\n  'olive oil',\n  'eggs',\n  'cake',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'green grapes',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'meatballs',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'chocolate',\n  'olive oil',\n  'french wine',\n  'salmon',\n  'rice',\n  'light mayo',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'french wine',\n  'vegetables mix',\n  'rice',\n  'clothes accessories',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'dessert wine',\n  'spaghetti',\n  'chicken',\n  'cake',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'flax seed',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'energy bar',\n  'green grapes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'cake',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'salt',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salt',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'red wine',\n  'mineral water',\n  'french fries',\n  'hand protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'energy drink',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'salmon',\n  'green tea',\n  'french fries',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'pasta',\n  'tomatoes',\n  'milk',\n  'frozen smoothie',\n  'sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'olive oil',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'antioxydant juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'almonds',\n  'eggs',\n  'strawberries',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'antioxydant juice',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'oil',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'butter',\n  'chicken',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'cake',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'oil',\n  'cereals',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'spinach',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'green tea',\n  'shampoo',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'mint green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'red wine',\n  'spaghetti',\n  'mineral water',\n  'salmon',\n  'eggs',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'shallot',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'spaghetti',\n  'muffins',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'tomato juice',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'yams',\n  'mineral water',\n  'muffins',\n  'frozen smoothie',\n  'hot dogs',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'soup',\n  'milk',\n  'olive oil',\n  'salmon',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'french wine',\n  'green grapes',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'dessert wine',\n  'ground beef',\n  'soup',\n  'salmon',\n  'eggs',\n  'gums',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'carrots',\n  'french fries',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cooking oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'eggs',\n  'cake',\n  'mashed potato',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'strong cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'soup',\n  'rice',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'avocado',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'herb & pepper',\n  'ground beef',\n  'pancakes',\n  'eggs',\n  'light cream',\n  'rice',\n  'champagne',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'french wine',\n  'vegetables mix',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'yams',\n  'butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'barbecue sauce',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'french fries',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'salmon',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'eggs',\n  'green tea',\n  'frozen smoothie',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'chicken',\n  'cider',\n  'green grapes',\n  'honey',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'milk',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'protein bar',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'bacon',\n  'nonfat milk',\n  'spinach',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'spinach',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'energy bar',\n  'honey',\n  'rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'chocolate',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'strawberries',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'salmon',\n  'honey',\n  'extra dark chocolate',\n  'green tea',\n  'mushroom cream sauce',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'tomatoes',\n  'soup',\n  'butter',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'tomato juice',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'whole wheat rice',\n  'cereals',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'vegetables mix',\n  'spinach',\n  'french fries',\n  'escalope',\n  'cauliflower',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'eggs',\n  'soda',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'shallot',\n  'strawberries',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'blueberries',\n  'oil',\n  'cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'frozen smoothie',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'butter',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'salmon',\n  'pancakes',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'salmon',\n  'eggs',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['magazines',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'extra dark chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'french wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'salt',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'escalope',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'fromage blanc',\n  'whole wheat rice',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'honey',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'shrimp',\n  'mineral water',\n  'barbecue sauce',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'french fries',\n  'escalope',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'soup',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'energy drink',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'grated cheese',\n  'herb & pepper',\n  'shrimp',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'red wine',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'mushroom cream sauce',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cake',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'almonds',\n  'cake',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'frozen vegetables',\n  'milk',\n  'butter',\n  'salmon',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'whole wheat pasta',\n  'whole wheat rice',\n  'chicken',\n  'green tea',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'olive oil',\n  'salmon',\n  'green beans',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'tomato sauce',\n  'mineral water',\n  'green grapes',\n  'eggs',\n  'gums',\n  'light cream',\n  'oil',\n  'green tea',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'honey',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'oil',\n  'ketchup',\n  'chili',\n  'pet food',\n  'eggplant',\n  'green tea',\n  'escalope',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'cake',\n  'blueberries',\n  'tea',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'salmon',\n  'carrots',\n  'frozen smoothie',\n  'pasta',\n  'mashed potato',\n  'shallot',\n  'light mayo',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'olive oil',\n  'whole wheat rice',\n  'eggplant',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'eggs',\n  'ham',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'brownies',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'pancakes',\n  'french fries',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pancakes',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'honey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'tomato sauce',\n  'olive oil',\n  'chili',\n  'french fries',\n  'cookies',\n  'salt',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'bramble',\n  'frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'meatballs',\n  'butter',\n  'eggs',\n  'salad',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'chocolate',\n  'olive oil',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pepper',\n  'soup',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'pancakes',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french wine',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'bramble',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'frozen vegetables',\n  'tomatoes',\n  'cake',\n  'chicken',\n  'green tea',\n  'toothpaste',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'olive oil',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'cooking oil',\n  'cereals',\n  'protein bar',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'burger sauce',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'whole wheat rice',\n  'yogurt cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'eggs',\n  'green tea',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'herb & pepper',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'salmon',\n  'nonfat milk',\n  'green grapes',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bug spray',\n  'oatmeal',\n  'sandwich',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'almonds',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'butter',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'pepper',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'rice',\n  'cooking oil',\n  'chocolate',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'butter',\n  'chocolate',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'chutney',\n  'cooking oil',\n  'asparagus',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'champagne',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'mineral water',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'almonds',\n  'cereals',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'champagne',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'oil',\n  'tomato juice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'french wine',\n  'french fries',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green grapes',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'oil',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'cake',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomatoes',\n  'ground beef',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cider',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'cookies',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'escalope',\n  'champagne',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'escalope',\n  'shallot',\n  'protein bar',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'parmesan cheese',\n  'spaghetti',\n  'meatballs',\n  'butter',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'tomato sauce',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'french fries',\n  'asparagus',\n  'low fat yogurt',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'light cream',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'chicken',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'energy bar',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'eggs',\n  'gums',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'pancakes',\n  'chocolate',\n  'escalope',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'yams',\n  'flax seed',\n  'salmon',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soda',\n  'pet food',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'olive oil',\n  'chicken',\n  'eggs',\n  'extra dark chocolate',\n  'melons',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'ground beef',\n  'yams',\n  'milk',\n  'strong cheese',\n  'salmon',\n  'muffins',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'tea',\n  'pancakes',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'turkey',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'shallot',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'butter',\n  'cooking oil',\n  'chocolate',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'butter',\n  'eggs',\n  'green tea',\n  'cottage cheese',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'whole wheat rice',\n  'cooking oil',\n  'chocolate',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'parmesan cheese',\n  'mineral water',\n  'olive oil',\n  'bacon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green grapes',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'milk',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'chocolate',\n  'french fries',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'gums',\n  'cooking oil',\n  'frozen smoothie',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'energy bar',\n  'vegetables mix',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'nonfat milk',\n  'cooking oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'low fat yogurt',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'parmesan cheese',\n  'butter',\n  'whole wheat rice',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'avocado',\n  'milk',\n  'chicken',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'black tea',\n  'french wine',\n  'pancakes',\n  'rice',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'mineral water',\n  'eggs',\n  'cake',\n  'oil',\n  'cooking oil',\n  'toothpaste',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'olive oil',\n  'black tea',\n  'vegetables mix',\n  'cooking oil',\n  'green tea',\n  'frozen smoothie',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cider',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'energy bar',\n  'eggs',\n  'cooking oil',\n  'pancakes',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'escalope',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'rice',\n  'barbecue sauce',\n  'clothes accessories',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'green tea',\n  'french fries',\n  'sandwich',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'salmon',\n  'green tea',\n  'cookies',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'soup',\n  'salmon',\n  'pancakes',\n  'chicken',\n  'mint green tea',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'almonds',\n  'french wine',\n  'yogurt cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'nonfat milk',\n  'eggs',\n  'gums',\n  'chocolate',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'ground beef',\n  'olive oil',\n  'energy bar',\n  'chicken',\n  'brownies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'cooking oil',\n  'frozen smoothie',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['vegetables mix',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'escalope',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'french wine',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'herb & pepper',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'almonds',\n  'muffins',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'salmon',\n  'antioxydant juice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'ground beef',\n  'soup',\n  'energy bar',\n  ' asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'butter',\n  'pancakes',\n  'french fries',\n  'frozen smoothie',\n  'white wine',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'frozen vegetables',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'shrimp',\n  'frozen vegetables',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'olive oil',\n  'almonds',\n  'cake',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'almonds',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'pet food',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'olive oil',\n  'energy bar',\n  'light cream',\n  'oil',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'mineral water',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bramble',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pepper',\n  'milk',\n  'eggs',\n  'green tea',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'butter',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'vegetables mix',\n  'eggs',\n  'whole wheat rice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'burger sauce',\n  'escalope',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'body spray',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'whole wheat pasta',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'eggs',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'black tea',\n  'french wine',\n  'cider',\n  'chutney',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'chicken',\n  'light cream',\n  'green tea',\n  'chocolate',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'blueberries',\n  'cooking oil',\n  'chocolate',\n  'cookies',\n  'champagne',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'salmon',\n  'muffins',\n  'chocolate',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'herb & pepper',\n  'parmesan cheese',\n  'milk',\n  'olive oil',\n  'pancakes',\n  'honey',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'butter',\n  'whole wheat rice',\n  'french fries',\n  'escalope',\n  'cookies',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'champagne',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'chicken',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'carrots',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'honey',\n  'green beans',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'light cream',\n  'rice',\n  'oil',\n  'chicken',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'mineral water',\n  'milk',\n  'eggs',\n  'low fat yogurt',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'tomato sauce',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'protein bar',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'honey',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'parmesan cheese',\n  'french wine',\n  'french fries',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'milk',\n  'olive oil',\n  'muffins',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'shrimp',\n  'spaghetti',\n  'olive oil',\n  'honey',\n  'strawberries',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'pancakes',\n  'light cream',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'avocado',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'burgers',\n  'almonds',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'clothes accessories',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'honey',\n  'whole wheat rice',\n  'green beans',\n  'french fries',\n  'frozen smoothie',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'almonds',\n  'strong cheese',\n  'cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'rice',\n  'eggplant',\n  'hand protein bar',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'nonfat milk',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'cookies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'bug spray',\n  'french fries',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'milk',\n  'cider',\n  'chicken',\n  'green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'chicken',\n  'salmon',\n  'pancakes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'chocolate',\n  'grated cheese',\n  'frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'butter',\n  'bacon',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'pepper',\n  'chocolate',\n  'fromage blanc',\n  'eggs',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'ground beef',\n  'avocado',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'green tea',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'avocado',\n  'bacon',\n  'oil',\n  'french fries',\n  'brownies',\n  'pancakes',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'chicken',\n  'cake',\n  'burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cooking oil',\n  'tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'soup',\n  'vegetables mix',\n  'pancakes',\n  'body spray',\n  'melons',\n  'protein bar',\n  'asparagus',\n  'mayonnaise',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'muffins',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'milk',\n  'gums',\n  'light cream',\n  'cooking oil',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'pancakes',\n  'french fries',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'tomato sauce',\n  'milk',\n  'extra dark chocolate',\n  'melons',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'chicken',\n  'chocolate',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'frozen smoothie',\n  'sandwich',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'frozen smoothie',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'rice',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'cream',\n  'black tea',\n  'almonds',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'parmesan cheese',\n  'ground beef',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'pancakes',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'soda',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'chicken',\n  'rice',\n  'oil',\n  'cooking oil',\n  'hot dogs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'chocolate',\n  'champagne',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'yogurt cake',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'olive oil',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'extra dark chocolate',\n  'cereals',\n  'french fries',\n  'frozen smoothie',\n  'pancakes',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'soup',\n  'avocado',\n  'milk',\n  'yogurt cake',\n  'energy drink',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'chocolate',\n  'ground beef',\n  'mineral water',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'olive oil',\n  'rice',\n  'antioxydant juice',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomato sauce',\n  'spaghetti',\n  'milk',\n  'pancakes',\n  'energy drink',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'strong cheese',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'cider',\n  'cooking oil',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'fromage blanc',\n  'vegetables mix',\n  'pancakes',\n  'honey',\n  'hot dogs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'milk',\n  'butter',\n  'black tea',\n  'eggs',\n  'frozen smoothie',\n  'light mayo',\n  'shampoo',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'cooking oil',\n  'frozen smoothie',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'blueberries',\n  'soda',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'ground beef',\n  'soup',\n  'milk',\n  'butter',\n  'honey',\n  'cake',\n  'mint green tea',\n  'brownies',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'napkins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'energy bar',\n  'butter',\n  'almonds',\n  'fromage blanc',\n  'eggs',\n  'cake',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'honey',\n  'pasta',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'milk',\n  'french wine',\n  'muffins',\n  'pancakes',\n  'champagne',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'cake',\n  'rice',\n  'cereals',\n  'french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'strong cheese',\n  'cereals',\n  'escalope',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'fresh tuna',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'cottage cheese',\n  'energy drink',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'honey',\n  'extra dark chocolate',\n  'carrots',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cottage cheese',\n  'strawberries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'avocado',\n  'cake',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'chocolate bread',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy bar',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'champagne',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'butter',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'green tea',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomato sauce',\n  'milk',\n  'butter',\n  'bacon',\n  'salmon',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'hot dogs',\n  'melons',\n  'protein bar',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'burger sauce',\n  'brownies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'pancakes',\n  'whole wheat rice',\n  'green tea',\n  'french fries',\n  'cookies',\n  'shallot',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'butter',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'honey',\n  'whole wheat rice',\n  'cooking oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'chili',\n  'pet food',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'butter',\n  'chicken',\n  'salt',\n  'mayonnaise',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'fromage blanc',\n  'honey',\n  'gums',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'sparkling water',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'butter',\n  'vegetables mix',\n  'green grapes',\n  'pancakes',\n  'eggs',\n  'cake',\n  'barbecue sauce',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'yams',\n  'eggs',\n  'burger sauce',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'eggs',\n  'chocolate',\n  'champagne',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cereals',\n  'salt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'chocolate',\n  'french fries',\n  'escalope',\n  'cookies',\n  'brownies',\n  'pancakes',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'yams',\n  'chicken',\n  'cooking oil',\n  'chocolate',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'bug spray',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'oil',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'chicken',\n  'fromage blanc',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'champagne',\n  'cookies',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'milk',\n  'butter',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'chicken',\n  'eggs',\n  'frozen smoothie',\n  'cauliflower',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'milk',\n  'energy bar',\n  'almonds',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'milk',\n  'cake',\n  'cooking oil',\n  'french fries',\n  'escalope',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'whole wheat rice',\n  'cake',\n  'frozen smoothie',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'red wine',\n  'pepper',\n  'spaghetti',\n  'fromage blanc',\n  'salmon',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'soup',\n  'chocolate',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'vegetables mix',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'parmesan cheese',\n  'cider',\n  'muffins',\n  'spinach',\n  'honey',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'mushroom cream sauce',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'almonds',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'butter',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'french fries',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'whole wheat rice',\n  'eggplant',\n  'chocolate',\n  'cookies',\n  'shallot',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'blueberries',\n  'light cream',\n  'rice',\n  'green tea',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'bacon',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'fromage blanc',\n  'ketchup',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'french fries',\n  'salt',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'yogurt cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'meatballs',\n  'milk',\n  'olive oil',\n  'chicken',\n  'honey',\n  'frozen smoothie',\n  'escalope',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'eggs',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'olive oil',\n  'blueberries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'oil',\n  'carrots',\n  'hand protein bar',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'chicken',\n  'fromage blanc',\n  'salmon',\n  'eggs',\n  'cake',\n  'frozen smoothie',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'butter',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'salmon',\n  'oil',\n  'cooking oil',\n  'frozen smoothie',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'escalope',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'carrots',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'soup',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cooking oil',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'olive oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'pancakes',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chicken',\n  'french fries',\n  'cottage cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'mineral water',\n  'muffins',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'spaghetti',\n  'milk',\n  'cider',\n  'eggs',\n  'honey',\n  'cake',\n  'green tea',\n  'french fries',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ...]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/16.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/16.html#modeling",
    "title": "Apriori",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'light cream', 'chicken'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'ground beef', 'herb & pepper'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'light cream', 'olive oil'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'shrimp', 'pasta'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts       = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Lift')\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/15.html#preprocessing",
    "title": "hierarchical clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/15.html#modeling",
    "title": "hierarchical clustering",
    "section": "Modeling",
    "text": "Modeling\n\nimport scipy.cluster.hierarchy as sch\n\ndendogram = sch.dendrogram(sch.linkage(x, method='ward'))\nplt.title('dendogram')\nplt.xlabel('Customers')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\nyh = hc.fit_predict(x)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/15.html#visualize",
    "title": "hierarchical clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[yh == 0, 0], x[yh == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[yh == 1, 0], x[yh == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[yh == 2, 0], x[yh == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[yh == 3, 0], x[yh == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[yh == 4, 0], x[yh == 4, 1], c='cyan', label='Cluster 5')\nplt.legend()\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/03.html#preprocessing",
    "title": "Multiple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/03.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# model automatically avoid dummy variable trap\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.2)\n\n# in multiple linear regression, we don't need to apply feature scaling",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/03.html#modeling",
    "title": "Multiple Linear Regression",
    "section": "modeling",
    "text": "modeling\n\nfrom sklearn.linear_model import LinearRegression\n\n# model automatically choose best model (dont need to apply 후진제거법)\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#predict",
    "href": "posts/02_areas/machine_learning/notes/03.html#predict",
    "title": "Multiple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), \n                      y_test.reshape(len(y_test), 1)), 1))\n\n[[ 93771.16 101004.64]\n [ 74466.6   78239.91]\n [ 50652.19  69758.98]\n [105956.49  99937.59]\n [145441.49 129917.04]\n [ 37211.29  64926.08]\n [ 71764.09  71498.49]\n [111249.19 108552.04]\n [ 59299.22  65200.33]\n [120840.79 118474.03]\n [162627.18 149759.96]\n [120230.67 126992.93]\n [166842.52 156991.12]\n [119593.28 108733.99]\n [123155.58 110352.25]\n [170183.11 155752.6 ]\n [198552.13 191050.39]\n [157465.39 132602.65]\n [ 98810.02  97427.84]\n [195490.18 192261.83]\n [142926.02 144259.4 ]\n [106210.25  96778.92]\n [ 89245.08  97483.56]\n [ 45068.42  14681.4 ]\n [ 41277.39  42559.73]\n [115735.66 105733.54]\n [176218.16 182901.99]\n [137986.12 141585.52]\n [ 88264.34  96479.51]\n [ 96578.58  89949.14]\n [ 82567.67  77798.83]\n [196724.5  191792.06]\n [111704.   111313.02]\n [102560.86 103282.38]\n [139310.55 124266.9 ]\n [ 87261.82  81005.76]\n [ 86587.2   96712.8 ]\n [107978.65 122776.86]\n [130333.68 134307.35]\n [188346.5  166187.94]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#evaluate",
    "href": "posts/02_areas/machine_learning/notes/03.html#evaluate",
    "title": "Multiple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.913139010635797",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/00.html#machine-learning-process",
    "href": "posts/02_areas/machine_learning/notes/00.html#machine-learning-process",
    "title": "overview",
    "section": "Machine Learning Process",
    "text": "Machine Learning Process\n\nData Pre-Processing\n\nimport data\nclean data\nsplit data trainig and testing\nfeature scailing\n\nnormalization: \\(\\frac{x - min(x)}{max(x) - min(x)}\\)\nstandardization: \\(\\frac{x - μ}{σ}\\)\n\n\nModeling\n\nbuild / train model\nmake predictions\n\nEvaluation\n\ncalculate performance metrix\nmake a verdict",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "overview"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/12.html#preprocessing",
    "title": "Decision Tree Classification",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/12.html#modeling---linear",
    "title": "Decision Tree Classification",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy')",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#predict",
    "href": "posts/02_areas/machine_learning/notes/12.html#predict",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[56  8]\n [ 8 28]]\n\n\n0.84",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/12.html#predict-1",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[56  8]\n [ 8 28]]\n\n\n0.84",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/10.html#preprocessing",
    "title": "Support Vector Machine",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/10.html#modeling---linear",
    "title": "Support Vector Machine",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#predict",
    "href": "posts/02_areas/machine_learning/notes/10.html#predict",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 4 35]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#modeling---non-linear",
    "href": "posts/02_areas/machine_learning/notes/10.html#modeling---non-linear",
    "title": "Support Vector Machine",
    "section": "Modeling - non-linear",
    "text": "Modeling - non-linear\n\nclassifier = SVC(kernel='rbf')\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/10.html#predict-1",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 4 35]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/05.html#preprocessing",
    "title": "Support Vector Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\ny = y.reshape(len(y), 1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#train",
    "href": "posts/02_areas/machine_learning/notes/05.html#train",
    "title": "Support Vector Regression",
    "section": "Train",
    "text": "Train\n\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='rbf')\nregressor.fit(x, y)\n\n/home/cryscham123/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n\n\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/05.html#visualize",
    "title": "Support Vector Regression",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(sc_x.inverse_transform(x), sc_y.inverse_transform(regressor.predict(x).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#high-resolution",
    "href": "posts/02_areas/machine_learning/notes/05.html#high-resolution",
    "title": "Support Vector Regression",
    "section": "High resolution",
    "text": "High resolution\n\nx_grid = np.arange(min(sc_x.inverse_transform(x)), max(sc_x.inverse_transform(x)), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(x_grid, sc_y.inverse_transform(regressor.predict(sc_x.transform(x_grid)).reshape(-1, 1)))\nplt.show()\n\n/tmp/ipykernel_12503/1939094151.py:1: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/07.html#preprocessing",
    "title": "random forest",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/07.html#modeling",
    "title": "random forest",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\n\n# 모델 학습\nregressor.fit(x, y)\n\nRandomForestRegressor(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=10, random_state=0)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#visualization",
    "href": "posts/02_areas/machine_learning/notes/07.html#visualization",
    "title": "random forest",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/06.html#preprocessing",
    "title": "Decision Tree Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/06.html#modeling",
    "title": "Decision Tree Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor()\n\n# 모델 학습\nregressor.fit(x, y)\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#visualization",
    "href": "posts/02_areas/machine_learning/notes/06.html#visualization",
    "title": "Decision Tree Regression",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#dag-skeleton",
    "href": "posts/02_areas/air_flow/notes/01.html#dag-skeleton",
    "title": "Coding pipeline",
    "section": "DAG skeleton",
    "text": "DAG skeleton\n\nfrom airflow import DAG\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='example_dag',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    pass\n\n\nDAG는 start_date / last_execution time + schedule_interval에 실행된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#operator",
    "href": "posts/02_areas/air_flow/notes/01.html#operator",
    "title": "Coding pipeline",
    "section": "Operator",
    "text": "Operator\n\noperator 하나 당 하나의 task만 실행하는게 좋다.\n\n\noperator type\n\nAction operators\n\nBashOperator\nPythonOperator\n\nTransfer operators\nSensor operators",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#providers",
    "href": "posts/02_areas/air_flow/notes/01.html#providers",
    "title": "Coding pipeline",
    "section": "Providers",
    "text": "Providers\n\nAirflow providers are a set of packages that contain operators, sensors, hooks, and other utilities to interact with external platforms and services.\nProviders are installed separately from Airflow and can be added to your environment as needed.\nIn Airflow core, Bash and Python operators, … are included\n\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nwith DAG(\n    dag_id='example_db',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    create_table = PostgresOperator(\n        task_id='create_table',\n        postgres_conn_id='postgres',\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS example_table (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(50)\n            );\n        \"\"\",\n    )\n\nDB에 접속하기 위해서 connection을 설정해야 한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#hook",
    "href": "posts/02_areas/air_flow/notes/01.html#hook",
    "title": "Coding pipeline",
    "section": "Hook",
    "text": "Hook",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html",
    "href": "posts/02_areas/air_flow/index.html",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#details",
    "href": "posts/02_areas/air_flow/index.html#details",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#tasks",
    "href": "posts/02_areas/air_flow/index.html#tasks",
    "title": "AirFlow",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#참고-자료",
    "href": "posts/02_areas/air_flow/index.html#참고-자료",
    "title": "AirFlow",
    "section": "참고 자료",
    "text": "참고 자료\n\nUdemy 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#related-posts",
    "href": "posts/02_areas/air_flow/index.html#related-posts",
    "title": "AirFlow",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html",
    "href": "posts/04_archives/adp_필기/notes/16.html",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "",
    "text": "시험 결과",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#intro",
    "href": "posts/04_archives/adp_필기/notes/16.html#intro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Intro",
    "text": "Intro\nadp 필기시험 34회차 결과가 나왔습니다. 당연히 합격했고요. 제가 말했죠? 서술형을 몰라도 객관식을 70개 이상 맞추면 합격이라고요. 저는 69개를 맞췄지만 말입니다. 하하.\n..사실 뭐 딱히 자랑스러운 결과는 아니긴 합니다. 서술형 1.5점 받은 주제에 데이터 분석 전문가라고 말 할 수 있을까요? 하지만 제가 합격한건 오직 필기. 전 아직 데이터 분석 전문가가 아닙니다. (제목을 봐주세요. 데이터 분석 전문가(진)입니다.)\n처음 adp 시험을 준비하려고 생각한 것은 데이터 분석 분야에 입문을 하기 위해서였기 때문에, 그런 측면에서 저의 목적은 달성했다고 생각합니다. 실기 시험은 더 구체적인 통계론이나 머신러닝 로직 등을 꽤 오랜시간 자세히 공부해서 치루려고 합니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#리뷰",
    "href": "posts/04_archives/adp_필기/notes/16.html#리뷰",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "리뷰",
    "text": "리뷰\n생각보다 시험에서 다루는 내용의 범위가 넓었습니다. 암기식 공부가 주가되는 자격증 시험의 특성상, 이는 굉장히 힘든 일이 아닐 수 없었습니다. 그래도 데이터 분석 분야의 내용들을 포괄적으로 다루다 보니, 확실히 입문을 하기에는 괜찮은 resource라고 생각합니다.\n그래도 몇 가지 아쉬운 점을 꼽자면, 2 단원 데이터 처리 기술들이 조금 old한 내용들을 다루는게 아닌가 하는 생각이 듭니다. 또, 5 단원 마지막 부분의 데이터 시각화 tool을 구체적으로 다루는 부분은 이걸 꼭 다 외워야하나 싶은 느낌이 들었고요. (근데 이번 시험에서 이 부분은 안 나왔습니다.) 사실 이 정도만 아는 것으로도 앞으로 학습을 이어나갈 수 있는 좋은 발판이 되어준다고 생각합니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#outro",
    "href": "posts/04_archives/adp_필기/notes/16.html#outro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Outro",
    "text": "Outro\n이제 실기 시험을 준비해야죠. 올해 볼까.. 내년에 볼까.. 고민입니다. 처음 목표는 졸업 전 까지 adp 자격증을 따는 거였고, 졸업까지 아직 2년이 남았으니까.. 여유롭게 준비해보려 합니다.\n그럼 저는 adp 실기 준비로 다시 돌아오겠습니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html",
    "href": "posts/04_archives/adp_필기/notes/15.html",
    "title": "시험을 보고 왔습니다.",
    "section": "",
    "text": "높게 솟은 수원 공고의 모습",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#intro",
    "href": "posts/04_archives/adp_필기/notes/15.html#intro",
    "title": "시험을 보고 왔습니다.",
    "section": "Intro",
    "text": "Intro\n수원 공고에서 진행된 adp 시험을 보고 왔습니다.\n고등학교 치고는 왠만한 대규모 성당급으로 상당히 넓다는 느낌이 들었습니다. 그냥 제가 나온 인문계 고등학교가 좁아서 그렇게 느껴진 것일 수도 있고요.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#시험",
    "href": "posts/04_archives/adp_필기/notes/15.html#시험",
    "title": "시험을 보고 왔습니다.",
    "section": "시험",
    "text": "시험\n저번 시험에서는 서술형에서 20점 만점에 4점을 받아서 굉장히 아쉬운 성적이 나와버렸죠. 사실 그렇다고 이번 시험에서 서술형을 엄청 잘 준비하거나 하진 않았습니다. 제가 게을러서 그렇다는걸 딱히 부정하는건 아니지만, 가장 큰 이유는 뭐가 나올지 예상을 할 수 없기 때문입니다.\n서술형에 나올 수 있는 주제 자체는 참고서에 나와있는 내용 안에서 등장하지만, 시험에서는 더 구체적인 수식과 구현 과정을 요구하는 경우가 많습니다. 아마도 adp 시험에서 서술형을 더 잘 대비하려면, 더 많은 외부 자료를 참고하거나, 배경지식을 더 쌓은 상태에서 도전해봐야 할 것 같습니다.\n하지만, 만약 서술형이 0점이 나온다고 해도 객관식을 80 문제중 70개 이상만 맞춘다면 통과할 수 있다는 사실. 뭐.. 그렇게까지 불가능한 것도 아니긴 합니다. 객관식은 참고서 내용으로 잘 커버할 수 있으니까요.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#outro",
    "href": "posts/04_archives/adp_필기/notes/15.html#outro",
    "title": "시험을 보고 왔습니다.",
    "section": "Outro",
    "text": "Outro\n실제로 꽤 느낌이 좋긴 합니다. 이번에도 서술형은 모르는 내용이 나오긴 했지만 말입니다.\n결과가 3월 중순에 나올 예정인데, 그 전까지는 smart contract 쪽으로 공부를 해볼 예정입니다.\n더 자세한 감상은 결과가 나온 후에 작성해보겠습니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/12.html#텍스트-마이닝",
    "href": "posts/04_archives/adp_필기/notes/12.html#텍스트-마이닝",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "텍스트 마이닝",
    "text": "텍스트 마이닝\n\n비정형 데이터를 구조화해서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정\n\n\n기능\n\n목표 기능: 문서 분류, 군집, 정보 추출, 문서 요약\n사용 기술: 자연어 처리, 컴퓨터 언어학\n\n\n\n과정\n\n\n텍스트 수집\n텍스트 전처리\n\ntm 패키지: 문서를 Corpus 객체로 변환해서 관리\n\nVCorpus: 문서를 Corpus로 변환해서 메모리에 저장\nPCorpus: 문서를 Corpus로 변환해서 디스크에 저장\nDirSource, DataframeSource, VectorSource: 데이터 소스 지정\ntm_map(x, FUN): x에 FUN을 적용\nDocumentTermMatrix: 문서-단어 빈도표 생성\nTermDocumentMatrix: 단어-문서 빈도표 생성\n\n전처리\n\n정제: 노이즈 제거\n토큰화\n\n단어 토큰화\n어절 토큰화\n형태소 토큰화\n품사 태깅\n\n불용어 처리: 불필요한 토큰 제거\n정제 / 정규화\n\n표기가 다른 같은 단어 통일\n대소문자 통일\n불필요한 단어 제거\n정규표현식으로 특수문자 제거\n\n어간 / 어근 추출\n텍스트 인코딩\n\none hot 인코딩\n말뭉치(BoW): 단어의 빈도수를 벡터로 표현\nTF-IDF: 문서 내 단어의 빈도 수 / 단어가 등장한 문서 수\n워드 임베딩\n\n\n텍스트 분석\n\n토픽 모델링\n감성 분석\n텍스트 분류\n텍스트 군집화\n\n텍스트 시각화\n\n워드 클라우드\n의미 연결망 분석\n\n\n\n\n\n정보 검색의 적절성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/12.html#사회연결망-분석",
    "href": "posts/04_archives/adp_필기/notes/12.html#사회연결망-분석",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "사회연결망 분석",
    "text": "사회연결망 분석\n\n2. 기법\n\n개인을 노드, 관계를 엣지로 해서 그래프 생성\n아래의 기준에 따라 구조 파악",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "href": "posts/04_archives/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "시각화 인사이트 프로세스의 의미",
    "text": "시각화 인사이트 프로세스의 의미\n\n1. 인사이트란 무엇인가\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해해야 한다.\n이를 위해 시각화 인사이트 방법이 필요하다.\n\n\n\nDIKW 피라미드와 시각화 관계\n\n\n\n\n2. 시각화와 인사이트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#탐색",
    "href": "posts/04_archives/adp_필기/notes/13.html#탐색",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "탐색",
    "text": "탐색\n\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해하는 과정\n객관적인 패턴을 찾는 용도\n\n\n1. 사용 가능한 데이터 확인\n\n데이터 접근\n\n이벤트 기록으로서 접근: 데이터로부터 통찰을 이끌어 내기 위해서 데이터 생성 원리를 파악해야 한다고 간주\n객체지향 관점에서의 접근: 데이터로부터 통찰을 이끌어 내기 위해서 전체 구조를 파악해야 한다고 간주\n\n데이터 명세화\n\n모든 데이터는 하나 이상의 차원과 측정값을 가지고 있다.\n이는 분석 형태에 따라, 차원이 될 수도 있고, 측정값이 될 수도 있다.\n\n\n\n\n2. 연결 고리의 확인\n데이터 명세서를 이용해 2개 이상의 데이터간 연결 고리를 확인해 봄\n\n공통 요소 찾기\n공통 요소로 변환하기: 데이터 타입이 달라도 공통 요소로 묶을 수 있다 (더 자세한 데이터를 덜 자세한 데이터로 변환. 반대는 불가)\n\n시간 데이터의 변환\n공간 데이터의 변환(지오코딩, 코로플레스 지도, X-Ray Map 사용 가능)\n계층 관계 변환: 상위 수준(덜 자세한)이라는 공통 요소로 변환. replace, lookup, vlookup 함수 사용 가능\n\n탐색 범위 설정: 차원과 측정값의 전체 조합 종류가 탐색 범위가 됨. 데이터를 구성하는 항목이 늘어날 수록 탐색 범위가 늘어남\n\n여러 데이터를 보유한 경우, 개별 데이터 안에서 먼저 탐색\n측정값 하나의 차원만 연결해 탐색\n같은 데이터 안에서 차원과 측정값을 맞바꾸면 다른 통찰을 얻을 수 있음\n어떤 통찰을 얻기 위해 비주얼 인사이트 프로세스를 사용하는 것인지 살펴본 후, 목표와 관련 있을 법한 조합을 만듦\n상식적으로 의미나 연계성이 없는 조합은 배제\n\n\n\n\n3. 관계의 탐색\n상관관계와 인과관계를 탐색\n\n이상값 처리: 시각화 도구를 통해 전체 구조를 파악한 후 처리\n차원과 측정값 유형에 따른 관계 파악 시각화\n\n시각화 도구 선정\n시간 데이터에서의 관계 파악: 구글 모션차트 사용 가능\n공간 데이터에서의 관계 파악: Arc GIS, X-Ray Map, 파워 맵 사용 가능\n비정형 데이터에서의 관계 파악\n\n워들: 주어진 텍스트에서 형태소 단위를 추출(NLP)해 빈도에 따라 시각화\n\n\n잘라보고 달리보기: 둘 이상의 차원과 측정값으로 이루어진 데이터를 여러 관점으로 살펴본다.\n\n잘라보기(slice): ex) 연령별, 성별 평균 체중 데이터 → 20세 이상, 40세 미만 남성들의 체중 패턴\n달리보기(dice): ex) 연령별, 성별 평균 체중 데이터 → 남성의 연령별 체중 패턴, 여성의 연령별 체중 패턴\nMS excel의 pivot, powerview, spreadsheet의 pivot table report 사용 가능\n\n내려다보고 올려보기\n\n내려다보기(Drill Down): 데이터를 하위 계층으로 세분화한다.\n올려보기(Reverse Driil Down): 데이터를 상위 계층으로 통합한다.\nTree map, Hyperbolic Tree\n\n척도의 조정: 스파크라인 차트 사용 가능",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#분석",
    "href": "posts/04_archives/adp_필기/notes/13.html#분석",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "분석",
    "text": "분석\n\n탐색을 통해 발견된 패턴을 분석하는 과정\n\n\n1. 분석 대상의 구체화\n\n2차 탐색: 관계들의 분석 우선순위 결정. 궁극적인 목표는 그냥 다시 한 번 더 검토하는 것\n분석 목표에 따른 분석 기법\n\n \n\n\n2. 분석 시각화 도구\n통계적 도구와 시각적 도구는 상호보완 관계\n\n\n3. 지표 설정과 분석\n\n지표: 어떤 현상의 강도를 평가하는 기준이 되는 수치\n\nex) KPI(Key Performance Indicator): 핵심 성과 지표. 목표 달성을 위한 세부적인 활동 결과물의 추진 정도나 수준을 측정하고 평가\n주로 함수식 구조를 가짐 (ex. 매출액 = 판매단가 * 판매량)\n요인 분석(factor analysis)를 통해 지표가 다른 요인과 설명력이 겹치는지 여부 확인할 수 있다.\n어떤 변화요인에 의해 지표의 흐름에 영향을 미쳤는지 파악하기 어렵다는 단점이 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#활용",
    "href": "posts/04_archives/adp_필기/notes/13.html#활용",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "활용",
    "text": "활용\n\n도출한 인사이트를 활용하는 과정\n\n\n1. 내부에서 적용\n\n기존 문제 해결 방식이나 설명 모델의 수정\n새로운 문제 해결 방식의 도입\n새롭게 발견한 가능성에 대한 구체적인 탐색과 발전\n\n\n\n2. 외부에 대한 설명, 설득과 시각화 도구\n설득이 필요하기 때문에 스토리텔링이 감미된 시각화 자료나, 인터렉티브 인포그래픽 활용\n\n\n3. 인사이트의 발전과 확장\n계속 잘 검토해 나가야함",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#통계분석의-이해",
    "href": "posts/04_archives/adp_필기/notes/10.html#통계분석의-이해",
    "title": "4 - 통계분석",
    "section": "통계분석의 이해",
    "text": "통계분석의 이해\n\n1. 표본 추출 방법\n\n단순랜덤 추출법\n계통추출법: k개씩 띄어서 랜덤으로 추출\n집락 추출법: 군집을 나눈 후, 군집 안에서 단순랜덤 추출\n층화 추출법: 이질적인 모집단에서, 비슷한 특성을 가진 층을 나눈 후, 각 층에서 단순랜덤 추출\n\n\n\n2. 척도\n\n명목척도\n순서척도\n구간척도: 더하기, 빼기 가능. 곱셈 나눗셈 불가능\n비율척도: 절대적 기준인 0이 존재, 사칙연산 가능\n\n\n\n3. 비모수 검정\n\n모집단에 대한 가정이 없이, 서열관계나 차이를 검정하는 방법\n분포의 형태가 동일하다, 동일하지 않다로 가정\n관측값들의 순위나 차이의 부호에 의존\n\n\n\n\n비모수 검정 예시",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#기초-통계분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#기초-통계분석",
    "title": "4 - 통계분석",
    "section": "기초 통계분석",
    "text": "기초 통계분석\n\n\n\n상관 분석 유형",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#통계분석의-방법론",
    "href": "posts/04_archives/adp_필기/notes/10.html#통계분석의-방법론",
    "title": "4 - 통계분석",
    "section": "통계분석의 방법론",
    "text": "통계분석의 방법론\n\nt 검정\n\n일표본\n대응표본\n독립표본\n\nANOVA\n\n일원분산분석\n이원분산분석\n다원분산분석\n\n다변량분석\n실험계획법\n\n요인배치법\n분할법\n교락법\n난괴법\n\n교차분석\n\n적합성 검정: k개의 범주들에 대한 관측값 갯수가 기댓값과 일치하는지 검정\n\n자유도: k-1\n각 집단의 \\(\\frac{(관측도수 - 기대도수)^2}{기대도수}\\)의 합이 카이제곱 분포를 따름\n\n독립성 검정\n\n자유도: (r - 1)(c - 1)\n\n동질성 검정: 독립성 검정이랑 유사",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#회귀분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#회귀분석",
    "title": "4 - 통계분석",
    "section": "회귀분석",
    "text": "회귀분석\n\n1. 가정\n\n선형성\n정규성: qq-plot, 대각선에 가까워야함\n등분산성: 수평선에 가까워야함\n독립성: 더빈 왓슨 검정(0~4), 2에 가까울수록 독립성이 있다.\n\n→ 가정을 충족하지 않을 경우, 회귀모델을 수정해야함\n\n이상치 → 관측값 제거\n선형성 → 독립변수 변환\n정규성, 등분산성 미충족 → 종속변수 변환\n\n변환: \\(x\\) → \\(x^λ\\)\n\n\n2. 회귀식\n\n\n\\(R^2 = \\frac{SSR}{SST}\\)\n\\(R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\)\n\n\n\n3. 다중공선성\n\n독립변수들 간에 강한 상관관계가 존재하는 경우\n\n상관계수: 변수간 상관계수를 직접 계산\n허용오차: 1 - \\(R^2\\). 0.1 이하면 다중공선성이 존재한다고 판단\nVIF: 허용 오차의 역수. 10 이상이면 다중공선성이 존재한다고 판단 → 변수 제거\n\n\n\n\n4. 최적화 회귀방정식\n\nAIC, BIC나 F-value를 크게 만드는 변수 제거\n\n\n전진 선택법: 상수항부터 시작해, 한번에 한개씩 독립변수 추가\n\n전체 변수 사용할 수 있지만 안정성이 낮음\n\n후진 선택법: 모든 독립변수를 포함한 후, 하나씩 제거. AIC가 더 이상 작아지지 않을 때까지\n\n안정성이 높지만 변수가 많을 때 시간이 오래 걸림\n\n단계 선택법: 전진, 후진 선택법을 혼합.\n\n이미 선택된 변수를 제거할 수 있음\n변수가 많으면 시간이 오래 걸림",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#고급-회귀분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#고급-회귀분석",
    "title": "4 - 통계분석",
    "section": "고급 회귀분석",
    "text": "고급 회귀분석\n\n1. 패널티 회귀분석\n지나치게 많은 독립변수를 갖는 모델에 페널티를 부과하는 방식\n\n릿지: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0에 근접하게 축소 (\\(l_2\\) 규제)\n\n회귀 계수가 비슷하고, 독립변수가 많을 때 효과가 좋다.\n\n라쏘: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0으로 만듦 (\\(l_1\\) 규제)\n\n회귀 계수 차이가 클 때 효과가 좋다.\n\n엘라스틱넷: 릿지와 라쏘를 혼합한 방법 (\\(l_1\\) + \\(l_2\\) 규제)\n\n\n\n2. 일반화 회귀분석\n\n종속변수가 연속형이면서 정규분포를 따르지 않을 때 사용\n\n\nlogistic 회귀모형\npoisson 회귀모형",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#시계열-분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#시계열-분석",
    "title": "4 - 통계분석",
    "section": "시계열 분석",
    "text": "시계열 분석\n\n시계열 데이터 생성\n탐색적 분석을 통해 데이터 이해\n\n시각화 작업으로 변통 패턴 관찰\n성분분해 작업으로 추세, 계절성분, 불규칙성분 분리\n\n추세(장기)\n계절(단기)\n순환(중장기)\n불규칙(설명 불가)\n\n\n미래 관측값에 대한 예측\n\n이동 평균법\n지수 평활법\nARIMA 기법\n\nAR모델: P시점 전의 자료가 현재에 주는 영향을 시계열 모형으로 구축. 과거 관측값을 이용하여 예측모델 생성. 감절\nMA모델: 시간이 지날수록 관측치의 평균값이 지속적으로 증가하거나 감소하는 경향 표현. 과거 오차항을 이용하여 예측모델 생성. 절감\nARIMA모델: 비정상 시계열. 차분이나 변환을 통해 정상시계열로 변환",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "href": "posts/04_archives/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "빅데이터 분석과 전략 인사이트",
    "text": "빅데이터 분석과 전략 인사이트\n빅테이터 분석은 분석을 통해 가치를 창출하는 것이 목적이다.\n\n일차원적인 분석: 해당 부서나 업무 영역에만 효과가 있다. 변화하는 환경에서 새로운 기회를 포착하기 어려움.\n전략도출 가치기반 분석: 일차원적인 분석을 통해 얻은 가치를 기반으로 활용 범위를 더 넓고 전략적으로 확장해야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "href": "posts/04_archives/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "전략 인사이트 도출을 위한 필요 역량",
    "text": "전략 인사이트 도출을 위한 필요 역량\n\n\n\n데이터 사이언티스트의 요구 역량\n\n\n외부 환경이 다음과 같이 변화함에 따라 인사이트 도출을 위한 인문학적 역량이 요구됨.\n\n컨버전스 → 디버전스\n생산 → 서비스\n생산 → 시장창조",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "href": "posts/04_archives/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "title": "3 - 분석 마스터 플랜",
    "section": "마스터 플랜 수립 프레임 워크",
    "text": "마스터 플랜 수립 프레임 워크\n\n\n\n마스터 플랜 수립 개요\n\n\n\n2. 수행 과제 도출 및 우선순위 평가\n\n\n\n일반적인 IT 프로젝트 우선순위 평가\n\n\n\n\n\nROI 관점\n\n\n위 기준에 따라 시급성과 난이도를 평가한 후, 아래 그림에 맞게 우선순위를 정한다.\n\n우선순위 기준을 시급성에 둔다면, 3 → 4 → 2 순, 난이도에 둔다면 3 → 1 → 2 순으로 우선순위를 정한다.\n\n\n3. 이행계획 수립\n\n\n\n로드맵 수립\n\n\n\n세부 이행계획 수립",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "href": "posts/04_archives/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "title": "3 - 분석 마스터 플랜",
    "section": "분석 거버넌스 체계 수립",
    "text": "분석 거버넌스 체계 수립\n\n1. 거버넌스 체계\n\n\n\n2. 데이터 분석 수준진단\n\n\n분석 준비도\n\n\n\n분석 성숙도\n\n\n\n\nCMMI(Capability Maturity Model Integration)\n\n\n분석 준비도와 성숙도를 통해 현재 분석 수준을 파악한다. 이후 아래의 그림에 맞춰 목표 방향을 설정한다.\n\n\n\n4. 데이터 거버넌스 체계 수립\n\n구성 요소:\n\n원칙\n조직\n프로세스\n\n\n\n\n\n체계\n\n\n\n데이터 표준화: 규칙같은거 통일하는거\n데이터 관리 체계: 라이프사이클 같은거 관리하는거\n레포지토리: 너가 아는 그거\n표준화 활동: 잘 지켜지는지 지속적으로 모니터링하는거\n\n\n\n5. 데이터 조직 및 인력방안 수립\n\n\n\n분석 조직 구조\n\n\n\n\n\n분석 조직 인력 구성\n\n\n\n\n6. 분석과제 관리 프로세스 수립\n\n\n\n분석 과제 관리 프로세스",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html",
    "href": "posts/04_archives/adp_필기/index.html",
    "title": "ADP 필기 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-02-02\n        종료일: 2025-02-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#details",
    "href": "posts/04_archives/adp_필기/index.html#details",
    "title": "ADP 필기 준비",
    "section": "Details",
    "text": "Details\n1회차 시도는 실패했지만, 이번엔 잘 되겠죠",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#참고-자료",
    "href": "posts/04_archives/adp_필기/index.html#참고-자료",
    "title": "ADP 필기 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#tasks",
    "href": "posts/04_archives/adp_필기/index.html#tasks",
    "title": "ADP 필기 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2025.01.20 10 am)\n                \n                2025.02.22 10:00 수원공업고등학교\n            \n\n            \n            \n                \n                    \n                    경기도 자격증 응시료 지원 신청\n                \n                아마도 5월 쯤 뜨지 않을까",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#related-posts",
    "href": "posts/04_archives/adp_필기/index.html#related-posts",
    "title": "ADP 필기 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html",
    "href": "posts/04_archives/vault/index.html",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#details",
    "href": "posts/04_archives/vault/index.html#details",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#tasks",
    "href": "posts/04_archives/vault/index.html#tasks",
    "title": "vault",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#참고-자료",
    "href": "posts/04_archives/vault/index.html#참고-자료",
    "title": "vault",
    "section": "참고 자료",
    "text": "참고 자료\n\nvault Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#related-posts",
    "href": "posts/04_archives/vault/index.html#related-posts",
    "title": "vault",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html",
    "href": "posts/04_archives/k8s/notes/5_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html#persistant-volume",
    "href": "posts/04_archives/k8s/notes/5_storage.html#persistant-volume",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html#storage-class",
    "href": "posts/04_archives/k8s/notes/5_storage.html#storage-class",
    "title": "김형훈의 학습 블로그",
    "section": "storage class",
    "text": "storage class\n\ndynamically provisioned persistant volumes.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#manual-scheduling",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#manual-scheduling",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "title": "김형훈의 학습 블로그",
    "section": "taints and tolerations",
    "text": "taints and tolerations\n\ntaints: a taint is a key-value pair that is applied to a node\ntolerations: a toleration is a key-value pair that is applied to a pod  \nnot garantee that the pod will be scheduled to the node",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#node-affinity",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#node-affinity",
    "title": "김형훈의 학습 블로그",
    "section": "node affinity",
    "text": "node affinity\n\n\n\nnode affinity",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#resource-limits-requests",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#resource-limits-requests",
    "title": "김형훈의 학습 블로그",
    "section": "resource limits, requests",
    "text": "resource limits, requests\n\nresource limits: the maximum amount of resources that a container can use\nresource requests: the amount of resources that a container is guaranteed to have\nresource quotas: the maximum amount of resources that a namespace can use\nlimit range: the minimum and maximum amount of resources that a container can use when it is created",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#static-pod",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#static-pod",
    "title": "김형훈의 학습 블로그",
    "section": "static pod",
    "text": "static pod\n\nstatic pod is a pod that is created by the kubelet on a node\nif kube-api is available, the kubelet will create the mirror pod in the api server. that is read-only  or in /etc/kubernetes/manifests",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#multiple-shedulers",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#multiple-shedulers",
    "title": "김형훈의 학습 블로그",
    "section": "multiple shedulers",
    "text": "multiple shedulers",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "title": "김형훈의 학습 블로그",
    "section": "configuring sheduler profile",
    "text": "configuring sheduler profile\n: single sheduler, multi profile",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/7_design_cluster.html",
    "href": "posts/04_archives/k8s/notes/7_design_cluster.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "href": "posts/04_archives/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "title": "김형훈의 학습 블로그",
    "section": "cluster upgrade process",
    "text": "cluster upgrade process\n - k8s supports up to recent 3 minor versions  ### kubeadm upgrade 1. upgrade kubeadm 2. command: kubeadm upgrade apply 3. upgrade kubelet and kubectl",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "href": "posts/04_archives/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "title": "EDA와 시각화",
    "section": "EDA(Exploratory Data Analysis)",
    "text": "EDA(Exploratory Data Analysis)\n: 데이터의 특징과 데이터에 내재된 관계를 알아내기 위해 그래프와 통계적 분석 방법을 활용하여 탐구하는 것\n\n주제\n\n저항성 강조: 부분적 변동(이상치 등)에 대한 민감성 확인\n잔차 계산\n자료변수의 재표현: 변수를 적당한 척도로 바꾸는 것\n그래프를 통한 현시성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#막대-그래프",
    "href": "posts/04_archives/adp_실기/notes/02.html#막대-그래프",
    "title": "EDA와 시각화",
    "section": "막대 그래프",
    "text": "막대 그래프\n범주형 데이터를 요약하고 시각적으로 비교하는 데 활용\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine_load\nwine['Class'] = wine_load.target\nwine['Class'] = wine['Class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nwine_type = wine['Class'].value_counts()\nwine_type\n\nClass\nclass_1    71\nclass_0    59\nclass_2    48\nName: count, dtype: int64\n\n\n\n# 수직 막대\nplt.bar(wine_type.index, wine_type.values, width=0.8, bottom=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 수평 막대\nplt.barh(wine_type.index, wine_type.values, height=0.8, left=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n각 범주의 값의 갯수 차이가 극단적인지 확인한다. 극단적일 경우, 전처리 과정에서 업/다운 샘플링 등을 통해 갯수가 유사해지도록 조정해야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#히스토그램",
    "href": "posts/04_archives/adp_실기/notes/02.html#히스토그램",
    "title": "EDA와 시각화",
    "section": "히스토그램",
    "text": "히스토그램\n연속형 데이터의 분포를 확인하는 데 활용\n\nplt.title('Wine alcohol histogram')\nplt.hist('alcohol', bins=8, range=(11, 15), color='purple', data=wine)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#box-plot",
    "href": "posts/04_archives/adp_실기/notes/02.html#box-plot",
    "title": "EDA와 시각화",
    "section": "box plot",
    "text": "box plot\n수치형 변수의 분포를 확인하는 그래프\n\nfrom sklearn.datasets import load_iris\n\niris_load = load_iris()\niris = pd.DataFrame(iris_load.data, columns=iris_load.feature_names)\niris['class'] = iris_load.target\niris['class'] = iris['class'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\nplt.boxplot(iris.drop(columns='class'))\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.boxplot(x=\"class\", y=\"sepal width (cm)\", data=iris)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#산점도",
    "href": "posts/04_archives/adp_실기/notes/02.html#산점도",
    "title": "EDA와 시각화",
    "section": "산점도",
    "text": "산점도\n두 개의 수치형 변수의 분포와 관계를 확인하는 그래프\n\nplt.title('iris scatter')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\n\nplt.scatter('sepal length (cm)', 'sepal width (cm)', data=iris, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', hue='class', data=iris, style='class')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#선그래프",
    "href": "posts/04_archives/adp_실기/notes/02.html#선그래프",
    "title": "EDA와 시각화",
    "section": "선그래프",
    "text": "선그래프\n\n수평 / 수직 선\n\nplt.hlines(y=-6, xmin=-10, xmax=10, colors='red', linestyles='solid')\nplt.vlines(x=0, ymin=-10, ymax=10, colors='blue', linestyles='dashed')\n\n\n\n\n\n\n\n\n\n\n함수식\n\ndef linear_func(x):\n    return 2*x + 1\n\nX = iris['sepal length (cm)']\nplt.plot(X, linear_func(X), c='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n회귀선\n\nimport numpy as np\n\nX, Y = iris['sepal length (cm)'], iris['sepal width (cm)']\nplt.scatter(X, Y, alpha=0.5)\na, b = np.polyfit(X, Y, 1)\nplt.plot(X, a*X + b, c='red')\nplt.show()\n\n\n\n\n\n\n\n\n2차 이상의 그래프는 X값에 대하여 정렬해야 한다.\n\niris2 = iris.sort_values(by='sepal length (cm)')\nX, Y = iris2['sepal length (cm)'], iris2['petal length (cm)']\nb2, b1, b0 = np.polyfit(X, Y, 2)\nplt.scatter(X, Y, alpha=0.5)\nplt.plot(X, b0 + b1*X + b2*X**2, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n꺾은선\n\nplt.plot('sepal length (cm)', 'petal length (cm)', data=iris2)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#상관관계-시각화",
    "href": "posts/04_archives/adp_실기/notes/02.html#상관관계-시각화",
    "title": "EDA와 시각화",
    "section": "상관관계 시각화",
    "text": "상관관계 시각화\n\n산점도 행렬\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(iris, alpha=0.5, figsize= (8, 8), diagonal='hist')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(iris, diag_kind='auto', hue='class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n상관계수 행렬 그래프\n\niris_corr = iris.drop(columns='class').corr(method='pearson')\nsns.heatmap(iris_corr, xticklabels=iris_corr.columns, yticklabels=iris_corr.columns, cmap=\"RdBu_r\", annot=True)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#pandas-profiling",
    "href": "posts/04_archives/adp_실기/notes/02.html#pandas-profiling",
    "title": "EDA와 시각화",
    "section": "Pandas Profiling",
    "text": "Pandas Profiling\n\n# from pandas_profiling import ProfileReport\n#\n# ProfileReport(iris)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/04.html",
    "href": "posts/04_archives/adp_실기/notes/04.html",
    "title": "머신 러닝",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "머신 러닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html",
    "href": "posts/04_archives/adp_실기/index.html",
    "title": "ADP 실기 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-02-05\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석python",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#details",
    "href": "posts/04_archives/adp_실기/index.html#details",
    "title": "ADP 실기 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#tasks",
    "href": "posts/04_archives/adp_실기/index.html#tasks",
    "title": "ADP 실기 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    파이썬 한권으로 끝내기 완독\n                \n                \n            \n            \n            \n                \n                    \n                    원서 접수 (2025.03.24 10 am)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#why-failed",
    "href": "posts/04_archives/adp_실기/index.html#why-failed",
    "title": "ADP 실기 준비",
    "section": "Why failed?",
    "text": "Why failed?\nTOFEL이 더 급하다.\n4학년 때 도전하자.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#related-posts",
    "href": "posts/04_archives/adp_실기/index.html#related-posts",
    "title": "ADP 실기 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/12_database.html",
    "href": "posts/04_archives/aws_saa/notes/12_database.html",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/12_database.html#database-types",
    "href": "posts/04_archives/aws_saa/notes/12_database.html#database-types",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "title": "김형훈의 학습 블로그",
    "section": "AWS WAF(Web Application Firewall)",
    "text": "AWS WAF(Web Application Firewall)\n\ndeploy on\n\nCloudFront (global)\nApplication Load Balancer (regional)\nAPI Gateway (regional)\nAppSync GraphQL API (regional)\ncognito (regional)\n\n\n\nfeatures\n\nprotect from SQL injection, cross-site scripting, and other web attacks\nIP blacklisting and whitelisting\nfilter HTTP headers / body / URI\nlimit the size of requests\ngeo-blocking\nrate limiting (DDoS protection)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Shield",
    "text": "AWS Shield\n\nDDoS protection service\nStandard and Advanced plan",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Firewall Manager",
    "text": "AWS Firewall Manager\n\ncentral management service to configure and manage WAF rules across accounts and applications",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon GuardDuty",
    "text": "Amazon GuardDuty\n\nthreat detection service\ngood for detect crypto currency mining",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Inspector",
    "text": "Amazon Inspector\n\nsecurity assessment service\ncontinuous assessment of applications for vulnerabilities and deviations from best practices",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Macie",
    "text": "Amazon Macie\n\ndata security and data privacy service\ndetect and protect sensitive data",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/08_cloudfront.html",
    "href": "posts/04_archives/aws_saa/notes/08_cloudfront.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/08_cloudfront.html#cloudfront",
    "href": "posts/04_archives/aws_saa/notes/08_cloudfront.html#cloudfront",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html",
    "title": "AWS Organization",
    "section": "",
    "text": "global service\ncontrol over multiple AWS accounts\nconsolidated billing\nshared reserved instances and savings plans across accounts ## service control policies (SCPs)\nIAM policy applied to OU or account except management account\n\n\n\n\ncloudwatch agent\n\n\n\n\n\ncloudwatch agent\n\n\n\n\n\nIAM role: cross-account access\nResource-based policy: cross-service access \n\n\n\n\n\nsupported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions \n\n\n\n\n\n\n\n\nAD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "title": "AWS Organization",
    "section": "",
    "text": "IAM role: cross-account access\nResource-based policy: cross-service access",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "title": "AWS Organization",
    "section": "",
    "text": "supported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#aws-directory-service",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#aws-directory-service",
    "title": "AWS Organization",
    "section": "",
    "text": "AD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet\n\n\n\n: stateless, allow/deny traffic in/out of subnet default, it allows all traffic \n\n\n\n\n\n\nVPC\n\n\n\n\n\n: private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.\n\n\n\n: VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC\n\n\n\n: connect on-premises network to AWS VPC \n\n\n\n: dedicated network connection between on-premises and AWS\n\n\n\n: IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#nat-gateway",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#nat-gateway",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#nacls",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#nacls",
    "title": "VPC",
    "section": "",
    "text": ": stateless, allow/deny traffic in/out of subnet default, it allows all traffic",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-peering",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-peering",
    "title": "VPC",
    "section": "",
    "text": "VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "title": "VPC",
    "section": "",
    "text": ": private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "title": "VPC",
    "section": "",
    "text": ": VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "title": "VPC",
    "section": "",
    "text": ": connect on-premises network to AWS VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#direct-connectdx",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#direct-connectdx",
    "title": "VPC",
    "section": "",
    "text": ": dedicated network connection between on-premises and AWS",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "title": "VPC",
    "section": "",
    "text": ": IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)\n\n\n\n\n\n\ndistribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer\n\n\n\n\n\n\n\nServer Name Indication\nALB and NLB and cloudFront support SNI\n\n\n\n\n\n\nALB and NLB support connection draining (deregestration delay)\n\n\n\n\n\n\n\nTarget tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "title": "ELB",
    "section": "",
    "text": "distribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#ssltls",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#ssltls",
    "title": "ELB",
    "section": "",
    "text": "Server Name Indication\nALB and NLB and cloudFront support SNI",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#connection-draining",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#connection-draining",
    "title": "ELB",
    "section": "",
    "text": "ALB and NLB support connection draining (deregestration delay)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#asg",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#asg",
    "title": "ELB",
    "section": "",
    "text": "Target tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "title": "Amazon RDS",
    "section": "",
    "text": "Amazon RDS is a managed relational database service that provides a highly available, scalable, and secure database.\nAmazon RDS supports multiple database engines:\n\nAmazon Aurora\nMySQL\nMariaDB\nPostgreSQL\nOracle\nMicrosoft SQL Server\n\nAmazon RDS provides the following features:\n\nAutomated backups\nMulti-AZ deployments\nRead replicas\nMonitoring\nSecurity\nScalability\nHigh availability ### auto scaling\n\nmust set maximum storage threshold\n\n\nfree storage is less then 10% of allocated storage\nlow-storage lasts at least 5 minutes\n6 hours have passed since last modification\n\n\n\n\nup to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance\n\n\n\n\n\n\nMySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second\n\n\n\n\n\n\n\n\nAutomated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore\n\n\n\n\n\n\n\n\n\nin-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "title": "Amazon RDS",
    "section": "",
    "text": "up to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "title": "Amazon RDS",
    "section": "",
    "text": "MySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "title": "Amazon RDS",
    "section": "",
    "text": "Automated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "title": "Amazon RDS",
    "section": "",
    "text": "in-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#define-iam",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#define-iam",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iampolicies",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iampolicies",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Policies",
    "text": "IAM:Policies\n\nUsers or Groups can be assigned JSON documents called policies\npolicies define permissions of the users(inline) or groups\nAWS apply the least privilege principle\n\n\nJSON exe\n{\n    {\n        \"Version\": \"2012-10-17\",\n        // optional: \"id\": \"...\",\n        \"Statement\": [\n            {\n                // optional: \"Sid\": \"...\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"s3:ListBucket\",\n                \"Resource\": \"arn:aws:s3:::example-bucket\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"s3:GetObject\",\n                    \"s3:PutObject\"\n                ],\n                \"Resource\": \"arn:aws:s3:::example-bucket/*\"\n            }\n        ]\n    }\n}\n\nEffect: Allow/Deny\nPrinciple: who can perform the action (account, user, role)\nAction: list of actions that are allowed or denied\nResource: list of resources that are allowed or denied\nCondition: when the policy is in effect (optional)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Roles for Services",
    "text": "IAM:Roles for Services\n\nRoles are used to delegate permissions to entities that you trust\n\n\ntrusted entities\n\nAWS account\n\nAWS services\n\nEC2, Lambda, CodeBuild, CodePipeline, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Security Tools",
    "text": "IAM:Security Tools\n\nIAM Credentials Report (account level):\nlist of all users and their various credentials\nIAM Access Advisor (user level):\nhow long each service has been active and when it was last used",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/00_region.html",
    "href": "posts/04_archives/aws_saa/notes/00_region.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "href": "posts/04_archives/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time\n\n\n\n\n\nlog data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3\n\n\n\n\n\ncollect more system-level metrics \n\n\n\n\n\nalarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm\n\n\n\n\n\ncron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#matrics",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#matrics",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#logs",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#logs",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "log data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "collect more system-level metrics",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "alarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "cron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#insights-events",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#insights-events",
    "title": "Amazon CloudWatch",
    "section": "Insights Events",
    "text": "Insights Events\n\ninsights events provide insights into the performance and availability of your AWS Account",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "title": "Introduction to Human Factors",
    "section": "what is human factors",
    "text": "what is human factors\n\nhuman factors = Ergonomics\na human-centered design philosophy &lt;-&gt; technology-centered design",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "component of human factors",
    "text": "component of human factors\n\nhuman: physical, cognitive, group\ntask: physical + cognitive + group\nenvirnment: working environment, systems",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "Goal of human factors",
    "text": "Goal of human factors\n\nReduce errors\nIncrease productivity\nEnhance safety\nEnhance comfort",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "title": "Research Method in Human Factors",
    "section": "reaserch meathods",
    "text": "reaserch meathods\n\ndescriptive Research\n관찰을 통해 데이터 묘사\n\n무엇을 측정할지\n어떻게 숫자로 표현할지\n\n\n대부분 평균, 표준편차를 대푯값으로 사용\n변수들 간의 관계를 파악하기 위해 상관분석, 회귀분석을 사용\n\n\ntypes of descriptive research\n\nobservational research\n\n\n관찰 연구를 계획할 때, 측정할 변수, 각 변수를 기록할 방법, 관찰이 이루어지는 조건, 관찰 기간 등을 식별\n\n\nsurvey research\n\n\n설문조사를 통해 데이터 수집\n\n\nincident and accident analysis\n\n\n사고나 오류를 분석하여 원인을 찾음\n사고나 오류를 줄이기 위한 대책을 마련\n\n\n\n\nexperimanetal Research\n하나 이상의 독립변수에 의도적인 변화를 주고, 그 변화가 하나 이상의 종속변수에 미치는 인과관계를 측정\n이때 다른 변수들은 통제한다\n\n예시\n\n휴대전화를 사용하는 것이 운전에 미치는 영향\n인센티브를 미리 주고 잘못 할 때마다 차감하는 것과, 잘할 때마다 인센티브를 주는 것의 차이\n\n\n\ntype of variables\n\nindependent(predictor, stratification) variable\ndependent(descriptive, criterion) variable\ncontrol variable: 이 값은 고정시키고 실험을 진행한다. 일반화하기 어렵게 한다.\nrandom variable (sigma): 통제할 수 없는 변수. 일반화하기 용이하다.\nconfounding variable: 수식에는 포함되지 않지만 주의해야하는 변수.\n\n\ndecide variables\noperational definition: 변수를 관찰 가능하고 측정 가능한 형태로 정의\n\nindependent variable\n\nRange: realistic / select a range taht will show the effect / pilot experiment\n\ndependent variable\n\nreliability: consistent. solution: increase the number of observations\nvalidity: measure what was intended\n\n\n\n\n\n\nwhy use experimental research?\n\nhumans are variable\n\n\nintra individual variability\ninter individual variability\n\n\nways to handle variability\n\n\nuse statistical techniques\ncontrol variability as much as possible\n\n\n\ntypes of experimental design\n\nsingle variable experiment\n\n\ntwo levels\nmulti levels\n\n\nfactorial design: 두개 이상의 독립변수를 조합하여 실험군을 만든다.\n\n\n변수 간 interaction effect을 확인할 수 있다.\nmore difficult to analyze\n2 x 2, 3 x 3, 2 x 2 x 2 등으로 설계한다.\nbetween-subject, within-subject를 모두 사용하는 mixed designs를 사용할 수 있음.\n\n\nbetween-subjects design: 각각의 실험군에 다른 사람들을 넣는다.\n\n\ngeneralibility 높다, intra person variability를 제거할 수 있다.\n\n\nwithin-subjects design: 같은 사람들을 다른 실험군에 넣는다.\n\n\ncost-effective, less variability, inter person variability를 제거할 수 있다.\n\n\n\n\nevaluation research\n시스템이나 제품이 목적을 충족하는지 평가\n\nusability testing: 사용자가 제품을 실제로 사용하면서 발생하는 문제점 파악\n\n태스크 완료 시간, 오류율, 사용자 만족도 등을 측정\n\ncost-benefit analysis: 제품 또는 시스템 도입의 경제성 평가\n\n직접 비용(하드웨어, 소프트웨어 구입비, training cost 등)\n예상되는 이익(생산성 향상, 오류 감소 등)을 비교 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "title": "Research Method in Human Factors",
    "section": "research design",
    "text": "research design\n\nqualitative research\n\n보통 마케팅에서 진행.\n\n\n\nquantitative research\n\nexperiments\ncorrelational observation\nsurveys and questionnaires\n\nsample: 랜덤하게 샘플링하는게 중요\nrating\nbias: 질문의 순서, 질문의 내용, 질문의 방향\n\narchival research\n\n\nfield study\n\nuncontrolled\nresults may be more generalizable to real-world situations\nhigher cost\ndifficult to replicate\ndifficult to control extraneous variables\n\n\n\nlab experiment\n\ncontrolled\nprecise replication\nlower cost\nmore flexibility\nreal-world generalizability may be limited",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "title": "Attention",
    "section": "Attention의 정의",
    "text": "Attention의 정의\n\nAttention acts as a means of focusing limited mental resources on the information and cognitive processes that are most salient at a given moment\nFocusing most salient at a given moment:\n\n주의는 Search light로 비유됨\n한 영역에 집중하면 다른 부분은 배제.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "title": "Attention",
    "section": "Attention의 네 가지 주요 측면",
    "text": "Attention의 네 가지 주요 측면\n\nSelective, Focused, Divided, Sustained Attention은 독립적이지 않으며, 상호작용하여 주의 과정 형성.\n\n\nFocused Attention (집중적 주의)\n\n특정 과업에 집중하고, 외부 방해 요인을 배제하는 능력.\n방해 요소(Distraction)를 최소화하여 현재 작업에 주의 집중.\n예시:\n\n냉장고에서 음식을 꺼내려는 도중 질문을 받으면 집중력이 분산되어 원래 작업을 잊어버릴 수 있음.\n\n\n\n\nDivided Attention (분할 주의)\n\n여러 작업을 동시에 수행하며 주의를 분배.\nSelective Attention과의 차이:\n\nSelective Attention: 특정 자극을 선택적으로 받아들임.\nDivided Attention: 여러 작업 간 우선순위를 메기고 주의 자원 분배.\n\n예시:\n\n운전 중 대화하며 라디오 듣기.\n각 작업에 필요한 시간과 노력을 어떻게 배분할지를 결정.\n\n\n\n\nSustained Attention (지속적 주의)\n\n주의가 높거나 낮거나보다는, 장시간 동안 주의를 유지하는 능력.\n높은 주의 레벨 필요 시:\n\n정보를 놓치는 경우가 발생할 가능성이 높음.\n여러 정보와 자극을 동시에 처리.\n예: 주식 거래에서 여러 종목을 모니터링.\n\n낮은 주의 레벨 시:\n\n자극 부족으로 주의 산만 발생.\n예: CCTV 감시 업무.\n\n\n\n\nSelective Attention (선택적 주의)\n\n여러 감각 자극 중 중요한 정보를 선택적으로 처리.\n시각, 청각, 촉각 등 다양한 감각 경로를 통해 들어오는 자극에서 의미 있는 정보 선별.\n예시:\n\n운전 중:\n\n표지판, 신호등, 앞차의 움직임 → 중요한 정보.\n옆 보행자의 얼굴이나 주변 불필요한 자극 → 중요하지 않은 정보.\n\n\n\n\n\nMental Workload (정신적 작업 부하)\n\n동시에 수행할 수 있는 과업이 몇 개인지 혹은 이 과업이 수행하기에 attention scale을 넘어가는 것인지 분석 용도\n측정 방법:\n\n주관적 설문:\n\n작업자가 느끼는 주관적 부담을 평가.\n\n생체 반응 분석:\n\n심박수, 뇌파 등 생리적 데이터를 활용.\n\n부과 과업(parallel tasking):\n\n추가 과업을 부여하여 작업 부하 평가.\n예시:\n\n운전 중 숫자 거꾸로 세기.\n특정 숫자를 기억하고 응답(예: N-back 테스트).\n\n\n\n워크로드 증가의 결과 특정 작업의 실패 확률 증가한다.\n\n\n\nAttention의 결정 요인\n\n의지\n\n개인의 목표와 필요에 따라 주의 집중.\n예: 차선 변경 시 후방 차량 확인.\n\ncaptured by salience and grouping\n\n공간, 강도, 색상, 크기, 음조 등 외부 요인.\n강렬한 자극이 주의를 끌 가능성 높음.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "title": "Attention",
    "section": "Selective Attention",
    "text": "Selective Attention\n\n특정한 자극(예: 시각적 또는 청각적 정보)에 주의를 집중하며, 다른 자극을 배제하는 과정.\n중요도에 따라 특정 정보를 선택적으로 처리하며 불필요한 정보는 억제.\nattention이 sensory memory로 부터 들어온 정보의 filter나 gateway나 bottle neck으로 작용한다고 봄\n\n\n작동 원리\n\nTop-down Processing (Mental model):\n\n개인의 경험과 목표에 기반하여 주의 집중 전략을 개발.\n예: 초보 운전때 앞만 보고 가다가 숙련이 되면 사이드미러 같은 주변도 보게 됨.\n\nBottom-up Processing (자극 기반 처리):\n\n강렬하거나 눈에 띄는 자극에 주의가 끌림.\n예: 갑작스러운 소리나 반짝이는 신호등.\n결정 요인\n\nSalience Source: 자극의 강도(밝기, 소리 크기 등).\n\nInformation Access Trade-offs: 특정 정보를 처리함으로써 얻는 이득.\n\n\n\n\nBottleneck Model\nEarly Selection Theory\n- sensory memory까지는 잘 오지만, Attention filter에 선택이 된게 처리가 되고 나머지는 처리가 안 된다.\n- 감각 단계에서 물리적 특성(의미가 아닌)을 기준으로 정보 필터링.\n- 폐기된 정보는 행동에 미치는 영향 없음.\n- 한계: 칵테일 파티 현상(의미 정보 처리 설명 불가).\n\nLate Selection Theory\n- 모든 정보가 cognition / working memory까지 전달 후 선택.\n- 식별되지 않은 정보는 작업 기억의 제한된 용량으로 인해 빠르게 잊혀짐.\n- 선택되지 않은 정보도 행동에 영향을 미침.\n- 광고 실험 - 인지하지 못하는 정보(빠르게 잊어버려서)에 의해서도 행동의 변화가 있을 것이다.\n\n\nTask\nGeneral orientation and scene scanning\n- 그림을 보거나 웹 브라우징\n감독 제어 (Supervisory Control)\n- 자동화된 시스템에서 이상 징후를 탐지.\n- 주로 AOI(Area of Interests)를 스캐닝 함.\nAOI는 여러개가 있음. 시간, 중요도, 과업의 컨텍스트에 따라서 다르게 설정됨\nspecific task-related information이 있는 물리적 위치\nAOI를 몇개를 만들고, 이들에 대한 시선의 이동을 어떻게 만들것인가가 중요한 issue - 예시:\n자율주행 차량 또는 산업 기계 감독.\n제어 패널에서 비정상적인 지표 확인 (예: 전력 공급 문제, 자원 부족).\n탐지 (Noticing)\n- 예상치 못한 사건이나 환경 변화 감지.\n- 예시:\n- CCTV로 비정상적인 활동 탐지.\n- 주요 시스템 성능의 갑작스러운 변화 인식.\n탐색 (Searching)\n- 방해 요소 속에서 특정 목표를 찾는 활동.\n- 예시:\n- 공항에서 수하물의 X-ray 검색.\n읽기 (Reading)\n- 책이나 디스플레이에서 정보를 읽고 이해.\n- 예시:\n- 계기판의 게이지 읽기.\n확인 (Confirming)\n- 작업이나 과정의 결과를 확인.\n- 예시:\n- 비행기 바퀴가 잘 내려왔는지 확인\n선택적 주의 과업 실패는 중요 정보를 놓치거나 잘못 해석하는 경우 발생할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "title": "Attention",
    "section": "SEEV Model",
    "text": "SEEV Model\nvisual attention에 영향을 주는 요소들을 설명\n\nBottom-up factors\n\nSalience: cue의 특징\nEffort: AOI로 이동하는데 드는 비용\n선형적으로 증가하는건 아니고, 그룹핑 할 수 있음.\n(Within foveal vision) 중심시에서 초점 변화. 멀리있는거에서 가까이 있는거 보는거 &lt; Eye movement &lt; Head movement &lt; Body\n중요한 정보는 cost가 작은 쪽에 배치를 해야함.\n\n\n\nTop-down factors\n\nExpectancy: 일어날 것 같은거에 주의를 더 많이 집중. mental model에 의해 예측 능력이 생길 수 있음.\nValue: 이것에 집중했을 때 얻는 이득, 보지 않았을 때 지불하는 비용\n\n\n\nGuidline\n\n중요한 AOI는 salience가 높아야함\n사용 빈도가 높은 AOI 사이의 거리는 가까워야함\n순차적인 디스플레이도 서로 가깝게 배치해야함.\n\n\n\nChange Blindness\n\n발생 원인\n\n멘탈 워크로드가 높은 경우\n눈에 띄는 변화(Slient change)는 발견하기 쉬움\n중심시에서 멀리 떨어진 곳에서 변화가 발생하면 탐지하기 어렵다.\n시야 밖에서 일어나는 변화는 인지하기 어려움(화면이 깜빡이면서 변하면 animation 효과가 안나타남)\n예상치 못한 변화는 탐지하기 어려움.(top-down processing)\n특정 위치를 응시(fixation)하고 있어도 집중(attention)이 부족하면 변화를 인지하지 못함.\n\n\n\n\nSearch Task의 유형\n\nSerial Search\n\n하나씩 순차적으로 탐색, 탐색 시간이 항목 수에 비례.\n\n예: 긴 텍스트 리스트에서 특정 단어 찾기. 같은 그림 2개 찾기\n\nParallel Search\n\n눈에 띄는 단서(pop-out effect)를 이용해 한 번에 탐색.\n\n5 search items is the same for 50 search items\n\npreattentive process로 유발됨\n\nParallel Search를 유도하는 방법\n\n색상, 크기, 대비(contrast), 회전\n\nmotion\n\nfeature를 adding하는건 찾기 쉬운데 missing하는건 찾기 어려움\n\nO안에서 Q 찾기 vs Q안에서 O 찾기\n\n깜빡이는 곳에서 안깜빡이는거 찾기 vs 안깜빡이는거에서 깜빡이는거 찾기",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "title": "Attention",
    "section": "Divided Attention",
    "text": "Divided Attention\n\n개념\n\n일반적으로 단순 작업보다는 멀티 태스킹이 많이 일어남.\n\n단순 작업: 라면 끓일 때 진짜 라면만 순서대로 끓임. (멀티테스킹 x)\n\n복잡 작업: 라면을 끓이면서 설거지도 하고, 반찬도 만들고, 카톡도 하고, … (멀티테스킹 o)\n\n여러 작업을 동시에 수행하면서 주의를 분배.\nAttention을 한계가 있는 자원으로 바라봄\n\n\n\nResource Model\n\nCentral Resource Theory:\n주의 자원을 단일 통으로 간주. 예: 교차로 진입 시 운전에만 집중, 라디오 듣기같은 다른 작업은 집중을 못함.\nMultiple Resource Theory:\n주의 자원이 감각기관의 특성, 과업의 특성에 따라 별개로 존재\n예: 시각(도로), 청각(라디오) 자원을 분리 사용.\n작업 간 유사성이 높을수록 분배 어려움.\n예: 운전 중 영화 감상(둘 다 시각 자원 사용).\n\n\n\n주의 자원과 훈련의 문제\n주의 자원은 고정인가, 훈련으로 확장 가능한가?\n\n리소스 차이는 명확히 증명되지 않았음.\n전략을 통한 과업 배분 및 우선순위 설정 → 성능 향상.\n반복적 학습과 자동화 → 개별 과업 및 주의 배분이 자연스럽게 효율화.\n\n\n\nAttention as capacity\n\n어떤 정보를 얼마나 주의 깊게 받아들일지 결정\n어떤 정보를 선택할 것인지는 disposition(형태), intentions, arousal, evaluation에 의존\n받아들이는 정보의 특성(visual, auditory), 반응(manual, vocal)에 따라 리소스 풀을 나눌 수 있다.\ntasks interfere to the degree that they tap into the same pool of resources\n\n\n\nUnitary Resource Model (단일 자원 모델)\n\n\n주의(attention)를 제한된 자원으로 봄\n과업 수행에 필요한 자원의 양이 가용 자원을 초과하면 성능 저하\n\n\n\nMultiple Resource Model (다중 자원 모델)\n\n서로 다른 유형의 과업은 다른 자원을 사용\n하지만 한쪽의 workload가 높으면 다른쪽에 영향을 미칠 수 있음.\n비주얼 테스트 두 개를 수행하는 것이 비주얼-청각 테스트보다 더 어려움\n한 객체의 두 가지 특징에 주의를 기울이는 것이 두 객체의 한 가지 특징에 주의를 기울이는 것보다 쉬움\nEx) 특성을 여러 막대로 보여주는것보다 육각형으로 보여주는게 더 보기 쉬움\n\n\n\nPerceptual Modalities\n\nAuditory,Visual, and Tactile Perceptual modalities에 사용하는 resource가 전부 다름\nVisual은 Focal과 Ambient가 서로 다른 자원을 사용함\nCross-modality가 15%정도 더 효과가 있음\ntactile은 auditory랑 비슷함.\n\n\n\ncoding\n\nspatial, verbal\nauditory verbal verbal and visual spatial manual is efficient\nverbal은 단 너무 길면 좋지 않다.\n모든 채널에서 다 쓸 수는 없다. (tactile 같은 경우에는 verbal 코딩이 없음)\n\n\n\nAttentional Allocation during Time-sharing: Skill or Ability?\n\nIf skill:\nAttentional allocation should be trainable\nSkills developed in one task transfer to unrelated tasks.\nIf ability:\nNo evidence supports the existence of a universal “multitasking ability.”\nPeople excel at specific tasks due to familiarity and automation, not inherent multitasking talent.\n\n\nPractical Implications\n\nOperator training:\nTraining must develop automaticity in single-task skills to reduce resource demand\nTraining of attentional allocation and time-sharing will help dual-task performance\nOperator selection:\ntime-sharing ability가 좋은 사람을 선택하는 것보다는 single-task performance가 좋고 자동화가 잘 사람을 선택하는 것이 더 좋음\n\n\n\n\nSystem design이나 multi-task performance를 측정할 때 좋은 것\n\nTask analysis나 multiple resource model를 사용하는 것이 좋다.\nTask의 어떤 면이 효율적으로 time-shared 될 것인가?\nTask의 어떤 면이 interference를 일으킬 것인가?\ninterference를 최소화하기 위해 어떻게 디자인 해야하나?\nex) driving할 때 손과 발을 따로 사용하게 하기\nTime-sharing efficiency, task performance, and mental workload를 고려해야한다\n(멘탈 워크로드 측정은 아직도 쉽지 않다.)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "title": "Control",
    "section": "Basic Control Task and Device",
    "text": "Basic Control Task and Device\n\n\n\n상태를 체크할 때는 toggle switch, 눌렀다 떼는 건 push button\n레버도 상태를 체크할 때 사용. 그 중 큰 힘이 필요한 경우. continuous setting에서는 slider가 쓰임\nselector switch는 lever랑은 다르게 discrete한 상태가 있음 (선풍기 버튼)\n조이스틱은 보통 가속도(2D, 멀리 밀면 빨리 가는 애)를 제어하거나 속도(1D, 버튼 조이스틱)를 제어하는데 사용. 마우스는 위치를 제어.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "title": "Control",
    "section": "15 principles to design of control device",
    "text": "15 principles to design of control device\n\nAttention principles\n\nProximity compatibility\n\n컨트롤 하고자 하는 대상과 컨트롤이 가까워야 한다.\n비상 스위치는 가까이 있어야 한다.\n\n\n\nAvoid resource competition\n\n같은 physical or cognitive resource를 사용하는 control은 피해야한다.\nex) 레버로 속도, 방향 모두 제어\n\n\n\n\nPerceptual principles\n\nMake accessible\n\nphysical accessibility: 손이 닿아야 한다.\ncognitive accessibility: 뭐 하는 control인지 이해하기 쉬워야 한다.\nex) 미는 손잡이는 flat하게 만든다.\n다양한 환경을 고려해야한다. (빛이나 소음이 많은 환경)\n\n\n\nMake discriminable\n\nvisual differentiation: 각각의 컨트롤 장비를 구분할 수 있게\nlogical grouping: 비슷한 기능을 하는 것끼리 묶어놓기\n\n\n\nExploit(활용) redundancy gain\n\n두개의 독립적인 정보를 제공하면 성능이 좋아진다.\n한 가지 정보가 없어도 다른 정보로 대체할 수 있다.\n\n\n\nAvoid absolute judgement limits\n\nworking memory limit(7)를 넘기지 말라.\ncontinuous vs with detents: 연속적인 조절에서 anchor point를 만들어주면 좋다.\n\n\n\n\nMemory Principles\n\nKnowledge in the world\n보편적으로 아는 표현을 사용\n\n\nBe consistent\n\n다른 상황에서도 예상 가능하고 일정한 방법으로 control이 가능해야한다.\n\n\n\nmake discriminable vs be consistent\n\n\n\n\nMental model principles\n\nLocation Compatibility\n\nSpatial Compatibility / physical similarity\n\n\n\nMovement Compatibility\n\n\n\n\nPopulation Stereotypes\n\nrotary controls: 시계방향으로 돌리면 커진다\nUp is on\nIncrease is right, Forward is faster\n\n\n\nResponse selection principles\n\nAvoid accidental activation\n\n사고로 눌리는 것을 방지해야한다.\n\n\n\nHick-Hyman Law\n\\(RT = a + b \\log_2(n+1)\\)\nN is the number of choices\n종류가 많아져도 그냥 몇개만 고민함\n\n\nDecision complexity advantage\n\n일반적으로 복잡한 선택을 적게 하는게 간단한 선택을 여러번 하는것보다 효율적이다\n\n\n\nFitt’s Law\n\n\nIndex of Difficulty: \\(ID = \\log_2(\\frac{2A}{W})\\)\nMovement Time: \\(MT = a + bID\\)\n\navoid accidental vs Fitt’s Law - target width가 구석에 있으면 width가 무한대가 된다.\n\n\nprovide feedback\ntouch screen은 haptic feedback이 없어서 불편함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "title": "Signal Detection Theory",
    "section": "Overview",
    "text": "Overview\n\n인간의 정보 처리 과정 중 perception에 관련된 것\nperception 단계에서 자극 뿐 아니라 노이즈도 같이 들어옴\n여러가지 신호 중 무엇이 중요한지 판단하는 것\nsiganal 탐지 과정을 정량적 모델로 분석하고 성능 평가가 목표\n인공지능 분야에서 중요성이 대두되고 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "title": "Signal Detection Theory",
    "section": "Example",
    "text": "Example\n\nQuality control inspector\n빵이나 과자가 찌그러졌는지 검사, 반도체 품질 검사. 요즘에는 기계가 대부분 담당\nDetection of a flashing warning light (or cctv)\n거수자 탐지\nAirport security guard\nDetecting peculiar patterns in medical imaging (x-ray)\n종양, 암세포 탐지\nMobile phone rings (sound)\nphantoms vibration\nMorning alarm is active or not (visual)\n\n주변의 제품, 서비스 문제 파악, 해결 디자인 제시, 검증",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Theory",
    "text": "Signal Detection Theory\n\nTrials\n\nSignal case(signal + noise): target이 존재\nNoise case(noise only): target이 없음\n\nResponse\n\nYes\nNo\n\n\n\n\nHit rate: P(Hit) = Number of Hits / Number of Signal Trials\nFalse alarm rate: P(FA) = Number of False Alarms / Number of Noise Trials\nMiss rate: P(Miss) = 1 - P(Hit)\nCorrect rejection rate: P(CR) = 1 - P(FA)\n\n\nWhat does it mean to detect?\n\nsignal is digital (exist / not exist)\nAbsolute threshold is exist\n\n\n\nAssumptions\n\n관찰자가 관찰할 수 있는 signal은 숫자나 변수로 표현할 수 있어야함\nsignal이 random variation이 있다\n피험자가 signal이 있는지 없는지 단순하게 표시할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "title": "Signal Detection Theory",
    "section": "Distribution of signal and noise",
    "text": "Distribution of signal and noise\n\n\nsensitivity index (d')\n\n값이 작으면 분간 힘듦\n값이 크면 분간 쉬움\nsignal의 성격에 따라 결정됨\n\nresponse bias (β)\n\ncriterion에 따라 yes라고 대답하는 비중과 no라고 대답하는 비중\n평가자에 따라 결정됨\n\nd′이 0, β가 50%면 그냥 랜덤으로 대답한 것과 같음\n\n\nd′ 계산\n\nP(M), P(CR) 계산\n표준 정규분포를 그림\nM과 CR의 z값을 찾음\nd′ = (0 - z(M)) + (z(CR) - 0)\n\n\n\nβ 계산\n\nd′과 관계 없이 조절\n\n\\(β = \\frac{P(X/(S+N))}{P(X/N)}\\)\n\\(\\ln β = d′λ_{center}\\)\nβ ~ 1: neutral\n\n\n\n\\(λ_{center}\\) 계산\n\\(λ_{center} = -\\frac{1}{2}(Z(FA)+Z(H))\\)\n\n\\(λ_{center}\\) = 0: ideal observer\n\\(λ_{center}\\) &lt; 0: liberal. yes라고 대답하는 비중이 늘어, hit rate가 높아지지만 false alarm rate도 높아짐\nex) 용의자를 찾는 경찰, 암세포 탐지\n\\(λ_{center}\\) &gt; 0: conservative. no라고 대답하는 비중이 늘어, correct rejection rate가 높아지지만 miss rate도 높아짐\nex) 억울한 죄인을 만들지 않으려는 범원 판결",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "title": "Signal Detection Theory",
    "section": "Optimal Response Criterion",
    "text": "Optimal Response Criterion\nsignal이 더 많은 환경, noise가 더 많은 환경이 있음. 즉, probability가 다를 수 있음\n또, Effects of payoffs가 있음\n\nsignal이 많은 환경 -&gt; criterion을 낮추는게 좋음. \\(β_{opt} &lt; 1\\)\nnoise가 많은 환경 -&gt; criterion을 높이는게 좋음. \\(β_{opt} &gt; 1\\)\n\\(β_{opt} = \\frac{P(N)}{P(S)} * \\frac{V(CR) + C(FA)}{V(H) + C(M)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "title": "Signal Detection Theory",
    "section": "Sluggish β",
    "text": "Sluggish β\n\n\n\nprobability 혹은 payoffs의 변화에 따라 bias가 optimal이랑 다르게 나옴\n\n\n\n\\(β_{opt}\\)가 낮은 경우, ideal보다 덜 conservative함.\n\\(β_{opt}\\)가 높은 경우, ideal보다 덜 risky함.\n확률에 의해 b가 조정될 때 더 많이 발생함.\n\n확률에 대한 계산이 잘못되는 경우\n평가자가 반복되는 반응에 bored해지는 경우",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "title": "Signal Detection Theory",
    "section": "ROC Curve",
    "text": "ROC Curve\n\n\n\nd′이 높아질 수록 false alarm 비중이 낮아지고, hit 비중이 높아짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Performance",
    "text": "Signal Detection Performance\n\nResponse Bias (β)\n\n잘 맞추면 보상을 준다\nfalse signals to raise signal rate\nFalse Alarm에서도 incentive를 준다.\n\n\n\nSensitivity (d′)\n\ngive feedback\nsignal을 조금 더 오래 보여줌\nsignal을 강조\nsignal을 움직이게\n휴식 시간을 충분히 줌\nsignal이 어떠넌지 잘 보여줌\n온갖 감각으로 signal을 보여줌",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "title": "중심 극한 정리",
    "section": "중심 극한 정리",
    "text": "중심 극한 정리\n평군이 μ이고, 분산이 \\(σ^2\\)인 모집단으로부터 추출한 확률표본 \\(X_1, X_2, ..., X_n\\)의 표본평균 \\(\\bar{X}\\)의 분포\n\n모집단의 분포와 상관 없이 \\(E(\\bar{X}) = μ\\), \\(Var(\\bar{X}) = \\frac{σ^2}{n}\\)\n정규 모집단일 경우 \\(\\bar{X}\\)가 정규분포를 따름\n정규 모집단이 아닐 경우\n\\(n \\geq 30\\) 이면 중심극한정리에 의해 \\(\\bar{X}\\)는 정규분포에 근사됨. (모집단의 skewed에 따라 더 큰 n이 필요할 수 있음)\n∴ \\(\\bar{X} \\sim N(μ, \\frac{σ^2}{n}), \\frac{\\bar{X} - μ}{σ/\\sqrt{n}} \\sim N(0, 1^2)\\)\n모집단의 분포가 이산, 연속 분포일 때 모두 적용 가능하다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "title": "중심 극한 정리",
    "section": "이항분포의 정규근사",
    "text": "이항분포의 정규근사",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "title": "확률변수와 확률분포",
    "section": "확률변수",
    "text": "확률변수\nsample space의 원소를 상호 배반인 event들로 분할하여 실수 값으로 대응시키는 함수\n\n이산확률변수: 확률변수가 취할 수 있는 값이 유한개 또는 무한개이지만 셀 수 있는 경우\n연속확률변수: 확률변수가 취할 수 있는 값이 실수의 구간이고 셀 수 없는 경우\n\n이산 표본공간 -&gt; 이산 확률변수\n연속 표본공간 -&gt; 연속 확률변수\n연속 표본공간 -&gt; 이산 확률변수",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "title": "확률변수와 확률분포",
    "section": "확률 분포",
    "text": "확률 분포\n\n표본공간 S에 정의된 확률변수 X의 모든 함수값들이 발생할 확률. 모집단의 확률구조를 나타냄\n확률 실험 -&gt; 표본공간 -&gt; 확률변수 -&gt; 확률분포\n\n\n이산확률분포\n확률 질량 함수(pmf): P(X=x) = f(x) =&gt; X가 x일 확률\n- 기하분포: 성공확률 p인 베르누이 시행을 독립적으로 반복했을 때 첫 번째 성공이 나타날 때까지의 시행횟수\n\n\n연속확률분포\n\n확률 밀도 함수(pdf): \\(\\int{f(x)}dx = 1\\)\n\\(\\int_a^b {f(x)}dx = P(a ≤ x ≤ b)\\) =&gt; x가 a와 b사이에 있을 확률\nP(X=x) = 0 (연속형 데이터여서 특정값을 가질 확률은 0)\nf(x) ≠ P(X=x)\nf(x)는 1보다 큰 값을 가질 수 있음\n누적분포함수(cdf): \\(F(x) = P(X ≤ x)\\) =&gt; \\(\\int_{-∞}^x{f(y)}dy\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "title": "확률변수와 확률분포",
    "section": "결합 확률분포",
    "text": "결합 확률분포\n\npmf: \\(P(X=x, Y=y) = f(x, y)\\)\npdf: \\(P(a ≤ X ≤ b, c ≤ Y ≤ d) = \\int_{a}^{b}\\int_{c}^{d}{f(x, y)}dydx\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "title": "확률변수와 확률분포",
    "section": "주변 확률분포",
    "text": "주변 확률분포\n\npmf: \\(f_X(x) = \\sum_y{f(x,y)}\\)\npdf: \\(f_X(x) = \\int_{-∞}^{∞}{f(x, y)}dy\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "title": "확률변수와 확률분포",
    "section": "조건부 확률분포",
    "text": "조건부 확률분포\n\n\\(f(x|y)\\) = \\(\\frac{joint}{marginal}\\) = \\(\\frac{f(x, y)}{f_Y(y)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "title": "확률변수와 확률분포",
    "section": "독립 확률변수",
    "text": "독립 확률변수\n\n모든 \\(x, y\\)에 대해 \\(f(x, y) = f_X(x)f_Y(y)\\)\n\n\n\n\\(f(x, y) = g(x) * h(y)\\)\n\nx, y 의 구간이 서로 간섭받지 않는다.\nX,Y는 독립이다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "title": "확률변수와 확률분포",
    "section": "확률변수의 변환",
    "text": "확률변수의 변환\n\ncdf를 이용한 변환\ncdf를 미분해서 pdf\n\n\n역함수가 존재할 경우\n\\(g(y) = f(u^{-1}(y)) * |\\frac{du^{-1}}{dy}|\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "title": "이산형 확률분포",
    "section": "",
    "text": "확률분포 정의 단계",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n각 시행의 결과는 성공(A) 또는 실패(B)\n성공 확률은 p, 실패 확률은 1-p\n각 시행은 서로 독립적 → 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면, 비복원 추출에서도 유효\n∴ S = {A,B}, f(1) = P(X=1) = p, f(0) = P(X=0) = 1-p\n\n\n\n베르누이 시행 예시",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(B(1,p)\\)\n\\(f(x) = p^x(1-p)^{1-x}, x = 0, 1\\)\n\\(E(x) = p\\)\n\\(Var(x) = p(1-p)\\)\n\\(m(t) = 1 - p + pe^t\\)\np = 0.5일 때, 분산은 0.25로 가장 큰 값을 가짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\nn번의 독립적인 베르누이 시행을 했을 때 성공 횟수 X\n서로 독립인 n개의 베르누이 분포의 합과 같다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim B(n,p)\\)\n\\(f(x) = {_n}C_x\\) \\(p^x(1-p)^{n-x}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = np\\)\n\\(Var(x) = np(1-p)\\)\n\\(m(t) = (1-p + pe^t)^n\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n성공 확률 p인 베르누이 시행을 반복하여 처음 성공할 때까지의 시행 횟수 X\n지수분포와 유사하다\n기하분포는 비기억 속성을 가진다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim G(p)\\)\n\\(f(x) = (1-p)^{x-1}p, x = 1, 2, 3, ...\\)\n\\(E(x) = \\frac{1}{p}\\)\n\\(Var(x) = \\frac{1-p}{p^2}\\)\n\\(m(t) = \\frac{pe^t}{1-qe^t}, (qe^t&lt;1), (q=1-p)\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = (1-p)^y\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n모집단의 크기에 비해 샘플의 크기가 작지 않은 경우, 비 복원 추출시 각각의 선택이 베르누이 시행이라 할 수 없다.\n\\(\\frac{r}{N} = p\\)로 일정할 때, N을 증가시키면, \\(HG(n, N, r)\\)은 \\(B(n, p)\\)로 수렴한다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim HG(n, N, r)\\)\n\\(f(x) = \\frac{\\binom{r}{x}\\binom{N-r}{n-x}}{\\binom{N}{n}}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = \\frac{nr}{N}\\)\n\\(Var(x) = \\frac{nr(N-r)(N-n)}{N^2(N-1)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수 X\n임의의 기간을 n 등분하여 각 등분에서 사건이 발생할 확률이 p라고 할 때, 발생횟수 기댓값 λ를 고정시킨 채로 n을 무한히 증가시킴\nn이 매우 크고 p가 매우 작을 때 이항분포를 포아송분포로 근사할 수 있다\n포아송 분포 + 포아송 분포 = 포아송 분포: \\(P(λ) + P(λ) = P(2λ)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim P(\\lambda)\\)\n\\(f(x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, x = 0, 1, 2, ...\\)\n\\(E(x) = \\lambda\\)\n\\(Var(x) = \\lambda\\)\n\\(m(t) = e^{\\lambda(e^t-1)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "title": "연속형 확률분포",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\nμ = 0, σ = 1인 정규분포",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "title": "연속형 확률분포",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\nα = ν/2, θ = 2인 감마분포\n자유도 ν에 따라 모양이 변함: 커질수록 정규분포에 가까워짐\n표기: \\(X \\sim χ^2(ν)\\)\n\\(E(x) = ν\\)\n\\(Var(x) = 2ν\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "title": "연속형 확률분포",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nα = 1, \\(λ = \\frac{1}{\\theta}\\)인 감마분포\nPoisson 분포에서 사건 발생 사이의 시간을 나타낼 수 있음\n표기: \\(X \\sim Exp(λ)\\)\n\\(f(x) = λe^{-λx}, x \\geq 0\\)\n\\(E(x) = \\frac{1}{λ}\\)\n\\(Var(x) = \\frac{1}{λ^2}\\)\n\\(P(X &gt; x) = e^{-λx}\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = e^{-λy}\\)\n포아송분포에서의 \\(\\frac{1}{λ}\\)와 동일\n비기억 특성을 가짐\n독립적으로 동일한 지수분포의 합은 감마분포 \\(Γ(n, \\frac{1}{\\lambda})\\)를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html",
    "title": "Database Design",
    "section": "",
    "text": "MS access is prototyping tool for mock-ups",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "title": "Database Design",
    "section": "Purpose of a Database Design",
    "text": "Purpose of a Database Design\nset of database specifications that can be implemented as a database in a DBMS\n\nconceptual design: non-DBMS specific\nlogical design: DBMS specific\nphysical design: DBMS specific but not implemented directly by humans",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "title": "Database Design",
    "section": "Logical Design(Relational Design)",
    "text": "Logical Design(Relational Design)\n\nCreate a table(relation) for each entity\n\nspecify primary key\nspecify properties for each column\n\ndata type\nconstraints\ndefault value\nnull status\n\nverify normalization: data structure의 complexity를 증가시킬 수도 있다 → denormalization: 조인 불필요, 조회 시 성능 향상 → datastructure complexity vs modification problems\n\nCreate relationships by placing foreign keys:\n\nStrong entity relationships\nID-dependent / non-ID-dependent weak entity relationships\nSubtypes\nRecursive",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "title": "Database Design",
    "section": "Representing Relationships",
    "text": "Representing Relationships\nid-dependent의 경우 부모의 primary key로 composite key 생성\nMaximum cardinality의 유형에 따라 관계 표현 방법이 달라짐\n\n1:1: foreign key를 어디에 두어도 상관 없음\nCREATE UNIQUE INDEX idx_1_1 ON table(foriegn_key);\n1:N: many(child) 쪽에 foreign key를 두는 것이 일반적\n1 side is called parent, many side is called child\nM:N\nData Modeling에서만 쓰임. database design에서는 intersection table을 사용하여 표현. intersection table은 두 entity의 primary key를 포함하는 composite key를 가짐\n만약 두 primary key 외의 attribute를 가진다면, association entity로 표현\nSupertype / Subtype: Supertype의 primary key를 Subtype의 primary key로 사용\nRecursive Relationship: 방향 이거 다시 보자\nN:M의 경우 virtual table을 생성하여 표현\n\n설문조사는\ndescriptive statistics\n남녀 비율, 경험 비율 등등도 포함되어야 한다.\n가중 평균으로 보여준다\n도서관 예약 시스템\n\n퇴설 처리 미흡\n좌석 이용 정보 파악\n앱 알림\n\n좌석 배치도 감이 안온다. 잔여시간도 안뜬다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "title": "ASP.NET",
    "section": "3-Tier Layers of Database System",
    "text": "3-Tier Layers of Database System\n\npresentation layer: user interface\napplication layer: web server(IIS)\ndata layer: database server",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "title": "ASP.NET",
    "section": "API Interface Standards for DB Access",
    "text": "API Interface Standards for DB Access\nDBMS에 접근하기 위한 표준 API\n\nODBC Open Database Connectivity\nDBMS-independent API\nJDBC: Java Database Connectivity\n\n&lt;a target=\"_blank\"&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "title": "ASP.NET",
    "section": "ASP (Active Server Pages)",
    "text": "ASP (Active Server Pages)\nserver side scripting(VBScript) language\nCGI: &lt;% %&gt;는 server에서 실행되는 코드",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "title": "ASP.NET",
    "section": "ASP 데이터베이스 연동",
    "text": "ASP 데이터베이스 연동\n&lt;%\n  Dim conn, connCmd, rs\n  Set connCmd = \"DSN=dsn_name; Database=dbname; UID=user;PWD=password\"\n  Set conn = Server.CreateObject(\"ADODB.Connection\")\n  Set rs = Server.CreateObject(\"ADODB.Recordset\")\n  conn.Open connCmd\n  rs.Open \"SELECT * FROM table_name\", conn\n%&gt;\n\n&lt;%\n  rs.getRows()\n\n  conn.Execute SQL\n%&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "title": "ASP.NET",
    "section": "오류 메세지 한글 설정",
    "text": "오류 메세지 한글 설정\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;%\n  Session.CodePage = 949\n  Response.CharSet = \"euc-kr\"\n  Response.AddHeader \"Pragma\",\"no-cache\"\n  Response.AddHeader \"cache-control\", \"no-staff\"\n  Response.Expires = -1\n%&gt;\n\nform tag 한글 깨짐 문제\n&lt;%\nSession.CodePage=\"65001\"\nResponse.CharSet=\"UTF-8\"\n%&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "title": "An Overview of Database",
    "section": "The Importance of DBs Today",
    "text": "The Importance of DBs Today\n\nDepend upon database: Internet, Web 2.0, IOT",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "title": "An Overview of Database",
    "section": "Why and How Databases are Used?",
    "text": "Why and How Databases are Used?\n\nThe purpose of a database is to keep track of thing\ndb store information that is more complicated than a simple spread sheet",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "title": "An Overview of Database",
    "section": "Problems with Lists (spread sheet)",
    "text": "Problems with Lists (spread sheet)\n\nRedundancy\n\n\n\n\n필요없는 column들이 중복됨\n\n\n\nMultiple Themes\n\n\n그 결과로, list에 나타날 때만 존재하는 informartion이 생김\n\n\nList Modification Issues\n\n\n\n\ndeletion problems, update problems, insertion problems",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "title": "An Overview of Database",
    "section": "Relational Databases",
    "text": "Relational Databases\n\nRelationa Model is methodology used as a solution for database design\nA relational database stores information in tables\n\nEach informational topic is stored in its own table\n\nEach theme in the list can be stored in a table\n\nTable = file = relation\ncolumn = fields = attribute\nrow = record = tuple",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "title": "An Overview of Database",
    "section": "SQL (Structured Query Language)",
    "text": "SQL (Structured Query Language)\n\ninternational standard for creating, processing, querying databases and their tables\ndb applications use SQL to retrieve, format, report, insert, delete, modify data for users\ncan combine table by join operation\n\nSELECT  CUSTOMER.CustomerLastName, \n        CUSTOMER.CustomerFirstName, \n        CUSTOMER.Phone,\n        COURSE.CourseDate, \n        ENROLLMENT.AmountPaid,\n        COURSE.Course, \n        COURSE.Fee\nFROM    CUSTOMER, ENROLLMENT, COURSE\nWHERE   CUSTOMER.CustomerNumber = ENROLLMENT.CustomerNumber -- join condition\n        AND  COURSE.CourseNumber = ENROLLMENT.CourseNumber; -- join condition",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "title": "An Overview of Database",
    "section": "Database System (DBS)",
    "text": "Database System (DBS)\n\n\n\nThe four components of database system\n\n\n\nUser: Employ database application to keep track of things\nUse forms to read, enter, query data\nproduce reports\nDatabase Application: web/mobile database applications, Forms, Reports\nDBMS: used to create, process, administer the database\nDatabase: self-describing collection of related tables\nuser data, metadata, index and other overhead data, application metadata(form, reports) are stored in db\nmetadata = about the structure of the database. &lt;-&gt; user data\n\n\nFunction of DBMS\n\nDB administration\n\nControl concurrency\nProvide security\nPerform backup and recovery\n\n\n\n\nReferential Integrity Constraints",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "title": "An Overview of Database",
    "section": "Personal vs Enterprise-class Database Systems",
    "text": "Personal vs Enterprise-class Database Systems\n\nPersonal: Access\nEnterprise-class(Organizational): Microsoft SQL server",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "title": "An Overview of Database",
    "section": "NoSQL databases",
    "text": "NoSQL databases\n\nNoSQL database = non-relational database",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "title": "An Overview of Database",
    "section": "Cloud databases",
    "text": "Cloud databases\nMain frame -&gt; Client/server -&gt; Cloud",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#data-and-information",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#data-and-information",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Data and information",
    "text": "Data and information\n\nData: raw facts. recorded facts\nInformation: meaningful context\nKnowledge: information + 가치",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "What is information system?",
    "text": "What is information system?\n\nSystem: a set of components that interact to achieve some purpose or goal\nInformation System: composed of hardware, software, data, procedures, people",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "System Analysis and Design",
    "text": "System Analysis and Design\n\nSystem analysis and design: process of creating and maintaining information systems\nclassic methodology: SDLC\n\n\nSDLC (System Development Life Cycle)\n\n\n\nSDLC\n\n\n\nSystem definitions: 예산 편상, 위험 분석, …\nRequirements analysis\nComponent design\nImplementation\nSystem maintenance\n\n\ndatabase development process\n\nRequirements analysis\ninput: the project plan\noutput: a set of approved requirements -&gt; data model (ER model로 conceptual design)\nsource: Use cases, Business rules\nComponent Design: Relational Database Design (상세 설계)\nImplementation",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#er-model",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#er-model",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "ER model",
    "text": "ER model\n\nEntities\n\nEntity class\nEntity instance\n\nAttributes: Data type, Properties(default, constraints)\nIdentifiers\n\nunique\nNonunique: identifies a set of instances\n\nRelationships\n\nbinary relationship\n\nMaximum cardinality: 1:1(A has a B), 1:N(A has a set of B), M:N\nMinimum cardinality: 0, 1\n\nternary relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Entit-Relationship Diagram",
    "text": "Entit-Relationship Diagram\n\nEntity classes: rectangle\nRelationships: diamond\nmaximum cardinality: inside the diamond\nminimum cardinality: oval or hash mark next to diamond\nstrong entity: 독자적으로 존재 가능. 강한개체 관계는 점선\nNon-ID-dependent: identifier에 다른 entity의 identifier가 포함되어 있지 않음. 점선으로 표기(non-identifying relationship)\nweak entity: 약, 강 관계는 실선. IS: rounded square, traditional: 2 layer square\nID-dependent: identifier에 다른 entity의 identifier가 포함되어 있음. 실선으로 표기(identifying relationship)\nassociative entity: relationship이 entity로 변환된 것.\nMany-to-many relationship을 2개의 1:N으로 변환\nsuper type, sub type: 상속관계. sub type is a super type\n\nexclusive: Discriminator attribute가 필요함\ninclusive\n\nrecursive relationship\nBusiness rule: build-in constraints, trigger, stored procedure, application code로 구현 가능\ndata model validation: form, report를 이용한 prototyping",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#통계학",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#통계학",
    "title": "확률과 통계 1 정리",
    "section": "통계학",
    "text": "통계학\n\n불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률",
    "title": "확률과 통계 1 정리",
    "section": "확률",
    "text": "확률\n\n고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n통계적 의미: 특정 사건이 발생하는 상대도수의 극한\n\n각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포 정의 단계",
    "text": "확률 분포 정의 단계\n\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n확률 변수: 확률실험의 결과를 수치로 나타낸 변수",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포",
    "text": "확률 분포\n\n이산 확률 분포: 이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n베르누이 분포: 각 시행은 서로 독립적이고, 실패와 성공 두 가지 결과만 존재.\n\n단 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면 비복원 추출에서도 유효\n\n이항 분포: n번의 독립적인 베르누이 시행을 수행하여 성공 횟수를 측정\n기하 분포: 성공 확률이 p인 베르누이 시행에서 첫 성공까지의 시행 횟수\n초기하 분포: 베르누이 시행이 아닌 시행에서 성공하는 횟수\n포아송 분포: 임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수\n\nn이 매우 크고, p가 매우 작을 때, 이항 분포를 포아송 분포로 근사할 수 있다.\n\n\n연속 확률 분포: 연속 표본 공간에서 정의 가능\n\n균일 분포\n정규 분포\n\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)\n\nt 분포\n\n자유도가 커질수록 표준 정규분포에 근사함.\n t(n)\n\nf 분포\n\n\\(F = \\frac{X_1/ν_1}{X_2/ν_2}\\), \\(X_1 \\sim χ^2(ν_1)\\), \\(X_2 \\sim χ^2(ν_2)\\)\n\n감마 분포\n\n카이제곱 분포: α = v/2, θ = 2 인 감마분포\n\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)\n\n지수 분포: 포아송 분포에서 사건 발생 간격의 분포\n\n\\(\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)\\), \\(θ = 1/λ\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#표본의-분포",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#표본의-분포",
    "title": "확률과 통계 1 정리",
    "section": "표본의 분포",
    "text": "표본의 분포\n\n샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n통계량: 표본의 특성을 나타내는 값\n추정량: 아래의 조건을 만족하는 통계량\n\n불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n\n\n표본 평균의 분포\n\n모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 \\(σ^2\\)이면, \\(\\bar{X}\\)의 평균은 μ이고, 분산은 \\(σ^2/n\\)인 정규분포를 따른다.\n\n단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리)\n\n만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n\n\n\n표본 분산의 분포\n\n정규 모집단으로 부터 나온 표본의 분산 S에 대하여, \\(\\frac{(n-1)S^2}{σ^2}\\)은 자유도가 n-1인 카이제곱 분포를 따른다.\n\n모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n\n두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#가설검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#가설검정",
    "title": "통계적 가설검정",
    "section": "가설검정",
    "text": "가설검정\n\n귀무가설(\\(H_0\\)): α의 값을 상한으로 지정하고 보수적으로 고려\n대립가설(\\(H_1\\))\n검정 통계량: \\(H_0\\)가 참이라고 가정했을 때, 표본에서 계산된 통계량\n기각역: \\(H_0\\)를 기각할 수 있는 범위\nType 1 error를 보통 보수적으로 지정하기 때문에 Type 2 error는 높아진다. (상충 관계)\n\n둘 다 줄이고 싶다면 n을 늘려야 한다.\n일반적으로 α(Type 1 error)를 고정하고 원하는 β(Type 2 error)를 만족하는 표본의 크기를 결정한다.\n\n상단측 검정\n하단측 검정\n양측 검정\n\n→ 기각역 계산에 주의하자\n\n모비율 차이 검정\n\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - D_0}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\)\n\\(\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\\)\n\n\n\nβ 계산\n\\(H_0\\)를 이용해 검정 통계량을 계산하고, 이를 이용해 기각역을 구한 후, \\(H_1\\)을 이용해 계산.\n\n\n대표본 단측 가설검정 표본 크기1\n\n\\(n = \\frac{(z_{1 - \\alpha} + z_{1 - \\beta})^2\\sigma^2}{(\\mu_1 - \\mu_2)^2}\\)\n\n\n\n가설 검정 절차와 신뢰구간의 관계\n\n\\(\\hat{θ} - z_{1-α/2}σ_{\\hat{θ}} ≤ θ_0 ≤ \\hat{θ} + z_{1-α/2}σ_{\\hat{θ}}\\)\n100(1-α)% 신뢰구간은 유의수준 α에서 귀무가설 \\(H_0: θ = θ_0\\)가 채택되는 모든 \\(θ_0\\) 값의 집합.\n\n\n\np-value\n\np-value: \\(H_0\\)를 기각시킬 수 있는 가장 작은 유의수준 α의 값 (즉 확률)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#series",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#series",
    "title": "pandas",
    "section": "Series",
    "text": "Series\n\nindex, value로 이루어진 데이터 구조\n\n\nimport pandas as pd\n\ns = pd.Series(['1', 3, 5, 7, 9])\n\n\ndata = {\n  'yo': {\n      'hey': 300\n  },\n  'hey': [200, 500],\n  'haha': [100, 90]\n}\ndata = pd.Series(data)\ndata.name = 'what'\ndata.index.name = 'kiki'\ndata\n\nkiki\nyo      {'hey': 300}\nhey       [200, 500]\nhaha       [100, 90]\nName: what, dtype: object",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "pandas"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/03.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/03.html",
    "title": "Support vector machine",
    "section": "",
    "text": "bias variance tradeoff\n\nmargin ↑, bias ↑, variance ↓\nmargin ↓, bias ↓, variance ↑\n\nsuppot vector: 임계값에 가까운 데이터 포인트\nlinear classification fomulation:\n\n\n\n\nif not linear solvable → kernel functions\nsoft margin: 이상치를 포함할 수 있는 마진\n\ncross validation으로 최적의 soft margin을 찾는다.\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Support vector machine"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/10.html#train-test-split",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/10.html#train-test-split",
    "title": "Data Transformation",
    "section": "train test split",
    "text": "train test split\n\nHold out method: sub sampling\n\n\nvalidation data set: 하이퍼파라미터를 튜닝\n층화추출법: stratified sampling\n\n\nresampling method: Hold out을 여러번 반복. variance 극복 bias 극복 x\n\ncross validation\n\n\nk-fold\nleave-one-out:\n\n\nbootstrap",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Data Transformation"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/10.html#data의-특성",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/10.html#data의-특성",
    "title": "Data Transformation",
    "section": "data의 특성",
    "text": "data의 특성\nvolume - Data size big - Data size small(long data) velocity variety veracity(정확성) value\n\nimport numpy as np\nfrom sklearn.model_selection import LeaveOneOut\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array(['a', 'b', 'c'])\nloo = LeaveOneOut()\nloo.get_n_splits(X)\n\n3\n\n\n\nfrom sklearn.model_selection import KFold\n\nX = np.array([[10, 20], [30, 40], [15, 19], [34, 41], [11, 21], [33, 39]])\ny = np.array([0, 1, 0, 1, 0, 1])\nkf = KFold(n_splits=3)\n\nkf.get_n_splits(X)\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nTRAIN: [2 3 4 5] TEST: [0 1]\nTRAIN: [0 1 4 5] TEST: [2 3]\nTRAIN: [0 1 2 3] TEST: [4 5]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Data Transformation"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/01.html#masking",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/01.html#masking",
    "title": "Numpy-2",
    "section": "Masking",
    "text": "Masking\n\nimport numpy as np\n\na = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nbool_indexing = (a % 2 == 0)\nprint(bool_indexing)\n\n[[False  True False]\n [ True False  True]\n [False  True False]]\n\n\n\nprint(a[bool_indexing])\n\n[2 4 6 8]\n\n\n\ndata = np.random.randn(4, 5)\nnp.around(data, 2) \n\narray([[ 1.48, -0.78,  1.11,  0.38, -1.24],\n       [-0.  , -0.25, -0.08,  0.62, -1.12],\n       [-2.27,  2.39, -0.85,  0.95, -1.6 ],\n       [ 1.72, -0.29,  1.04,  0.48, -0.14]])\n\n\n\ndata = np.array([[ 1.87883804, -0.39056004, 1.18374625, -0.91699153, 0.23666417],\n[ 0.28408269, 1.14786861, -1.54178089, 0.12426074, 0.54734241],\n[-1.67396474, -1.88974809, -0.09876402, 1.05047587, 1.31776863],\n[-0.27404289, -0.73640766, -0.16014918,\n-1.03578294, 0.62956063]])\ndata[data[:, 0] &lt;0, 1:3] = 1.0\n\n\na[::-1]\n\narray([[7, 8, 9],\n       [4, 5, 6],\n       [1, 2, 3]])\n\n\n\na = np.array([[1, 2, 3], [4, 5, 6]])\nprint(a.reshape(3, 2))\n\n[[1 2]\n [3 4]\n [5 6]]\n\n\n\nprint(a.T)\n\n[[1 4]\n [2 5]\n [3 6]]\n\n\n\nimport matplotlib.pyplot as plt\n\nx1 = np.arange(0, 10, 0.1)\nx2 = np.sin(x1)\n\nplt.plot(x1, x2)\nplt.show()\n\n\n\n\n\n\n\n\n\ny1=np.sin(x1)\ny2=np.cos(x1)\nplt.plot(x1, y1, label='sin')\nplt.plot(x1, y2, linestyle='--', label='cos')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('sin and cos')\nplt.legend()\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Numpy-2"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/00.html#수업-요약",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/00.html#수업-요약",
    "title": "Numpy",
    "section": "수업 요약",
    "text": "수업 요약\n\nnumpy랑 pandas 비교, broadcast\nnumpy 다차원 pandas 2차원\npython list 보다 빠름\n차원을 rank, 크기를 shape\n\n\nimport numpy as np\n\nv1 = np.arange(1, 10, 2) ** 2\nprint(v1)\n\n[ 1  9 25 49 81]\n\n\n\nv2 = np.arange(3, 10, 1, dtype=float)\nprint(v2)\n\n[3. 4. 5. 6. 7. 8. 9.]\n\n\n\n수업으로 공부하지 말고 따로 올려준 자료로 공부해라\n\n\na = np.zeros((2,2))\nprint(a)\n\n[[0. 0.]\n [0. 0.]]\n\n\n\na = np.ones((2, 3))\nprint(a)\n\n[[1. 1. 1.]\n [1. 1. 1.]]\n\n\n\nlst1 = [[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n        [9, 10, 11],\n        [12, 13, 14]]\na = np.array(lst1)\na[[1, 2]]\n\narray([[4, 5, 6],\n       [7, 8, 9]])\n\n\n\na = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nb = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\na + b\n\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18]])",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Numpy"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#선형-계획을-푸는-알고리즘",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#선형-계획을-푸는-알고리즘",
    "title": "Linear Programming Algorithm",
    "section": "선형 계획을 푸는 알고리즘",
    "text": "선형 계획을 푸는 알고리즘\n\n그래프를 사용하는 방법\n변수가 3개 이하인 경우, 그래프를 그려서 해를 찾을 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#simplex-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#simplex-method",
    "title": "Linear Programming Algorithm",
    "section": "Simplex Method",
    "text": "Simplex Method\n\n기하학적 이해\n\n\nInitialization: Collect 1 CFP. 일반적으로 원점을 선택\nOptimality test: find better adj\n\nObj ⋅ (adj - cur) &gt; 0: better\nObj ⋅ (adj - cur) = 0: not changed\nObj ⋅ (adj - cur) &lt; 0: worse\n\n\n\n\n대수적 풀이\n\nbasic solution: 제약식의 변수 중 일부를 기저 변수로 선택하고, 나머지를 0으로 설정하여 얻는 해.\n\n만약 기저변수가 0인 경우, 이를 퇴화라고 부른다.\n\nbasic feasible solution: 모든 변수가 0 이상인 basic solution. 즉, 제약식을 모두 만족하는 해.\n비 기저변수(non basic variable): free variable. 변수의 수 - 방정식의 수 만큼 존재.\n기저 변수(bais variable): pivot variable.\n풀이는 생략\n\n\n\nSimplex Tableau\n\n풀이는 생략\n\n\n\n비 표준형 모델에서의 적용\n\n제약식이 = 인 경우: 인공 변수 추가. 목적 함수에 Big-M 방법을 사용하여 표현.\n\nTableau에서 인공변수의 계수를 0으로 만들어서 진행.\n\n제약식이 ≥ 인 경우: slack 변수랑 surplus variable 추가. surplus variable에 대하여 목적 함수에 Big-M 방법을 사용하여 표현.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/00.html",
    "href": "posts/01_projects/bs_3_1/notes/computer/00.html",
    "title": "intro",
    "section": "",
    "text": "개별 과제는 없음\n8주차, 9주차 안나옴\n11주차 시험. LMS로 봄\n14주차 발표는 유튜브로 찍어서\n15주차 학교 안나옴\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "title": "Matching Supply with Demand",
    "section": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지",
    "text": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지\n\n운영하는 관점에서 수요와 공급을 바라볼 예정\n기업을 바라보는 관점: 유/무형의 제품을 생산해서 수요(양, timing, 품질, …)에 맞게 공급하기 위해 노력하는 집단",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급 법칙",
    "text": "수요와 공급 법칙\n\n\n\n파란색 - 수요, 빨간색 - 공급\n\n\n\n경제학 기본적인 법칙. 가격 조정은 건강한 시스템의 증거라고 봄\n운영 관리자(OM)인 우리는 이거랑 다르게 바라봄.\n\nExcess demand = lost revenue\nExcess supply = wasted resources\n가격 조정만으로 수요와 공급 맞추기 어려움\n\n끊임없이 변하는 수요와 비 탄력적인 공급으로 인해 수요와 공급을 맞추기 어렵다.\n과학적 도구로 최대한 수요를 예측하고, 탄력적인 공급을 하는 방법을 찾아야 한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급이 안 맞는 사례",
    "text": "수요와 공급이 안 맞는 사례\n\n푸바오를 보기 위해 사람들이 몰림\n\n수요 공급의 불균형은 안 좋은 효과를 가져옴\n\n마스크, 먹태깡: 수요는 빠르게 변하는데, 공급은 느리게 변함\n\n수요를 예측하고, 설비를 미리 준비하는 과학적 도구가 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소매업\n철광석 공장\n응급실\n심박조율기\n항공 여행\n\n\n\n\n공급\n소비자 전자제품\n철광석\n의료 서비스\n의료 장비\n특정 항공편 좌석\n\n\n수요\n새로운 비디오 시스템을 구매하는 소비자\n제철소\n긴급한 의료 서비스 수요\n심박조율기가 특정 시간과 장소에서 필요한 심장외과 의사\n특정 시간과 목적지로의 여행\n\n\n공급이 수요를 초과\n재고 비용이 높고, 재고 회전율이 낮음\n가격 하락\n의사, 간호사 및 인프라가 충분히 활용되지 않음\n심박조율기가 재고로 남아 있음\n빈 좌석 발생\n\n\n수요가 공급을 초과\n포기한 이익 기회; 소비자 불만족\n가격 상승\n응급실 혼잡 및 지연; 구급차 우회 가능성\n포기된 이익 (일반적으로 의료적 위험과는 관련 없음)\n초과 예약으로 인해 고객이 다른 항공편을 이용해야 함 (이익 손실)\n\n\n공급과 수요를 맞추기 위한 조치\n수요 예측; 신속한 대응\n가격이 지나치게 하락하면 생산 시설이 폐쇄됨\n예측된 수요에 맞춘 인력 배치; 우선순위 설정\n여러 장소에서 심박조율기를 보관하는 유통 시스템\n동적 가격 책정; 예약 정책\n\n\n관리적 중요성\n소비자 전자제품 소매업의 단위당 재고 비용이 종종 순이익을 초과함\n가격 경쟁이 치열하여 주요 초점은 공급 비용 절감에 맞춰짐\n치료 또는 이송 지연이 사망과 연관된 사례 있음\n대부분의 제품(가치 2만 달러)이 사용되기 전에 영업 사원의 차량 트렁크에서 4~5개월 동안 대기함1\n전체 좌석의 약 30%가 빈 채로 운항되며, 좌석 이용률이 1~2%만 증가해도 이익과 손실이 갈림2",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템의 performance",
    "text": "생산 시스템의 performance\n\n서로 상충됨. business 목표에 맞게 balance를 잘 맞춰야함\n\ncost\nquality: 품질이 얼마나 좋고 일관되냐\nvariety: 다양한 사용자의 니즈를 얼마나 잘 맞추냐\ntime",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템 관리를 배우면 할 수 있는 것",
    "text": "생산 시스템 관리를 배우면 할 수 있는 것\n\n비효율성 분석\n상충관계에 대한 의사결정\n신기술 등에 대한 평가",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#footnotes",
    "title": "Matching Supply with Demand",
    "section": "각주",
    "text": "각주\n\n\n뭔소리지↩︎\n뭔소리지↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#간트-차트1",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#간트-차트1",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "간트 차트1",
    "text": "간트 차트1\n\n\n\n방사선조영실 작업 칸트 차트\n\n\n\n작업시간 / 처리시간을 표현\n프로세스 상의 작업 순서와 소요시간, 상호관계를 볼 수 있음\n대기 / 지연2: 수요와 공급의 불일치, 작업들에 존재하는 불확실성으로 생기는 것",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-평가를-위한-3가지-요소",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-평가를-위한-3가지-요소",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "프로세스 평가를 위한 3가지 요소",
    "text": "프로세스 평가를 위한 3가지 요소\n\n흐름률(flow rate / throughput): 실제 흐름 단위가 프로세스에 진입 / 떠나는 비율\n\n흐름단위 수 / 단위 시간\n흐름률이 오르면 생산 능력이 오른다.\n유량: 특정한 시간동안 관찰하는 양\n매출 원가(들어온 가격 기준)를 흐름률로 바라볼 수 있다.\n\n흐름시간(flow time): 하나의 흐름단위가 프로세스상에 머무는 시간\n\n흐름시간이 줄어들면 수요-공급 사이의 시간도 줄어든다.\n\n재고(inventory): 프로세스 상에 존재하는 흐름단위 수\n\n가공중인 제품 WIP(work-in-process)도 재고에 포함\n저량(stock): 특정 시점에서 관찰하는 양\n\n\n\n\n\n\n\n\n\n\n\n프로세스\n\n\n\n흐름(flow): 작업이 진행되는 것을 tracking\n\n단위: 일반적으로 산출물의 단위로 정의",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#리틀의-법칙",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#리틀의-법칙",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "리틀의 법칙",
    "text": "리틀의 법칙\n위의 세개와 수요 공급간의 관계가 있다.\n\n\n\n기울기는 흐름률\n\n\n\nI = R * T (항상 성립)1\n\nI: 평균 재고 (flow time 동안 들어온 input)\nR: 평균 흐름률\nT: 평균 흐름시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#재고를-카운트하는-방법",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#재고를-카운트하는-방법",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고를 카운트하는 방법",
    "text": "재고를 카운트하는 방법\n\ninput이 여러개일 경우 단순히 흐름단위만으로 재고를 표현하기 어려울 수 있다.\n\n\nIn terms of $s: I. 원가 기준\nIn terms of days-of-supply(DOS, 공급일수): I / R = T\nIn terms of inventory turns(재고 회전율): R / I = 1 / T\n\n\nTurns and DOS at Kohl’s and Walmart\n\n\n\n두 회사의 재무재표\n\n\n\n위의 사례에서 두 기업의 전략을 볼 수 있음.\nKohl’s: 회전률이 낮은 대신 마진을 높임\nWalmart: 마진이 낮은 대신 회전률을 높임\n\n\n\n재고가 부담이 되는 이유\n\n이자비용\n유지비용\n\n재고가 구식으로 변함\n물리적으로 부식됨\n사라질 수 있음\n저장공간과 추가적인 간접비 유발\n품질의 저하에 따르는 추가적인 비용 존재\n\n재고 비용4: 연간 재고 유지 비용 / 연간 재고 회전율",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#재고-유지의-다섯-가지-이유4",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#재고-유지의-다섯-가지-이유4",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고 유지의 다섯 가지 이유5",
    "text": "재고 유지의 다섯 가지 이유5\n\n재고 유지는 기업 입장에서 부담이 되지만 그럼에도 불구하고 유지하는 이유가 있다.\n\n\n수송중재고: 프로세스에 존재하는 재고\n계절재고: 공급 능력은 고정되어 있는데 수요는 변동하는 경우(예측 가능한 수요), 미리 만들어둠\n주기재고: 한 번에 많이 사는게 싸다. 규모의 경제 이용한 비용 절감\n완충재고(buffer, decoupling): 프로세스상의 작업 사이의 지속적 공급을 가능하게 해줌. 단 제고가 계속 쌓이지 않게 line balancing(각 프로세스에서 진행하는 일의 양의 밸런스)을 해줘야 함\n안전재고: 불확실성에 대비해 예측된 수요보다 더 많이 재고를 유지함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#footnotes",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "각주",
    "text": "각주\n\n\n시험문제에 더 복잡하게 낸다고 하시긴 함↩︎\n이것들의 차이와 의미하는 바, 사례를 보고 어떤걸 의미하는지 알아야 한다.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#setup과-생산주기",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#setup과-생산주기",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "setup과 생산주기",
    "text": "setup과 생산주기\n\nsetup: 기계를 준비하는데 필요한 것\n\n정확히 하나의 제품을 만드는 경우에도 setup이 필요함\n생산하는 양에 관계없이 setup 시간이 일정함\nsequence dependent setup: 순서에 따라 setup 시간이 달라짐\n\n생산주기(production cycle): setup + 생산의 과정을 반복\n\nsetup은 아무것도 못하고 시간을 버림",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#배치-생산과정",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#배치-생산과정",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "배치 생산과정",
    "text": "배치 생산과정\n\nbatch1: 부품 집합을 흐름 단위로 사용\n생산 주기: batch size만큼 생산하는 주기\n처리능력: \\(\\frac{batch size}{setup time + (batch size * processing time per unit)}\\)\n\nbatch size가 무한히 커질수록 \\(\\frac{1}{p}\\)로 수렴\nsetuptime이 0이여도 \\(\\frac{1}{p}\\)\n\n\n\n\nbatch는 클 수록 좋은가?\n\nbatch size가 커질수록 처리능력이 증가하지만 재고가 많아짐\n→ 처리능력 제약적 상황에서 bottleneck의 batch size를 늘리고, 수요 제약적 상황에서 non-bottleneck의 batch size를 줄이는게 좋음\n→ \\(\\frac{B}{S + Bp} = R → B = \\frac{SR}{1 - Rp}\\)\nR보다 크면 쓸데없이 제고가 쌓이고, 작으면 capacity가 낮아짐\n\nS가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨\np가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/scholarships/2.html",
    "href": "posts/01_projects/bs_3_1/notes/scholarships/2.html",
    "title": "성적 장학금",
    "section": "",
    "text": "오~예~ (남은 등록금 300만원을 대출 받으며)\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Scholarships",
      "성적 장학금"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html",
    "href": "posts/01_projects/bs_3_1/index.html",
    "title": "학부 3학년 1학기",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-06-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#details",
    "href": "posts/01_projects/bs_3_1/index.html#details",
    "title": "학부 3학년 1학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 1학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#tasks",
    "href": "posts/01_projects/bs_3_1/index.html#tasks",
    "title": "학부 3학년 1학기",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    푸른등대 기부장학금 - 두나무UDC 신청 (~2025-01-20 18:00)\n                \n                불합격\n            \n            \n            \n                \n                \n                    2025 DB 드림리더 장학생 신청 (~2025-01-10)\n                \n                잘할 자신이 없다\n            \n            \n            \n                \n                \n                    경기도 학자금대출 이자 지원 신청 (~2025.02.14 18:00)\n                \n                신청 완료\n            \n            \n            \n                \n                    \n                    학과 근로 신청\n                \n                신청 완료\n            \n\n            \n            \n                \n                    \n                    KMOOC 학점 인정 신청 (2025-03-10~)\n                \n                하나는 기간이 지나 버렸다. 종강 전에 하나만 더 듣자.\n            \n\n            \n            \n                \n                \n                    봉사활동 계획서 작성 (2025-03-11~)\n                \n                작성 완료\n            \n            \n            \n                \n                \n                    OR 과제 1 (~2025-03-16 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 2 (~2025-03-23 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                    \n                    교수님과 커피\n                \n                나 커피 못 마시는데\n            \n\n            \n            \n                \n                \n                    데이터마이닝 팀과제 주제 제안\n                \n                일단 완성\n            \n            \n            \n                \n                \n                    OR 과제 3 (~2025-04-06 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 4 (~2025-04-13 23:59)\n                \n                그만...\n            \n            \n            \n                \n                \n                    data mining 1차 과제 ppt 완성\n                \n                done\n            \n            \n            \n                \n                    \n                    진로 지도 상담 받기\n                \n                \n            \n\n            \n            \n                \n                \n                    컴퓨팅적 사고 발표 ppt 만들기\n                \n                \n            \n            \n            \n                \n                    \n                    데이터마이닝 2차 과제 준비\n                \n                노션에 정리 중\n            \n\n            \n            \n                \n                    \n                    교통비 지원금 신청\n                \n                \n            \n\n            \n            \n                \n                \n                    OR 과제 6 (~2025-05-18 23:59)\n                \n                힘들다\n            \n            \n            \n                \n                \n                    OR 과제 7 (~2025-05-25 23:59)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#필요한-자료",
    "href": "posts/01_projects/bs_3_1/index.html#필요한-자료",
    "title": "학부 3학년 1학기",
    "section": "필요한 자료",
    "text": "필요한 자료\n\n자기소개서 작성 1\n교육 이수 증빙자료\nPortfolio: 큰일 났다. 진짜 못만들었다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#참고-자료",
    "href": "posts/01_projects/bs_3_1/index.html#참고-자료",
    "title": "학부 3학년 1학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#related-posts",
    "href": "posts/01_projects/bs_3_1/index.html#related-posts",
    "title": "학부 3학년 1학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "경제적 주문량 모형",
    "text": "경제적 주문량 모형\n\n외부 공급자에게 부품을 주문하여 생산 및 배송이 이루어지는 경우\n단위시간당 발생하는 비용이 적을수록 좋다\n\n\n\n\n재고량 패턴(재고가 0이 되었을 때 정확히 도착하도록 주문할 수 있다고 가정)\n\n\n\nQ: 한 번에 주문하는 양\nR: 수요(기울기)\n주문 주기: \\(\\frac{Q}{R}\\)\n평균 재고량: \\(\\frac{Q}{2}\\)\n\n\n구매비용(purchase cost / variable cost): 단위 시간 당 구매비용은 Q에 영향을 받지 않음\n단위 재고 비용(h)\n\n단위 시간 당 발생하는 재고 비용: \\(h\\frac{Q}{2}\\)\n\n셋업(주문) 비용 (Fixed cost) (k): 주문량과 무관\n\n단위 시간 당 발생하는 셋업 비용: \\(\\frac{k}{\\frac{Q}{R}}\\)\n\n\n\n목적 함수: \\(C(Q) = \\frac{KR}{Q} + \\frac{hQ}{2}\\)\n경제적 주문량(EOQ): \\(Q^* = \\sqrt{\\frac{2KR}{h}}\\)\n\nK: 주문비용\nR: 수요량\nh: 단위 재고비용\n\nEOQ만큼 주문할 때 단위 시간당 비용\n\n\\(C(Q^*) = \\sqrt{2KhR}\\)\n\n단위당 비용 = \\(\\frac{C(Q^*)}{R} = \\sqrt{\\frac{2Kh}{R}}\\)\n수요가 증가함에 따라 EOQ는 늘어나는데 단위 당 비용은 감소\n\\(\\frac{C(Q)}{C(Q^*)} = \\frac{1}{2}(\\frac{Q^*}{Q} + \\frac{Q}{Q^*})\\)\n\\(\\frac{1}{주문 주기} ≠ 재고 회전율\\)\n\n주문 주기는 Q개가 다 없어지는 시간\n회전율은 Q/2개의 재고가 다 없어지는 시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "",
    "text": "다다음주 월요일 - 수업 x\nIU가 100 이하여도 대기가 발생할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/11.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/11.html",
    "title": "Homework - 2",
    "section": "",
    "text": "import pandas as pd\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv(\"./_data/class/FFvote.csv\", encoding='utf-8')\ndataset.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngender_female\ngender_male\nregion_Chuncheung\nregion_Honam\nregion_Sudo\nregion_Youngnam\nregion_others\nedu\nincome\nage\nscore_gov\nscore_progress\nscore_intention\nvote\nparties\n\n\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1.0\n0.666667\n0.666667\n0.25\n0.25\n0.75\n1\n2\n\n\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0.5\n0.666667\n0.666667\n0.25\n0.75\n0.50\n0\n3\n\n\n2\n2\n0\n1\n0\n1\n0\n0\n0\n0.0\n0.333333\n1.000000\n0.00\n0.50\n0.45\n1\n4\n\n\n3\n3\n1\n0\n0\n0\n1\n0\n0\n0.5\n0.000000\n0.666667\n1.00\n0.75\n0.40\n1\n1\n\n\n4\n4\n0\n1\n0\n0\n1\n0\n0\n0.0\n0.333333\n1.000000\n0.75\n0.50\n0.35\n1\n1\n\n\n\n\n\n\n\n\nX = dataset.loc[:, 'gender_female':'score_intention'].values\ny = dataset['vote'].values\n\n어떻게 하면 scaling을 간단하게 처리할 수 있을까 고민하던 중 sklearn pipeline 문서의 Safety 부분을 참고해서 작성해봤습니다.\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n      StandardScaler(),\n      KNeighborsClassifier(n_neighbors=42)\n)\n\nfor k in (3, 4, 5):\n      kf = KFold(n_splits=k, shuffle=False)\n      accuracies = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy')\n      print(f\"Accuracy for K={k}: {round(accuracies.mean(), 2)}\")\n\nAccuracy for K=3: 0.71\nAccuracy for K=4: 0.71\nAccuracy for K=5: 0.72\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework - 2"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (단일 자원)",
    "text": "대기시간 예측 (단일 자원)\n\n가정\n\n내재활용률은 100% 미만\n\nif D &gt; C, 대기 - 처리능력 부족 (+ 변동성)\nif D &lt; C, 대기 - 변동성\n\n안정적 도착: 평균 고객 수가 시점에 의존하지 않고, 길이에만 의존함\n\n만약 프로세스가 안정적이지 않다면 더 짧은 시간간격으로 나누어 접근\n\n지수분포를 따르는 도착간격\n\n\\(CV_a = 1\\)\n비기억 특성\n\n\n\n\n변수\n\na: 평균 도착 간격 (줄 기준)\np: 평균 서비스 시간\n\\(CV_a\\): 도착간격의 변동계수\n\\(CV_p\\): 서비스 시간의 변동계수\n\\(T_q\\): 대기 시간\n\\(I_q\\): 대기열의 재고\n\\(I_p\\): 서비스 중 재고\n\n\n\n\n공식\n\ncapacity: \\(\\frac{1}{p}\\)\nflow rate = demand(수요 제약적 상황을 가정하니까): \\(\\frac{1}{a}\\)\nutilization: \\(\\frac{p}{a}\\)\nT: \\(T_q\\) + p\n\\(I_p\\): (1 - u) * 0 + u * 1 = u\nI = \\(I_q\\) + \\(I_p\\) = \\(I_q\\) + utilization\n\\(T_q = p * \\frac{u}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n도착 간격이 지수분포를 따르지 않는 경우 근사치만을 제공\n\n\\(I_q = \\frac{1}{a} * T_q = \\frac{T_q}{a}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/12.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/12.html",
    "title": "Dataminig 1차 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Dataminig 1차 발표 ppt"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "buffer or suffer",
    "text": "buffer or suffer\n\nbuffer 제고가 없으면 처리능력이 떨어질 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#대표본-가설검정-표본-크기",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#대표본-가설검정-표본-크기",
    "title": "통계적 가설검정",
    "section": "대표본 가설검정 표본 크기",
    "text": "대표본 가설검정 표본 크기\n\n\\(n = \\frac{(z_{1 - \\alpha} + z_{1 - \\beta})^2\\sigma^2}{(\\mu_1 - \\mu_2)^2}\\)\np-value: \\(H_0\\)를 기각시킬 수 있는 가장 작은 유의수준 α의 값",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "title": "통계적 가설검정",
    "section": "소표본 가설검정",
    "text": "소표본 가설검정\n\n쌍체표본: 두 집단이 독립이 아니고 서로 연관되어 있는 경우\n\n각 쌍의 차이를 계산하여 단일 표본으로 변환 후 분석할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html",
    "title": "classification with trees",
    "section": "",
    "text": "gini 지수가 낮을수록, misclassification error가 낮을수록, entropy가 낮을수록 Information gain이 높을수록, gain ratio가 클수록 순도가 높고 좋다. 최댓값은 0.5. entropy는 최댓값 1.\n과적합 pruning 할 때 misclassification error를 기준으로 한다.\n의사결정 트리는 데이터 마이닝에서 가장 널리 사용되는 분류 기법 중 하나로, 데이터의 패턴을 트리 구조로 표현하여 예측 모델을 구축한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#분산-검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#분산-검정",
    "title": "통계적 가설검정",
    "section": "분산 검정",
    "text": "분산 검정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "title": "ANOVA",
    "section": "여러 모집단의 평균 비교",
    "text": "여러 모집단의 평균 비교\n\n여러 모집단에 대해 2개씩 비교를 하면 α가 커짐\n\\(t(n_1 + n_2 - 2)^2 = F(1, n_1 + n_2 -2)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#통계적-검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#통계적-검정",
    "title": "통계적 가설검정",
    "section": "통계적 검정",
    "text": "통계적 검정\n\n가설 수립\n표본 추출\n통계량 계산\n가설 채택 / 기각",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#footnotes",
    "title": "통계적 가설검정",
    "section": "각주",
    "text": "각주\n\n\n양측은?↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html",
    "title": "시험 범위",
    "section": "",
    "text": "2주차\n\n순서, 시간복잡도 Big O 계산하는거. python code 5줄 이내로 간단한거로 나옴.\n재귀 알고리즘 간단한 소스코드, 결과 물어봄\n\n3~5주차\n\n연결리스트\n\n이중 연결리스트: 객관식\n원형: 시간복잡도, method 일부분? 전체 빈칸?\n\n\n5~6주차\n\nstack\n\nt자 철로 출력\nstack을 이용하는 응용\nstack을 이용하지 않는 중위 -&gt; 후위, 전위 표기\n\n\n7주차\n\n원형 queue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#intro",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#intro",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Intro",
    "text": "Intro\n\n지금까지는 변동성을 고려하지 않았지만 프로세스 성과 평가에 중요한 영향을 미친다.\n변동성이 대기시간에 미치는 영향을 살펴본다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#example",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#example",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Example",
    "text": "Example\n\n\n변동성\n\n불규칙한 도착 간격\n서비스 시간의 변동성\n영향: 재고, 대기시간, 산출 손실\n\nIU가 100 이하여도 대기가 발생할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-원인",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-원인",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "변동성의 원인",
    "text": "변동성의 원인\n\n흐름단위의 input (\\(CV_a\\))\n\nrandom arrival\nincoming quality\nproduct mix\n\nprocessing time의 변동성 (\\(CV_p\\))\n\n그냥 내재적인 변동성\n숙련도 (일을 못해서 오래걸림)\n품질 (재작업)\n\n자원의 무작위적 가용성\n\n자원 고장\n작업자 출근 안함\nsetup time\n\n복수의 흐름단위가 무작위적 경로결정\n\n경로의 변동성\n\n\n\n변동성의 측정: \\(\\frac{표준편차}{평균}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-측정",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-측정",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "변동성의 측정",
    "text": "변동성의 측정\n\n표준편차에 근거하여 측정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#풀링의-효과",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#풀링의-효과",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "풀링의 효과",
    "text": "풀링의 효과\n\n대기할 수 있는 방법은 여러가지가 있다\n대기시간을 줄일 수 있는 방법에 사람을 많이 뽑는것 외에 다른 고려 요소\n풀링: 대기열이 독립적이냐 아니냐",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "프로세스 흐름 분석",
    "text": "프로세스 흐름 분석\n\n간트 차트\n\n\n\n방사선조영실 작업 칸트 차트\n\n\n\n작업시간을 표현\n프로세스 상의 작업 순서와 소요시간, 상호관계를 볼 수 있음\n대기: 수요와 공급의 불일치, 작업들에 존재하는 불확실성으로 생기는 것\n\n\n\n프로세스 평가를 위한 3가지 요소\n\n흐름률(flow rate / throughput): 실제 흐름 단위가 프로세스에 진입, 떠나는 비율\n\n\\(\\frac{흐름단위 수}{단위 시간}\\)\n흐름률이 오르면 생산 능력이 오른다.\n유량: 특정한 시간동안 관찰하는 양\n매출 원가(들어온 가격 기준)를 흐름률로 바라볼 수 있다.\n\n흐름시간(flow time): 하나의 흐름단위가 프로세스상에 머무는 시간\n\n흐름시간이 줄어들면 수요-공급 사이의 시간도 줄어든다.\n\n재고(inventory): 프로세스 상에 존재하는 흐름단위 수\n\n매출 원가 기준\n가공중인 제품 WIP(work-in-process)도 재고에 포함\n저량(stock): 특정 시점에서 관찰하는 양\n\n\n\n\n\n\n\n\n\n\n\n프로세스\n\n\n\n흐름(flow): 작업이 진행되는 것을 tracking\n\n단위: 일반적으로 산출물의 단위로 정의\n\n위의 그림에서 흐름 단위는 상품 1개, 서비스 받은 고객 1명",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고 관리 메커니즘",
    "text": "재고 관리 메커니즘\n\n재고를 카운트 하는 방법\n\ninput이 여러개일 경우 단순히 흐름단위만으로 재고를 표현하기 어려울 수 있다.\n\n\nIn terms of $s: I. 원가 기준\nIn terms of days-of-supply(DOS, 공급일수): \\(\\frac{I}{R} = T\\)\nIn terms of inventory turns(재고 회전율): \\(\\frac{R}{I} = \\frac{1}{T}\\)\n\n\n\nTurns and DOS at Kohl’s and Walmart\n\n\n\n두 회사의 재무재표\n\n\n\n위의 사례에서 두 기업의 전략을 볼 수 있음.\nKohl’s: 회전률이 낮은 대신 마진을 높임\nWalmart: 마진이 낮은 대신 회전률을 높임\n공급 일수와 회전율은 반비례 관계에 있다.\n\n\n\n재고가 부담이 되는 이유\n\n이자비용\n유지비용\n\n재고가 구식으로 변함\n물리적으로 부식됨\n사라질 수 있음\n저장공간과 추가적인 간접비 유발\n품질의 저하에 따르는 추가적인 비용 존재\n\n제품 당 재고 비용: \\(\\frac{단위 시간 당 재고 유지 비용}{단위 시간 당 재고 회전율}\\)\n\n\n\n재고 유지의 다섯 가지 이유2\n\n재고 유지는 기업 입장에서 부담이 되지만 그럼에도 불구하고 유지하는 이유가 있다.\n\n\n수송중재고(pipeline): 프로세스에 존재하는 재고\n계절재고(seasonal): 공급 능력은 고정되어 있는데 수요는 변동하는 경우(예측 가능한 수요), 미리 만들어둠\n\n계절 재고 vs 주기 재고\n계절 재고 vs 안전 재고\n\n계절 재고: 수요가 예측 가능할 때\n안전 재고: 수요가 예측 불가능할 때\n\n\n주기재고(cycle): 한 번에 많이 사는게 싸다. 규모의 경제 이용한 비용 절감\n완충재고(buffer, decoupling): 프로세스상의 작업 사이의 지속적 공급을 가능하게 해줌. 단 제고가 계속 쌓이지 않게 line balancing(각 프로세스에서 진행하는 일의 양의 밸런스)을 해줘야 함\n안전재고(safety): 불확실성에 대비해 예측된 수요보다 더 많이 재고를 유지함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "href": "posts/01_projects/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "title": "Linear Programming Algorithm",
    "section": "선형 계획을 푸는 알고리즘",
    "text": "선형 계획을 푸는 알고리즘\n\n그래프를 사용하는 방법\n변수가 3개 이하인 경우, 그래프를 그려서 해를 찾을 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/03.html#simplex-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/03.html#simplex-method",
    "title": "Linear Programming Algorithm",
    "section": "Simplex Method",
    "text": "Simplex Method\n\n기하학적 이해\n\n\nInitialization: Collect 1 CFP. 일반적으로 원점을 선택\nOptimality test: find better adj\n\nObj ⋅ (adj - cur) &gt; 0: better\nObj ⋅ (adj - cur) = 0: not changed\nObj ⋅ (adj - cur) &lt; 0: worse\n\n\n\n\n대수적 풀이\n\nbasic solution: 제약식의 변수 중 일부를 기저 변수로 선택하고, 나머지를 0으로 설정하여 얻는 해.\n\n만약 기저변수가 0인 경우, 이를 퇴화라고 부른다.\n\nbasic feasible solution: 모든 변수가 0 이상인 basic solution. 즉, 제약식을 모두 만족하는 해.\n비 기저변수(non basic variable): free variable. 변수의 수 - 방정식의 수 만큼 존재.\n기저 변수(bais variable): pivot variable.\n풀이는 생략\n\n\n\nSimplex Tableau\n\n풀이는 생략\n\n\n\n비 표준형 모델에서의 적용\n\n제약식이 = 인 경우: 인공 변수 추가. 목적 함수에 Big-M 방법을 사용하여 표현.\n\nTableau에서 인공변수의 계수를 0으로 만들어서 진행.\n\n제약식이 ≥ 인 경우: slack 변수랑 surplus variable 추가. surplus variable에 대하여 목적 함수에 Big-M 방법을 사용하여 표현.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "쌍대이론의 본질",
    "text": "쌍대이론의 본질\n\n모든 선형계획 문제는 쌍대문제를 가진다:\n\n원문제(Primal): 예를 들어 이익 최대화.\n쌍대문제(Dual): 자원비용 최소화.\n\n\n\n\n\n원 문제와 쌍대 문제의 관계\n\n\n\n원-쌍대 관계의 성질\n\n원문제의 최적해가 존재하면 쌍대문제의 최적해도 존재하며, 두 목적함수값은 같다.\n원문제의 해로부터 쌍대해를 읽을 수 있고, 그 역도 성립한다.\n쌍대해는 자원의 경제적 가치(잠재가격, shadow price)를 의미한다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "원-쌍대 관계와 상보기저해",
    "text": "원-쌍대 관계와 상보기저해\n\n상보해(Complementary Solutions)\n\n원문제의 기저해와 쌍대문제의 기저해는 서로 직접적으로 대응한다.\n최적해에서는 원문제와 쌍대문제의 목적함수값이 같다.\n\n\n\n상보여유성\n\n원문제의 기저변수가 0이 아니면, 대응 쌍대변수는 0이고, 그 반대도 성립한다.\n이 속성은 심플렉스 방법의 반복과정에서 두 문제의 해가 어떻게 연동되는지 설명한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "다른 원문제 형태의 쌍대문제",
    "text": "다른 원문제 형태의 쌍대문제\n\n비표준형(등식제약식, 변수의 음수 허용 등)에서도 쌍대문제는 항상 존재\n\n등식제약식은 쌍대에서 해당 쌍대변수의 부호제약을 제거(음수 허용)한다.\n변수의 음수 허용은 쌍대에서 등식제약식으로 나타난다.\n\n\n\nSOB(Sensible-Odd-Bizarre) 법칙\n\n원문제의 제약식 및 변수의 형태(≤, =, ≥, 비음, 무제약 등)에 따라 쌍대문제의 대응 형태를 쉽게 결정하는 규칙.\n대칭성: 쌍대문제의 쌍대는 원문제이므로, 두 문제의 관계는 완전히 대칭적이다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#business-analytics",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#business-analytics",
    "title": "Intro",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nData Analysis\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\n\nOperations Research\n\nPrescriptive Analytics: What should we do? (Optimization)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#process-of-or-study",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#process-of-or-study",
    "title": "Intro",
    "section": "Process of OR Study",
    "text": "Process of OR Study\n\n\n\n\n\nflowchart LR\n  A(Collect data) --&gt; B(Define the problem)\n  B --&gt; C{Data are sufficient?}\n  C --&gt;|No| A\n  C --&gt;|Yes| D(Formulate a model)\n  D --&gt; E(Solve the model)\n  E --&gt; F{Model is good?}\n  F --&gt;|Yes| G(Interpret results make suggestions)\n  F --&gt;|No| D",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "title": "Intro",
    "section": "LP Model (표준형)",
    "text": "LP Model (표준형)\n\n제한된 자원을 경쟁하는 활동들에게 가능한 최적으로 분배하거나 이와 비슷한 수학적 구조를 가진 문제를 다루는 방법\n\n\\[\\begin{aligned}\nmax & \\sum_{i=1}^{n} c_i x_i \\\\\ns.t. & \\sum_{i=1}^{n} a_{ij} x_i \\leq b_j, j ≤ m \\\\\n& x_1, x_2, ..., x_n ≥ 0\n\\end{aligned}\\]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#lp의-가정",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#lp의-가정",
    "title": "Intro",
    "section": "LP의 가정",
    "text": "LP의 가정\n\n선형계획은 현실을 단순화한 모델로, 아래의 네 가지 가정이 완벽히 맞지 않을 수 있음.\n작은 불일치는 허용 가능하며, 민감도 분석으로 보완.\n심각한 위반 시 대안 모델(비선형계획, 정수계획 등)을 사용하나, 선형계획의 강력한 알고리즘이 유리하므로 초기 분석에 활용 후 필요 시 복잡한 모델로 전환.\n\n\n비례성(Proportionality)\n\n정의: 목적함수와 제약식에서 활동 수준(예: xx)에 대한 기여도가 선형(비례적)으로 표현됨.\n위반 사례:\n\n초기 투자비용(고정비용)이 있어 \\(Z=3x_1−1\\)이 되는 경우, 비례성이 깨짐.\n규모의 경제로 한계 이익이 증가하면 비례성이 위반됨.\n한계 이익이 감소(예: 마케팅 비용 증가)하면 역시 비례성이 깨짐.\n\n대안: 비례성이 깨지면 비선형계획(12장)이나 혼합정수계획(11장)을 고려.\n\n가합성(Additivity)\n\n정의: 목적함수와 제약식의 값이 각 활동의 개별 기여도의 합으로 표현됨. 즉, 변수 간 교차곱이 없음.\n위반 사례:\n\n제품 간 보완적 상호작용(예: 공동 광고 효과)으로 \\(Z=3x_1+5x_2+x_1x_2\\)가 됨.\n경쟁적 상호작용(예: 설비 공유로 비효율 발생)으로 \\(Z=3x_1+5x_2−x_1x_2\\)가 됨.\n\n대안: 가합성이 위반되면 비선형계획(12장)으로 전환.\n\n가분성(Divisibility)\n\n정의: 의사결정 변수가 실수 값을 가질 수 있음. 즉, 활동 수준이 정수로 제한되지 않음.\n위반 사례: 변수가 정수로 제한되면(예: 배치 단위가 1, 2, 3만 가능) 가분성이 깨짐.\n대안: 정수계획(11장) 사용.\n\n확실성(Certainty)\n\n정의: 모델의 매개변수(예: \\(c_j, a_{ij}, b_i\\))가 알려진 상수로 고정. 해당 상수는 미래 예측에 기반하므로 불확실성이 존재.\n대응: 불확실성이 크면 민감도 분석(6.7절)으로 최적해의 변화를 확인하거나, 확률변수를 도입한 모델(23장) 사용.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#overview",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#overview",
    "title": "Simplex Method (part 5)",
    "section": "Overview",
    "text": "Overview\n\n심플렉스 방법의 기하학적·대수적 원리, 행렬형 알고리즘, 그리고 그 실용적 응용(민감도 분석 등)을 체계적으로 설명\n심플렉스 방법은 선형계획 문제에서 최적해를 꼭짓점 가능해(CPF)에서 찾으며, 행렬 연산을 통해 컴퓨터로 효율적으로 구현할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "title": "Simplex Method (part 5)",
    "section": "Simplex 방법의 기초",
    "text": "Simplex 방법의 기초\n\n꼭짓점 가능해와 제약식 경계\n\n선형계획 문제의 해는 가능해 영역(feasible region)의 경계에 존재한다.\n이 제약식들을 등호(=)로 바꾼 제약식 경계식을 만들 수 있다.\n\n함수 제약식\n\n≤: slack variable\n=: artificial variable\n≥: surplus variable, artificial variable\n\n비음 제약식\n\nnon-restricted: \\(x_i = x_i^{+} - x_i^{-}\\)\n≤ 0: \\(x_i = -x_i^{+}\\)\n\n\n이 경계식들은 2차원에서는 선, 3차원에서는 평면, n차원에서는 초평면(hyperplane)을 형성한다.\n\n\n\n꼭짓점 가능해의 세 가지 주요 속성\n\n최적해의 위치\n\n최적해가 유일하면, 그것은 꼭짓점 가능해이다.\n최적해가 여러 개라면, 그 중 적어도 두 개는 인접 꼭짓점 가능해이다.\n즉, 최적해는 항상 꼭짓점 가능해(혹은 그 선분)에 존재한다.\n\n유한성\n\n꼭짓점 가능해의 개수는 유한합니다.\nm+n개의 제약식 중 n개를 선택하는 조합의 수는 유한하므로, 이론적으로 모든 꼭짓점 가능해를 열거해 비교할 수도 있습니다. 하지만 실제로는 심플렉스 방법이 훨씬 적은 수만 탐색한다.\n\n최적성의 충분조건\n\n인접 꼭짓점 중 더 좋은 해가 없으면, 현재 해가 최적해임이 보장된다.\n\n\n\n\n심플렉스 방법의 핵심 알고리즘 구조\n심플렉스 방법은 다음과 같은 반복 구조를 가진다\n\n초기 꼭짓점 가능해(기저해) 선택\n인접 꼭짓점으로 이동(목적함수 값이 개선되는 방향)\n더 이상 개선이 불가능하면 종료, 그 해가 최적해임을 보장",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "title": "Simplex Method (part 5)",
    "section": "행렬형의 Simplex",
    "text": "행렬형의 Simplex\n\n행렬형 심플렉스 방법의 기본 구조\n표준형 선형계획 문제를 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\text{Maximize} \\quad &Z=c^Tx \\\\\n\\text{Subject to} \\quad &Ax=b, x≥0 \\\\\n\\end{aligned}\n\\]\n\n여기서 A는 m×n 행렬, x는 n차원 변수 벡터, b는 m차원 상수 벡터, c는 n차원 계수 벡터이다.\n여유변수(slack variable)등을 도입해 모든 제약식을 등식으로 바꾼다.\n\n\n\n행렬 연산을 활용한 반복 과정\n\n(제일 처음 단계의 경우 3, 4단계 먼저 진행)\n\n\n진입기저변수(Entering Variable) 선택\n탈락기저변수(Leaving Variable) 선택\n\n최소비율법(minimum ratio test) 사용\n\n새로운 기저 가능해 결정\n\n기저변수 식별\n기저행렬(Basic Matrix, B): m개의 기저변수에 대해 m×m 행렬 \\(B\\)와 \\(B^{-1}\\)를 만든다. 1\n기저해(Basic Solution) 계산: \\(x_B=B^{-1}b\\)\n목적함수 값 계산: \\(Z=c_B^TB^{−1}b\\)\n\n최적화 검사\n\n비기저변수의 계수(감소계수, reduced cost)를 계산\n\n계산식: \\(c_B^TB^{−1}a_n - c_n\\)\nslack 변수: \\(c_B^TB^{-1}\\)\n\n최적일 경우 종료",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#footnotes",
    "title": "Simplex Method (part 5)",
    "section": "각주",
    "text": "각주\n\n\n역행렬 구하는 법. 2차원 말고는 그냥 그 방식으로 풀자.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#footnotes",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "각주",
    "text": "각주\n\n\nbatch 1개는 부품 집합 1 단위 의미↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (복수 자원)",
    "text": "대기시간 예측 (복수 자원)\n\ncapacity: \\(\\frac{m}{p}\\)\nflow rate: \\(\\frac{p}{am}\\)\n\\(I = I_q + I_p = I_q + mu\\)\n\\(T_q = \\frac{p}{m} * \\frac{u^{\\sqrt{2(m+1)} - 1}}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n근사치만을 제공\n\nservice level: \\(P(T_q ≤ TWT)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#풀링",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#풀링",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "풀링",
    "text": "풀링\n\n대기할 수 있는 방법은 여러가지가 있다\n대기시간을 줄일 수 있는 방법에 사람을 많이 뽑는것 외에 다른 고려 요소\n\n\n\n풀링의 효과\n\n풀링되는 시스템이 서로 완전히 독립\n다양한 input을 처리할 수 있어야 함.\n→ 대기시간, 대기 인원 감소",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#질문",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#질문",
    "title": "확률과 통계 1 정리",
    "section": "질문",
    "text": "질문\n\n\n\n이 공식은 무조건 t분포에서만 쓰이는건가?\n\n\n\n\n\n이거 어떻게 푸는거야\n\n\n\n모분산을 모르고 표본분산을 쓰면 무조건 t분포?\n쌍체표본은 무조건 t분포?\n적당한 β값은 존재하지 않은건가? 아니면 10%가 너무 큰건가?\n분모에서 분산 어떻게 추정함?\n모집단의 분포와 관계없이 표본분산 \\(S^2\\)은 \\(σ^2\\)의 불편추정량이다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#single-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#single-linked-list",
    "title": "시험 범위",
    "section": "single linked list",
    "text": "single linked list\n\n안 나오지만 그냥 외우셈",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "title": "시험 범위",
    "section": "doubly linked list",
    "text": "doubly linked list",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "title": "시험 범위",
    "section": "circular linked list",
    "text": "circular linked list\n\n\n\n\n\n5~6주차\n\nstack\n\nt자 철로 출력\nstack을 이용하는 응용\nstack을 이용하지 않는 중위 -&gt; 후위, 전위 표기\n\n\n7주차\n\n원형 queue",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#single-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#single-linked-list",
    "title": "잘 외웠는지 체크",
    "section": "Single Linked List",
    "text": "Single Linked List\nclass SL:\n    class Node:\n        def __init__(self, item, next):\n          self.item = item\n          self.next = next\n\n    def __init__(self):\n        self.head = None\n        self.size = 0\n\n    def is_empty(self): return self.size == 0\n    def size(self): return self.size\n\n    def insert_front(self, item):\n        if self.is_empty():\n            self.head = Node(item, None)\n        else:\n            self.head = Node(item, self.head)\n        size += 1\n\n    def insert_after(self, item, p):\n        p.next = Node(item, p.next)\n        size += 1\n\n    def delete_front(self):\n        if self.is_empty():\n            return print(error)\n        x = self.head\n        self.head = self.head.next\n        size -= 1\n        return x.item\n\n    def delete_after(self, p):\n        if self.is_empty():\n            return print(error)\n        x = p.next\n        p.next = x.next\n        size -= 1 \n        return x.item\n\n    def print():\n        p = self.head\n        while p != None:\n            if p.next != None:\n                print(p.item, ' -&gt; ', end='')\n            else:\n                print(p.item)\n            p = p.next",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#double-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#double-linked-list",
    "title": "잘 외웠는지 체크",
    "section": "Double Linked List",
    "text": "Double Linked List\nclass DL:\n    class Node:\n        def __init__(self, item, prev, next):\n            self.item = item\n            self.prev = prev\n            self.next = next\n\n    def __init__(self):\n        self.head = Node(None, None, None)\n        self.tail = Node(None, self.head, None)\n        self.head.next = self.tail\n        self.size = 0\n\n    def is_empty(self): return self.size == 0\n    def size(self): return self.size\n\n    def insert_before(self, p, item):\n        f = p.prev\n        n = Node(item, f, p)\n        f.next = n\n        p.prev = n\n        size += 1\n\n    def insert_after(self, p, item):\n        f = p.next\n        n = Node(item, p, f)\n        f.prev = n\n        p.next = n\n        size += 1\n\n    def delete(self, p):\n        f = p.prev\n        r = p.next\n        f.next = r\n        r.prev = f\n        size -= 1\n        return p.item\n    def print(self):\n        if not self.is_empty():\n            p = self.head.next\n            while p != self.tail:\n                if p.next != self.tail:\n                    print(p.item, ' &lt;-&gt; ', end='')\n                else:\n                    print(p.item)\n                p = p.next",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#circular-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#circular-linked-list",
    "title": "잘 외웠는지 체크",
    "section": "Circular Linked List",
    "text": "Circular Linked List\nclass Clist:\n    class Node:\n        def __init__(self, item, next):\n            self.item = item\n            self.next = next\n\n    def __init__(self):\n        self.last = None\n        self.size = 0\n\n    def is_empty(self):\n        return self.size == 0\n    def size(self):\n        return self.size\n    def front(self):\n        if self.is_empty():\n            raise Error('underflow')\n        x = self.last.next\n        return x.item\n    def insert(self, item):\n        n = Node(item, None)\n        if self.is_empty():\n            n.next = n\n            self.last = n\n        else:\n            n.next = self.last.next\n            self.last.next = n\n        self.size += 1\n    def delete(self):\n        if self.is_empty():\n            raise Error('underflow')\n        x = self.last.next\n        if self.size == 1:\n            self.last = None\n        else:\n            self.last.next = x.next\n        self.size -= 1\n        return x.item\n\n    def print(self):\n        if self.is_empty():\n            print('empty')\n        else:\n            f = self.last.next\n            p = f\n            while p.next != f:\n                print(p.item, ' -&gt; ', end='')\n                p = p.next\n            print(p.item)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---circular-array",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---circular-array",
    "title": "잘 외웠는지 체크",
    "section": "Queue - circular array",
    "text": "Queue - circular array\nMAX_SIZE = 30\nclass ca:\n    def __init__(self):\n        self.front = 0\n        self.rear = 0\n        self.items = [None] * MAX_SIZE\n    def is_empty(self): return self.front == self.rear\n    def is_full(self): return (self.rear + 1) % MAX_SIZE == self.front\n    def size(self): return (self.rear - self.front + MAX_SIZE) % MAX_SIZE\n    def peek(self):\n        if not self.is_empty():\n            return self.items[(self.front + 1) % MAX_SIZE]\n    def enqueue(self, item):\n        if not self.is_full():\n            self.rear = (self.rear + 1) % MAX_SIZE\n            self.items[self.rear] = item\n    def dequeue(self):\n        if not self.is_empty():\n            self.front = (self.front + 1) % MAX_SIZE\n            return self.items[self.front]\n\n    def print(self):",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---single-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---single-linked-list",
    "title": "잘 외웠는지 체크",
    "section": "Queue - Single linked list",
    "text": "Queue - Single linked list\nclass Node:\n    def __init__(self, item, next):\n      self.item = item\n      self.next = next\n\ndef add(item):\n    global front\n    global tail\n    global size\n\n    n = Node(item, None)\n    if size == 0:\n        front = n\n    else:\n        rear.next = n\n    rear = n\n    size += 1\n\ndef remove():\n    global front\n    global tail\n    global size\n\n    if not size == 0:\n        x = front\n        front = front.next\n        if front == tail:\n            tail = None\n        size -= 1\n        return x.item\n\ndef print_q():\n    global front\n    global tail\n    global size\n\n    if not size == 0:\n        p = front\n        while p != tail:\n            print(p.item, ' -&gt; ', end='')\n            p = p.next\n        print(p.item)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---circular-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#queue---circular-linked-list",
    "title": "잘 외웠는지 체크",
    "section": "Queue - circular linked list",
    "text": "Queue - circular linked list\nclass CQ:\n    class Node:\n        def __init__(self, item, next):\n          self.item = item\n          self.next = next\n    def __init__(self):\n        self.last = None\n\n    def is_empty(self):\n        return self.last == None\n    def clear(self):\n        self.last = None\n    def peek(self):\n        if not self.is_empty():\n            return self.last.next.item\n    def size(self):\n        if self.is_empty():\n            return 0\n        count = 1\n        p = self.last.next\n        while p != self.last:\n            count += 1\n            p = p.next\n        return count\n\n    def enqueue(self, item):\n        n = Node(item, None)\n        if self.is_empty():\n            n.next = n\n            self.last = n\n        else:\n            n.next = self.last.next\n            self.last.next = n\n            self.last = n\n\n    def dequeue(self):\n        if not self.is_empty():\n            x = self.last.next\n            if x == self.last:\n                self.last = None\n            else:\n                self.last.next = x.next\n            return x.item\n\n    def print(self):\n        if not self.is_empty():\n            p = self.last.next\n            while p != self.last:\n                print(p.item, ' -&gt; ', end='')\n                p = p.next\n            print(p.item)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "잘 외웠는지 체크"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/1.html",
    "href": "posts/01_projects/bs_3_1/notes/others/1.html",
    "title": "자기 소개서",
    "section": "",
    "text": "자기소개 및 가치관 (500자 이내)\n\n저는 데이터 분석과 IT 인프라 설계 분야에 깊은 관심을 가지고 있는 산업공학과 학생입니다. 산업공학을 전공하며 시스템 최적화와 데이터 기반 의사결정에 대한 이론을 배우며 데이터 분석 및 IT 인프라 설계 분야에 관심을 가지게 되었고, 이를 실무에 적용할 수 있는 지식을 학습하고자 42서울 교육기관에서 2년 동안 IT 관련 학습을 진행했습니다. 또한 이 기간 동안 AWS와 ADsP(Advanced Data Analytics Semi-Professional) 자격증을 취득하며 클라우드 컴퓨팅과 데이터 분석에 대한 기초 역량을 쌓았습니다.\n저는 효율적이고 신뢰할 수 있는 시스템을 구축하는 것을 가장 중요한 가치로 삼고 있습니다. 이러한 시스템은 데이터 손실과 보안 위협을 방지할 뿐만 아니라, 장기적인 성장의 토대가 되기 때문입니다. 현재는 데이터와 블록체인 기술을 활용하여 복잡한 문제를 단순화하고, 효율적인 해결책을 찾는 데 큰 관심을 가지고 있습니다. 앞으로도 지속적인 학습과 경험을 통해 해당 분야에서 전문성을 키워가고자 합니다.\n\n졸업 후 IT 및 블록체인 분야에 관련해서 이루고자 하는 꿈과 선정 사유 (500자 이내)\n\n저는 데이터 분석, IT 인프라, 블록체인 기술을 융합하여 현실의 복잡한 문제들을 해결하고 혁신적인 가치를 창출하는 데 기여하고 싶습니다. 전공 수업과 프로젝트를 통해 데이터가 지닌 잠재력을 배워가면서, 동시에 데이터의 신뢰성과 보안이라는 중요한 과제에 대해서도 깊이 고민하게 되었습니다. 특히 42서울에서의 학습 경험을 통해, 안전하고 효율적인 데이터 활용을 위해서는 IT 인프라와 블록체인 기술의 역할이 매우 중요하다는 것을 깨달았습니다. 이러한 경험들을 바탕으로 IT 인프라와 블록체인 기술에 더욱 관심을 가지게 되었고, 관련 기술 서적과 온라인 자료를 통해 꾸준히 학습하며 이해의 폭을 넓혀가고 있습니다. 앞으로도 끊임없이 배우고 성장하여 데이터의 가치를 안전하게 실현할 수 있는 시스템을 만드는 데 기여하고 싶습니다.\n\n목표 달성을 위한 그간의 성과 및 계획 (500자 이내)\n\n저의 주요 성과로는 42서울에서의 프로젝트 경험과 AWS, ADsP 자격증 취득을 들 수 있습니다. 42서울에서 진행한 Solidity 기반 이더리움 스마트 컨트랙트 설계 및 배포 프로젝트를 통해 블록체인의 핵심 원리와 실제 활용 방안을 학습했습니다. 또한 Vagrant, Kubernetes(K8s), ArgoCD, GitLab helm 배포 프로젝트를 수행하며 온프레미스 환경에서의 인프라 설계와 개발 환경 관리 역량을 키웠고, 이를 통해 클라우드와 온프레미스 환경의 IT 인프라 운영에 대한 실질적인 이해도를 높일 수 있었습니다. 향후 계획으로는 학부 과정에 충실히 임하면서 데이터사이언스 대학원 진학을 위한 준비를 체계적으로 진행하고자 합니다. 대학원에서는 빅데이터 처리, 머신러닝, 딥러닝 등 데이터 분석의 핵심 기술을 심도 있게 학습하고자 합니다. 이와 병행하여 온라인 강좌 수강과 실전 프로젝트 수행을 통해 IT 인프라 및 블록체인 분야의 역량을 지속적으로 강화하고, 각종 공모전 참여를 통해 실력을 검증받고자 합니다. 궁극적으로는 이러한 기술들을 융합하여 데이터의 신뢰성과 보안을 보장하고, 효율적인 시스템을 설계하는 전문가로 성장하고 싶습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "자기 소개서"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-2",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-2",
    "title": "봉사",
    "section": "봉사 2",
    "text": "봉사 2",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/2.html",
    "href": "posts/01_projects/bs_3_1/notes/others/2.html",
    "title": "성적 장학금",
    "section": "",
    "text": "오~예~ (남은 등록금 300만원을 대출 받으며)\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "성적 장학금"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "title": "봉사",
    "section": "봉사 계획서 2",
    "text": "봉사 계획서 2",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-실습일지",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-실습일지",
    "title": "봉사",
    "section": "봉사 실습일지",
    "text": "봉사 실습일지",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "title": "봉사",
    "section": "봉사 결과보고서",
    "text": "봉사 결과보고서",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/01.html",
    "href": "posts/01_projects/bs_3_1/notes/computer/01.html",
    "title": "1차 발표 script",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "1차 발표 script"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/01.html#카페인-과다섭취-모니터링-앱",
    "href": "posts/01_projects/bs_3_1/notes/computer/01.html#카페인-과다섭취-모니터링-앱",
    "title": "1차 발표 script",
    "section": "카페인 과다섭취 모니터링 앱",
    "text": "카페인 과다섭취 모니터링 앱\n\n사람마다 카페인 민감도가 다르기 때문에 개인에 맞춰 학습 -&gt; 초기에는 일반적인 사용자에 맞춰, 이후 개인화 모델로 전환 1. 일반 모델 학습 2. 일반 모델로 진행하다가 2주 정도 데이터가 쌓이면 fine-tuning 시작\n하루 하루가 독립적이지 않으므로, Stateful LSTM 기반 시계열 모델 적용\n한계점:\n\n카페인 외 수면에 영향을 미치는 요인(오늘 기준)은 고려하지 않음.\n사용자가 꾸준히 데이터를 입력한다는 가정 필요\n사용자 별 맞춤 학습을 하려면 서버 과부화가 생길 수 있음. (근데 이건 뭐 llm들 그런식으로 동작하니까…내장 ai 같은게 개발된다면 괜찮을지도?)\n사용자의 패턴이 갑자기 변하면 예측력이 떨어질 수 있음. (최근 데이터에 더 높은 가중치를 부여하지만 여전히 한계점이 있음)\n생물학적 메커니즘을 학습한게 아니라, 단순히 카페인 섭취량을 바탕으로 수면의 질을 판단하는 것이기 때문에 예측력이 빈약할 수있음.\n수면의 질 지수가 주관적임\n\n\n\n변수:\n\n전날 수면의 질(카페인 외 다른 요인들의 영향도 반영할 수 있다) (설문조사를 통해)\n00~23시 카페인 섭취 량\n수면 시작 시간\n수면 종료 시간\n\n예측: 오늘 수면의 질\n사용자가 전날 수면의 질, 오늘 카페인 섭취 량과 목표 수면 시작 시간, 목표 수면 종료 시간을 알려주면 몇시에 커피 몇잔까지 가능한지 알려준다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "1차 발표 script"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/02.html",
    "href": "posts/01_projects/bs_3_1/notes/computer/02.html",
    "title": "컴퓨팅적사고 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "컴퓨팅적사고 발표 ppt"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#example-food-truck",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#example-food-truck",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: Food Truck",
    "text": "example: Food Truck\n\n변동(동일한 확률 가정)\n\n수요\n공급할 수 있는 양\n\n수요와 공급이 동시에 발생하지 않는 경우로 인해 평균 흐름률이 실제랑 다름.\n\n변동성이 흐름률에 영향을 미침\nbuffer가 있으면 흐름률 높일 수 있음\n\nbuffer가 없다면?",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#example-병원-외상-센터",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#example-병원-외상-센터",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: 병원 외상 센터",
    "text": "example: 병원 외상 센터\n\n\n대기해야 할 상황이 있으면 다른 병원으로 이동\n\ndiversion 상태, loss(service를 못 받음)\n\n\n\nDiversion 상태 확률\n\nD &lt; C 가정하지 않음\n도착 간격은 지수분포 가정 (processing time 분포는 가정 안함)\n대기하지 않고 바로 이탈한다고 가정\n\\(P_m\\): 내재활용률과 자원의 수에 의해 결정됨\n\\(r = um = \\frac{p}{a}\\), 해야하는 일의 양을 의미\n\n단위: Erlang\n\n\n\n\n\nErlang Loss Table\n\n\n\n들어온 인원: \\(\\frac{1}{a}(1 - P_m(r))\\)\n안 들어온 인원: \\(\\frac{1}{a}P_m(r)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html",
    "title": "test",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "title": "classification with trees",
    "section": "What is Data minig",
    "text": "What is Data minig\n\n데이터 마이닝은 대량의 데이터에서 암시적이고 이전에 알려지지 않았던 잠재적으로 유용한 지식이나 패턴을 추출하는 과정입니다.\n\n\n종류\n\n지도 학습1: 주어진 학습 데이터를 이용하여 목표 속성의 값을 예측하는 모델을 생성하는 과정으로, 입력(속성)과 출력(정답)이 모두 주어진 데이터를 바탕으로 학습\n비지도 학습",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#분류",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#분류",
    "title": "classification with trees",
    "section": "분류",
    "text": "분류\n\n목표\n\n새로운 데이터에 대해서도 정확한 예측이 가능한 일반화된 모델을 만드는 것\n이를 위해 과거 데이터를 학습용과 테스트용으로 나누어 모델의 성능을 검증",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "title": "classification with trees",
    "section": "의사결정 트리의 구조와 원리",
    "text": "의사결정 트리의 구조와 원리\n\n의사결정 트리는 노드와 가지로 구성된 계층적 구조로, 각 노드는 특성(attribute)을 나타내며 가지는 테스트 결과를 표현.\n잎 노드는 클래스 레이블이나 클래스 분포를 나타냄\n의사결정 트리 구축은 주로 탐욕적 전략(Greedy strategy)을 사용하며, 각 단계에서 가장 좋은 분할 기준을 선택함.\n대표적인 의사결정 트리 알고리즘으로는 CART, ID3, C4.5, SLIQ, SPRINT 등이 있다.\n\n\n노드 불순도 측정 방법\n의사결정 트리에서 최적의 분할을 결정하기 위해 다양한 불순도 측정 방법이 사용됩니다:\n\nGini Index: 노드의 불순도를 측정하는 방법으로, 1-∑[p(j|t)]²로 계산됩니다. 값이 0에 가까울수록 순수한 노드를 의미합니다.\nEntropy(엔트로피): 노드의 동질성을 측정하는 방법으로, -∑p(j|t)log₂p(j|t)로 계산됩니다. 0일 때 완전히 동질적인 노드를 의미합니다.\nInformation Gain(정보 이득): 분할 전후의 엔트로피 차이로, 분할로 인해 얻어지는 불확실성 감소량을 의미합니다. 높은 정보 이득은 해당 속성이 데이터를 잘 나누는 것을 의미합니다.\nGain Ratio(이득 비율): 정보 이득을 분할의 내재 정보량(Split Information)으로 나눈 값으로, 분기가 많은 속성에 대한 편향을 줄이기 위해 고안되었습니다.\n\n\n\n트리 분할 기준\n트리 분할 시 고려해야 할 주요 이슈는 다음과 같습니다: - 데이터 분할 방법 선택 - 속성의 테스트 조건 명시 - 최고의 분할 정의 - 트리 분기 종료 시점 결정\n최적의 분할은 불순도를 최소화하는 방향으로 이루어지며, CART는 Gini 기반 분할을, ID3와 C4.5는 Information Gain 기반 분할을 주로 사용합니다.\n\n\n모델 평가 기준\n의사결정 트리 모델의 평가는 다음과 같은 기준으로 이루어집니다: - 테스트 세트에서의 정확도(%) - 오류율 - 혼동 행렬(Confusion Matrix) - 속도와 확장성 - 노이즈와 결측값 처리 능력",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#결론",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#결론",
    "title": "classification with trees",
    "section": "결론",
    "text": "결론\n의사결정 트리는 직관적이고 이해하기 쉬운 분류 모델을 제공하지만, 과적합(overfitting)이나 데이터 단편화와 같은 문제가 발생할 수 있습니다. 이를 해결하기 위해 C4.5와 같은 알고리즘은 Gain Ratio를 도입하여 분기가 많은 속성에 대한 편향을 줄이는 방법을 제시했습니다.\n의사결정 트리의 성공적인 구축을 위해서는 적절한 불순도 측정 방법 선택, 가지치기(pruning), 그리고 다양한 속성 선택 기준의 이해가 필요합니다. 이러한 방법들을 통해 보다 정확하고 일반화된 모델을 구축할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#footnotes",
    "title": "classification with trees",
    "section": "각주",
    "text": "각주\n\n\n규칙 기반 시스템 != 연관 규칙 학습↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#빈번-패턴-마이닝",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#빈번-패턴-마이닝",
    "title": "association rule mining",
    "section": "빈번 패턴 마이닝",
    "text": "빈번 패턴 마이닝\n\n데이터 집합에서 자주 발생하는 패턴을 찾는 과정\n연관 규칙 학습 뿐만 아니라 다양한 데이터 마이닝 기법의 기초가 됨\n빈발 패턴: 패턴의 거래 수가 최소 지지도를 넘는 것\n\n빈발 패턴의 하위 집합은 빈발 패턴\n\nmax frequent: 직속 상위 집합들 모두가 빈번 집합이 아닌 빈번 집합",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#유형",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#유형",
    "title": "association rule mining",
    "section": "유형",
    "text": "유형\n\n불리언 / 양적 연관 규칙\n단일/다차원\n단일/다중 수준",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#apirori",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#apirori",
    "title": "association rule mining",
    "section": "Apirori",
    "text": "Apirori\n\n후보 생성 → 검사 → 가지치기\nfrequent itemsets을 찾는 알고리즘\n\nmonotone property → 가지치기",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#완전-확률화-계획법",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#완전-확률화-계획법",
    "title": "ANOVA",
    "section": "완전 확률화 계획법",
    "text": "완전 확률화 계획법\n\n실험계획법\n일원 분산분석이 공정한 결과를 내기 위한 실험 조건",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산-분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산-분석",
    "title": "ANOVA",
    "section": "일원 분산 분석",
    "text": "일원 분산 분석\n\n정규성, 독립성, 등분산성\n\n→ 오차의 독, 정, 불편성, 등분산성\n오차(표본 - 잔차): 관심 없는 다른 모든 요인에 의해 발생하는 오차\n효과: \\(τ_i: μ_i - μ\\)\n\n\\(Y_{ij} = μ + τ_i + ε_{ij}\\)\n\\(Y_{ij} - \\bar{Y} = (\\bar{Y_i} - \\bar{Y}) + (Y_{ij} - \\bar{Y_i})\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table1",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table1",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "Erlang Loss Table1",
    "text": "Erlang Loss Table1\n대기? 버퍼?",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#footnotes",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "각주",
    "text": "각주\n\n\ndiversion 확률, 꽉 차있을 확률, 도착한 환자가 서비스 받을 확률, 다른 병원으로 갈 확률 시험에 나온다.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#buffer의-역할",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#buffer의-역할",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer의 역할",
    "text": "buffer의 역할\n\n\n\n변동성으로 인해 capacity가 낮아지는 이유\n\n\n\n\n변동성이 없다면 cycle time은 1/capacity\n변동성이 있다면 cycle time은 늘어남. (시뮬레이션으로 계산)\n버퍼가 있으면 1/capacity로 점점 줄어듦.\ncell layout을 사용하면 cycle time을 제일 많이 줄일 수 있음.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "title": "프로젝트 관리",
    "section": "주경로 기법(Critical Path Method, CPM)",
    "text": "주경로 기법(Critical Path Method, CPM)\n\n프로젝트 완료 시간 계산\n종속성 path(가장 짧은 시간) 중 가장 긴 path가 critial path\n\n이 path의 활동이 지연되면 프로젝트 전체가 지연됨\n프로세스 흐름도에서 개별작업의 처리능력이 중요했던것과 달리 프로젝트 종료 시간이 중요.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "Erlang Loss Table",
    "text": "Erlang Loss Table\n\n\n\n얼랑 솔실 공식1",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "title": "프로젝트 관리",
    "section": "프로젝트 vs 프로세스",
    "text": "프로젝트 vs 프로세스\n\n프로젝트: 일회성, 하나의 흐름단위\n프로세스: 지속적, 여러 흐름 단위",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "href": "posts/01_projects/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "쌍대 심플렉스 방법",
    "text": "쌍대 심플렉스 방법\n\n쌍대 문제 제약식에 -1 하고 slack 변수 추가해서 반대로 품.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/10.html#상한-기법",
    "href": "posts/01_projects/bs_3_1/notes/OR/10.html#상한-기법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "상한 기법",
    "text": "상한 기법\n\n일단은 대수적으로 푸는 방법만 배움.\n변수가 0일 때 뿐만 아니라 upper bound일 때도 non-basic variable로 취급.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석",
    "title": "ANOVA",
    "section": "분산분석",
    "text": "분산분석\n\n요인, 인자 (factor): class\n수준 (level): class 값\n처리 (treatment): 요인과 수준의 조합\n반응치 (response): 관측치\n\n\n완전 확률화 계획법\n\n일원 분산분석이 공정한 결과를 내기 위한 실험 조건\n처리 i에 해당되는 모집단으로부터 독립인 표본 \\(n_i\\)개를 랜덤으로 샘플링함으로써, k개의 서로 다른 모집단으로부터 독립인 random sample들을 얻는 것과 같음\n반복 수가 같을 필요는 없다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/11.html",
    "href": "posts/01_projects/bs_3_1/notes/OR/11.html",
    "title": "수송문제와 할당 문제들",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "민감도 분석",
    "text": "민감도 분석",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "6.7 민감도 분석 적용 – 요약 및 주요 내용",
    "text": "6.7 민감도 분석 적용 – 요약 및 주요 내용\n6.7절 “민감도 분석 적용”은 선형계획(Linear Programming) 문제에서 민감도 분석(Sensitivity Analysis)을 실제로 어떻게 적용하는지, 그리고 다양한 매개변수 변화가 최적해에 어떤 영향을 미치는지 구체적으로 설명하는 부분입니다.\n\n주요 내용 요약\n\n민감도 분석의 출발점\n민감도 분석은 보통 자원(b₁, b₂, …, bₘ)의 공급량 변화가 해에 미치는 영향을 분석하는 것으로 시작합니다. 이는 실제 모델에서 자원의 양을 조정할 수 있는 융통성이 크기 때문입니다.\n우변(b) 변화의 영향\n자원(b)의 값이 변하면, 최종 심플렉스 표의 우변만 바뀌고 나머지(행 0의 비기저변수 계수 등)는 변하지 않을 수 있습니다. 이때는 우변만 수정해서 해가 여전히 가능(feasible)한지(기저변수 값이 모두 음이 아닌지) 확인하면 됩니다. 만약 불가능해지면 쌍대심플렉스법 등으로 재최적화가 필요합니다.\n증분 분석\n자원의 값이 변화할 때, 변화분만큼의 영향(증분)을 계산해서 새로운 해와 목적함수 값을 빠르게 구할 수 있습니다.\n허용범위(Allowable Range)\n각 자원(b)의 변화가 해의 가능성과 최적성을 유지할 수 있는 범위를 계산합니다. 이 범위 내에서는 잠재가격(dual price, shadow price)이 유효하게 적용됩니다.\n동시 변화와 100% 규칙\n여러 자원의 값이 동시에 변할 때, 각 변화가 허용범위 내에서 차지하는 비율의 합이 100%를 넘지 않으면 잠재가격을 이용한 해석이 유효합니다.\n목적함수 계수 변화\n비기저변수나 기저변수의 목적함수 계수(c)가 변할 때 해가 어떻게 변하는지, 허용범위를 어떻게 계산하는지 설명합니다.\n새로운 제약식 추가\n모델에 새로운 제약식이 추가되면, 기존 최적해가 여전히 가능해인지 확인하고, 아니라면 심플렉스 표에 새로운 행을 추가해 재최적화를 진행합니다.\n파라메트릭 분석\n하나 또는 여러 매개변수를 연속적으로 변화시키면서 최적해가 어떻게 달라지는지 체계적으로 분석합니다.\n\n\n\n예시: Wyndor Glass Co. 모델\n\nb₂(자원 2의 공급량)가 12에서 24로 증가하면, 기저해가 더 이상 가능하지 않게 되고, 쌍대심플렉스법을 통해 새로운 최적해를 구해야 함을 보여줍니다.\n허용범위 내에서만 자원의 변화에 대해 잠재가격이 유효하며, 이를 벗어나면 해가 바뀌고 잠재가격도 달라집니다.\n여러 자원이 동시에 변할 때 100% 규칙을 적용해, 변화의 합이 100%를 넘지 않으면 기존 해석이 유효함을 설명합니다.\n\n\n\n실무적 의의\n\n실제 기업(예: Pacific Lumber Company)의 대규모 산림관리 최적화 문제에 민감도 분석이 어떻게 적용되어, 불확실성 하에서 더 나은 의사결정과 수익 증대에 기여했는지 사례로 제시합니다.\n\n\n요약:\n6.7절은 선형계획의 해가 자원, 목적함수 계수, 제약식 등 모델의 매개변수 변화에 얼마나 민감한지, 그리고 이런 변화가 있을 때 해를 신속하게 갱신하거나 재최적화하는 절차를 구체적으로 다룹니다. 이를 통해 실제 의사결정에서 불확실성을 관리하고, 최적화 모델의 실용성을 높일 수 있음을 보여줍니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html",
    "title": "의사결정 트리",
    "section": "",
    "text": "불순도가 가장 낮은 leaves를 root에 두고, 그 다음 불순도가 낮은 leaf를 그 아래에 두는 방식으로 트리를 구성한다. leaf 노드의 과반수가 같은 클래스를 가지면 그 클래스를 리턴한다. overfit을 방비하기 위해 pruning을 하거나 max depth를 설정한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#과정",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#과정",
    "title": "의사결정 트리",
    "section": "과정",
    "text": "과정\n\n루트 노드에서 시작전체 데이터셋을 기준으로 시작하여 가장 좋은 분할속성(feature)을 선택\n분할 기준 평가각 속성에 대해 데이터를 분할했을 때의 분할 평가함수 적용\n\ngini index\nentropy\ninformation gain\ngain ratio\n\n최적의 분할 선택 평가된 기준 중 가장 순도가 높은 점수를 갖는 속성을 선택 (greedy)\n재귀적으로 하위 노드 분할분할된 하위 데이터에 대해 위의 과정을 반복\n종료 조건 만족 시 정지",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#algorithm",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#algorithm",
    "title": "의사결정 트리",
    "section": "Algorithm",
    "text": "Algorithm\n\nCART\n\n분할 기준\n\n분류: gini index\n회귀: MSE\n\n모든 분할에서 이진 트리로 분할\n사후 가지치기\n\n비용 복잡도 가지치기: \\(Total SSR + α(leaf size)\\)이 제일 작은 트리 선택\nα는 cross validation으로 결정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#pattern-minig",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#pattern-minig",
    "title": "association rule mining",
    "section": "Pattern minig",
    "text": "Pattern minig\n\nBasic Concepts\n\npattern: dataset 안에서 함께 자주 발생하는 subsequences, substructures, set of items\n\n이 pattern은 인과관계를 의미하진 않는다.\n\nAssociation rule minig: 최소 지지도나 신뢰도를 넘는 모든 항목에 대해 pattern을 찾는다.\n\n\n\nApplications\n\nassociation rule, correlation, classification, clustering data mining의 기반이 될 수 있다.\n장바구니 분석\n연속 구매 분석\n\n\n\nTerminologies\n\n지지도(Support): 전체 거래 중 특정 항목 집합이 포함된 거래의 비율.\n신뢰도(Confidence): 항목 X를 포함하는 거래 중에서 항목 Y도 함께 포함하는 거래의 비율.\n빈발 패턴(frequent): 최소 지지도를 넘는 pattern\n\n빈발 항목 집합(frequent itemset): 단순한 묶음\n빈발 시퀀스\n\n\n\n\nclosed pattern\n\nx가 빈발이고, 지지도가 상위 집합들과 다른 집합 (지지도는 상위로 갈 수록 떨어짐)\n지지도 정보를 유지할 수 있다.\n\n신뢰도 계산할 때 사용할 수 있음\n\n\n\n\nmax patterns\n\nx가 빈발이고, 상위 집합들 모두가 빈발 집합이 아닌 집합\n지지도 정보는 유지되지 않음.\n\n신뢰도 계산할 때 사용할 수 없어서 사실 상 결과 요약 외의 용도는 없음\n\nDownward closure property: 어떤 itemset이 빈발하지 않으면, 그 모든 superset은 무조건 빈발하지 않는다. 대우도 성립\n\n교수님은 anti-monotone property로 설명하셨지만 이게 더 자주 사용되는 용어",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#association-rule",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#association-rule",
    "title": "association rule mining",
    "section": "Association Rule",
    "text": "Association Rule\n\nfind frequent itemsets\n\nApriori (breadth-first search)\nFP-Growth\nEclat (depth-first search)\n\ngenerate association rules\n\n모든 빈발 itemset I에 대해 모든 I의 subset s로 ‘s -&gt; (I - s)’ 규칙을 생성\n최소 신뢰도 조건을 만족하는 규칙만 남김",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#algorithm",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#algorithm",
    "title": "association rule mining",
    "section": "Algorithm",
    "text": "Algorithm\n\nApirori\n\nMonotone 성질을 이용하여 빈발하지 않는 집합은 후보에서 제거\n\n\nscan DB once to get 1-itemsets\n반복\n\nk개의 itemset에 대해 k+1-itemset의 후보를 생성\n\nself-join: k-itemset을 두 개 합쳐서 k+1-itemset을 생성\nprune: k+1-itemset을 생성할 때, k-itemset의 subset이 모두 빈발해야 k+1-itemset이 빈발할 수 있다.\n\nk+1-itemset 후보에 대해 DB를 scan하여 빈발한 itemset을 찾는다.\nk += 1\n빈발 itemset이 없으면 종료\n\n\n\nApriori의 단점: DB scan을 여러 번 해야함, 후보가 많아질 수 있음\n\n후보 수를 줄이는 방법: Hashing\n\n\n\n\nDHP (Direct Hashing and Pruning)\n\nHash값이 같은 itemset의 count를 합하고, minimum support를 넘는 itemset만 남김\nHash table을 매번 만드는 번거로움이 있지만, apriori보다 빠름\n반복\n\n빈발 항목 찾기, 후보 해시 테이블 생성\n\n데이터베이스를 scan하여 최소 지지도를 넘는 1-itemset 후보를 찾음\n동시에 조합 가능한 2-itemset을 만들어 mapping된 해시 테이블 bucket에 count += 1\n\n가지치기\n\n1-itemset 후보를 이용해 self-join, prune으로 2-itemset 후보 생성\n완성된 후보를 1단계에서 만든 hash table에 매핑해서 최소 지지도를 넘는 2-itemset bucket이 아닐 경우 배제",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#기본-데이터-정보-확인",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#기본-데이터-정보-확인",
    "title": "test",
    "section": "기본 데이터 정보 확인",
    "text": "기본 데이터 정보 확인\n\ndf.head()\n\n\n\n\n\n\n\n\nq48a01\nq48a02\nq48a03\nq48a04\nq48a05\nq48a06\nq48a07\nq48a08\nq48a09\nq48a10\nq48a11\nq48a12\nq48b1\nq48b2\nq48b3\nq48c1\nq48c2\nq48c3\nq48c4\nq48c5\nq48c6\nq48c7\nq48c8\nq48c9\nq48d1\nq48d2\nq48d3\nq48d4\nq48d5\nq48d6\nq49a01\nq49a02\nq49a03\nq49a04\nq49a05\nq49a06\nq49a07\nq49a08\nq49a09\nq49a10\nq49a11\nq49a12\nq49a13\nq49a14\nq49a15\nq49a16\nq49a17\nq50\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0\n3.0\n2.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n2.0\n3.0\n3.0\n3.0\n4.0\n1.0\n1.0\n1.0\n1.0\n2.0\n5.0\n1.0\n3.0\n3.0\n3.0\n3.0\n3.0\n2.0\n2.0\n1.0\n1.0\n1.0\n3.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n\n\n2.0\n1.0\n1.0\n2.0\n5.0\n3.0\n4.0\n3.0\n2.0\n4.0\n1.0\n3.0\n3.0\n1.0\n1.0\n3.0\n4.0\n5.0\n3.0\n4.0\n4.0\n5.0\n2.0\n5.0\n5.0\n3.0\n3.0\n2.0\n2.0\n4.0\n3.0\n2.0\n2.0\n2.0\n1.0\n5.0\n5.0\n3.0\n5.0\n3.0\n3.0\n3.0\n5.0\n3.0\n5.0\n4.0\n3.0\n4.0\n2.0\n\n\n3.0\n3.0\n2.0\n1.0\n5.0\n5.0\n4.0\n1.0\n1.0\n3.0\n1.0\n3.0\n5.0\n3.0\n2.0\n3.0\n3.0\n2.0\n1.0\n5.0\n3.0\n1.0\n1.0\n5.0\n4.0\n4.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n5.0\n3.0\n4.0\n5.0\n3.0\n3.0\n4.0\n5.0\n5.0\n5.0\n4.0\n4.0\n5.0\n4.0\n1.0\n3.0\n2.0\n\n\n4.0\n3.0\n3.0\n3.0\n3.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3.0\n3.0\n5.0\n5.0\n5.0\n4.0\n4.0\n1.0\n5.0\n1.0\n1.0\n1.0\n5.0\n5.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n3.0\n1.0\n3.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n5.0\n\n\n5.0\n3.0\n3.0\n3.0\n3.0\n2.0\n2.0\n2.0\n1.0\n1.0\n1.0\n4.0\n4.0\n4.0\n4.0\n3.0\n2.0\n4.0\n2.0\n4.0\n2.0\n2.0\n2.0\n3.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n4.0\n4.0\n4.0\n3.0\n4.0\n4.0\n4.0\n3.0\n2.0\n1.0\n1.0\n3.0\n5.0\n3.0\n4.0\n5.0\n5.0\n5.0\n3.0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 3449 entries, 1.0 to 3449.0\nData columns (total 48 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   q48a01  3449 non-null   float64\n 1   q48a02  3449 non-null   float64\n 2   q48a03  3449 non-null   float64\n 3   q48a04  3449 non-null   float64\n 4   q48a05  3449 non-null   float64\n 5   q48a06  3449 non-null   float64\n 6   q48a07  3449 non-null   float64\n 7   q48a08  3449 non-null   float64\n 8   q48a09  3449 non-null   float64\n 9   q48a10  3449 non-null   float64\n 10  q48a11  3449 non-null   float64\n 11  q48a12  3449 non-null   float64\n 12  q48b1   3449 non-null   float64\n 13  q48b2   3449 non-null   float64\n 14  q48b3   3449 non-null   float64\n 15  q48c1   3449 non-null   float64\n 16  q48c2   3449 non-null   float64\n 17  q48c3   3449 non-null   float64\n 18  q48c4   3449 non-null   float64\n 19  q48c5   3449 non-null   float64\n 20  q48c6   3449 non-null   float64\n 21  q48c7   3449 non-null   float64\n 22  q48c8   3449 non-null   float64\n 23  q48c9   3449 non-null   float64\n 24  q48d1   3449 non-null   float64\n 25  q48d2   3449 non-null   float64\n 26  q48d3   3449 non-null   float64\n 27  q48d4   3449 non-null   float64\n 28  q48d5   3449 non-null   float64\n 29  q48d6   3449 non-null   float64\n 30  q49a01  3449 non-null   float64\n 31  q49a02  3449 non-null   float64\n 32  q49a03  3449 non-null   float64\n 33  q49a04  3449 non-null   float64\n 34  q49a05  3449 non-null   float64\n 35  q49a06  3449 non-null   float64\n 36  q49a07  3449 non-null   float64\n 37  q49a08  3449 non-null   float64\n 38  q49a09  3449 non-null   float64\n 39  q49a10  3449 non-null   float64\n 40  q49a11  3449 non-null   float64\n 41  q49a12  3449 non-null   float64\n 42  q49a13  3449 non-null   float64\n 43  q49a14  3449 non-null   float64\n 44  q49a15  3449 non-null   float64\n 45  q49a16  3449 non-null   float64\n 46  q49a17  3449 non-null   float64\n 47  q50     3449 non-null   float64\ndtypes: float64(48)\nmemory usage: 1.3 MB\n\n\n\npd.set_option('display.max_columns', None)\ndf.describe()\n\n\n\n\n\n\n\n\nq48a01\nq48a02\nq48a03\nq48a04\nq48a05\nq48a06\nq48a07\nq48a08\nq48a09\nq48a10\nq48a11\nq48a12\nq48b1\nq48b2\nq48b3\nq48c1\nq48c2\nq48c3\nq48c4\nq48c5\nq48c6\nq48c7\nq48c8\nq48c9\nq48d1\nq48d2\nq48d3\nq48d4\nq48d5\nq48d6\nq49a01\nq49a02\nq49a03\nq49a04\nq49a05\nq49a06\nq49a07\nq49a08\nq49a09\nq49a10\nq49a11\nq49a12\nq49a13\nq49a14\nq49a15\nq49a16\nq49a17\nq50\n\n\n\n\ncount\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.00000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n3449.000000\n\n\nmean\n2.997101\n3.044361\n3.290229\n2.840533\n2.964627\n2.565671\n2.121774\n1.697304\n1.770368\n1.61873\n2.922586\n3.167295\n3.429110\n3.508843\n3.456654\n3.320093\n3.644245\n2.029284\n2.960568\n2.355755\n2.227312\n2.900261\n3.627718\n3.674398\n2.630618\n2.089591\n2.173963\n3.132792\n3.455204\n3.161206\n3.059148\n2.877646\n2.779936\n2.602493\n3.216874\n3.393737\n2.810670\n3.174543\n2.073645\n2.051319\n2.196869\n2.536097\n2.563642\n2.673239\n2.547985\n2.489707\n2.694984\n3.478110\n\n\nstd\n0.852179\n0.879684\n0.910827\n1.040131\n1.019201\n1.009723\n0.960480\n0.823098\n0.888854\n0.80089\n1.152060\n1.118354\n0.850296\n0.856255\n0.867994\n1.034584\n0.965244\n0.857958\n1.258456\n1.072555\n1.053085\n1.093337\n1.024071\n0.966574\n1.098328\n0.937631\n1.062930\n1.114290\n1.019720\n1.060300\n1.098398\n1.073889\n1.097346\n1.094706\n1.096792\n1.065068\n1.156963\n1.096679\n0.994663\n0.939886\n1.019435\n1.230151\n1.189947\n1.130768\n1.146758\n1.182564\n1.198307\n0.864785\n\n\nmin\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.00000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n25%\n3.000000\n3.000000\n3.000000\n2.000000\n2.000000\n2.000000\n1.000000\n1.000000\n1.000000\n1.00000\n2.000000\n2.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n1.000000\n2.000000\n2.000000\n1.000000\n2.000000\n3.000000\n3.000000\n2.000000\n1.000000\n1.000000\n2.000000\n3.000000\n3.000000\n2.000000\n2.000000\n2.000000\n2.000000\n3.000000\n3.000000\n2.000000\n2.000000\n1.000000\n1.000000\n1.000000\n1.000000\n2.000000\n2.000000\n2.000000\n2.000000\n2.000000\n3.000000\n\n\n50%\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n2.000000\n2.000000\n2.000000\n1.00000\n3.000000\n3.000000\n3.000000\n4.000000\n3.000000\n3.000000\n4.000000\n2.000000\n3.000000\n2.000000\n2.000000\n3.000000\n4.000000\n4.000000\n3.000000\n2.000000\n2.000000\n3.000000\n4.000000\n3.000000\n3.000000\n3.000000\n3.000000\n2.000000\n3.000000\n3.000000\n3.000000\n3.000000\n2.000000\n2.000000\n2.000000\n2.000000\n3.000000\n3.000000\n2.000000\n2.000000\n3.000000\n4.000000\n\n\n75%\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n3.000000\n3.000000\n2.000000\n2.000000\n2.00000\n4.000000\n4.000000\n4.000000\n4.000000\n4.000000\n4.000000\n4.000000\n2.000000\n4.000000\n3.000000\n3.000000\n4.000000\n4.000000\n4.000000\n3.000000\n3.000000\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n4.000000\n4.000000\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n3.000000\n4.000000\n4.000000\n\n\nmax\n9.000000\n5.000000\n5.000000\n9.000000\n9.000000\n5.000000\n5.000000\n9.000000\n5.000000\n5.00000\n5.000000\n9.000000\n5.000000\n9.000000\n9.000000\n5.000000\n5.000000\n9.000000\n5.000000\n5.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n5.000000\n9.000000\n5.000000\n9.000000\n5.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n9.000000\n5.000000\n9.000000\n5.000000\n5.000000",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#결측치-처리-및-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#결측치-처리-및-분석",
    "title": "test",
    "section": "2. 결측치 처리 및 분석",
    "text": "2. 결측치 처리 및 분석\n\n# 결측치 비율 확인\nmissing_percentage = df.isnull().sum() / len(df) * 100\nmissing_df = pd.DataFrame({\n    '결측치 개수': df.isnull().sum(),\n    '결측치 비율(%)': missing_percentage\n})\n\n# 결측치가 있는 열만 표시\nmissing_df = missing_df[missing_df['결측치 개수'] &gt; 0].sort_values('결측치 비율(%)', ascending=False)\n\nif not missing_df.empty:\n    print(\"결측치가 있는 열:\")\n    missing_df.head(10)  # 상위 10개\nelse:\n    print(\"결측치가 없습니다.\")\n\n결측치가 있는 열:",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#변수-선택-및-차원-축소",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#변수-선택-및-차원-축소",
    "title": "test",
    "section": "3. 변수 선택 및 차원 축소",
    "text": "3. 변수 선택 및 차원 축소\n\n# 수치형 변수만 선택\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_df = df[numeric_cols]\n\n# 변수의 분산 확인\nprint(f\"수치형 변수 개수: {len(numeric_cols)}\")\nvariances = numeric_df.var()\nlow_var_cols = variances[variances &lt; 0.01].index.tolist()\nprint(f\"분산이 매우 낮은 변수 개수: {len(low_var_cols)}\")\n\n# 상관관계가 높은 변수 쌍 확인\nif len(numeric_cols) &lt;= 30:  # 변수가 적으면 전체 상관관계 행렬\n    corr_matrix = numeric_df.corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n    plt.title('상관관계 행렬')\n    plt.tight_layout()\nelse:  # 변수가 많으면 상위 30개만\n    sampled_cols = numeric_cols[:30]\n    corr_matrix = df[sampled_cols].corr()\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n    plt.title('상위 30개 변수의 상관관계 행렬')\n    plt.tight_layout()\n\n수치형 변수 개수: 567\n분산이 매우 낮은 변수 개수: 19",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#pca-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#pca-분석",
    "title": "test",
    "section": "4. PCA 분석",
    "text": "4. PCA 분석\n\n# 표본 추출 (변수가 많아 시각화할 때 필요)\nif len(numeric_cols) &gt; 100:\n    # 가장 분산이 큰 변수 100개 선택\n    top_var_cols = variances.sort_values(ascending=False).head(100).index.tolist()\n    pca_df = numeric_df[top_var_cols]\nelse:\n    pca_df = numeric_df\n\n# 결측치 처리\npca_df = pca_df.fillna(pca_df.mean())\n\n# 스케일링\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(pca_df)\n\n# PCA 실행\npca = PCA()\npca_result = pca.fit_transform(scaled_data)\n\n# 설명된 분산 비율\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='-')\nplt.xlabel('주성분 개수')\nplt.ylabel('누적 설명된 분산 비율')\nplt.title('PCA 누적 설명된 분산')\nplt.grid()\nplt.axhline(y=0.8, color='r', linestyle='--')\nplt.tight_layout()\n\n# 필요한 주성분 수 계산\nn_components = np.argmax(cumulative_variance &gt;= 0.8) + 1\nprint(f\"80% 분산을 설명하는데 필요한 주성분 수: {n_components}\")\n\n80% 분산을 설명하는데 필요한 주성분 수: 47",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#변수-중요도-및-특성-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#변수-중요도-및-특성-시각화",
    "title": "test",
    "section": "5. 변수 중요도 및 특성 시각화",
    "text": "5. 변수 중요도 및 특성 시각화\n\n# 첫 두 개의 주성분으로 산점도 그리기\nplt.figure(figsize=(10, 8))\nplt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\nplt.xlabel('주성분 1')\nplt.ylabel('주성분 2')\nplt.title('첫 두 주성분으로 데이터 시각화')\nplt.grid()\nplt.tight_layout()\n\n# 첫 두 개의 주성분에 대한 변수 기여도\nif len(pca_df.columns) &lt;= 30:\n    component_df = pd.DataFrame(\n        pca.components_[:2].T, \n        columns=['PC1', 'PC2'], \n        index=pca_df.columns\n    )\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(component_df, cmap='coolwarm', center=0)\n    plt.title('첫 두 주성분에 대한 변수 기여도')\n    plt.tight_layout()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#특성-간-관계-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#특성-간-관계-시각화",
    "title": "test",
    "section": "6. 특성 간 관계 시각화",
    "text": "6. 특성 간 관계 시각화\n\n# 중요 특성 몇 개만 선택하여 페어플롯 생성\nif len(numeric_cols) &gt; 5:\n    # 분산이 가장 큰 상위 5개 변수\n    top5_vars = variances.sort_values(ascending=False).head(5).index.tolist()\n    \n    plt.figure(figsize=(12, 10))\n    sns.pairplot(df[top5_vars])\n    plt.suptitle('상위 5개 변수의 페어플롯', y=1.02)\n    plt.tight_layout()\n\n&lt;Figure size 1152x960 with 0 Axes&gt;",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#요약-및-결론",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#요약-및-결론",
    "title": "test",
    "section": "7. 요약 및 결론",
    "text": "7. 요약 및 결론\n\n# 데이터 요약 정보\nprint(f\"데이터셋 크기: {df.shape[0]}개 관측치, {df.shape[1]}개 변수\")\nprint(f\"수치형 변수: {len(numeric_cols)}개\")\nprint(f\"범주형 변수: {len(df.select_dtypes(include=['object']).columns)}개\")\nprint(f\"80% 분산을 설명하는데 필요한 주성분 수: {n_components}\")\n\n데이터셋 크기: 3449개 관측치, 567개 변수\n수치형 변수: 567개\n범주형 변수: 0개\n80% 분산을 설명하는데 필요한 주성분 수: 47",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#표준화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#표준화",
    "title": "test",
    "section": "표준화",
    "text": "표준화\n\ndf.to_csv('_data/tmp.csv')",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#pca",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#pca",
    "title": "test",
    "section": "PCA",
    "text": "PCA\n\npca = PCA(n_components=0.9)\nX_pca = pca.fit_transform(df)\n\n# PCA 결과 시각화\nplt.figure(figsize=(10,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Explained Variance Ratio')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#군집-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#군집-분석",
    "title": "test",
    "section": "군집 분석",
    "text": "군집 분석\n\n# 요인 점수로 군집 분석 수행\nlibrary(cluster)\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# 모든 CFA 요인 점수를 추출 (결측치가 없는 행만 선택)\ncluster_data &lt;- merged_df %&gt;%\n  select(contains(\"_w\")) %&gt;%\n  na.omit()\n\n# 사용할 행의 ID 저장\ncluster_row_ids &lt;- rownames(cluster_data)\n\n# 데이터 표준화\nscaled_data &lt;- scale(cluster_data)\n\n# 최적의 군집 수 찾기\nset.seed(123)\nfviz_nbclust(scaled_data, kmeans, method = \"silhouette\", k.max = 10) +\n  labs(title = \"실루엣 방법을 통한 최적 군집 수 결정\")\n\n\n\n\n\n\n\n# 엘보우 방법도 함께 확인\nfviz_nbclust(scaled_data, kmeans, method = \"wss\", k.max = 10) +\n  labs(title = \"엘보우 방법을 통한 최적 군집 수 결정\")\n\n\n\n\n\n\n\n# 최적 군집 수를 결정 (결과에 따라 k 값 설정)\nk &lt;- 4  # 분석 결과에 따라 조정 가능\n\n# K-means 군집 분석 수행\nset.seed(123)\nkm_result &lt;- kmeans(scaled_data, centers = k, nstart = 25)\n\n# 군집 결과 시각화\nfviz_cluster(km_result, data = scaled_data,\n             ellipse.type = \"convex\",\n             palette = \"jco\",\n             ggtheme = theme_minimal(),\n             main = \"K-means 군집 분석 결과\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n# 군집 결과를 데이터프레임에 추가\ncluster_results &lt;- data.frame(\n  id = cluster_row_ids,\n  cluster = km_result$cluster\n)\n\n# 원본 데이터와 군집 결과 병합\nmerged_with_clusters &lt;- merge(\n  merged_df, \n  cluster_results, \n  by.x = \"id\", \n  by.y = \"id\"\n)\n\n# q11과 군집 간의 교차표 만들기\n# 먼저 df6_origin에서 id와 q11, ua2만 추출\nq11_data &lt;- merged_df[, c(\"id\", \"q11_ua2_subgroup\")]\n\n# q11=1인 경우 ua2 값으로 세분화, 그렇지 않은 경우 q11 값 그대로 사용\nq11_data$group_value &lt;- merged_df$q11_ua2_subgroup\n\n# 군집 결과와 q11 데이터 병합\ncluster_q11 &lt;- merge(\n  cluster_results,\n  q11_data,\n  by = \"id\"\n)\n\n# 군집별 빈도표 생성 (레이블 생성 없이 값 그대로 사용)\ncluster_q11_table &lt;- table(cluster_q11$cluster, cluster_q11$group_value)\n\n# 빈도표 백분율 계산\ncluster_q11_percent &lt;- prop.table(cluster_q11_table, margin = 1) * 100\n\n# 결과 테이블 출력\nkable(cluster_q11_table, \n      caption = \"군집별 빈도표\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n군집별 빈도표\n\n\n과학계열| 기타\n열| 미분류/미결\n| 예체능계열| 인문계열|\n\n\n\n\n\n\n133\n50\n45\n95\n125\n\n\n200\n31\n105\n75\n204\n\n\n131\n21\n78\n64\n140\n\n\n141\n31\n48\n89\n145\n\n\n\n\n\n\n\n\n\nfrequency_data_only &lt;- cluster_q11_table\ncluster_names &lt;- rownames(cluster_q11_table)\nif (is.null(cluster_names)) {\n  cluster_names &lt;- paste0(\"Cluster_\", 1:nrow(cluster_q11_table))\n}\n\nq11_detailed_categories &lt;- colnames(frequency_data_only)\nmax_freq_cluster_indices_per_q11 &lt;- apply(frequency_data_only, 2, which.max)\ndominant_cluster_names_per_q11 &lt;- cluster_names[max_freq_cluster_indices_per_q11]\n\n\n# --- 4. 결과를 군집별로 정리하여 저장 ---\n# 각 군집 ID를 이름으로 하는 리스트를 만듭니다.\nq11_codes_dominant_in_each_cluster &lt;- lapply(cluster_names, function(x) character(0))\nnames(q11_codes_dominant_in_each_cluster) &lt;- cluster_names\n\n# 각 상세 q11 범주에 대해, 그 범주가 가장 많이 나타나는 군집을 찾아 리스트에 추가합니다.\nfor (i in 1:length(q11_detailed_categories)) {\n  q11_code &lt;- q11_detailed_categories[i]\n  dominant_cluster_for_this_code &lt;- dominant_cluster_names_per_q11[i]\n  \n  q11_codes_dominant_in_each_cluster[[dominant_cluster_for_this_code]] &lt;- \n    c(q11_codes_dominant_in_each_cluster[[dominant_cluster_for_this_code]], q11_code)\n}\n\n\n# --- 5. 최종 출력 (군집별로 정리된 상세 q11 범주 목록) ---\ncat(\"\\n=== 각 군집별로 빈도가 가장 높은 상세 q11 범주 목록 ===\\n\")\n\n\n=== 각 군집별로 빈도가 가장 높은 상세 q11 범주 목록 ===\n\nfor (cluster_name in names(q11_codes_dominant_in_each_cluster)) {\n  q11_codes_list &lt;- q11_codes_dominant_in_each_cluster[[cluster_name]]\n  \n  if (length(q11_codes_list) &gt; 0) {\n    # 예시 출력 형식에 맞게 ( )로 묶어서 출력\n    cat(paste0(cluster_name, \": (\", paste(q11_codes_list, collapse = \", \"), \")\\n\"))\n  } else {\n    cat(paste0(cluster_name, \": (이 군집이 가장 지배적인 상세 q11 범주는 없음)\\n\"))\n  }\n}\n\n1: (기타계열, 예체능계열)\n2: (과학계열, 미분류/미결정, 인문계열)\n3: (이 군집이 가장 지배적인 상세 q11 범주는 없음)\n4: (이 군집이 가장 지배적인 상세 q11 범주는 없음)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#exploratory-factor-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#exploratory-factor-analysis",
    "title": "test",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nchi_sq, p_value = calculate_bartlett_sphericity(df)\nkmo_all, kmo_model = calculate_kmo(df)\nprint(f\"Bartlett's Test p-value: {p_value:.4f}\")\nprint(f\"KMO Measure: {kmo_model:.3f}\")\n\n# 병렬분석 함수 정의\ndef parallel_analysis(data, n_sim=100, q=95):\n    real_eigen = FactorAnalyzer(rotation=None, method='minres').fit(data).get_eigenvalues()[0]\n    sim_eigen = []\n    for _ in range(n_sim):\n        X_sim = np.random.normal(size=data.shape)\n        fa = FactorAnalyzer(rotation=None, method='minres').fit(X_sim)\n        sim_eigen.append(fa.get_eigenvalues()[0])\n    cutoff = np.percentile(sim_eigen, q=q, axis=0)\n    n_factors = sum(real_eigen &gt; cutoff)\n    return n_factors\n\n# 병렬분석으로 요인 수 결정\nn_factors = parallel_analysis(df)\nprint(f\"Suggested number of factors: {n_factors}\")\n\n# EFA 실행\nfa = FactorAnalyzer(\n    n_factors=n_factors,\n    method='minres',\n    rotation='oblimin',\n    use_smc=True\n)\nfa.fit(df)\n\n# 요인 부하량 테이블 생성\nloadings = pd.DataFrame(\n    fa.loadings_,\n    columns=[f'Factor{i+1}' for i in range(n_factors)],\n    index=df.columns\n)\n\n# 0.3 미만 적재량 마스킹\nloadings_masked = loadings.mask(np.abs(loadings) &lt; 0.3, '')\n\n# 시각화\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    loadings_masked.replace('', np.nan),\n    annot=True,\n    cmap='coolwarm',\n    center=0,\n    fmt=\".2f\",\n    linewidths=0.5\n)\nplt.title(f'Exploratory Factor Analysis (Oblimin Rotation)\\n{n_factors} Factors from Parallel Analysis')\nplt.show()\n\nBartlett's Test p-value: 0.0000\nKMO Measure: 0.871\nSuggested number of factors: 12",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#confirmatory-factor-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#confirmatory-factor-analysis",
    "title": "test",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\nmodel_dict = {\n    # 자아존중감 (Self-Esteem)\n    \"Positive_Self_Esteem\": [\"q48a01\", \"q48a02\", \"q48a03\"],  # 좋은 사람이라고 생각하는 정도\n    \"Negative_Self_Esteem\": [\"q48a04\", \"q48a05\", \"q48a06\"],  # 안좋은 사람이라고 생각하는 정도\n    \n    # 일탈적 자아 낙인 (Deviant Self-Stigma)\n    \"Deviant_Self_Stigma\": [\"q48a07\", \"q48a08\", \"q48a09\", \"q48a10\"],\n    \n    # 주위 사람들에 대한 애착 (Attachment)\n    \"Attachment\": [\"q48a11\", \"q48a12\"],\n    \n    # 자기신뢰감 (Self-Confidence)\n    \"Self_Confidence\": [\"q48b1\", \"q48b2\", \"q48b3\"],\n    \n    # 공격성 (Aggression)\n    \"Aggression\": [\"q48c1\", \"q48c2\", \"q48c3\", \"q48c4\", \"q48c5\", \"q48c6\"],\n    \n    # 정서적 조절감 (Emotional Regulation)\n    \"Emotional_Regulation\": [\"q48c7\", \"q48c8\", \"q48c9\"],\n    \n    # 성역할정체감 (Gender Identity)\n    \"Female_Gender_Identity\": [\"q48d1\", \"q48d2\", \"q48d3\"],  # 여자 정체성\n    \"Male_Gender_Identity\": [\"q48d4\", \"q48d5\", \"q48d6\"],    # 남자 정체성\n    \n    # 스트레스 (Stress)\n    \"Parental_Stress\": [\"q49a01\", \"q49a02\", \"q49a03\", \"q49a04\"],\n    \"Academic_Stress\": [\"q49a05\", \"q49a06\", \"q49a07\", \"q49a08\"],\n    \"Peer_Stress\": [\"q49a09\", \"q49a10\", \"q49a11\"],\n    \"Appearance_Stress\": [\"q49a12\", \"q49a13\", \"q49a14\"],\n    \"Material_Stress\": [\"q49a15\", \"q49a16\", \"q49a17\"],\n    \n    # 삶의 만족도 (Life Satisfaction)\n    \"Life_Satisfaction\": [\"q50\"]\n}\n\n# 모델 사양 파싱\nmodel_spec = ModelSpecificationParser.parse_model_specification_from_dict(df, model_dict)\n\n# CFA 실행\ncfa = ConfirmatoryFactorAnalyzer(model_spec, disp=True)\nfactor_scores = cfa.fit_transform(df.values)\n\nfactor_scores_df = pd.DataFrame(\n    factor_scores,\n    columns=model_dict.keys()\n)\n\n This problem is unconstrained.\n\n\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =          888     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  2.21088D+05    |proj g|=  3.24469D+03\n\nAt iterate    1    f=  2.14989D+05    |proj g|=  1.77167D+03\n\n\n\n Bad direction in the line search;\n   refresh the lbfgs memory and restart the iteration.\n\n\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n  888      2     42      2     0     0   1.772D+03   2.150D+05\n  F =   214988.57589872958     \n\nABNORMAL_TERMINATION_IN_LNSRCH                              \n\n\n\n Line search cannot locate an adequate point after MAXLS\n  function and gradient evaluations.\n  Previous x, f and g restored.\n Possible causes: 1 error in function or gradient evaluation;\n                  2 rounding error dominate computation.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html",
    "title": "예측",
    "section": "",
    "text": "4페이지 집중 못함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#판단적-기법정성적-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#판단적-기법정성적-기법",
    "title": "예측",
    "section": "판단적 기법(정성적 기법)",
    "text": "판단적 기법(정성적 기법)\n\n전문가의 경험과 직관에 의존하여 예측하는 기법\n비정량적 / 주관적 데이터로 정량적인 예측치를 구함\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치\n추세분석:치전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법\n시장조사법\n전문가 의견 종합법\n사례유추법: 비슷한 제품이랑 비교\n델파이 기법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#불확실성-하에서의-프로젝트-관리",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#불확실성-하에서의-프로젝트-관리",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "불확실성 하에서의 프로젝트 관리",
    "text": "불확실성 하에서의 프로젝트 관리\n\nCPM(Critical Path Method): 소요 시간이 정확하게 알려져 있다고 가정\nPERT(Program Evaluation and Review Technique): 소요 시간이 불확실하다고 가정\n\n\n가정\n\n주 경로는 각 활동의 평균 소요시간으로구함\n주 경로는 바뀌지 않음\n각 활동의 소요시간은 독립적임\n\n\n\n특징\n\n\\(d_1, d_2, ..., d_n\\)이 확률변수이면 프로젝트 완성시간 (\\(X\\))도 확률변수\n\\(d_1, d_2, ..., d_n\\)이 정규분포를 따른다면 \\(X\\)도 정규분포를 따름",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-load",
    "title": "preprocessing",
    "section": "Data Load",
    "text": "Data Load\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survey)\n\nLoading required package: grid\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\nlibrary(semPlot)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(ROSE) # ROSE 패키지 추가 (불균형 데이터 처리)\n\nLoaded ROSE 0.0-4\n\nID_VAR &lt;- \"id\"\nWEIGHT_VAR &lt;- \"wt2\"\nOUTCOME_VAR &lt;- \"status_category\"\n\ntarget_pred_var &lt;- c(ID_VAR, \"wt1\", \"q33a01\", \"q33a02\", \"q33a03\", \"q33a04\", \"q33a05\", \"q33a06\",\n                    \"q48a07\", \"q48a08\", \"q48a09\", \"q48a10\",\n                    \"q49a01\", \"q49a02\", \"q49a03\", \"q49a04\",\n                    \"q33a07\", \"q33a08\", \"q33a09\",\n                    \"q49a15\", \"q49a16\", \"q49a17\",\n                    \"q49a09\", \"q49a10\", \"q49a11\",\n                    \"q48b1\", \"q48b2\", \"q48b3\",\n                    \"q12a01\", \"q12a02\", \"q12a03\",\n                    \"q48a04\", \"q48a05\", \"q48a06\",\n                    \"q49a05\", \"q49a06\", \"q49a08\")\n\ncfa_model &lt;- '\n  # 1. 부모애착\n  parent_attachment =~ q33a01 + q33a02 + q33a03 + q33a04 + q33a05 + q33a06\n  # 2. 일탈적 자아 낙인\n  deviant_esteem =~ q48a07 + q48a08 + q48a09 + q48a10\n  # 3. 부모에 의한 스트레스\n  parent_stress =~ q49a01 + q49a02 + q49a03 + q49a04\n  # 4. 부모감독\n  parent_monitoring =~ q33a07 + q33a08 + q33a09\n  # 5. 물질적 요인으로 인한 스트레스\n  desire_stress =~ q49a15 + q49a16 + q49a17\n  # 6. 친구로 인한 스트레스\n  friend_stress =~ q49a09 + q49a10 + q49a11\n  # 7. 자기신뢰감\n  self_confidence =~ q48b1 + q48b2 + q48b3\n  # 8. 상급학교 의존도\n  higher_school_dependence =~ q12a01 + q12a02 + q12a03\n  # 9. 부정적 자아존중감\n  neg_esteem =~ q48a04 + q48a05 + q48a06\n  # 10. 학업으로 인한 스트레스\n  academic_stress =~ q49a05 + q49a06 + q49a08\n'\n\ndf1_origin &lt;- read.csv('_data/student_1.csv')\ndf2_origin &lt;- read.csv('_data/student_2.csv')\ndf3_origin &lt;- read.csv('_data/student_3.csv')\ndf4_origin &lt;- read.csv('_data/student_4.csv')\ndf5_origin &lt;- read.csv('_data/student_5.csv')\ndf6_origin &lt;- read.csv('_data/student_6.csv')",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "title": "preprocessing",
    "section": "CFA (Confirmatory Factor Analysis)",
    "text": "CFA (Confirmatory Factor Analysis)\n\nmerged_df &lt;- data.frame()\nfor (i in 1:5) {\n  ID_VAR &lt;- \"id\"\n  df &lt;- get(paste0(\"df\", i, \"_origin\"))\n  df_analysis &lt;- df[target_pred_var]\n  df_clean &lt;- na.omit(df_analysis)\n  cfa_fit &lt;- cfa(\n    cfa_model,\n    data = df_clean,\n    sampling.weights = \"wt1\",\n    estimator = \"MLR\",\n    se = \"robust.cluster\",\n    test = \"satorra.bentler\")\n  factor_scores &lt;- lavPredict(cfa_fit)\n  factor_scores_df &lt;- cbind(df_clean[ID_VAR], as.data.frame(factor_scores))\n  names(factor_scores_df)[-1] &lt;- paste0(names(factor_scores_df)[-1], \"_w\", i)\n  if (i == 1) {\n    merged_df &lt;- factor_scores_df\n  } else {\n    merged_df &lt;- merge(merged_df, factor_scores_df, by = ID_VAR, all = TRUE)\n  }\n}\n\nWarning: lavaan-&gt;lav_options_set():  \n   observed.information for ALL test statistics is set to h1.\nWarning: lavaan-&gt;lav_options_set():  \n   observed.information for ALL test statistics is set to h1.\nWarning: lavaan-&gt;lav_options_set():  \n   observed.information for ALL test statistics is set to h1.\nWarning: lavaan-&gt;lav_options_set():  \n   observed.information for ALL test statistics is set to h1.\nWarning: lavaan-&gt;lav_options_set():  \n   observed.information for ALL test statistics is set to h1.\n\nclassify_status &lt;- function(q11_value) {\n  if (q11_value %in% c(1, 5, 6, 7, 8, 71, 81, 91, 10, 101, 11, 111)) {\n    return(\"active\")\n  } else if (q11_value %in% c(2, 3, 4, 9, 12, 13, 14)) {\n    return(\"passive\")\n  } else {\n    return(NA)\n  }\n}\n\nextra_df &lt;- df6_origin[, c(\"id\", \"q11\", \"wt2\", \"sex\", \"yy\", \"area\")]\nextra_df$status_category &lt;- mapply(classify_status, extra_df$q11)\nmerged_df &lt;- merge(merged_df, extra_df, by = \"id\", all.x = TRUE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#잠재-요인-점수-추출-및-dbscan-클러스터링",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#잠재-요인-점수-추출-및-dbscan-클러스터링",
    "title": "test",
    "section": "잠재 요인 점수 추출 및 DBSCAN 클러스터링",
    "text": "잠재 요인 점수 추출 및 DBSCAN 클러스터링\nDBSCAN(Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반 클러스터링 알고리즘으로, 임의 형태의 클러스터를 발견할 수 있습니다.\n\n# 필요한 패키지 로드\nlibrary(dbscan)  # DBSCAN 알고리즘\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(cluster) # 실루엣 계수 계산\nlibrary(fpc)     # 클러스터 통계 (Dunn 지수 등)\n\n\nAttaching package: 'fpc'\n\n\nThe following object is masked from 'package:dbscan':\n\n    dbscan\n\nlibrary(clValid) # 클러스터 타당성 검증\nlibrary(factoextra) # 클러스터 시각화\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# CFA에서 얻은 잠재 요인 점수 추출\nlatent_scores &lt;- lavPredict(cfa_fit)\nlatent_scores_df &lt;- as.data.frame(latent_scores)\n\nlatent_scores_df &lt;- na.omit(latent_scores_df)\n\n# 데이터 스케일링 (DBSCAN에는 중요)\nscaled_scores &lt;- scale(latent_scores_df)\n\n\n# 다양한 eps와 minPts 값 조합 실험\neps_values &lt;- seq(0.5, 1.5, by = 0.1)\nminPts_values &lt;- c(3, 4, 5, 6)\n\nresults &lt;- expand.grid(eps = eps_values, minPts = minPts_values)\nresults$silhouette &lt;- NA\nresults$dunn &lt;- NA\nresults$clusters &lt;- NA\n\nfor(i in 1:nrow(results)) {\n  # DBSCAN 실행\n    db &lt;- dbscan::dbscan(scaled_scores, eps = results$eps[i], minPts = results$minPts[i])\n  \n  # 클러스터 수 (노이즈 제외)\n  n_clusters &lt;- length(unique(db$cluster[db$cluster &gt; 0]))\n    results$clusters[i] &lt;- n_clusters\n    \n    # 클러스터 정보 저장 (문제 해결용)\n    if(i == 1) {\n      cat(\"첫 번째 파라미터 조합 클러스터링 결과:\\n\")\n      print(table(db$cluster))\n    }\n  \n  # 클러스터가 2개 이상인 경우에만 평가 지표 계산\n  if(n_clusters &gt;= 2) {\n    # 실루엣 계수 계산 (노이즈 제외)\n    non_noise_idx &lt;- which(db$cluster &gt; 0)\n    if(length(non_noise_idx) &gt; 0) {\n      sil &lt;- silhouette(db$cluster[non_noise_idx], \n                        dist(scaled_scores[non_noise_idx, ]))\n      results$silhouette[i] &lt;- mean(sil[,3])\n    }\n    \n    # Dunn 지수 계산 (fpc 패키지)\n    dist_matrix &lt;- dist(scaled_scores)\n    results$dunn[i] &lt;- dunn(dist_matrix, db$cluster)\n  }\n}\n\n첫 번째 파라미터 조합 클러스터링 결과:\n\n   0    1    2    3    4 \n2927    3    5   20    3 \n\n# 결과 확인 (NA 값 제거)\nvalid_results &lt;- results[!is.na(results$silhouette) & !is.na(results$dunn), ]\n\n# 최적의 파라미터 선택\n# 실루엣 계수와 Dunn 지수를 모두 고려\nvalid_results$combined_score &lt;- scale(valid_results$silhouette) + scale(valid_results$dunn)\nbest_params &lt;- valid_results[which.max(valid_results$combined_score), ]\n\ncat(\"최적의 파라미터:\\n\")\n\n최적의 파라미터:\n\ncat(\"eps =\", best_params$eps, \", minPts =\", best_params$minPts, \"\\n\")\n\neps = 0.6 , minPts = 3 \n\ncat(\"실루엣 계수 =\", best_params$silhouette, \", Dunn 지수 =\", best_params$dunn, \"\\n\")\n\n실루엣 계수 = 0.8253902 , Dunn 지수 = 1.555603 \n\ncat(\"클러스터 수 =\", best_params$clusters, \"\\n\")\n\n클러스터 수 = 3 \n\n# 결과 시각화\nggplot(valid_results, aes(x = eps, y = minPts, color = silhouette, size = dunn)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(title = \"DBSCAN 파라미터에 따른 클러스터링 성능\",\n       x = \"eps\", y = \"minPts\",\n       color = \"실루엣 계수\", size = \"Dunn 지수\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# 최적의 파라미터로 DBSCAN 실행\nbest_db &lt;- dbscan::dbscan(scaled_scores, \n                 eps = best_params$eps, \n                 minPts = best_params$minPts)\n\n# 클러스터 결과 확인\ncluster_table &lt;- table(best_db$cluster)\nprint(cluster_table)\n\n\n   0    1    2    3 \n2924    3   28    3 \n\n# 클러스터 수 재확인 (노이즈 제외)\nactual_clusters &lt;- length(unique(best_db$cluster[best_db$cluster &gt; 0]))\ncat(\"실제 클러스터 수 (노이즈 제외):\", actual_clusters, \"\\n\")\n\n실제 클러스터 수 (노이즈 제외): 3 \n\ncat(\"파라미터 탐색 시 예측된 클러스터 수:\", best_params$clusters, \"\\n\")\n\n파라미터 탐색 시 예측된 클러스터 수: 3 \n\n# 클러스터 수 불일치 확인\nif(actual_clusters != best_params$clusters) {\n  cat(\"경고: 예측된 클러스터 수와 실제 클러스터 수가 다릅니다.\\n\")\n  cat(\"이는 DBSCAN 알고리즘의 특성 또는 데이터 전처리 차이 때문일 수 있습니다.\\n\")\n}\n\n# 결과 시각화 (PCA로 차원 축소하여 2D로 표현)\npca_result &lt;- prcomp(scaled_scores, scale = FALSE)\npca_data &lt;- as.data.frame(pca_result$x[, 1:2])\npca_data$cluster &lt;- factor(best_db$cluster)\n\n# PCA 시각화\nggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"DBSCAN 클러스터링 결과 (PCA 기반 시각화)\",\n       x = \"주성분 1\", y = \"주성분 2\")\n\n\n\n\n\n\n\n# 클러스터별 잠재변수 평균 차이 비교\nif(sum(best_db$cluster &gt; 0) &gt; 0) {  # 노이즈가 아닌 샘플이 있는 경우\n  cluster_means &lt;- aggregate(latent_scores_df, by = list(cluster = best_db$cluster), mean)\n  print(cluster_means)\n  \n  # 클러스터별 잠재변수 분포 시각화\n  latent_scores_long &lt;- latent_scores_df\n  latent_scores_long$cluster &lt;- factor(best_db$cluster)\n  latent_scores_long &lt;- reshape2::melt(latent_scores_long, id.vars = \"cluster\")\n  \n  ggplot(latent_scores_long[latent_scores_long$cluster != 0, ], \n         aes(x = variable, y = value, fill = cluster)) +\n    geom_boxplot() +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"클러스터별 잠재 요인 분포\",\n         x = \"잠재 요인\", y = \"요인 점수\")\n}\n\n  cluster   pos_esteem   neg_esteem deviant_esteem self_confidence   aggression\n1       0  0.003510103 -0.004933594    -0.01226843     0.004703159 -0.005493053\n2       1 -0.378992943  0.456951642     1.09363231    -0.564248262  0.469865008\n3       2 -0.366145747  0.480263522     1.14058116    -0.463159195  0.521178731\n4       3  0.375172471 -0.130801241     0.21857307     0.303055167  0.019696142\n  emotional_regulation female_identity male_identity parent_stress\n1          0.003249365    -0.006336888    0.00393931  -0.002779819\n2         -0.338314895     0.566937685   -0.38506129  -0.414885351\n3         -0.333829298     0.595599452   -0.36180569   0.374526500\n4          0.287007366     0.050487988   -0.07759969  -0.371297928\n  academic_stress friend_stress appearance_stress desire_stress\n1   -0.0002226621  -0.009322332      -0.004173755  -0.003621459\n2   -0.9312536616   0.102563696      -0.386943239  -0.449711030\n3    0.1087473350   0.953083980       0.461211900   0.418486827\n4    0.1332998718   0.088151845       0.150318978   0.073549043",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#클러스터-해석-및-특성-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#클러스터-해석-및-특성-분석",
    "title": "test",
    "section": "클러스터 해석 및 특성 분석",
    "text": "클러스터 해석 및 특성 분석\n\n# 클러스터별 특성 분석\nif(length(unique(best_db$cluster[best_db$cluster &gt; 0])) &gt;= 2) {\n  # 각 잠재변수별 클러스터 간 차이 검정 (ANOVA)\n  anova_results &lt;- list()\n  for(var in colnames(latent_scores_df)) {\n    formula &lt;- as.formula(paste(var, \"~ cluster\"))\n    # 노이즈가 아닌 데이터만 사용\n    subset_data &lt;- data.frame(\n      y = latent_scores_df[best_db$cluster &gt; 0, var],\n      cluster = factor(best_db$cluster[best_db$cluster &gt; 0])\n    )\n    anova_results[[var]] &lt;- summary(aov(y ~ cluster, data = subset_data))\n  }\n  \n  # 유의미한 차이가 있는 변수 확인\n  significant_vars &lt;- c()\n  for(var in names(anova_results)) {\n    p_value &lt;- anova_results[[var]][[1]]$`Pr(&gt;F)`[1]\n    if(p_value &lt; 0.05) {\n      significant_vars &lt;- c(significant_vars, var)\n      cat(var, \": p-value =\", p_value, \"\\n\")\n    }\n  }\n  \n  # 클러스터별 중요 특성 시각화 (유의미한 차이가 있는 변수만)\n  if(length(significant_vars) &gt; 0) {\n    significant_data &lt;- latent_scores_df[, significant_vars]\n    significant_data$cluster &lt;- factor(best_db$cluster)\n    \n    # 노이즈 제외\n    significant_data &lt;- significant_data[significant_data$cluster != 0, ]\n    \n    # 레이더 차트 데이터 준비\n    radar_data &lt;- aggregate(. ~ cluster, data = significant_data, mean)\n    radar_data_long &lt;- reshape2::melt(radar_data, id.vars = \"cluster\")\n    \n    # 레이더 차트 대신 클러스터별 특성을 히트맵으로 시각화\n    # 클러스터별 변수 평균값 계산\n    cluster_means &lt;- aggregate(. ~ cluster, data = significant_data, mean)\n    \n    # 히트맵 데이터 준비\n    heatmap_data &lt;- as.matrix(cluster_means[, -1])  # cluster 열 제외\n    rownames(heatmap_data) &lt;- paste(\"Cluster\", cluster_means$cluster)\n    \n    # 데이터 스케일링 (변수 간 비교 가능하도록)\n    scaled_heatmap &lt;- scale(t(heatmap_data))  # 변수를 행으로, 클러스터를 열로 변환\n    \n    # 히트맵 그리기\n    library(pheatmap)\n    pheatmap(scaled_heatmap, \n             main = \"클러스터별 유의미한 변수 특성\",\n             color = colorRampPalette(c(\"navy\", \"white\", \"firebrick3\"))(100),\n             cluster_rows = TRUE,  # 변수 클러스터링\n             cluster_cols = FALSE,  # 클러스터 순서 유지\n             fontsize_row = 8,\n             fontsize_col = 10)\n    \n    # 클러스터별 중요 변수의 평균값 막대 그래프\n    cluster_long &lt;- reshape2::melt(cluster_means, id.vars = \"cluster\")\n    \n    # 각 클러스터별 변수 평균값 시각화\n    ggplot(cluster_long, aes(x = variable, y = value, fill = factor(cluster))) +\n      geom_bar(stat = \"identity\", position = \"dodge\") +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n      labs(title = \"클러스터별 중요 변수 평균값\",\n           x = \"잠재 변수\", y = \"평균값\",\n           fill = \"클러스터\") +\n      scale_fill_brewer(palette = \"Set1\")\n  }\n}\n\npos_esteem : p-value = 6.558728e-24 \nneg_esteem : p-value = 1.504417e-17 \ndeviant_esteem : p-value = 1.515274e-35 \nself_confidence : p-value = 0.0001062653 \naggression : p-value = 1.161138e-24 \nemotional_regulation : p-value = 1.425289e-34 \nfemale_identity : p-value = 1.598818e-21 \nmale_identity : p-value = 2.29179e-30 \nparent_stress : p-value = 1.587271e-23 \nacademic_stress : p-value = 1.111085e-10 \nfriend_stress : p-value = 1.606266e-58 \nappearance_stress : p-value = 9.877758e-40 \ndesire_stress : p-value = 7.040077e-55",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/16.html#data-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/16.html#data-load",
    "title": "test2",
    "section": "Data Load",
    "text": "Data Load\n\n# 데이터 로드 및 필요 패키지 불러오기\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# CSV 파일 읽기 (row.names=1은 첫 번째 열을 행 이름으로 사용)\ndf_origin &lt;- read.csv('_data/student_6.csv', row.names=1)\n\n# 원본 데이터 복사\ndf &lt;- df_origin[c('q11', 'yy')]\n\nsummary(df$yy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  87.00   89.00   89.00   89.22   89.00   90.00     595 \n\n# 데이터 구조 확인\nstr(df)\n\n'data.frame':   3449 obs. of  2 variables:\n $ q11: num  NA 10 NA 1 NA 1 NA 1 NA NA ...\n $ yy : num  NA 89 NA 90 NA 90 NA 90 NA NA ...\n\n# 칼럼별 결측치 개수 확인\nmissing_values &lt;- colSums(is.na(df))\nprint(\"칼럼별 결측치 개수:\")\n\n[1] \"칼럼별 결측치 개수:\"\n\nprint(missing_values)\n\nq11  yy \n616 595 \n\n# 결측치 비율 확인(%)\nmissing_percentage &lt;- colMeans(is.na(df)) * 100\nmissing_df &lt;- data.frame(\n  Column = names(missing_percentage),\n  Missing_Count = missing_values,\n  Missing_Percentage = missing_percentage\n)\nprint(\"칼럼별 결측치 비율(%):\")\n\n[1] \"칼럼별 결측치 비율(%):\"\n\nprint(missing_df)\n\n    Column Missing_Count Missing_Percentage\nq11    q11           616           17.86025\nyy      yy           595           17.25138\n\n# q11 빈도표 생성\nq11_freq &lt;- table(df$q11)\nprint(\"q11 빈도표:\")\n\n[1] \"q11 빈도표:\"\n\nprint(q11_freq)\n\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   19 \n1416  524  256   14   41  332  116    1    1   32    1    1   33   21   15    1 \n  21   25   31   33   71   81  101  111 \n   1    5    6    2    5    1    7    1 \n\n# 빈도와 비율을 함께 보여주는 데이터프레임 생성\nq11_freq_df &lt;- as.data.frame(q11_freq)\ncolnames(q11_freq_df) &lt;- c(\"q11_값\", \"빈도\")\nq11_freq_df$비율 &lt;- prop.table(q11_freq) * 100\nq11_freq_df$누적비율 &lt;- cumsum(q11_freq_df$비율)\n\n# 총 빈도를 마지막 행에 추가\ntotal_row &lt;- data.frame(\n  \"q11_값\" = \"총계\",\n  \"빈도\" = sum(q11_freq_df$빈도),\n  \"비율\" = sum(q11_freq_df$비율),\n  \"누적비율\" = NA\n)\nq11_freq_df &lt;- rbind(q11_freq_df, total_row)\nprint(q11_freq_df)\n\n   q11_값 빈도         비율  누적비율\n1       1 1416  49.98235086  49.98235\n2       2  524  18.49629368  68.47864\n3       3  256   9.03635722  77.51500\n4       4   14   0.49417579  78.00918\n5       5   41   1.44722909  79.45641\n6       6  332  11.71902577  91.17543\n7       7  116   4.09459936  95.27003\n8       8    1   0.03529827  95.30533\n9       9    1   0.03529827  95.34063\n10     10   32   1.12954465  96.47017\n11     11    1   0.03529827  96.50547\n12     12    1   0.03529827  96.54077\n13     13   33   1.16484292  97.70561\n14     14   21   0.74126368  98.44688\n15     15   15   0.52947406  98.97635\n16     19    1   0.03529827  99.01165\n17     21    1   0.03529827  99.04695\n18     25    5   0.17649135  99.22344\n19     31    6   0.21178962  99.43523\n20     33    2   0.07059654  99.50582\n21     71    5   0.17649135  99.68232\n22     81    1   0.03529827  99.71761\n23    101    7   0.24708789  99.96470\n24    111    1   0.03529827 100.00000\n25   총계 2833 100.00000000        NA\n\n# 빈도 막대 그래프\nbarplot(q11_freq, main=\"q11 빈도 분포\", xlab=\"q11 값\", ylab=\"빈도\", col=\"skyblue\")\n\n\n\n\n\n\n\n# ggplot2를 활용한 빈도 그래프\nlibrary(ggplot2)\nggplot(q11_freq_df, aes(x=`q11_값`, y=빈도)) +\n  geom_bar(stat=\"identity\", fill=\"steelblue\") +\n  geom_text(aes(label=빈도), vjust=-0.3) +\n  theme_minimal() +\n  labs(title=\"q11 빈도 분포\", x=\"q11 값\", y=\"빈도\")",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test2"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#시계열-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#시계열-기법",
    "title": "예측",
    "section": "시계열 기법",
    "text": "시계열 기법\n\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가까울 수록 가중치를 크게 부여\n지수평활법: 과거의 모든 데이터를 가중 평균\n\n지수평활계수(α): 최근의 값을 더 높은 가중치가 부여되도록 추정\n\\(\\hat{y_{t+1}} = αy_t+ (1-α)\\hat{y_t} = \\hat{y_t} + α(y_t - \\hat{y_t}\\)\n예측치와 관측치 중 어디에 중점을 둘 지에 따라서 α 결정\n== 오차를 어느정도 반영할지에 따라서 α 결정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#상관관계-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#상관관계-기법",
    "title": "예측",
    "section": "상관관계 기법",
    "text": "상관관계 기법\n\n선행 지수법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#계절성-수요예측",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#계절성-수요예측",
    "title": "예측",
    "section": "계절성 수요예측",
    "text": "계절성 수요예측\n\n계절\n추세",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "title": "프로젝트 관리",
    "section": "불확실성 하에서의 프로젝트 관리",
    "text": "불확실성 하에서의 프로젝트 관리\n\nCPM(Critical Path Method): 소요 시간이 정확하게 알려져 있다고 가정\nPERT(Program Evaluation and Review Technique): 소요 시간이 불확실하다고 가정\n\n\n가정\n\n주 경로는 각 활동의 평균 소요시간으로구함\n주 경로는 바뀌지 않음\n각 활동의 소요시간은 독립적임\n\n\n\n특징\n\n\\(d_1, d_2, ..., d_n\\)이 확률변수이면 프로젝트 완성시간 (\\(X\\))도 확률변수\n\\(d_1, d_2, ..., d_n\\)이 정규분포를 따른다면 \\(X\\)도 정규분포를 따름",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#train-test-split",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#train-test-split",
    "title": "test",
    "section": "train test split",
    "text": "train test split",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest-모델링",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest-모델링",
    "title": "test",
    "section": "Random Forest 모델링",
    "text": "Random Forest 모델링\n\n# 가중치 확인 (있으면 사용)\nif(\"w1\" %in% names(train_data)) {\n  # 가중치 정규화\n  train_data$w1_norm &lt;- train_data$w1 / sum(train_data$w1) * nrow(train_data)\n  \n  # 가중치를 적용한 Random Forest 모델 학습\n  rf_model &lt;- randomForest(\n    status_code ~ ., \n    data = train_data[, c(\"status_code\", feature_vars)],\n    ntree = 500,\n    mtry = floor(sqrt(length(feature_vars))),\n    weights = train_data$w1_norm,\n    importance = TRUE\n  )\n} else {\n  # 가중치 없이 Random Forest 모델 학습\n  rf_model &lt;- randomForest(\n    status_code ~ ., \n    data = train_data[, c(\"status_code\", feature_vars)],\n    ntree = 500,\n    mtry = floor(sqrt(length(feature_vars))),\n    importance = TRUE\n  )\n}\n\n# 모델 요약\nprint(rf_model)\n\n\nCall:\n randomForest(formula = status_code ~ ., data = train_data[, c(\"status_code\",      feature_vars)], ntree = 500, mtry = floor(sqrt(length(feature_vars))),      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 7\n\n        OOB estimate of  error rate: 3.9%\nConfusion matrix:\n     1  2 3 class.error\n1 1581  0 0   0.0000000\n2   25 69 0   0.2659574\n3   42  0 0   1.0000000\n\n# 변수 중요도 시각화\nvarImpPlot(rf_model, main = \"변수 중요도\", n.var = 20)\n\n\n\n\n\n\n\n# 상위 중요 변수 추출\nimportance_df &lt;- as.data.frame(importance(rf_model))\nimportance_df$Variable &lt;- rownames(importance_df)\nimportance_df &lt;- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]\nkable(head(importance_df, 10), caption = \"상위 10개 중요 변수\")\n\n\n상위 10개 중요 변수\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\nMeanDecreaseAccuracy\nMeanDecreaseGini\nVariable\n\n\n\n\nacademic_stress_w6\n48.688136\n54.4391516\n11.9711933\n52.349626\n84.352094\nacademic_stress_w6\n\n\nhigher_school_dependence_w5\n2.867711\n3.0454153\n0.3523582\n4.026224\n5.315765\nhigher_school_dependence_w5\n\n\ndeviant_esteem_w3\n6.786002\n1.8135155\n1.7218146\n7.085437\n5.260721\ndeviant_esteem_w3\n\n\ndeviant_esteem_w5\n5.199977\n-0.7217035\n1.5585534\n4.810958\n4.509423\ndeviant_esteem_w5\n\n\nhigher_school_dependence_w6\n1.759168\n3.1717633\n3.3209804\n3.691522\n4.406056\nhigher_school_dependence_w6\n\n\nhigher_school_dependence_w4\n3.785954\n1.5526308\n-0.6850892\n3.802252\n4.244204\nhigher_school_dependence_w4\n\n\nparent_stress_w6\n7.260727\n-0.5756950\n-0.4532738\n7.063251\n3.929955\nparent_stress_w6\n\n\nacademic_stress_w4\n5.791030\n-0.2609228\n1.5789768\n5.327081\n3.741553\nacademic_stress_w4\n\n\ndeviant_esteem_w1\n5.403712\n-0.2976806\n0.7909437\n5.151211\n3.583555\ndeviant_esteem_w1\n\n\nacademic_stress_w3\n6.735353\n-0.9431100\n-1.1050319\n5.931922\n3.570494\nacademic_stress_w3",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#모델-평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#모델-평가",
    "title": "test",
    "section": "모델 평가",
    "text": "모델 평가\n\n# 테스트 데이터에 대한 예측\npredictions &lt;- predict(rf_model, newdata = test_data[, feature_vars])\n\n# 혼동 행렬 계산\nconf_matrix &lt;- confusionMatrix(predictions, test_data$status_code)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   1   2   3\n         1 677  11  17\n         2   0  28   1\n         3   0   0   0\n\nOverall Statistics\n                                          \n               Accuracy : 0.9605          \n                 95% CI : (0.9437, 0.9734)\n    No Information Rate : 0.9223          \n    P-Value [Acc &gt; NIR] : 1.811e-05       \n                                          \n                  Kappa : 0.6472          \n                                          \n Mcnemar's Test P-Value : 2.239e-06       \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            1.0000  0.71795  0.00000\nSpecificity            0.5088  0.99856  1.00000\nPos Pred Value         0.9603  0.96552      NaN\nNeg Pred Value         1.0000  0.98440  0.97548\nPrevalence             0.9223  0.05313  0.02452\nDetection Rate         0.9223  0.03815  0.00000\nDetection Prevalence   0.9605  0.03951  0.00000\nBalanced Accuracy      0.7544  0.85825  0.50000\n\n# 성능 지표 계산\naccuracy &lt;- conf_matrix$overall[\"Accuracy\"]\ncat(\"정확도:\", round(accuracy * 100, 2), \"%\\n\")\n\n정확도: 96.05 %\n\n# 클래스별 성능 지표\nclass_metrics &lt;- conf_matrix$byClass\nprint(kable(class_metrics, caption = \"클래스별 성능 지표\"))\n\n\n\nTable: 클래스별 성능 지표\n\n|         | Sensitivity| Specificity| Pos Pred Value| Neg Pred Value| Precision|    Recall|        F1| Prevalence| Detection Rate| Detection Prevalence| Balanced Accuracy|\n|:--------|-----------:|-----------:|--------------:|--------------:|---------:|---------:|---------:|----------:|--------------:|--------------------:|-----------------:|\n|Class: 1 |   1.0000000|   0.5087719|      0.9602837|      1.0000000| 0.9602837| 1.0000000| 0.9797395|  0.9223433|      0.9223433|            0.9604905|         0.7543860|\n|Class: 2 |   0.7179487|   0.9985612|      0.9655172|      0.9843972| 0.9655172| 0.7179487| 0.8235294|  0.0531335|      0.0381471|            0.0395095|         0.8582549|\n|Class: 3 |   0.0000000|   1.0000000|            NaN|      0.9754768|        NA| 0.0000000|        NA|  0.0245232|      0.0000000|            0.0000000|         0.5000000|\n\n# 다중 클래스 ROC 곡선 (One-vs-Rest 방식)\n# 예측 확률 추출\npred_probs &lt;- predict(rf_model, newdata = test_data[, feature_vars], type = \"prob\")\n\n# 클래스별 ROC 곡선 계산 및 시각화\npar(mfrow = c(2, 2))  # 2x2 그래프 레이아웃\nauc_values &lt;- c()\n\nfor(i in 1:length(levels(test_data$status_code))) {\n  class_label &lt;- levels(test_data$status_code)[i]\n  \n  # 이진 분류 문제로 변환 (현재 클래스 vs 나머지)\n  binary_actual &lt;- ifelse(test_data$status_code == class_label, 1, 0)\n  \n  # ROC 곡선 계산\n  roc_obj &lt;- roc(binary_actual, pred_probs[, i])\n  auc_values[i] &lt;- auc(roc_obj)\n  \n  # ROC 곡선 그리기\n  plot(roc_obj, main = paste(\"ROC 곡선 - 클래스\", class_label),\n       col = \"blue\", lwd = 2)\n  abline(a = 0, b = 1, lty = 2, col = \"gray\")\n  legend(\"bottomright\", legend = paste(\"AUC =\", round(auc_values[i], 3)))\n}\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n# 전체 성능 지표 요약\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\ncat(\"\\n클래스별 AUC 값:\\n\")\n\n\n클래스별 AUC 값:\n\nfor(i in 1:length(levels(test_data$status_code))) {\n  class_label &lt;- levels(test_data$status_code)[i]\n  cat(class_label, \":\", round(auc_values[i], 3), \"\\n\")\n}\n\n1 : 0.938 \n2 : 0.961 \n3 : 0.852 \n\ncat(\"평균 AUC:\", round(mean(auc_values), 3), \"\\n\")\n\n평균 AUC: 0.917 \n\n# 주요 결과를 테이블로 요약\nresult_summary &lt;- data.frame(\n  Metric = c(\"정확도\", \"가중평균 F1 점수\", paste0(\"클래스 \", levels(test_data$status_code), \" AUC\")),\n  Value = c(round(accuracy, 3), round(mean(class_metrics[, \"F1\"], na.rm = TRUE), 3), round(auc_values, 3))\n)\nkable(result_summary, caption = \"모델 성능 요약\")\n\n\n모델 성능 요약\n\n\nMetric\nValue\n\n\n\n\n정확도\n0.960\n\n\n가중평균 F1 점수\n0.902\n\n\n클래스 1 AUC\n0.938\n\n\n클래스 2 AUC\n0.961\n\n\n클래스 3 AUC\n0.852\n\n\n\n\n# feature_vars 별로 상자 그림 그리기\nimportant_vars &lt;- head(importance_df$Variable, 5)  # 상위 5개 중요 변수만 선택\npar(mfrow = c(3, 2))\nfor(var in important_vars) {\n  boxplot(train_data[[var]] ~ train_data$status_code, \n          main = paste(\"상태별\", var),\n          xlab = \"상태 코드\", \n          ylab = var,\n          col = rainbow(length(levels(train_data$status_code))))\n}\n\n# 원래 레이아웃으로 복원\npar(mfrow = c(1, 1))",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest",
    "title": "test",
    "section": "Random Forest",
    "text": "Random Forest",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#평가",
    "title": "test",
    "section": "평가",
    "text": "평가",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#가중치-및-층화-정보-추가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#가중치-및-층화-정보-추가",
    "title": "preprocessing",
    "section": "가중치 및 층화 정보 추가",
    "text": "가중치 및 층화 정보 추가\n\nweight_strata_df &lt;- df6_origin[, c(\"id\", \"wt2\", \"area\")]\nmerged_df &lt;- merge(merged_df, weight_strata_df, by = \"id\", all.x = TRUE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "title": "preprocessing",
    "section": "데이터 전처리 및 train test split",
    "text": "데이터 전처리 및 train test split\n\npred_vars &lt;- names(merged_df)[grep(\"_w[1-5]$\", names(merged_df))]\nmodel_df &lt;- merged_df %&gt;%\n  select(!!sym(ID_VAR), !!sym(OUTCOME_VAR), !!sym(WEIGHT_VAR), all_of(pred_vars))\nmodel_df &lt;- model_df %&gt;%\n  filter(complete.cases(.))\nmodel_df[[OUTCOME_VAR]] &lt;- factor(model_df[[OUTCOME_VAR]])\nlevels(model_df[[OUTCOME_VAR]]) &lt;- make.names(levels(model_df[[OUTCOME_VAR]]))\n\nX &lt;- model_df[, pred_vars]\ny &lt;- model_df[[OUTCOME_VAR]]\nweights &lt;- model_df[[WEIGHT_VAR]]\n\nset.seed(1234)\ntrain_index &lt;- createDataPartition(y, p = 0.7, list = FALSE, times = 1)\n\nX_train &lt;- X[train_index, , drop = FALSE]\nX_test &lt;- X[-train_index, , drop = FALSE]\ny_train &lt;- y[train_index]\ny_test &lt;- y[-train_index]\nweights_train &lt;- weights[train_index]\nweights_test &lt;- weights[-train_index]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest-모델-학습-및-평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#random-forest-모델-학습-및-평가",
    "title": "analysis",
    "section": "Random Forest 모델 학습 및 평가",
    "text": "Random Forest 모델 학습 및 평가\n\nrf_model &lt;- randomForest(\n  x = X_train,\n  y = y_train,\n  weights = weights_train,\n  ntree = 500,\n  mtry = floor(sqrt(ncol(X_train))),\n  importance = TRUE,\n  sampsize = table(y_train),\n  replace = TRUE\n)\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\ny_pred &lt;- predict(rf_model, X_test)\nactual_classes &lt;- y_test\npredicted_classes &lt;- y_pred\ntest_weights &lt;- weights_test\noutcome_levels &lt;- levels(actual_classes)\nweighted_confusion_matrix &lt;- matrix(0,\n                                   nrow = length(outcome_levels),\n                                   ncol = length(outcome_levels),\n                                   dimnames = list(Actual = outcome_levels, Predicted = outcome_levels))\nfor (i in 1:length(actual_classes)) {\n  actual_cat &lt;- as.character(actual_classes[i])\n  predicted_cat &lt;- as.character(predicted_classes[i])\n  weight &lt;- test_weights[i]\n  weighted_confusion_matrix[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix[actual_cat, predicted_cat] + weight\n}\nprint(round(weighted_confusion_matrix, 2))\n\n               Predicted\nActual          comprehensive   modern\n  comprehensive      39299.10 16593.20\n  modern             16626.57 44867.97\n\ntotal_weighted_sum &lt;- sum(weighted_confusion_matrix)\nweighted_accuracy &lt;- sum(diag(weighted_confusion_matrix)) / total_weighted_sum\ncat(\"\\n=== 가중 성능 지표 ===\\n\")\n\n\n=== 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy, 4), \"\\n\")\n\n가중 정확도: 0.717 \n\nweighted_precision &lt;- numeric(length(outcome_levels))\nweighted_recall &lt;- numeric(length(outcome_levels))\nweighted_f1_score &lt;- numeric(length(outcome_levels))\nnames(weighted_precision) &lt;- names(weighted_recall) &lt;- names(weighted_f1_score) &lt;- outcome_levels\n\nfor (cat in outcome_levels) {\n  TP &lt;- weighted_confusion_matrix[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix[, cat]) - TP # 해당 열의 합 - TP\n  FN &lt;- sum(weighted_confusion_matrix[cat, ]) - TP # 해당 행의 합 - TP\n  weighted_precision[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score[cat] &lt;- ifelse((weighted_precision[cat] + weighted_recall[cat]) == 0, 0,\n                                    2 * (weighted_precision[cat] * weighted_recall[cat]) / (weighted_precision[cat] + weighted_recall[cat]))\n}\ncat(\"\\n범주별 가중 정밀도:\\n\")\n\n\n범주별 가중 정밀도:\n\nprint(round(weighted_precision, 4))\n\ncomprehensive        modern \n       0.7027        0.7300 \n\ncat(\"\\n범주별 가중 재현율:\\n\")\n\n\n범주별 가중 재현율:\n\nprint(round(weighted_recall, 4))\n\ncomprehensive        modern \n       0.7031        0.7296 \n\ncat(\"\\n범주별 가중 F1-score:\\n\")\n\n\n범주별 가중 F1-score:\n\nprint(round(weighted_f1_score, 4))\n\ncomprehensive        modern \n       0.7029        0.7298 \n\ny_pred_prob &lt;- predict(rf_model, X_test, type = \"prob\")\ncat(\"\\n비가중 ROC 곡선 (참고용):\\n\")\n\n\n비가중 ROC 곡선 (참고용):\n\nplot_roc &lt;- function() {\n  num_classes &lt;- length(levels(y_test))\n  if (num_classes &lt;= 4 && num_classes &gt; 0) { # 범주가 1개 이하이면 플롯 불가능\n    par(mfrow = c(2, ceiling(num_classes/2)))\n  } else if (num_classes &gt; 4) {\n    par(mfrow = c(2, 2)) # 범주가 많으면 일부만 표시하거나 레이아웃 조정 필요\n    warning(\"범주가 4개 이상입니다. 일부 ROC 곡선만 표시될 수 있습니다. 레이아웃을 조정하거나 플롯 코드 를 수정하세요.\")\n  } else {\n    cat(\"ROC 곡선을 그릴 범주가 부족합니다.\\n\")\n    return(numeric(0)) # 빈 numeric 반환\n  }\n  auc_values &lt;- numeric(num_classes)\n  names(auc_values) &lt;- levels(y_test)\n  for (i in 1:num_classes) {\n    class_label &lt;- levels(y_test)[i]\n    if(class_label %in% unique(y_test) && class_label %in% colnames(y_pred_prob)) {\n      roc_obj &lt;- roc(response = ifelse(y_test == class_label, 1, 0), predictor = y_pred_prob[, class_label])\n      auc_values[i] &lt;- auc(roc_obj)\n      plot(roc_obj, main = paste(\"ROC for\", class_label, \"(Unweighted)\"), col = \"blue\", lwd = 2)\n      abline(a = 0, b = 1, lty = 2, col = \"gray\")\n      text(0.5, 0.3, paste(\"AUC =\", round(auc_values[i], 3)), col = \"red\")\n      } else {\n        cat(\"클래스\", class_label, \"에 대한 ROC 곡선을 그릴 수 없습니다 (데이터 부족).\\n\")\n        auc_values[i] &lt;- NA # AUC 값에 NA 할당\n        plot.new() # 빈 플롯 생성\n        text(0.5, 0.5, paste(\"No ROC for\", class_label))\n      }\n  }\n  par(mfrow = c(1, 1))\n  return(auc_values)\n}\nauc_values_unweighted &lt;- plot_roc()\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\ncat(\"\\n각 클래스별 비가중 AUC:\\n\")\n\n\n각 클래스별 비가중 AUC:\n\nprint(data.frame(Class = levels(y_test), AUC_Unweighted = auc_values_unweighted))\n\n                      Class AUC_Unweighted\ncomprehensive comprehensive      0.7502979\nmodern               modern      0.7502979",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#모델-튜닝",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#모델-튜닝",
    "title": "analysis",
    "section": "모델 튜닝",
    "text": "모델 튜닝\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  savePredictions = \"final\",\n  classProbs = TRUE,\n  summaryFunction = multiClassSummary\n)\nparam_grid &lt;- expand.grid(\n  mtry = floor(sqrt(ncol(X_train)))\n)\nif (ncol(X_train) == 1) {\n  param_grid &lt;- expand.grid(mtry = 1)\n}\nrf_tuned &lt;- train(\n  x = X_train,\n  y = y_train,\n  method = \"rf\",\n  metric = \"Accuracy\",\n  weights = weights_train,\n  trControl = ctrl,\n  tuneGrid = param_grid,\n  importance = TRUE,\n  ntree = 500\n)\nprint(rf_tuned)\n\nRandom Forest \n\n1167 samples\n  51 predictor\n   2 classes: 'comprehensive', 'modern' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 934, 933, 934, 933, 934 \nResampling results:\n\n  logLoss    AUC        prAUC      Accuracy  Kappa      F1        Sensitivity\n  0.6417572  0.6903978  0.6498775  0.666641  0.3308735  0.645206  0.6388862  \n  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision  Recall   \n  0.69167      0.6517788       0.6796336       0.6517788  0.6388862\n  Detection_Rate  Balanced_Accuracy\n  0.3033344       0.6652781        \n\nTuning parameter 'mtry' was held constant at a value of 7\n\ny_pred_tuned &lt;- predict(rf_tuned, X_test)\nactual_classes_tuned &lt;- y_test\npredicted_classes_tuned &lt;- y_pred_tuned\ntest_weights_tuned &lt;- weights_test\nweighted_confusion_matrix_tuned &lt;- matrix(0,\n                                         nrow = length(levels(actual_classes_tuned)),\n                                         ncol = length(levels(actual_classes_tuned)),\n                                         dimnames = list(Actual = levels(actual_classes_tuned), Predicted = levels(actual_classes_tuned)))\n\nfor (i in 1:length(actual_classes_tuned)) {\n  actual_cat &lt;- as.character(actual_classes_tuned[i])\n  predicted_cat &lt;- as.character(predicted_classes_tuned[i])\n  weight &lt;- test_weights_tuned[i]\n   if (actual_cat %in% levels(actual_classes_tuned) && predicted_cat %in% levels(actual_classes_tuned)) {\n      weighted_confusion_matrix_tuned[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_tuned[actual_cat, predicted_cat] + weight\n  } else {\n      warning(paste(\"튜닝 모델: 유효하지 않은 범주 발견: 실제 =\", actual_cat, \", 예측 =\", predicted_cat, \"인 개체 (인덱스:\", i, \")\"))\n  }\n}\ncat(\"\\n=== 튜닝된 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== 튜닝된 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_tuned, 2)) # 보기 좋게 반올림하여 출력\n\n               Predicted\nActual          comprehensive   modern\n  comprehensive      39478.52 16413.77\n  modern             15729.27 45765.28\n\ntotal_weighted_sum_tuned &lt;- sum(weighted_confusion_matrix_tuned)\nweighted_accuracy_tuned &lt;- sum(diag(weighted_confusion_matrix_tuned)) / total_weighted_sum_tuned\ncat(\"\\n=== 튜닝된 모델 가중 성능 지표 ===\\n\")\n\n\n=== 튜닝된 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_tuned, 4), \"\\n\")\n\n가중 정확도: 0.7262 \n\nweighted_precision_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nweighted_recall_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nweighted_f1_score_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nnames(weighted_precision_tuned) &lt;- names(weighted_recall_tuned) &lt;- names(weighted_f1_score_tuned) &lt;- levels(actual_classes_tuned)\nfor (cat in levels(actual_classes_tuned)) {\n  TP &lt;- weighted_confusion_matrix_tuned[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_tuned[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_tuned[cat, ]) - TP\n\n  weighted_precision_tuned[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_tuned[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_tuned[cat] &lt;- ifelse((weighted_precision_tuned[cat] + weighted_recall_tuned[cat]) == 0, 0,\n                                    2 * (weighted_precision_tuned[cat] * weighted_recall_tuned[cat]) / (weighted_precision_tuned[cat] + weighted_recall_tuned[cat]))\n}\n\ncat(\"\\n튜닝된 모델 범주별 가중 정밀도:\\n\")\n\n\n튜닝된 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_tuned, 4))\n\ncomprehensive        modern \n       0.7151        0.7360 \n\ncat(\"\\n튜닝된 모델 범주별 가중 재현율:\\n\")\n\n\n튜닝된 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_tuned, 4))\n\ncomprehensive        modern \n       0.7063        0.7442 \n\ncat(\"\\n튜닝된 모델 범주별 가중 F1-score:\\n\")\n\n\n튜닝된 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_tuned, 4))\n\ncomprehensive        modern \n       0.7107        0.7401 \n\nimportance_obj_tuned &lt;- varImp(rf_tuned)\n\nif(\"importance\" %in% names(importance_obj_tuned)) {\n  imp_df_tuned &lt;- importance_obj_tuned$importance\n} else {\n  imp_df_tuned &lt;- importance_obj_tuned\n}\n\nvar_names_tuned &lt;- rownames(imp_df_tuned)\nif(\"Overall\" %in% colnames(imp_df_tuned)) {\n  var_importance_tuned &lt;- imp_df_tuned$Overall\n} else {\n  var_importance_tuned &lt;- imp_df_tuned[,1]\n}\n\nimportance_df_tuned &lt;- data.frame(\n  Variable = var_names_tuned,\n  Importance = var_importance_tuned\n)\nimportance_df_tuned &lt;- importance_df_tuned[order(importance_df_tuned$Importance, decreasing = TRUE),]\n\n# 상위 10개 변수 출력 (튜닝된 모델)\ntop_vars_tuned &lt;- head(importance_df_tuned$Variable, 10)\ncat(\"\\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\\n\")\n\n\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\n\nprint(top_vars_tuned)\n\n [1] \"deviant_esteem_w3\"    \"neg_esteem_w3\"        \"neg_esteem_w4\"       \n [4] \"desire_stress_w5\"     \"desire_stress_w3\"     \"parent_attachment_w2\"\n [7] \"deviant_esteem_w5\"    \"neg_esteem_w5\"        \"academic_stress_w1\"  \n[10] \"desire_stress_w2\"",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#최종-평가-및-결론",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#최종-평가-및-결론",
    "title": "test",
    "section": "최종 평가 및 결론",
    "text": "최종 평가 및 결론\n\n# # 최종 모델 성능 요약 비교\n# # 원 모델과 튜닝 모델의 가중 정확도 비교\n# final_metrics &lt;- data.frame(\n#   Metric = c(\"가중 정확도\"), # 비가중 정확도 대신 가중 정확도만 비교\n#   Original_Model = c(round(weighted_accuracy * 100, 2)),\n#   Tuned_Model = c(round(weighted_accuracy_tuned * 100, 2))\n# )\n#\n# # 결과 출력\n# cat(\"\\n=== 최종 모델 가중 정확도 비교 ===\\n\")\n# print(kable(final_metrics, caption = \"원본 모델 vs 튜닝 모델 가중 정확도 비교\"))\n#\n# # 클래스별 가중 성능 지표 (튜닝된 모델 결과 사용 권장)\n# cat(\"\\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\\n\")\n# class_weighted_metrics_tuned &lt;- data.frame(\n#   Class = levels(y_test), # y_test 또는 actual_classes_tuned 사용 가능\n#   Weighted_Precision = round(weighted_precision_tuned, 4),\n#   Weighted_Recall = round(weighted_recall_tuned, 4),\n#   Weighted_F1 = round(weighted_f1_score_tuned, 4)\n# )\n# print(kable(class_weighted_metrics_tuned, caption = \"튜닝된 모델 클래스별 가중 성능 지표\"))\n#\n#\n# # 최종 변수 중요도 요약 (튜닝된 모델 결과 사용)\n# cat(\"\\n=== 튜닝된 모델 변수 중요도 ===\\n\")\n# print(kable(head(importance_df_tuned, 10), caption = \"튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\"))\n#\n#\n# # 결론 요약\n# cat(\"\\n=== 결론 요약 ===\\n\")\n# cat(\"1. 위 모델은 학생들의 진로 상태(\", OUTCOME_VAR, \")를 예측하기 위해 이전 시점의 요인 점수들을 독립변수로 활용했습니다.\\n\",\n#   \"2. 6차년도 종단면 가중치(\", WEIGHT_VAR, \")를 모델 학습에 반영했으며, 테스트 데이터에서 가중치 고려 성능 평가를 수행했습니다.\\n\",\n#   \"3. 랜덤 포레스트 모델의 원래 버전과 하이퍼파라미터 튜닝 버전 모두 학습 및 가중 평가했습니다.\\n\",\n#   \"4. 최종 튜닝된 모델의 테스트 데이터에서의 **가중 정확도**는 약 \", round(weighted_accuracy_tuned * 100, 2), \"%입니다.\\n\",\n#   \"5. 각 진로 상태 범주별 모델의 가중 정밀도, 가중 재현율, 가중 F1-score는 위의 '튜닝된 모델 범주별 가중 성능 지표' 표를 참고하십시오.\\n\",\n#   \"6. 예측에 가장 중요한 변수는 위의 '튜닝된 모델 상위 10개 중요 변수' 표를 참고하십시오.\\n\",\n#   \"7. 이 결과는 6차년도 종단면 가중치를 통해 원 모집단(1차 조사 시점의 중2 청소년들)에 대해 일정 수준 일반화하여 해석할 수 있습니다.\\n\")",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#기본-인구통계학적-특성에-대한-도수분포표",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#기본-인구통계학적-특성에-대한-도수분포표",
    "title": "test",
    "section": "기본 인구통계학적 특성에 대한 도수분포표",
    "text": "기본 인구통계학적 특성에 대한 도수분포표\n\n# df6_origin에서 태어난 년도(yy), 성별(sex), q11 칼럼 추출\nif(all(c(\"id\", \"yy\", \"sex\", \"q11\") %in% names(df6_origin))) {\n  # 필요한 칼럼만 추출\n  demo_df &lt;- df6_origin[, c(\"id\", \"yy\", \"sex\", \"q11\")]\n  \n  # q11 칼럼이 이미 merged_df에 있는 경우 삭제 (중복 방지)\n  if(\"q11\" %in% names(merged_df)) {\n    merged_df$q11 &lt;- NULL\n  }\n  \n  # 이미 데이터가 있는 경우를 체크\n  if(any(c(\"yy\", \"sex\") %in% names(merged_df))) {\n    # 선택적으로 필요한 변수만 추출\n    cols_to_add &lt;- setdiff(c(\"yy\", \"sex\", \"q11\"), names(merged_df))\n    if(length(cols_to_add) &gt; 0) {\n      # 필요한 칼럼만 포함하는 새 데이터프레임 생성\n      demo_df_filtered &lt;- demo_df[, c(\"id\", cols_to_add)]\n      # merged_df와 병합\n      merged_df &lt;- merge(merged_df, demo_df_filtered, by = \"id\", all.x = TRUE)\n    }\n  } else {\n    # 모든 인구통계학 변수 추가\n    merged_df &lt;- merge(merged_df, demo_df, by = \"id\", all.x = TRUE)\n  }\n  merged_df &lt;- na.omit(merged_df)\n  \n  # 태어난 년도(yy)에 대한 도수분포표\n  yy_table &lt;- table(merged_df$yy, useNA = \"ifany\")\n  yy_prop &lt;- prop.table(yy_table) * 100\n  \n  # 성별(sex)에 대한 도수분포표\n  sex_table &lt;- table(merged_df$sex, useNA = \"ifany\")\n  sex_prop &lt;- prop.table(sex_table) * 100\n  \n  # q11에 대한 도수분포표 (merged_df에 q11이 있는지 확인)\n  if(\"q11\" %in% names(merged_df)) {\n    q11_table &lt;- table(merged_df$q11, useNA = \"ifany\")\n    q11_prop &lt;- prop.table(q11_table) * 100\n  } else {\n    # df6_origin에서 직접 q11 도수분포표 생성\n    q11_table &lt;- table(df6_origin$q11, useNA = \"ifany\")\n    q11_prop &lt;- prop.table(q11_table) * 100\n  }\n  \n  # 도수분포표 출력\n  cat(\"## 태어난 년도(yy) 도수분포표\\n\")\n  print(kable(data.frame(\n    출생년도 = names(yy_table),\n    빈도 = as.vector(yy_table),\n    비율 = round(as.vector(yy_prop), 2)\n  )))\n  \n  cat(\"\\n## 성별(sex) 도수분포표\\n\")\n  print(kable(data.frame(\n    성별코드 = names(sex_table),\n    빈도 = as.vector(sex_table),\n    비율 = round(as.vector(sex_prop), 2)\n  )))\n  \n  # 성별 레이블 추가\n  sex_labels &lt;- data.frame(\n    성별코드 = 1:2,\n    성별 = c(\"남자\", \"여자\")\n  )\n  \n  sex_freq_with_labels &lt;- merge(\n    data.frame(성별코드 = names(sex_table), 빈도 = as.vector(sex_table), 비율 = round(as.vector(sex_prop), 2)),\n    sex_labels, \n    by = \"성별코드\", \n    all.x = TRUE\n  )\n  \n  cat(\"\\n## 성별(sex) 도수분포표 (레이블 포함)\\n\")\n  print(kable(sex_freq_with_labels[, c(\"성별코드\", \"성별\", \"빈도\", \"비율\")]))\n  \n  cat(\"\\n## q11 응답 도수분포표\\n\")\n  print(kable(data.frame(\n    응답값 = names(q11_table),\n    빈도 = as.vector(q11_table),\n    비율 = round(as.vector(q11_prop), 2)\n  )))\n  \n  # q11 응답에 대한 레이블 설명 추가\n  q11_labels &lt;- data.frame(\n    응답값 = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"71\", \"81\", \"91\"),\n    응답내용 = c(\n      \"고등학교 재학\", \"고등학교 중퇴\", \"고등학교 졸업, 미진학\", \n      \"대학교 재학\", \"대학교 중퇴\", \"대학교 졸업\", \n      \"취업\", \"가사\", \"무직\", \n      \"취업+진학\", \"가사+진학\", \"무직+진학\"\n    )\n  )\n  \n  q11_freq_with_labels &lt;- merge(\n    data.frame(응답값 = names(q11_table), 빈도 = as.vector(q11_table), 비율 = round(as.vector(q11_prop), 2)),\n    q11_labels, \n    by = \"응답값\", \n    all.x = TRUE\n  )\n  \n  cat(\"\\n## q11 응답 도수분포표 (레이블 포함)\\n\")\n  print(kable(q11_freq_with_labels[, c(\"응답값\", \"응답내용\", \"빈도\", \"비율\")]))\n  \n} else {\n  warning(\"df6_origin에 id, yy, sex 또는 q11 칼럼 중 일부가 없습니다.\")\n}\n\n## 태어난 년도(yy) 도수분포표\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n|출생년도 | 빈도|  비율|\n|:--------|----:|-----:|\n|88       |    2|  0.08|\n|89       | 1895| 77.32|\n|90       |  554| 22.60|\n\n## 성별(sex) 도수분포표\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n|성별코드 | 빈도|  비율|\n|:--------|----:|-----:|\n|1        | 1165| 47.53|\n|2        | 1286| 52.47|\n\n## 성별(sex) 도수분포표 (레이블 포함)\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n|성별코드 |성별 | 빈도|  비율|\n|:--------|:----|----:|-----:|\n|1        |남자 | 1165| 47.53|\n|2        |여자 | 1286| 52.47|\n\n## q11 응답 도수분포표\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n|응답값 | 빈도|  비율|\n|:------|----:|-----:|\n|1      | 1236| 50.43|\n|2      |  475| 19.38|\n|3      |  222|  9.06|\n|4      |   13|  0.53|\n|5      |   39|  1.59|\n|6      |  273| 11.14|\n|7      |   95|  3.88|\n|8      |    1|  0.04|\n|9      |    1|  0.04|\n|10     |   23|  0.94|\n|11     |    1|  0.04|\n|13     |   30|  1.22|\n|14     |   16|  0.65|\n|15     |    8|  0.33|\n|19     |    1|  0.04|\n|21     |    1|  0.04|\n|25     |    1|  0.04|\n|31     |    2|  0.08|\n|33     |    1|  0.04|\n|71     |    4|  0.16|\n|81     |    1|  0.04|\n|101    |    6|  0.24|\n|111    |    1|  0.04|\n\n## q11 응답 도수분포표 (레이블 포함)\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n|응답값 |응답내용              | 빈도|  비율|\n|:------|:---------------------|----:|-----:|\n|1      |고등학교 재학         | 1236| 50.43|\n|10     |NA                    |   23|  0.94|\n|101    |NA                    |    6|  0.24|\n|11     |NA                    |    1|  0.04|\n|111    |NA                    |    1|  0.04|\n|13     |NA                    |   30|  1.22|\n|14     |NA                    |   16|  0.65|\n|15     |NA                    |    8|  0.33|\n|19     |NA                    |    1|  0.04|\n|2      |고등학교 중퇴         |  475| 19.38|\n|21     |NA                    |    1|  0.04|\n|25     |NA                    |    1|  0.04|\n|3      |고등학교 졸업, 미진학 |  222|  9.06|\n|31     |NA                    |    2|  0.08|\n|33     |NA                    |    1|  0.04|\n|4      |대학교 재학           |   13|  0.53|\n|5      |대학교 중퇴           |   39|  1.59|\n|6      |대학교 졸업           |  273| 11.14|\n|7      |취업                  |   95|  3.88|\n|71     |취업+진학             |    4|  0.16|\n|8      |가사                  |    1|  0.04|\n|81     |가사+진학             |    1|  0.04|\n|9      |무직                  |    1|  0.04|",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#도수분포표-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#도수분포표-시각화",
    "title": "test",
    "section": "도수분포표 시각화",
    "text": "도수분포표 시각화\n\n# ggplot2 라이브러리 로드\nlibrary(ggplot2)\n\n# 1. 태어난 년도(yy) 막대 그래프\nif(exists(\"yy_table\")) {\n  yy_df &lt;- data.frame(\n    출생년도 = names(yy_table),\n    빈도 = as.vector(yy_table)\n  )\n  \n  # NA 값이 있으면 제거하거나 \"결측\" 등의 레이블로 변경\n  yy_df$출생년도[is.na(yy_df$출생년도)] &lt;- \"결측\"\n  \n  ggplot(yy_df, aes(x = 출생년도, y = 빈도)) +\n    geom_bar(stat = \"identity\", fill = \"skyblue\") +\n    theme_minimal() +\n    labs(title = \"출생년도별 분포\",\n         x = \"출생년도\",\n         y = \"빈도수\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}\n\n\n\n\n\n\n\n# 2. 성별(sex) 막대 그래프\nif(exists(\"sex_freq_with_labels\")) {\n  # NA 값 처리\n  sex_freq_with_labels$성별[is.na(sex_freq_with_labels$성별)] &lt;- \"결측\"\n  \n  ggplot(sex_freq_with_labels, aes(x = 성별, y = 빈도)) +\n    geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n    theme_minimal() +\n    labs(title = \"성별 분포\",\n         x = \"성별\",\n         y = \"빈도수\") +\n    geom_text(aes(label = 빈도), vjust = -0.3)\n}",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#독립변수간-상관관계-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#독립변수간-상관관계-분석",
    "title": "test",
    "section": "독립변수간 상관관계 분석",
    "text": "독립변수간 상관관계 분석\n\n# 필요한 패키지 로드\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n# 상관관계 분석을 위한 독립변수 선택 (요인 점수만 선택)\nfactor_vars &lt;- names(merged_df)[grep(\"_w[1-6]$\", names(merged_df))]\ncorrelation_df &lt;- merged_df[, factor_vars]\n\n# 결측치가 있는 행은 제외\ncorrelation_df &lt;- correlation_df[complete.cases(correlation_df), ]\n\n# 상관행렬 계산\ncorr_matrix &lt;- cor(correlation_df, use = \"pairwise.complete.obs\")\n\n# 상관행렬 확인 (크기가 크므로 첫 10x10만 출력)\ncat(\"상관행렬 (처음 10x10):\\n\")\n\n상관행렬 (처음 10x10):\n\nprint(round(corr_matrix[1:10, 1:10], 2))\n\n                            parent_attachment_w1 deviant_esteem_w1\nparent_attachment_w1                        1.00             -0.27\ndeviant_esteem_w1                          -0.27              1.00\nparent_stress_w1                           -0.49              0.31\nparent_monitoring_w1                        0.66             -0.26\ndesire_stress_w1                           -0.35              0.36\nfriend_stress_w1                           -0.14              0.26\nself_confidence_w1                          0.29             -0.18\nhigher_school_dependence_w1                 0.16             -0.08\nneg_esteem_w1                              -0.29              0.52\nacademic_stress_w1                         -0.26              0.20\n                            parent_stress_w1 parent_monitoring_w1\nparent_attachment_w1                   -0.49                 0.66\ndeviant_esteem_w1                       0.31                -0.26\nparent_stress_w1                        1.00                -0.26\nparent_monitoring_w1                   -0.26                 1.00\ndesire_stress_w1                        0.53                -0.28\nfriend_stress_w1                        0.26                -0.12\nself_confidence_w1                     -0.08                 0.26\nhigher_school_dependence_w1             0.08                 0.17\nneg_esteem_w1                           0.42                -0.23\nacademic_stress_w1                      0.72                -0.16\n                            desire_stress_w1 friend_stress_w1\nparent_attachment_w1                   -0.35            -0.14\ndeviant_esteem_w1                       0.36             0.26\nparent_stress_w1                        0.53             0.26\nparent_monitoring_w1                   -0.28            -0.12\ndesire_stress_w1                        1.00             0.38\nfriend_stress_w1                        0.38             1.00\nself_confidence_w1                     -0.18            -0.18\nhigher_school_dependence_w1             0.04             0.04\nneg_esteem_w1                           0.45             0.41\nacademic_stress_w1                      0.56             0.38\n                            self_confidence_w1 higher_school_dependence_w1\nparent_attachment_w1                      0.29                        0.16\ndeviant_esteem_w1                        -0.18                       -0.08\nparent_stress_w1                         -0.08                        0.08\nparent_monitoring_w1                      0.26                        0.17\ndesire_stress_w1                         -0.18                        0.04\nfriend_stress_w1                         -0.18                        0.04\nself_confidence_w1                        1.00                        0.23\nhigher_school_dependence_w1               0.23                        1.00\nneg_esteem_w1                            -0.26                        0.01\nacademic_stress_w1                       -0.12                        0.11\n                            neg_esteem_w1 academic_stress_w1\nparent_attachment_w1                -0.29              -0.26\ndeviant_esteem_w1                    0.52               0.20\nparent_stress_w1                     0.42               0.72\nparent_monitoring_w1                -0.23              -0.16\ndesire_stress_w1                     0.45               0.56\nfriend_stress_w1                     0.41               0.38\nself_confidence_w1                  -0.26              -0.12\nhigher_school_dependence_w1          0.01               0.11\nneg_esteem_w1                        1.00               0.55\nacademic_stress_w1                   0.55               1.00\n\n# kable로 더 보기 좋게 출력\nkable(round(corr_matrix, 2), caption = \"요인 점수 간 상관관계\")\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n요인 점수 간 상관관계\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparent_attachment_w1\ndeviant_esteem_w1\nparent_stress_w1\nparent_monitoring_w1\ndesire_stress_w1\nfriend_stress_w1\nself_confidence_w1\nhigher_school_dependence_w1\nneg_esteem_w1\nacademic_stress_w1\nparent_attachment_w2\ndeviant_esteem_w2\nparent_stress_w2\nparent_monitoring_w2\ndesire_stress_w2\nfriend_stress_w2\nself_confidence_w2\nhigher_school_dependence_w2\nneg_esteem_w2\nacademic_stress_w2\nparent_attachment_w3\ndeviant_esteem_w3\nparent_stress_w3\nparent_monitoring_w3\ndesire_stress_w3\nfriend_stress_w3\nself_confidence_w3\nhigher_school_dependence_w3\nneg_esteem_w3\nacademic_stress_w3\nparent_attachment_w4\ndeviant_esteem_w4\nparent_stress_w4\nparent_monitoring_w4\ndesire_stress_w4\nfriend_stress_w4\nself_confidence_w4\nhigher_school_dependence_w4\nneg_esteem_w4\nacademic_stress_w4\nparent_attachment_w5\ndeviant_esteem_w5\nparent_stress_w5\nparent_monitoring_w5\ndesire_stress_w5\nfriend_stress_w5\nself_confidence_w5\nhigher_school_dependence_w5\nneg_esteem_w5\nacademic_stress_w5\nparent_attachment_w6\ndeviant_esteem_w6\nparent_stress_w6\nparent_monitoring_w6\ndesire_stress_w6\nfriend_stress_w6\nself_confidence_w6\nhigher_school_dependence_w6\nneg_esteem_w6\nacademic_stress_w6\n\n\n\n\nparent_attachment_w1\n1.00\n-0.27\n-0.49\n0.66\n-0.35\n-0.14\n0.29\n0.16\n-0.29\n-0.26\n0.59\n-0.20\n-0.29\n0.44\n-0.23\n-0.19\n0.17\n0.11\n-0.18\n-0.19\n0.53\n-0.19\n-0.22\n0.38\n-0.17\n-0.11\n0.19\n0.10\n-0.15\n-0.03\n0.45\n-0.16\n-0.18\n0.35\n-0.18\n-0.11\n0.17\n0.13\n-0.13\n0.00\n0.45\n-0.16\n-0.16\n0.34\n-0.16\n-0.09\n0.16\n0.11\n-0.14\n0.01\n0.41\n-0.18\n-0.17\n0.33\n-0.10\n-0.08\n0.11\n0.04\n-0.15\n-0.06\n\n\ndeviant_esteem_w1\n-0.27\n1.00\n0.31\n-0.26\n0.36\n0.26\n-0.18\n-0.08\n0.52\n0.20\n-0.18\n0.40\n0.18\n-0.19\n0.23\n0.20\n-0.13\n-0.13\n0.22\n0.13\n-0.19\n0.40\n0.12\n-0.19\n0.20\n0.16\n-0.10\n-0.12\n0.19\n-0.01\n-0.13\n0.33\n0.11\n-0.15\n0.15\n0.14\n-0.10\n-0.13\n0.13\n-0.07\n-0.15\n0.33\n0.09\n-0.14\n0.17\n0.12\n-0.09\n-0.16\n0.18\n-0.05\n-0.12\n0.31\n0.10\n-0.11\n0.13\n0.11\n-0.11\n-0.06\n0.18\n0.02\n\n\nparent_stress_w1\n-0.49\n0.31\n1.00\n-0.26\n0.53\n0.26\n-0.08\n0.08\n0.42\n0.72\n-0.28\n0.20\n0.48\n-0.16\n0.32\n0.20\n-0.06\n0.04\n0.22\n0.37\n-0.28\n0.19\n0.40\n-0.18\n0.28\n0.16\n-0.06\n0.04\n0.20\n0.22\n-0.21\n0.17\n0.33\n-0.13\n0.26\n0.15\n-0.06\n0.02\n0.20\n0.20\n-0.21\n0.13\n0.28\n-0.13\n0.22\n0.08\n-0.07\n0.01\n0.19\n0.14\n-0.17\n0.12\n0.27\n-0.11\n0.20\n0.09\n-0.08\n0.03\n0.18\n0.11\n\n\nparent_monitoring_w1\n0.66\n-0.26\n-0.26\n1.00\n-0.28\n-0.12\n0.26\n0.17\n-0.23\n-0.16\n0.42\n-0.17\n-0.16\n0.51\n-0.18\n-0.15\n0.17\n0.14\n-0.15\n-0.12\n0.39\n-0.17\n-0.12\n0.46\n-0.13\n-0.09\n0.15\n0.12\n-0.12\n0.01\n0.31\n-0.15\n-0.09\n0.38\n-0.13\n-0.08\n0.14\n0.15\n-0.08\n0.04\n0.33\n-0.14\n-0.09\n0.39\n-0.14\n-0.08\n0.12\n0.13\n-0.10\n0.03\n0.30\n-0.17\n-0.09\n0.38\n-0.09\n-0.07\n0.09\n0.07\n-0.12\n-0.01\n\n\ndesire_stress_w1\n-0.35\n0.36\n0.53\n-0.28\n1.00\n0.38\n-0.18\n0.04\n0.45\n0.56\n-0.22\n0.22\n0.31\n-0.17\n0.48\n0.25\n-0.08\n-0.01\n0.22\n0.32\n-0.21\n0.18\n0.25\n-0.17\n0.43\n0.17\n-0.10\n-0.01\n0.22\n0.19\n-0.14\n0.13\n0.18\n-0.11\n0.36\n0.14\n-0.09\n-0.01\n0.17\n0.17\n-0.16\n0.14\n0.18\n-0.15\n0.36\n0.11\n-0.08\n-0.01\n0.18\n0.12\n-0.16\n0.11\n0.21\n-0.12\n0.35\n0.16\n-0.11\n0.01\n0.20\n0.07\n\n\nfriend_stress_w1\n-0.14\n0.26\n0.26\n-0.12\n0.38\n1.00\n-0.18\n0.04\n0.41\n0.38\n-0.13\n0.12\n0.18\n-0.12\n0.20\n0.33\n-0.12\n0.04\n0.22\n0.23\n-0.14\n0.13\n0.18\n-0.13\n0.19\n0.29\n-0.14\n0.02\n0.20\n0.15\n-0.09\n0.11\n0.15\n-0.08\n0.17\n0.27\n-0.14\n-0.01\n0.20\n0.13\n-0.12\n0.09\n0.13\n-0.11\n0.17\n0.23\n-0.12\n-0.02\n0.20\n0.11\n-0.12\n0.10\n0.20\n-0.06\n0.20\n0.25\n-0.17\n0.01\n0.22\n0.10\n\n\nself_confidence_w1\n0.29\n-0.18\n-0.08\n0.26\n-0.18\n-0.18\n1.00\n0.23\n-0.26\n-0.12\n0.20\n-0.16\n-0.09\n0.16\n-0.11\n-0.17\n0.44\n0.17\n-0.19\n-0.09\n0.17\n-0.15\n-0.05\n0.15\n-0.12\n-0.13\n0.39\n0.17\n-0.16\n0.00\n0.12\n-0.10\n0.00\n0.11\n-0.11\n-0.08\n0.37\n0.15\n-0.15\n0.00\n0.12\n-0.13\n-0.01\n0.09\n-0.13\n-0.09\n0.37\n0.16\n-0.17\n0.02\n0.10\n-0.11\n-0.05\n0.08\n-0.10\n-0.10\n0.30\n0.10\n-0.17\n-0.04\n\n\nhigher_school_dependence_w1\n0.16\n-0.08\n0.08\n0.17\n0.04\n0.04\n0.23\n1.00\n0.01\n0.11\n0.11\n-0.09\n0.07\n0.11\n0.02\n0.00\n0.16\n0.38\n-0.04\n0.10\n0.11\n-0.09\n0.10\n0.10\n0.04\n0.03\n0.13\n0.35\n0.00\n0.17\n0.07\n-0.07\n0.11\n0.07\n0.05\n0.05\n0.10\n0.31\n0.05\n0.22\n0.08\n-0.08\n0.12\n0.07\n0.04\n0.04\n0.11\n0.30\n0.00\n0.20\n0.09\n-0.05\n0.09\n0.07\n0.08\n0.07\n0.10\n0.20\n0.01\n0.04\n\n\nneg_esteem_w1\n-0.29\n0.52\n0.42\n-0.23\n0.45\n0.41\n-0.26\n0.01\n1.00\n0.55\n-0.17\n0.23\n0.25\n-0.14\n0.28\n0.22\n-0.18\n-0.01\n0.38\n0.29\n-0.20\n0.22\n0.23\n-0.16\n0.27\n0.22\n-0.17\n0.00\n0.37\n0.20\n-0.12\n0.18\n0.17\n-0.11\n0.24\n0.21\n-0.17\n-0.03\n0.30\n0.15\n-0.13\n0.18\n0.15\n-0.12\n0.25\n0.17\n-0.15\n-0.04\n0.34\n0.12\n-0.14\n0.17\n0.19\n-0.10\n0.23\n0.15\n-0.17\n0.01\n0.32\n0.08\n\n\nacademic_stress_w1\n-0.26\n0.20\n0.72\n-0.16\n0.56\n0.38\n-0.12\n0.11\n0.55\n1.00\n-0.14\n0.13\n0.38\n-0.09\n0.32\n0.20\n-0.09\n0.07\n0.26\n0.45\n-0.17\n0.12\n0.33\n-0.11\n0.30\n0.16\n-0.10\n0.06\n0.25\n0.31\n-0.11\n0.10\n0.26\n-0.06\n0.27\n0.16\n-0.08\n0.03\n0.23\n0.28\n-0.12\n0.08\n0.21\n-0.07\n0.24\n0.08\n-0.08\n0.01\n0.22\n0.19\n-0.09\n0.08\n0.22\n-0.05\n0.23\n0.13\n-0.10\n0.05\n0.22\n0.10\n\n\nparent_attachment_w2\n0.59\n-0.18\n-0.28\n0.42\n-0.22\n-0.13\n0.20\n0.11\n-0.17\n-0.14\n1.00\n-0.25\n-0.43\n0.68\n-0.28\n-0.22\n0.27\n0.15\n-0.21\n-0.23\n0.59\n-0.20\n-0.25\n0.41\n-0.18\n-0.14\n0.21\n0.13\n-0.19\n-0.03\n0.52\n-0.18\n-0.20\n0.37\n-0.18\n-0.12\n0.20\n0.11\n-0.15\n0.01\n0.50\n-0.19\n-0.19\n0.38\n-0.19\n-0.12\n0.18\n0.10\n-0.19\n-0.01\n0.44\n-0.18\n-0.18\n0.33\n-0.13\n-0.09\n0.15\n0.05\n-0.18\n-0.07\n\n\ndeviant_esteem_w2\n-0.20\n0.40\n0.20\n-0.17\n0.22\n0.12\n-0.16\n-0.09\n0.23\n0.13\n-0.25\n1.00\n0.34\n-0.21\n0.38\n0.41\n-0.07\n-0.12\n0.35\n0.24\n-0.19\n0.41\n0.13\n-0.18\n0.19\n0.20\n-0.10\n-0.10\n0.19\n0.02\n-0.14\n0.34\n0.13\n-0.15\n0.16\n0.16\n-0.10\n-0.11\n0.18\n-0.02\n-0.14\n0.35\n0.10\n-0.13\n0.15\n0.14\n-0.09\n-0.09\n0.17\n-0.05\n-0.11\n0.31\n0.09\n-0.10\n0.11\n0.13\n-0.10\n-0.04\n0.17\n-0.01\n\n\nparent_stress_w2\n-0.29\n0.18\n0.48\n-0.16\n0.31\n0.18\n-0.09\n0.07\n0.25\n0.38\n-0.43\n0.34\n1.00\n-0.24\n0.58\n0.47\n-0.04\n0.06\n0.33\n0.77\n-0.30\n0.17\n0.43\n-0.18\n0.30\n0.22\n-0.10\n0.06\n0.22\n0.28\n-0.25\n0.14\n0.37\n-0.16\n0.26\n0.18\n-0.10\n0.06\n0.23\n0.24\n-0.25\n0.15\n0.35\n-0.16\n0.25\n0.17\n-0.08\n0.04\n0.21\n0.20\n-0.20\n0.13\n0.31\n-0.13\n0.21\n0.15\n-0.07\n0.06\n0.19\n0.09\n\n\nparent_monitoring_w2\n0.44\n-0.19\n-0.16\n0.51\n-0.17\n-0.12\n0.16\n0.11\n-0.14\n-0.09\n0.68\n-0.21\n-0.24\n1.00\n-0.21\n-0.17\n0.21\n0.20\n-0.14\n-0.16\n0.43\n-0.17\n-0.13\n0.52\n-0.13\n-0.11\n0.18\n0.14\n-0.12\n0.01\n0.37\n-0.15\n-0.11\n0.44\n-0.16\n-0.10\n0.15\n0.14\n-0.10\n0.04\n0.39\n-0.15\n-0.11\n0.45\n-0.15\n-0.10\n0.13\n0.15\n-0.11\n0.04\n0.33\n-0.16\n-0.10\n0.38\n-0.10\n-0.06\n0.12\n0.07\n-0.13\n-0.01\n\n\ndesire_stress_w2\n-0.23\n0.23\n0.32\n-0.18\n0.48\n0.20\n-0.11\n0.02\n0.28\n0.32\n-0.28\n0.38\n0.58\n-0.21\n1.00\n0.54\n-0.03\n0.01\n0.33\n0.59\n-0.19\n0.19\n0.27\n-0.16\n0.46\n0.23\n-0.11\n-0.01\n0.25\n0.24\n-0.16\n0.16\n0.23\n-0.11\n0.40\n0.19\n-0.12\n0.03\n0.22\n0.20\n-0.16\n0.15\n0.22\n-0.13\n0.37\n0.18\n-0.07\n0.00\n0.19\n0.15\n-0.16\n0.15\n0.22\n-0.14\n0.35\n0.18\n-0.11\n0.02\n0.19\n0.06\n\n\nfriend_stress_w2\n-0.19\n0.20\n0.20\n-0.15\n0.25\n0.33\n-0.17\n0.00\n0.22\n0.20\n-0.22\n0.41\n0.47\n-0.17\n0.54\n1.00\n-0.11\n-0.01\n0.33\n0.48\n-0.17\n0.20\n0.20\n-0.14\n0.22\n0.33\n-0.16\n0.00\n0.22\n0.14\n-0.13\n0.17\n0.20\n-0.11\n0.21\n0.29\n-0.14\n0.00\n0.21\n0.12\n-0.15\n0.17\n0.18\n-0.11\n0.20\n0.28\n-0.14\n0.00\n0.21\n0.12\n-0.13\n0.20\n0.20\n-0.09\n0.20\n0.26\n-0.13\n0.04\n0.21\n0.06\n\n\nself_confidence_w2\n0.17\n-0.13\n-0.06\n0.17\n-0.08\n-0.12\n0.44\n0.16\n-0.18\n-0.09\n0.27\n-0.07\n-0.04\n0.21\n-0.03\n-0.11\n1.00\n0.21\n-0.26\n-0.06\n0.15\n-0.16\n-0.03\n0.13\n-0.08\n-0.13\n0.41\n0.17\n-0.21\n0.00\n0.12\n-0.12\n-0.02\n0.11\n-0.10\n-0.11\n0.36\n0.19\n-0.16\n-0.02\n0.10\n-0.14\n-0.01\n0.07\n-0.10\n-0.13\n0.36\n0.16\n-0.17\n-0.01\n0.09\n-0.13\n-0.06\n0.06\n-0.08\n-0.09\n0.34\n0.07\n-0.17\n-0.04\n\n\nhigher_school_dependence_w2\n0.11\n-0.13\n0.04\n0.14\n-0.01\n0.04\n0.17\n0.38\n-0.01\n0.07\n0.15\n-0.12\n0.06\n0.20\n0.01\n-0.01\n0.21\n1.00\n-0.01\n0.12\n0.11\n-0.10\n0.08\n0.12\n0.01\n0.02\n0.12\n0.43\n-0.02\n0.17\n0.07\n-0.10\n0.09\n0.10\n0.00\n0.02\n0.11\n0.41\n0.04\n0.22\n0.09\n-0.11\n0.11\n0.08\n0.02\n0.04\n0.11\n0.38\n0.00\n0.22\n0.06\n-0.06\n0.08\n0.06\n0.06\n0.04\n0.08\n0.27\n0.01\n0.06\n\n\nneg_esteem_w2\n-0.18\n0.22\n0.22\n-0.15\n0.22\n0.22\n-0.19\n-0.04\n0.38\n0.26\n-0.21\n0.35\n0.33\n-0.14\n0.33\n0.33\n-0.26\n-0.01\n1.00\n0.38\n-0.20\n0.19\n0.17\n-0.16\n0.22\n0.21\n-0.20\n0.00\n0.36\n0.14\n-0.15\n0.19\n0.17\n-0.16\n0.23\n0.21\n-0.20\n-0.04\n0.34\n0.15\n-0.17\n0.18\n0.15\n-0.11\n0.22\n0.19\n-0.18\n-0.05\n0.33\n0.09\n-0.14\n0.17\n0.16\n-0.10\n0.15\n0.16\n-0.15\n-0.02\n0.28\n0.08\n\n\nacademic_stress_w2\n-0.19\n0.13\n0.37\n-0.12\n0.32\n0.23\n-0.09\n0.10\n0.29\n0.45\n-0.23\n0.24\n0.77\n-0.16\n0.59\n0.48\n-0.06\n0.12\n0.38\n1.00\n-0.18\n0.09\n0.34\n-0.12\n0.31\n0.22\n-0.12\n0.09\n0.25\n0.37\n-0.14\n0.09\n0.29\n-0.10\n0.28\n0.21\n-0.13\n0.09\n0.27\n0.35\n-0.16\n0.09\n0.28\n-0.10\n0.27\n0.18\n-0.10\n0.07\n0.23\n0.28\n-0.13\n0.08\n0.25\n-0.09\n0.25\n0.15\n-0.08\n0.08\n0.19\n0.12\n\n\nparent_attachment_w3\n0.53\n-0.19\n-0.28\n0.39\n-0.21\n-0.14\n0.17\n0.11\n-0.20\n-0.17\n0.59\n-0.19\n-0.30\n0.43\n-0.19\n-0.17\n0.15\n0.11\n-0.20\n-0.18\n1.00\n-0.27\n-0.43\n0.66\n-0.26\n-0.21\n0.29\n0.13\n-0.27\n-0.09\n0.61\n-0.22\n-0.28\n0.42\n-0.20\n-0.17\n0.19\n0.13\n-0.18\n-0.01\n0.55\n-0.20\n-0.24\n0.38\n-0.21\n-0.15\n0.18\n0.13\n-0.22\n-0.03\n0.49\n-0.18\n-0.24\n0.37\n-0.14\n-0.12\n0.15\n0.06\n-0.21\n-0.09\n\n\ndeviant_esteem_w3\n-0.19\n0.40\n0.19\n-0.17\n0.18\n0.13\n-0.15\n-0.09\n0.22\n0.12\n-0.20\n0.41\n0.17\n-0.17\n0.19\n0.20\n-0.16\n-0.10\n0.19\n0.09\n-0.27\n1.00\n0.27\n-0.21\n0.30\n0.34\n-0.18\n-0.13\n0.41\n0.06\n-0.18\n0.47\n0.19\n-0.16\n0.19\n0.22\n-0.15\n-0.12\n0.23\n-0.01\n-0.16\n0.42\n0.13\n-0.13\n0.15\n0.16\n-0.12\n-0.12\n0.21\n-0.04\n-0.14\n0.39\n0.12\n-0.13\n0.13\n0.16\n-0.14\n-0.08\n0.21\n-0.01\n\n\nparent_stress_w3\n-0.22\n0.12\n0.40\n-0.12\n0.25\n0.18\n-0.05\n0.10\n0.23\n0.33\n-0.25\n0.13\n0.43\n-0.13\n0.27\n0.20\n-0.03\n0.08\n0.17\n0.34\n-0.43\n0.27\n1.00\n-0.21\n0.50\n0.40\n-0.12\n0.10\n0.32\n0.60\n-0.29\n0.17\n0.52\n-0.17\n0.28\n0.21\n-0.07\n0.09\n0.23\n0.31\n-0.28\n0.13\n0.43\n-0.16\n0.28\n0.18\n-0.06\n0.08\n0.23\n0.28\n-0.20\n0.13\n0.36\n-0.10\n0.20\n0.15\n-0.06\n0.05\n0.19\n0.10\n\n\nparent_monitoring_w3\n0.38\n-0.19\n-0.18\n0.46\n-0.17\n-0.13\n0.15\n0.10\n-0.16\n-0.11\n0.41\n-0.18\n-0.18\n0.52\n-0.16\n-0.14\n0.13\n0.12\n-0.16\n-0.12\n0.66\n-0.21\n-0.21\n1.00\n-0.22\n-0.16\n0.24\n0.14\n-0.18\n0.00\n0.41\n-0.18\n-0.17\n0.51\n-0.19\n-0.14\n0.15\n0.13\n-0.13\n0.02\n0.39\n-0.18\n-0.14\n0.47\n-0.18\n-0.13\n0.12\n0.15\n-0.15\n0.02\n0.34\n-0.19\n-0.14\n0.44\n-0.13\n-0.10\n0.13\n0.06\n-0.16\n-0.05\n\n\ndesire_stress_w3\n-0.17\n0.20\n0.28\n-0.13\n0.43\n0.19\n-0.12\n0.04\n0.27\n0.30\n-0.18\n0.19\n0.30\n-0.13\n0.46\n0.22\n-0.08\n0.01\n0.22\n0.31\n-0.26\n0.30\n0.50\n-0.22\n1.00\n0.43\n-0.18\n0.00\n0.37\n0.44\n-0.20\n0.19\n0.31\n-0.14\n0.52\n0.24\n-0.16\n0.01\n0.27\n0.27\n-0.18\n0.17\n0.26\n-0.14\n0.47\n0.20\n-0.10\n0.01\n0.25\n0.20\n-0.16\n0.15\n0.26\n-0.12\n0.40\n0.18\n-0.13\n0.03\n0.25\n0.10\n\n\nfriend_stress_w3\n-0.11\n0.16\n0.16\n-0.09\n0.17\n0.29\n-0.13\n0.03\n0.22\n0.16\n-0.14\n0.20\n0.22\n-0.11\n0.23\n0.33\n-0.13\n0.02\n0.21\n0.22\n-0.21\n0.34\n0.40\n-0.16\n0.43\n1.00\n-0.26\n0.00\n0.38\n0.28\n-0.16\n0.22\n0.23\n-0.12\n0.23\n0.43\n-0.20\n0.00\n0.26\n0.16\n-0.15\n0.19\n0.22\n-0.10\n0.21\n0.35\n-0.17\n0.00\n0.27\n0.20\n-0.14\n0.18\n0.22\n-0.10\n0.22\n0.30\n-0.18\n0.02\n0.27\n0.05\n\n\nself_confidence_w3\n0.19\n-0.10\n-0.06\n0.15\n-0.10\n-0.14\n0.39\n0.13\n-0.17\n-0.10\n0.21\n-0.10\n-0.10\n0.18\n-0.11\n-0.16\n0.41\n0.12\n-0.20\n-0.12\n0.29\n-0.18\n-0.12\n0.24\n-0.18\n-0.26\n1.00\n0.18\n-0.30\n-0.07\n0.20\n-0.13\n-0.08\n0.18\n-0.17\n-0.18\n0.48\n0.13\n-0.24\n-0.06\n0.18\n-0.11\n-0.06\n0.12\n-0.15\n-0.17\n0.42\n0.13\n-0.23\n-0.06\n0.17\n-0.12\n-0.07\n0.14\n-0.11\n-0.12\n0.40\n0.08\n-0.23\n-0.04\n\n\nhigher_school_dependence_w3\n0.10\n-0.12\n0.04\n0.12\n-0.01\n0.02\n0.17\n0.35\n0.00\n0.06\n0.13\n-0.10\n0.06\n0.14\n-0.01\n0.00\n0.17\n0.43\n0.00\n0.09\n0.13\n-0.13\n0.10\n0.14\n0.00\n0.00\n0.18\n1.00\n0.04\n0.26\n0.10\n-0.11\n0.12\n0.08\n-0.02\n0.04\n0.14\n0.47\n0.04\n0.25\n0.08\n-0.12\n0.14\n0.06\n0.03\n0.04\n0.12\n0.40\n0.00\n0.24\n0.07\n-0.08\n0.09\n0.06\n0.04\n0.05\n0.12\n0.26\n0.01\n0.07\n\n\nneg_esteem_w3\n-0.15\n0.19\n0.20\n-0.12\n0.22\n0.20\n-0.16\n0.00\n0.37\n0.25\n-0.19\n0.19\n0.22\n-0.12\n0.25\n0.22\n-0.21\n-0.02\n0.36\n0.25\n-0.27\n0.41\n0.32\n-0.18\n0.37\n0.38\n-0.30\n0.04\n1.00\n0.38\n-0.19\n0.19\n0.22\n-0.14\n0.22\n0.27\n-0.24\n0.03\n0.44\n0.22\n-0.17\n0.18\n0.19\n-0.09\n0.24\n0.24\n-0.20\n0.00\n0.39\n0.18\n-0.17\n0.19\n0.21\n-0.10\n0.23\n0.24\n-0.23\n0.00\n0.38\n0.07\n\n\nacademic_stress_w3\n-0.03\n-0.01\n0.22\n0.01\n0.19\n0.15\n0.00\n0.17\n0.20\n0.31\n-0.03\n0.02\n0.28\n0.01\n0.24\n0.14\n0.00\n0.17\n0.14\n0.37\n-0.09\n0.06\n0.60\n0.00\n0.44\n0.28\n-0.07\n0.26\n0.38\n1.00\n-0.06\n-0.03\n0.32\n-0.02\n0.23\n0.16\n-0.07\n0.22\n0.24\n0.52\n-0.05\n-0.04\n0.28\n-0.01\n0.26\n0.16\n-0.04\n0.18\n0.19\n0.41\n-0.04\n-0.01\n0.23\n0.00\n0.21\n0.13\n-0.05\n0.12\n0.16\n0.13\n\n\nparent_attachment_w4\n0.45\n-0.13\n-0.21\n0.31\n-0.14\n-0.09\n0.12\n0.07\n-0.12\n-0.11\n0.52\n-0.14\n-0.25\n0.37\n-0.16\n-0.13\n0.12\n0.07\n-0.15\n-0.14\n0.61\n-0.18\n-0.29\n0.41\n-0.20\n-0.16\n0.20\n0.10\n-0.19\n-0.06\n1.00\n-0.20\n-0.44\n0.69\n-0.25\n-0.19\n0.27\n0.12\n-0.21\n-0.05\n0.62\n-0.19\n-0.28\n0.45\n-0.21\n-0.15\n0.19\n0.12\n-0.22\n-0.07\n0.52\n-0.18\n-0.24\n0.38\n-0.12\n-0.11\n0.14\n0.07\n-0.19\n-0.08\n\n\ndeviant_esteem_w4\n-0.16\n0.33\n0.17\n-0.15\n0.13\n0.11\n-0.10\n-0.07\n0.18\n0.10\n-0.18\n0.34\n0.14\n-0.15\n0.16\n0.17\n-0.12\n-0.10\n0.19\n0.09\n-0.22\n0.47\n0.17\n-0.18\n0.19\n0.22\n-0.13\n-0.11\n0.19\n-0.03\n-0.20\n1.00\n0.26\n-0.22\n0.28\n0.36\n-0.15\n-0.15\n0.46\n-0.04\n-0.19\n0.43\n0.15\n-0.16\n0.16\n0.16\n-0.12\n-0.14\n0.21\n-0.04\n-0.14\n0.39\n0.12\n-0.12\n0.12\n0.17\n-0.12\n-0.07\n0.22\n0.01\n\n\nparent_stress_w4\n-0.18\n0.11\n0.33\n-0.09\n0.18\n0.15\n0.00\n0.11\n0.17\n0.26\n-0.20\n0.13\n0.37\n-0.11\n0.23\n0.20\n-0.02\n0.09\n0.17\n0.29\n-0.28\n0.19\n0.52\n-0.17\n0.31\n0.23\n-0.08\n0.12\n0.22\n0.32\n-0.44\n0.26\n1.00\n-0.26\n0.45\n0.41\n-0.13\n0.14\n0.39\n0.56\n-0.32\n0.16\n0.49\n-0.18\n0.31\n0.20\n-0.08\n0.08\n0.27\n0.30\n-0.23\n0.18\n0.39\n-0.12\n0.24\n0.20\n-0.09\n0.05\n0.24\n0.15\n\n\nparent_monitoring_w4\n0.35\n-0.15\n-0.13\n0.38\n-0.11\n-0.08\n0.11\n0.07\n-0.11\n-0.06\n0.37\n-0.15\n-0.16\n0.44\n-0.11\n-0.11\n0.11\n0.10\n-0.16\n-0.10\n0.42\n-0.16\n-0.17\n0.51\n-0.14\n-0.12\n0.18\n0.08\n-0.14\n-0.02\n0.69\n-0.22\n-0.26\n1.00\n-0.21\n-0.17\n0.22\n0.13\n-0.18\n-0.01\n0.46\n-0.18\n-0.19\n0.54\n-0.15\n-0.12\n0.14\n0.12\n-0.16\n-0.03\n0.37\n-0.16\n-0.14\n0.45\n-0.07\n-0.06\n0.09\n0.05\n-0.14\n-0.04\n\n\ndesire_stress_w4\n-0.18\n0.15\n0.26\n-0.13\n0.36\n0.17\n-0.11\n0.05\n0.24\n0.27\n-0.18\n0.16\n0.26\n-0.16\n0.40\n0.21\n-0.10\n0.00\n0.23\n0.28\n-0.20\n0.19\n0.28\n-0.19\n0.52\n0.23\n-0.17\n-0.02\n0.22\n0.23\n-0.25\n0.28\n0.45\n-0.21\n1.00\n0.40\n-0.23\n0.01\n0.42\n0.44\n-0.22\n0.16\n0.27\n-0.17\n0.49\n0.19\n-0.14\n0.01\n0.28\n0.22\n-0.16\n0.18\n0.23\n-0.13\n0.43\n0.17\n-0.15\n0.02\n0.28\n0.10\n\n\nfriend_stress_w4\n-0.11\n0.14\n0.15\n-0.08\n0.14\n0.27\n-0.08\n0.05\n0.21\n0.16\n-0.12\n0.16\n0.18\n-0.10\n0.19\n0.29\n-0.11\n0.02\n0.21\n0.21\n-0.17\n0.22\n0.21\n-0.14\n0.24\n0.43\n-0.18\n0.04\n0.27\n0.16\n-0.19\n0.36\n0.41\n-0.17\n0.40\n1.00\n-0.25\n0.01\n0.45\n0.32\n-0.17\n0.23\n0.24\n-0.11\n0.24\n0.40\n-0.16\n-0.01\n0.31\n0.20\n-0.13\n0.21\n0.26\n-0.09\n0.22\n0.36\n-0.17\n0.05\n0.29\n0.11\n\n\nself_confidence_w4\n0.17\n-0.10\n-0.06\n0.14\n-0.09\n-0.14\n0.37\n0.10\n-0.17\n-0.08\n0.20\n-0.10\n-0.10\n0.15\n-0.12\n-0.14\n0.36\n0.11\n-0.20\n-0.13\n0.19\n-0.15\n-0.07\n0.15\n-0.16\n-0.20\n0.48\n0.14\n-0.24\n-0.07\n0.27\n-0.15\n-0.13\n0.22\n-0.23\n-0.25\n1.00\n0.16\n-0.36\n-0.18\n0.19\n-0.13\n-0.09\n0.12\n-0.18\n-0.19\n0.51\n0.14\n-0.30\n-0.10\n0.17\n-0.12\n-0.10\n0.12\n-0.12\n-0.14\n0.44\n0.06\n-0.24\n-0.07\n\n\nhigher_school_dependence_w4\n0.13\n-0.13\n0.02\n0.15\n-0.01\n-0.01\n0.15\n0.31\n-0.03\n0.03\n0.11\n-0.11\n0.06\n0.14\n0.03\n0.00\n0.19\n0.41\n-0.04\n0.09\n0.13\n-0.12\n0.09\n0.13\n0.01\n0.00\n0.13\n0.47\n0.03\n0.22\n0.12\n-0.15\n0.14\n0.13\n0.01\n0.01\n0.16\n1.00\n0.07\n0.33\n0.09\n-0.13\n0.14\n0.08\n0.03\n0.04\n0.11\n0.47\n-0.02\n0.25\n0.09\n-0.07\n0.07\n0.08\n0.05\n0.02\n0.10\n0.30\n0.00\n0.04\n\n\nneg_esteem_w4\n-0.13\n0.13\n0.20\n-0.08\n0.17\n0.20\n-0.15\n0.05\n0.30\n0.23\n-0.15\n0.18\n0.23\n-0.10\n0.22\n0.21\n-0.16\n0.04\n0.34\n0.27\n-0.18\n0.23\n0.23\n-0.13\n0.27\n0.26\n-0.24\n0.04\n0.44\n0.24\n-0.21\n0.46\n0.39\n-0.18\n0.42\n0.45\n-0.36\n0.07\n1.00\n0.44\n-0.19\n0.19\n0.23\n-0.12\n0.27\n0.24\n-0.22\n-0.01\n0.43\n0.19\n-0.13\n0.18\n0.20\n-0.09\n0.22\n0.22\n-0.22\n0.03\n0.38\n0.10\n\n\nacademic_stress_w4\n0.00\n-0.07\n0.20\n0.04\n0.17\n0.13\n0.00\n0.22\n0.15\n0.28\n0.01\n-0.02\n0.24\n0.04\n0.20\n0.12\n-0.02\n0.22\n0.15\n0.35\n-0.01\n-0.01\n0.31\n0.02\n0.27\n0.16\n-0.06\n0.25\n0.22\n0.52\n-0.05\n-0.04\n0.56\n-0.01\n0.44\n0.32\n-0.18\n0.33\n0.44\n1.00\n-0.04\n-0.05\n0.31\n0.00\n0.28\n0.17\n-0.11\n0.21\n0.24\n0.47\n-0.02\n-0.01\n0.23\n0.01\n0.25\n0.14\n-0.11\n0.16\n0.22\n0.17\n\n\nparent_attachment_w5\n0.45\n-0.15\n-0.21\n0.33\n-0.16\n-0.12\n0.12\n0.08\n-0.13\n-0.12\n0.50\n-0.14\n-0.25\n0.39\n-0.16\n-0.15\n0.10\n0.09\n-0.17\n-0.16\n0.55\n-0.16\n-0.28\n0.39\n-0.18\n-0.15\n0.18\n0.08\n-0.17\n-0.05\n0.62\n-0.19\n-0.32\n0.46\n-0.22\n-0.17\n0.19\n0.09\n-0.19\n-0.04\n1.00\n-0.24\n-0.45\n0.71\n-0.32\n-0.21\n0.26\n0.13\n-0.32\n-0.14\n0.59\n-0.21\n-0.30\n0.43\n-0.16\n-0.12\n0.18\n0.08\n-0.24\n-0.09\n\n\ndeviant_esteem_w5\n-0.16\n0.33\n0.13\n-0.14\n0.14\n0.09\n-0.13\n-0.08\n0.18\n0.08\n-0.19\n0.35\n0.15\n-0.15\n0.15\n0.17\n-0.14\n-0.11\n0.18\n0.09\n-0.20\n0.42\n0.13\n-0.18\n0.17\n0.19\n-0.11\n-0.12\n0.18\n-0.04\n-0.19\n0.43\n0.16\n-0.18\n0.16\n0.23\n-0.13\n-0.13\n0.19\n-0.05\n-0.24\n1.00\n0.23\n-0.20\n0.24\n0.33\n-0.17\n-0.16\n0.44\n-0.03\n-0.18\n0.47\n0.15\n-0.16\n0.13\n0.20\n-0.13\n-0.08\n0.24\n0.02\n\n\nparent_stress_w5\n-0.16\n0.09\n0.28\n-0.09\n0.18\n0.13\n-0.01\n0.12\n0.15\n0.21\n-0.19\n0.10\n0.35\n-0.11\n0.22\n0.18\n-0.01\n0.11\n0.15\n0.28\n-0.24\n0.13\n0.43\n-0.14\n0.26\n0.22\n-0.06\n0.14\n0.19\n0.28\n-0.28\n0.15\n0.49\n-0.19\n0.27\n0.24\n-0.09\n0.14\n0.23\n0.31\n-0.45\n0.23\n1.00\n-0.26\n0.50\n0.41\n-0.13\n0.18\n0.41\n0.66\n-0.29\n0.18\n0.48\n-0.17\n0.29\n0.22\n-0.11\n0.08\n0.27\n0.15\n\n\nparent_monitoring_w5\n0.34\n-0.14\n-0.13\n0.39\n-0.15\n-0.11\n0.09\n0.07\n-0.12\n-0.07\n0.38\n-0.13\n-0.16\n0.45\n-0.13\n-0.11\n0.07\n0.08\n-0.11\n-0.10\n0.38\n-0.13\n-0.16\n0.47\n-0.14\n-0.10\n0.12\n0.06\n-0.09\n-0.01\n0.45\n-0.16\n-0.18\n0.54\n-0.17\n-0.11\n0.12\n0.08\n-0.12\n0.00\n0.71\n-0.20\n-0.26\n1.00\n-0.23\n-0.13\n0.16\n0.12\n-0.17\n-0.05\n0.43\n-0.17\n-0.18\n0.51\n-0.12\n-0.06\n0.12\n0.05\n-0.16\n-0.07\n\n\ndesire_stress_w5\n-0.16\n0.17\n0.22\n-0.14\n0.36\n0.17\n-0.13\n0.04\n0.25\n0.24\n-0.19\n0.15\n0.25\n-0.15\n0.37\n0.20\n-0.10\n0.02\n0.22\n0.27\n-0.21\n0.15\n0.28\n-0.18\n0.47\n0.21\n-0.15\n0.03\n0.24\n0.26\n-0.21\n0.16\n0.31\n-0.15\n0.49\n0.24\n-0.18\n0.03\n0.27\n0.28\n-0.32\n0.24\n0.50\n-0.23\n1.00\n0.43\n-0.19\n0.04\n0.45\n0.44\n-0.25\n0.19\n0.33\n-0.18\n0.50\n0.23\n-0.18\n0.02\n0.32\n0.14\n\n\nfriend_stress_w5\n-0.09\n0.12\n0.08\n-0.08\n0.11\n0.23\n-0.09\n0.04\n0.17\n0.08\n-0.12\n0.14\n0.17\n-0.10\n0.18\n0.28\n-0.13\n0.04\n0.19\n0.18\n-0.15\n0.16\n0.18\n-0.13\n0.20\n0.35\n-0.17\n0.04\n0.24\n0.16\n-0.15\n0.16\n0.20\n-0.12\n0.19\n0.40\n-0.19\n0.04\n0.24\n0.17\n-0.21\n0.33\n0.41\n-0.13\n0.43\n1.00\n-0.27\n0.06\n0.42\n0.40\n-0.17\n0.20\n0.27\n-0.09\n0.22\n0.37\n-0.20\n0.02\n0.27\n0.07\n\n\nself_confidence_w5\n0.16\n-0.09\n-0.07\n0.12\n-0.08\n-0.12\n0.37\n0.11\n-0.15\n-0.08\n0.18\n-0.09\n-0.08\n0.13\n-0.07\n-0.14\n0.36\n0.11\n-0.18\n-0.10\n0.18\n-0.12\n-0.06\n0.12\n-0.10\n-0.17\n0.42\n0.12\n-0.20\n-0.04\n0.19\n-0.12\n-0.08\n0.14\n-0.14\n-0.16\n0.51\n0.11\n-0.22\n-0.11\n0.26\n-0.17\n-0.13\n0.16\n-0.19\n-0.27\n1.00\n0.15\n-0.39\n-0.15\n0.19\n-0.15\n-0.12\n0.11\n-0.14\n-0.18\n0.50\n0.06\n-0.30\n-0.09\n\n\nhigher_school_dependence_w5\n0.11\n-0.16\n0.01\n0.13\n-0.01\n-0.02\n0.16\n0.30\n-0.04\n0.01\n0.10\n-0.09\n0.04\n0.15\n0.00\n0.00\n0.16\n0.38\n-0.05\n0.07\n0.13\n-0.12\n0.08\n0.15\n0.01\n0.00\n0.13\n0.40\n0.00\n0.18\n0.12\n-0.14\n0.08\n0.12\n0.01\n-0.01\n0.14\n0.47\n-0.01\n0.21\n0.13\n-0.16\n0.18\n0.12\n0.04\n0.06\n0.15\n1.00\n-0.02\n0.34\n0.07\n-0.10\n0.10\n0.08\n0.07\n0.04\n0.11\n0.36\n0.00\n0.09\n\n\nneg_esteem_w5\n-0.14\n0.18\n0.19\n-0.10\n0.18\n0.20\n-0.17\n0.00\n0.34\n0.22\n-0.19\n0.17\n0.21\n-0.11\n0.19\n0.21\n-0.17\n0.00\n0.33\n0.23\n-0.22\n0.21\n0.23\n-0.15\n0.25\n0.27\n-0.23\n0.00\n0.39\n0.19\n-0.22\n0.21\n0.27\n-0.16\n0.28\n0.31\n-0.30\n-0.02\n0.43\n0.24\n-0.32\n0.44\n0.41\n-0.17\n0.45\n0.42\n-0.39\n-0.02\n1.00\n0.38\n-0.24\n0.25\n0.30\n-0.14\n0.26\n0.26\n-0.28\n0.01\n0.49\n0.14\n\n\nacademic_stress_w5\n0.01\n-0.05\n0.14\n0.03\n0.12\n0.11\n0.02\n0.20\n0.12\n0.19\n-0.01\n-0.05\n0.20\n0.04\n0.15\n0.12\n-0.01\n0.22\n0.09\n0.28\n-0.03\n-0.04\n0.28\n0.02\n0.20\n0.20\n-0.06\n0.24\n0.18\n0.41\n-0.07\n-0.04\n0.30\n-0.03\n0.22\n0.20\n-0.10\n0.25\n0.19\n0.47\n-0.14\n-0.03\n0.66\n-0.05\n0.44\n0.40\n-0.15\n0.34\n0.38\n1.00\n-0.09\n0.00\n0.32\n-0.04\n0.29\n0.19\n-0.10\n0.18\n0.23\n0.18\n\n\nparent_attachment_w6\n0.41\n-0.12\n-0.17\n0.30\n-0.16\n-0.12\n0.10\n0.09\n-0.14\n-0.09\n0.44\n-0.11\n-0.20\n0.33\n-0.16\n-0.13\n0.09\n0.06\n-0.14\n-0.13\n0.49\n-0.14\n-0.20\n0.34\n-0.16\n-0.14\n0.17\n0.07\n-0.17\n-0.04\n0.52\n-0.14\n-0.23\n0.37\n-0.16\n-0.13\n0.17\n0.09\n-0.13\n-0.02\n0.59\n-0.18\n-0.29\n0.43\n-0.25\n-0.17\n0.19\n0.07\n-0.24\n-0.09\n1.00\n-0.22\n-0.50\n0.68\n-0.28\n-0.20\n0.24\n0.05\n-0.32\n-0.13\n\n\ndeviant_esteem_w6\n-0.18\n0.31\n0.12\n-0.17\n0.11\n0.10\n-0.11\n-0.05\n0.17\n0.08\n-0.18\n0.31\n0.13\n-0.16\n0.15\n0.20\n-0.13\n-0.06\n0.17\n0.08\n-0.18\n0.39\n0.13\n-0.19\n0.15\n0.18\n-0.12\n-0.08\n0.19\n-0.01\n-0.18\n0.39\n0.18\n-0.16\n0.18\n0.21\n-0.12\n-0.07\n0.18\n-0.01\n-0.21\n0.47\n0.18\n-0.17\n0.19\n0.20\n-0.15\n-0.10\n0.25\n0.00\n-0.22\n1.00\n0.24\n-0.19\n0.22\n0.35\n-0.23\n-0.04\n0.48\n0.01\n\n\nparent_stress_w6\n-0.17\n0.10\n0.27\n-0.09\n0.21\n0.20\n-0.05\n0.09\n0.19\n0.22\n-0.18\n0.09\n0.31\n-0.10\n0.22\n0.20\n-0.06\n0.08\n0.16\n0.25\n-0.24\n0.12\n0.36\n-0.14\n0.26\n0.22\n-0.07\n0.09\n0.21\n0.23\n-0.24\n0.12\n0.39\n-0.14\n0.23\n0.26\n-0.10\n0.07\n0.20\n0.23\n-0.30\n0.15\n0.48\n-0.18\n0.33\n0.27\n-0.12\n0.10\n0.30\n0.32\n-0.50\n0.24\n1.00\n-0.24\n0.49\n0.43\n-0.19\n0.10\n0.41\n0.34\n\n\nparent_monitoring_w6\n0.33\n-0.11\n-0.11\n0.38\n-0.12\n-0.06\n0.08\n0.07\n-0.10\n-0.05\n0.33\n-0.10\n-0.13\n0.38\n-0.14\n-0.09\n0.06\n0.06\n-0.10\n-0.09\n0.37\n-0.13\n-0.10\n0.44\n-0.12\n-0.10\n0.14\n0.06\n-0.10\n0.00\n0.38\n-0.12\n-0.12\n0.45\n-0.13\n-0.09\n0.12\n0.08\n-0.09\n0.01\n0.43\n-0.16\n-0.17\n0.51\n-0.18\n-0.09\n0.11\n0.08\n-0.14\n-0.04\n0.68\n-0.19\n-0.24\n1.00\n-0.23\n-0.11\n0.15\n0.04\n-0.19\n-0.11\n\n\ndesire_stress_w6\n-0.10\n0.13\n0.20\n-0.09\n0.35\n0.20\n-0.10\n0.08\n0.23\n0.23\n-0.13\n0.11\n0.21\n-0.10\n0.35\n0.20\n-0.08\n0.06\n0.15\n0.25\n-0.14\n0.13\n0.20\n-0.13\n0.40\n0.22\n-0.11\n0.04\n0.23\n0.21\n-0.12\n0.12\n0.24\n-0.07\n0.43\n0.22\n-0.12\n0.05\n0.22\n0.25\n-0.16\n0.13\n0.29\n-0.12\n0.50\n0.22\n-0.14\n0.07\n0.26\n0.29\n-0.28\n0.22\n0.49\n-0.23\n1.00\n0.44\n-0.24\n0.09\n0.43\n0.24\n\n\nfriend_stress_w6\n-0.08\n0.11\n0.09\n-0.07\n0.16\n0.25\n-0.10\n0.07\n0.15\n0.13\n-0.09\n0.13\n0.15\n-0.06\n0.18\n0.26\n-0.09\n0.04\n0.16\n0.15\n-0.12\n0.16\n0.15\n-0.10\n0.18\n0.30\n-0.12\n0.05\n0.24\n0.13\n-0.11\n0.17\n0.20\n-0.06\n0.17\n0.36\n-0.14\n0.02\n0.22\n0.14\n-0.12\n0.20\n0.22\n-0.06\n0.23\n0.37\n-0.18\n0.04\n0.26\n0.19\n-0.20\n0.35\n0.43\n-0.11\n0.44\n1.00\n-0.30\n0.08\n0.40\n0.18\n\n\nself_confidence_w6\n0.11\n-0.11\n-0.08\n0.09\n-0.11\n-0.17\n0.30\n0.10\n-0.17\n-0.10\n0.15\n-0.10\n-0.07\n0.12\n-0.11\n-0.13\n0.34\n0.08\n-0.15\n-0.08\n0.15\n-0.14\n-0.06\n0.13\n-0.13\n-0.18\n0.40\n0.12\n-0.23\n-0.05\n0.14\n-0.12\n-0.09\n0.09\n-0.15\n-0.17\n0.44\n0.10\n-0.22\n-0.11\n0.18\n-0.13\n-0.11\n0.12\n-0.18\n-0.20\n0.50\n0.11\n-0.28\n-0.10\n0.24\n-0.23\n-0.19\n0.15\n-0.24\n-0.30\n1.00\n0.03\n-0.44\n-0.14\n\n\nhigher_school_dependence_w6\n0.04\n-0.06\n0.03\n0.07\n0.01\n0.01\n0.10\n0.20\n0.01\n0.05\n0.05\n-0.04\n0.06\n0.07\n0.02\n0.04\n0.07\n0.27\n-0.02\n0.08\n0.06\n-0.08\n0.05\n0.06\n0.03\n0.02\n0.08\n0.26\n0.00\n0.12\n0.07\n-0.07\n0.05\n0.05\n0.02\n0.05\n0.06\n0.30\n0.03\n0.16\n0.08\n-0.08\n0.08\n0.05\n0.02\n0.02\n0.06\n0.36\n0.01\n0.18\n0.05\n-0.04\n0.10\n0.04\n0.09\n0.08\n0.03\n1.00\n0.08\n0.13\n\n\nneg_esteem_w6\n-0.15\n0.18\n0.18\n-0.12\n0.20\n0.22\n-0.17\n0.01\n0.32\n0.22\n-0.18\n0.17\n0.19\n-0.13\n0.19\n0.21\n-0.17\n0.01\n0.28\n0.19\n-0.21\n0.21\n0.19\n-0.16\n0.25\n0.27\n-0.23\n0.01\n0.38\n0.16\n-0.19\n0.22\n0.24\n-0.14\n0.28\n0.29\n-0.24\n0.00\n0.38\n0.22\n-0.24\n0.24\n0.27\n-0.16\n0.32\n0.27\n-0.30\n0.00\n0.49\n0.23\n-0.32\n0.48\n0.41\n-0.19\n0.43\n0.40\n-0.44\n0.08\n1.00\n0.19\n\n\nacademic_stress_w6\n-0.06\n0.02\n0.11\n-0.01\n0.07\n0.10\n-0.04\n0.04\n0.08\n0.10\n-0.07\n-0.01\n0.09\n-0.01\n0.06\n0.06\n-0.04\n0.06\n0.08\n0.12\n-0.09\n-0.01\n0.10\n-0.05\n0.10\n0.05\n-0.04\n0.07\n0.07\n0.13\n-0.08\n0.01\n0.15\n-0.04\n0.10\n0.11\n-0.07\n0.04\n0.10\n0.17\n-0.09\n0.02\n0.15\n-0.07\n0.14\n0.07\n-0.09\n0.09\n0.14\n0.18\n-0.13\n0.01\n0.34\n-0.11\n0.24\n0.18\n-0.14\n0.13\n0.19\n1.00\n\n\n\n\n# corrplot을 사용하여 상관관계 히트맵 생성\ncorrplot(corr_matrix, \n         method = \"color\",\n         type = \"upper\", \n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.cex = 0.7,\n         diag = FALSE,\n         title = \"요인 점수 간 상관관계\",\n         mar = c(0, 0, 1, 0))\n\n\n\n\n\n\n\n# ggplot2를 사용하여 상관관계 히트맵 생성\n# 데이터 구조 변환\ncorr_df &lt;- melt(corr_matrix)\nnames(corr_df) &lt;- c(\"Var1\", \"Var2\", \"value\")\n\n# 히트맵 생성\ncorrelation_heatmap_plot &lt;- ggplot(corr_df, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6),\n        axis.text.y = element_text(size = 6),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title = \"요인 점수 간 상관관계 히트맵\", x = \"\", y = \"\", fill = \"상관계수\") +\n  coord_fixed()\n\noutput_filename &lt;- \"correlation_heatmap.png\"\n\n# ggsave 함수를 사용하여 저장\n# plot 인자에 저장할 플롯 객체를 지정합니다.\n# width와 height 인자로 그림의 크기를 조절할 수 있습니다 (인치 단위).\n# dpi 인자로 해상도를 조절합니다 (dots per inch). 보고서용은 300dpi 이상 권장.\nggsave(filename = output_filename,\n       plot = correlation_heatmap_plot, # 저장할 플롯 객체 지정\n       width = 8, # 너비 (인치)\n       height = 8, # 높이 (인치)\n       dpi = 300) # 해상도\n\n# 추가 분석: 높은 상관관계를 가진 변수 쌍 확인\n# 대각선 위쪽만 사용하여 중복 피함\nhigh_corr &lt;- which(abs(corr_matrix) &gt; 0.7 & upper.tri(corr_matrix), arr.ind = TRUE)\n\nif(nrow(high_corr) &gt; 0) {\n  high_corr_pairs &lt;- data.frame(\n    Var1 = rownames(corr_matrix)[high_corr[, 1]],\n    Var2 = colnames(corr_matrix)[high_corr[, 2]],\n    Correlation = corr_matrix[high_corr]\n  )\n  \n  # 상관계수 크기순으로 정렬\n  high_corr_pairs &lt;- high_corr_pairs[order(abs(high_corr_pairs$Correlation), decreasing = TRUE), ]\n  \n  cat(\"\\n높은 상관관계(|r| &gt; 0.7)를 가진 변수 쌍:\\n\")\n  print(high_corr_pairs)\n  kable(high_corr_pairs, caption = \"높은 상관관계를 가진 변수 쌍 (|r| &gt; 0.7)\")\n} else {\n  cat(\"\\n상관계수 절댓값이 0.7을 초과하는 변수 쌍이 없습니다.\\n\")\n}\n\n\n높은 상관관계(|r| &gt; 0.7)를 가진 변수 쌍:\n                  Var1                 Var2 Correlation\n2     parent_stress_w2   academic_stress_w2   0.7699974\n1     parent_stress_w1   academic_stress_w1   0.7205420\n3 parent_attachment_w5 parent_monitoring_w5   0.7079990\n\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n높은 상관관계를 가진 변수 쌍 (|r| &gt; 0.7)\n\n\n\nVar1\nVar2\nCorrelation\n\n\n\n\n2\nparent_stress_w2\nacademic_stress_w2\n0.7699974\n\n\n1\nparent_stress_w1\nacademic_stress_w1\n0.7205420\n\n\n3\nparent_attachment_w5\nparent_monitoring_w5\n0.7079990",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "title": "예측",
    "section": "예측의 품질 평가",
    "text": "예측의 품질 평가",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/11.html#총괄생산계획",
    "href": "posts/01_projects/bs_3_1/notes/product/11.html#총괄생산계획",
    "title": "총괄생산계획",
    "section": "총괄생산계획",
    "text": "총괄생산계획\n\n중기 범위(6-18개월) 기간에 대한 유사한 제품 묶음 수준에서 수요-공급 균형을 맞추기 위한 생산 계획\n제품 묶음\n\n생산 공정, 시간, 비용, 필요 자원 등의 측면에서 유사.\n개별 제품에 비해 예측 정확도가 높음\n\n계획 수립 후 주기적으로 업데이트",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/13.html",
    "href": "posts/01_projects/bs_3_1/notes/OR/13.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "13"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#c50-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#c50-모델",
    "title": "analysis",
    "section": "C50 모델",
    "text": "C50 모델\n\n# C5.0 의사결정나무 모델 학습 및 평가\nlibrary(C50)\nc50_model &lt;- C5.0(x = X_train, y = y_train, weights = weights_train, trials = 10)\ncat(\"C5.0 모델 학습 완료.\\n\")\n\nC5.0 모델 학습 완료.\n\n# 모델 요약\nprint(summary(c50_model))\n\n\nCall:\nC5.0.default(x = X_train, y = y_train, trials = 10, weights = weights_train)\n\n\nC5.0 [Release 2.07 GPL Edition]     Thu May 22 11:36:18 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 1167 cases (52 attributes) from undefined.data\nUsing relative case weighting\n\n-----  Trial 0:  -----\n\nDecision tree:\n\nneg_esteem_w3 &lt;= -0.4917164:\n:...academic_stress_w2 &lt;= 0.255909:\n:   :...self_confidence_w4 &lt;= 0.3803211:\n:   :   :...self_confidence_w3 &lt;= -0.4378077:\n:   :   :   :...neg_esteem_w4 &lt;= -0.3359894: modern (6.9)\n:   :   :   :   neg_esteem_w4 &gt; -0.3359894: comprehensive (6.5)\n:   :   :   self_confidence_w3 &gt; -0.4378077:\n:   :   :   :...academic_stress_w5 &lt;= 0.5332705: modern (128.4/10.1)\n:   :   :       academic_stress_w5 &gt; 0.5332705:\n:   :   :       :...higher_school_dependence_w3 &lt;= 0.4061199: comprehensive (6.6/0.8)\n:   :   :           higher_school_dependence_w3 &gt; 0.4061199: modern (7.5)\n:   :   self_confidence_w4 &gt; 0.3803211:\n:   :   :...parent_stress_w2 &gt; 0.2899708: comprehensive (4.6)\n:   :       parent_stress_w2 &lt;= 0.2899708:\n:   :       :...self_confidence_w2 &lt;= 0.569523:\n:   :           :...academic_stress_w2 &gt; -0.8532773: modern (18.3)\n:   :           :   academic_stress_w2 &lt;= -0.8532773:\n:   :           :   :...friend_stress_w3 &lt;= -0.5425196: modern (3)\n:   :           :       friend_stress_w3 &gt; -0.5425196: comprehensive (3.3)\n:   :           self_confidence_w2 &gt; 0.569523:\n:   :           :...higher_school_dependence_w3 &lt;= -0.7532299: modern (3.6)\n:   :               higher_school_dependence_w3 &gt; -0.7532299:\n:   :               :...higher_school_dependence_w1 &lt;= 0.6168386: comprehensive (10.5)\n:   :                   higher_school_dependence_w1 &gt; 0.6168386: modern (5.4/0.8)\n:   academic_stress_w2 &gt; 0.255909:\n:   :...deviant_esteem_w3 &gt; 0.09987827: comprehensive (7.5)\n:       deviant_esteem_w3 &lt;= 0.09987827:\n:       :...academic_stress_w3 &gt; 0.6635044: comprehensive (5.7)\n:           academic_stress_w3 &lt;= 0.6635044:\n:           :...deviant_esteem_w5 &gt; 0.5742826: modern (4.8)\n:               deviant_esteem_w5 &lt;= 0.5742826:\n:               :...parent_monitoring_w5 &gt; 0.497577: modern (7.6)\n:                   parent_monitoring_w5 &lt;= 0.497577:\n:                   :...deviant_esteem_w3 &lt;= -0.5728644: comprehensive (14.6/2.5)\n:                       deviant_esteem_w3 &gt; -0.5728644: modern (8.9/1.4)\nneg_esteem_w3 &gt; -0.4917164:\n:...parent_attachment_w2 &lt;= 0.2406168:\n    :...parent_attachment_w2 &lt;= -0.9545448:\n    :   :...self_confidence_w5 &lt;= -1.140358: modern (4.6/0.9)\n    :   :   self_confidence_w5 &gt; -1.140358: comprehensive (52.8/4.4)\n    :   parent_attachment_w2 &gt; -0.9545448:\n    :   :...desire_stress_w5 &lt;= -0.9080333:\n    :       :...academic_stress_w3 &lt;= -0.8450933: comprehensive (5.5/0.8)\n    :       :   academic_stress_w3 &gt; -0.8450933: modern (27.4/3)\n    :       desire_stress_w5 &gt; -0.9080333:\n    :       :...deviant_esteem_w4 &lt;= 0.2627467:\n    :           :...deviant_esteem_w2 &gt; 0.9321506: comprehensive (22.5/2.4)\n    :           :   deviant_esteem_w2 &lt;= 0.9321506:\n    :           :   :...academic_stress_w4 &gt; 0.07257767: comprehensive (195.8/75.8)\n    :           :       academic_stress_w4 &lt;= 0.07257767:\n    :           :       :...self_confidence_w2 &gt; 0.3842234:\n    :           :           :...desire_stress_w2 &lt;= 1.305564: modern (35.3/2.9)\n    :           :           :   desire_stress_w2 &gt; 1.305564: comprehensive (2.8)\n    :           :           self_confidence_w2 &lt;= 0.3842234:\n    :           :           :...academic_stress_w3 &gt; -0.3870669:\n    :           :               :...friend_stress_w3 &lt;= 1.042437: modern (82/29)\n    :           :               :   friend_stress_w3 &gt; 1.042437: comprehensive (4.8)\n    :           :               academic_stress_w3 &lt;= -0.3870669:\n    :           :               :...neg_esteem_w2 &gt; 1.443583: modern (2.5)\n    :           :                   neg_esteem_w2 &lt;= 1.443583: [S1]\n    :           deviant_esteem_w4 &gt; 0.2627467:\n    :           :...neg_esteem_w3 &gt; 1.136711: modern (7.5/1.7)\n    :               neg_esteem_w3 &lt;= 1.136711:\n    :               :...parent_attachment_w1 &gt; 0.0524049: comprehensive (30.7/0.8)\n    :                   parent_attachment_w1 &lt;= 0.0524049:\n    :                   :...academic_stress_w5 &lt;= -0.6976507: comprehensive (14.9)\n    :                       academic_stress_w5 &gt; -0.6976507:\n    :                       :...self_confidence_w5 &gt; 0.273435:\n    :                           :...parent_stress_w5 &lt;= -0.2001466: comprehensive (4.8)\n    :                           :   parent_stress_w5 &gt; -0.2001466: modern (10.5/0.7)\n    :                           self_confidence_w5 &lt;= 0.273435:\n    :                           :...deviant_esteem_w1 &lt;= 0.1817347: comprehensive (19.5)\n    :                               deviant_esteem_w1 &gt; 0.1817347: [S2]\n    parent_attachment_w2 &gt; 0.2406168:\n    :...desire_stress_w3 &gt; 1.006586:\n        :...self_confidence_w2 &gt; 0.7153788: modern (3.3)\n        :   self_confidence_w2 &lt;= 0.7153788:\n        :   :...parent_attachment_w1 &lt;= 0.6579127: comprehensive (22.8/0.9)\n        :       parent_attachment_w1 &gt; 0.6579127: modern (4.1)\n        desire_stress_w3 &lt;= 1.006586:\n        :...desire_stress_w5 &lt;= 0.07439398: modern (152.5/36.5)\n            desire_stress_w5 &gt; 0.07439398:\n            :...parent_stress_w4 &lt;= -0.5802217: modern (10.5)\n                parent_stress_w4 &gt; -0.5802217:\n                :...neg_esteem_w3 &gt; 1.155261: modern (6.4)\n                    neg_esteem_w3 &lt;= 1.155261:\n                    :...parent_attachment_w3 &lt;= 0.01178261: comprehensive (24.6/3.4)\n                        parent_attachment_w3 &gt; 0.01178261:\n                        :...academic_stress_w3 &lt;= -0.7931573: comprehensive (4.4)\n                            academic_stress_w3 &gt; -0.7931573:\n                            :...higher_school_dependence_w4 &gt; 1.088369: modern (7.7)\n                                higher_school_dependence_w4 &lt;= 1.088369: [S3]\n\nSubTree [S1]\n\nparent_stress_w3 &gt; -0.6894929: comprehensive (29/2)\nparent_stress_w3 &lt;= -0.6894929:\n:...friend_stress_w4 &lt;= -0.7487749: comprehensive (2.3)\n    friend_stress_w4 &gt; -0.7487749: modern (4.6)\n\nSubTree [S2]\n\nfriend_stress_w3 &gt; 0.3602169: comprehensive (23.7/1.7)\nfriend_stress_w3 &lt;= 0.3602169:\n:...parent_attachment_w2 &gt; -0.2574169: comprehensive (15.9/2.6)\n    parent_attachment_w2 &lt;= -0.2574169:\n    :...parent_monitoring_w5 &lt;= -0.05316973: modern (14.8/1)\n        parent_monitoring_w5 &gt; -0.05316973: comprehensive (3.4)\n\nSubTree [S3]\n\nself_confidence_w1 &gt; 1.034353: comprehensive (6.4)\nself_confidence_w1 &lt;= 1.034353:\n:...deviant_esteem_w4 &gt; 0.4704975: modern (7.3)\n    deviant_esteem_w4 &lt;= 0.4704975:\n    :...deviant_esteem_w4 &lt;= -0.4019238:\n        :...parent_stress_w3 &lt;= -0.1467977: modern (13.8)\n        :   parent_stress_w3 &gt; -0.1467977:\n        :   :...desire_stress_w5 &lt;= 0.3345925: comprehensive (3.1)\n        :       desire_stress_w5 &gt; 0.3345925: modern (7.5/1.8)\n        deviant_esteem_w4 &gt; -0.4019238:\n        :...desire_stress_w3 &gt; -0.01291398: comprehensive (14.2/1.2)\n            desire_stress_w3 &lt;= -0.01291398:\n            :...desire_stress_w5 &lt;= 0.3627517: comprehensive (2.6)\n                desire_stress_w5 &gt; 0.3627517: modern (4.7)\n\n-----  Trial 1:  -----\n\nDecision tree:\n\ndeviant_esteem_w3 &lt;= 0.2006587:\n:...academic_stress_w1 &gt; 0.4023996:\n:   :...deviant_esteem_w1 &gt; 0.6481321: comprehensive (11.7/0.6)\n:   :   deviant_esteem_w1 &lt;= 0.6481321:\n:   :   :...friend_stress_w3 &gt; 0.7180749: modern (22.2/3.7)\n:   :       friend_stress_w3 &lt;= 0.7180749:\n:   :       :...parent_stress_w2 &gt; 1.076355: comprehensive (8.8)\n:   :           parent_stress_w2 &lt;= 1.076355:\n:   :           :...neg_esteem_w1 &gt; 0.8253638: modern (20.6/4.1)\n:   :               neg_esteem_w1 &lt;= 0.8253638:\n:   :               :...parent_attachment_w4 &lt;= -0.2527352:\n:   :                   :...self_confidence_w2 &lt;= 0.972608: comprehensive (29.5/2.5)\n:   :                   :   self_confidence_w2 &gt; 0.972608: modern (2.6)\n:   :                   parent_attachment_w4 &gt; -0.2527352: [S1]\n:   academic_stress_w1 &lt;= 0.4023996:\n:   :...friend_stress_w4 &gt; 0.8264473: comprehensive (31.6/10.3)\n:       friend_stress_w4 &lt;= 0.8264473:\n:       :...higher_school_dependence_w4 &gt; 0.9983951:\n:           :...parent_attachment_w3 &gt; 0.3970405: modern (27.7)\n:           :   parent_attachment_w3 &lt;= 0.3970405:\n:           :   :...parent_stress_w1 &lt;= -0.7770947: comprehensive (7/0.5)\n:           :       parent_stress_w1 &gt; -0.7770947: modern (36.3/4.3)\n:           higher_school_dependence_w4 &lt;= 0.9983951:\n:           :...parent_monitoring_w4 &lt;= 0.557053:\n:               :...parent_stress_w3 &gt; 0.6818147: comprehensive (19.9/5.8)\n:               :   parent_stress_w3 &lt;= 0.6818147:\n:               :   :...parent_stress_w1 &lt;= -0.7150363: modern (58.8/4.6)\n:               :       parent_stress_w1 &gt; -0.7150363:\n:               :       :...academic_stress_w2 &lt;= 0.246583: modern (193.8/51.8)\n:               :           academic_stress_w2 &gt; 0.246583: [S2]\n:               parent_monitoring_w4 &gt; 0.557053:\n:               :...self_confidence_w2 &lt;= -1.067314: comprehensive (6.6)\n:                   self_confidence_w2 &gt; -1.067314:\n:                   :...neg_esteem_w2 &gt; 0.80349: comprehensive (5.3)\n:                       neg_esteem_w2 &lt;= 0.80349:\n:                       :...higher_school_dependence_w1 &lt;= -1.170888: modern (8.4)\n:                           higher_school_dependence_w1 &gt; -1.170888:\n:                           :...self_confidence_w2 &lt;= -0.4732084: modern (6.8)\n:                               self_confidence_w2 &gt; -0.4732084: [S3]\ndeviant_esteem_w3 &gt; 0.2006587:\n:...friend_stress_w2 &gt; 1.123414:\n    :...parent_attachment_w4 &lt;= -0.6665184: comprehensive (4.6)\n    :   parent_attachment_w4 &gt; -0.6665184:\n    :   :...self_confidence_w3 &lt;= 0.2077743: modern (27.8/2.4)\n    :       self_confidence_w3 &gt; 0.2077743: comprehensive (2.5)\n    friend_stress_w2 &lt;= 1.123414:\n    :...friend_stress_w2 &lt;= -0.5327595:\n        :...parent_monitoring_w4 &gt; 0.5466406: modern (10.6)\n        :   parent_monitoring_w4 &lt;= 0.5466406:\n        :   :...higher_school_dependence_w2 &lt;= -1.80912: comprehensive (6.3)\n        :       higher_school_dependence_w2 &gt; -1.80912:\n        :       :...parent_attachment_w5 &lt;= -0.5709003: comprehensive (12.8/1.7)\n        :           parent_attachment_w5 &gt; -0.5709003:\n        :           :...neg_esteem_w5 &lt;= -0.4700277: modern (9.2)\n        :               neg_esteem_w5 &gt; -0.4700277: [S4]\n        friend_stress_w2 &gt; -0.5327595:\n        :...parent_monitoring_w1 &lt;= -0.473584: comprehensive (104/18.5)\n            parent_monitoring_w1 &gt; -0.473584:\n            :...academic_stress_w5 &lt;= 0.02488093:\n                :...desire_stress_w5 &gt; 0.6130046: comprehensive (15.4)\n                :   desire_stress_w5 &lt;= 0.6130046:\n                :   :...deviant_esteem_w4 &lt;= -0.376675: modern (13.7/4.6)\n                :       deviant_esteem_w4 &gt; -0.376675: comprehensive (68.7/14.6)\n                academic_stress_w5 &gt; 0.02488093:\n                :...friend_stress_w1 &gt; 1.115224: comprehensive (8.8/0.4)\n                    friend_stress_w1 &lt;= 1.115224:\n                    :...friend_stress_w3 &gt; 0.9951192: comprehensive (9.5)\n                        friend_stress_w3 &lt;= 0.9951192:\n                        :...parent_monitoring_w5 &gt; 0.4652432: comprehensive (18.8/4.5)\n                            parent_monitoring_w5 &lt;= 0.4652432:\n                            :...self_confidence_w1 &lt;= -0.6392726: comprehensive (6.7/0.6)\n                                self_confidence_w1 &gt; -0.6392726:\n                                :...parent_attachment_w5 &gt; 0.1718637: modern (27.5/1.7)\n                                    parent_attachment_w5 &lt;= 0.1718637: [S5]\n\nSubTree [S1]\n\nhigher_school_dependence_w5 &gt; 1.091035: comprehensive (12.1/1.1)\nhigher_school_dependence_w5 &lt;= 1.091035:\n:...parent_stress_w5 &gt; 0.09165927: modern (24.6/3.5)\n    parent_stress_w5 &lt;= 0.09165927:\n    :...higher_school_dependence_w4 &lt;= -1.250813: modern (5.7)\n        higher_school_dependence_w4 &gt; -1.250813:\n        :...higher_school_dependence_w5 &lt;= 0.8657749: comprehensive (35/8.7)\n            higher_school_dependence_w5 &gt; 0.8657749: modern (3.8)\n\nSubTree [S2]\n\nhigher_school_dependence_w4 &lt;= -1.66591: comprehensive (5.5)\nhigher_school_dependence_w4 &gt; -1.66591:\n:...self_confidence_w3 &lt;= -0.4129246: modern (17.3/2.2)\n    self_confidence_w3 &gt; -0.4129246:\n    :...parent_monitoring_w5 &lt;= -0.9591953: modern (4.5)\n        parent_monitoring_w5 &gt; -0.9591953:\n        :...higher_school_dependence_w5 &lt;= -1.16502: modern (3.1)\n            higher_school_dependence_w5 &gt; -1.16502: comprehensive (32.5/6.2)\n\nSubTree [S3]\n\nacademic_stress_w1 &gt; -0.1763002: comprehensive (22.1/3.7)\nacademic_stress_w1 &lt;= -0.1763002:\n:...deviant_esteem_w5 &gt; -0.1599706: modern (8)\n    deviant_esteem_w5 &lt;= -0.1599706:\n    :...friend_stress_w4 &gt; -0.02617266: modern (5)\n        friend_stress_w4 &lt;= -0.02617266:\n        :...parent_stress_w4 &lt;= -0.5714737: modern (13.6/5.2)\n            parent_stress_w4 &gt; -0.5714737: comprehensive (14.2/0.8)\n\nSubTree [S4]\n\nhigher_school_dependence_w2 &gt; 0.5933306: comprehensive (14.3/1.7)\nhigher_school_dependence_w2 &lt;= 0.5933306:\n:...parent_attachment_w3 &lt;= 0.1044953: modern (23.1/2.9)\n    parent_attachment_w3 &gt; 0.1044953: comprehensive (11.1/3.9)\n\nSubTree [S5]\n\nacademic_stress_w2 &lt;= -0.328288: comprehensive (7.8/0.8)\nacademic_stress_w2 &gt; -0.328288:\n:...friend_stress_w4 &lt;= -0.1490846: comprehensive (4.9/0.7)\n    friend_stress_w4 &gt; -0.1490846:\n    :...parent_attachment_w3 &lt;= -0.5992901: modern (12.7)\n        parent_attachment_w3 &gt; -0.5992901:\n        :...desire_stress_w4 &lt;= 1.105612: modern (37.2/10.1)\n            desire_stress_w4 &gt; 1.105612: comprehensive (4.3)\n\n-----  Trial 2:  -----\n\nDecision tree:\n\ndesire_stress_w5 &lt;= 0.3115232:\n:...neg_esteem_w3 &gt; -0.3126045:\n:   :...parent_attachment_w5 &gt; -0.6469256: modern (444.7/203)\n:   :   parent_attachment_w5 &lt;= -0.6469256:\n:   :   :...desire_stress_w2 &gt; 1.420854: modern (2.6)\n:   :       desire_stress_w2 &lt;= 1.420854:\n:   :       :...deviant_esteem_w5 &lt;= -0.5263632: modern (4.7/0.6)\n:   :           deviant_esteem_w5 &gt; -0.5263632: comprehensive (32.7/3.1)\n:   neg_esteem_w3 &lt;= -0.3126045:\n:   :...neg_esteem_w3 &lt;= -1.363796: modern (17.1)\n:       neg_esteem_w3 &gt; -1.363796:\n:       :...deviant_esteem_w1 &gt; 0.183979: modern (61.3/10.2)\n:           deviant_esteem_w1 &lt;= 0.183979:\n:           :...desire_stress_w5 &gt; -0.366286:\n:               :...higher_school_dependence_w2 &lt;= 1.482369: modern (46.4/5.3)\n:               :   higher_school_dependence_w2 &gt; 1.482369: comprehensive (3)\n:               desire_stress_w5 &lt;= -0.366286:\n:               :...parent_monitoring_w5 &lt;= -0.638469: modern (7.4)\n:                   parent_monitoring_w5 &gt; -0.638469:\n:                   :...higher_school_dependence_w4 &lt;= -0.2796632: comprehensive (23/3)\n:                       higher_school_dependence_w4 &gt; -0.2796632:\n:                       :...self_confidence_w4 &lt;= 0.3357325: modern (25.4/4.1)\n:                           self_confidence_w4 &gt; 0.3357325:\n:                           :...neg_esteem_w3 &lt;= -1.188902: modern (4.1)\n:                               neg_esteem_w3 &gt; -1.188902:\n:                               :...parent_attachment_w5 &lt;= -0.1211987: modern (4/0.6)\n:                                   parent_attachment_w5 &gt; -0.1211987:\n:                                   :...neg_esteem_w1 &lt;= -0.02616754: comprehensive (37.5/7.7)\n:                                       neg_esteem_w1 &gt; -0.02616754: modern (4.8/0.5)\ndesire_stress_w5 &gt; 0.3115232:\n:...parent_stress_w1 &lt;= -0.05522731:\n    :...deviant_esteem_w5 &lt;= -0.5497233: modern (26.4/3.2)\n    :   deviant_esteem_w5 &gt; -0.5497233:\n    :   :...higher_school_dependence_w4 &gt; 0.5713524: modern (19.6/4)\n    :       higher_school_dependence_w4 &lt;= 0.5713524:\n    :       :...parent_stress_w5 &lt;= -0.4999616: comprehensive (7.9)\n    :           parent_stress_w5 &gt; -0.4999616:\n    :           :...parent_stress_w4 &lt;= -0.3156172: comprehensive (28.8/5.7)\n    :               parent_stress_w4 &gt; -0.3156172:\n    :               :...academic_stress_w4 &lt;= -0.5514578: modern (9.7)\n    :                   academic_stress_w4 &gt; -0.5514578:\n    :                   :...parent_attachment_w1 &lt;= -0.3572552: comprehensive (8.3)\n    :                       parent_attachment_w1 &gt; -0.3572552:\n    :                       :...friend_stress_w2 &gt; 1.269789: comprehensive (3.4)\n    :                           friend_stress_w2 &lt;= 1.269789: [S1]\n    parent_stress_w1 &gt; -0.05522731:\n    :...parent_attachment_w1 &lt;= -1.31776: modern (15.1/3.9)\n        parent_attachment_w1 &gt; -1.31776:\n        :...friend_stress_w2 &gt; 0.8899145:\n            :...higher_school_dependence_w2 &lt;= -1.145506: comprehensive (7.3)\n            :   higher_school_dependence_w2 &gt; -1.145506:\n            :   :...parent_attachment_w4 &lt;= -0.8235376: comprehensive (2.8)\n            :       parent_attachment_w4 &gt; -0.8235376: modern (29.2/5.7)\n            friend_stress_w2 &lt;= 0.8899145:\n            :...higher_school_dependence_w4 &lt;= -0.3952348: comprehensive (71.7/9.6)\n                higher_school_dependence_w4 &gt; -0.3952348:\n                :...self_confidence_w1 &gt; 0.804032: comprehensive (8.5)\n                    self_confidence_w1 &lt;= 0.804032:\n                    :...deviant_esteem_w2 &gt; 0.261834:\n                        :...parent_attachment_w5 &lt;= -0.4891729: comprehensive (11.3/1.1)\n                        :   parent_attachment_w5 &gt; -0.4891729:\n                        :   :...desire_stress_w5 &lt;= 0.3453267: comprehensive (3.7)\n                        :       desire_stress_w5 &gt; 0.3453267: modern (32.8/6.4)\n                        deviant_esteem_w2 &lt;= 0.261834:\n                        :...higher_school_dependence_w2 &lt;= -1.112529: modern (9.7/1.3)\n                            higher_school_dependence_w2 &gt; -1.112529:\n                            :...desire_stress_w1 &gt; 0.5448515: comprehensive (36.8/1.1)\n                                desire_stress_w1 &lt;= 0.5448515:\n                                :...deviant_esteem_w5 &gt; 0.8505611: modern (5.6/0.4)\n                                    deviant_esteem_w5 &lt;= 0.8505611: [S2]\n\nSubTree [S1]\n\nacademic_stress_w4 &lt;= 0.1729998: comprehensive (22/6.9)\nacademic_stress_w4 &gt; 0.1729998: modern (32.9/8.4)\n\nSubTree [S2]\n\nacademic_stress_w2 &gt; 0.4319452: comprehensive (14.5)\nacademic_stress_w2 &lt;= 0.4319452:\n:...parent_monitoring_w3 &gt; 0.5183533: comprehensive (11.5/0.9)\n    parent_monitoring_w3 &lt;= 0.5183533:\n    :...deviant_esteem_w4 &lt;= 0.2289638: modern (22.9/7.3)\n        deviant_esteem_w4 &gt; 0.2289638: comprehensive (5.7)\n\n-----  Trial 3:  -----\n\nDecision tree:\n\nneg_esteem_w3 &lt;= -0.5306732:\n:...neg_esteem_w3 &lt;= -1.368326: modern (17.6)\n:   neg_esteem_w3 &gt; -1.368326:\n:   :...higher_school_dependence_w3 &lt;= -1.189619: modern (16.8/0.5)\n:       higher_school_dependence_w3 &gt; -1.189619:\n:       :...self_confidence_w2 &gt; 0.5176293: comprehensive (33/9.5)\n:           self_confidence_w2 &lt;= 0.5176293:\n:           :...friend_stress_w5 &gt; 0.147894: comprehensive (19.9/5.9)\n:               friend_stress_w5 &lt;= 0.147894:\n:               :...parent_stress_w1 &lt;= -0.4626315: modern (39.5/3.8)\n:                   parent_stress_w1 &gt; -0.4626315:\n:                   :...academic_stress_w1 &gt; 0.484723: comprehensive (10.6/2.1)\n:                       academic_stress_w1 &lt;= 0.484723:\n:                       :...neg_esteem_w1 &lt;= -0.6939729: comprehensive (15.3/5.3)\n:                           neg_esteem_w1 &gt; -0.6939729: modern (30.8/3.3)\nneg_esteem_w3 &gt; -0.5306732:\n:...self_confidence_w2 &gt; 0.6270063:\n    :...parent_stress_w3 &gt; 1.015424: comprehensive (7.3/0.5)\n    :   parent_stress_w3 &lt;= 1.015424:\n    :   :...parent_stress_w5 &lt;= -0.5939033: comprehensive (15.9/4.7)\n    :       parent_stress_w5 &gt; -0.5939033: modern (49.9/7.8)\n    self_confidence_w2 &lt;= 0.6270063:\n    :...parent_attachment_w1 &lt;= 0.5002198: comprehensive (775.9/291.7)\n        parent_attachment_w1 &gt; 0.5002198:\n        :...higher_school_dependence_w4 &gt; 1.176983: modern (9.5)\n            higher_school_dependence_w4 &lt;= 1.176983:\n            :...deviant_esteem_w5 &gt; 0.2503748: comprehensive (25.3/5.3)\n                deviant_esteem_w5 &lt;= 0.2503748:\n                :...higher_school_dependence_w5 &lt;= -0.5852311: modern (15.6/1.2)\n                    higher_school_dependence_w5 &gt; -0.5852311:\n                    :...deviant_esteem_w4 &gt; 0.8331017: comprehensive (5.6)\n                        deviant_esteem_w4 &lt;= 0.8331017:\n                        :...friend_stress_w4 &lt;= -0.2859541:\n                            :...deviant_esteem_w4 &lt;= -0.06133959: comprehensive (26.3/5.6)\n                            :   deviant_esteem_w4 &gt; -0.06133959: modern (3.6)\n                            friend_stress_w4 &gt; -0.2859541:\n                            :...parent_attachment_w2 &lt;= -0.2530124: comprehensive (3.8)\n                                parent_attachment_w2 &gt; -0.2530124: modern (45.1/10.5)\n\n-----  Trial 4:  -----\n\nDecision tree:\n\nneg_esteem_w3 &gt; -0.3147962:\n:...deviant_esteem_w4 &lt;= 0.5021676:\n:   :...parent_attachment_w2 &gt; -0.9545448: modern (702.3/335.8)\n:   :   parent_attachment_w2 &lt;= -0.9545448:\n:   :   :...desire_stress_w4 &lt;= 0.7441784: comprehensive (25.4/0.9)\n:   :       desire_stress_w4 &gt; 0.7441784: modern (11.7/4.1)\n:   deviant_esteem_w4 &gt; 0.5021676:\n:   :...desire_stress_w2 &gt; 1.187062: modern (12.3/3.1)\n:       desire_stress_w2 &lt;= 1.187062:\n:       :...higher_school_dependence_w3 &lt;= 0.9438877: comprehensive (108.6/27)\n:           higher_school_dependence_w3 &gt; 0.9438877: modern (7.7/0.5)\nneg_esteem_w3 &lt;= -0.3147962:\n:...parent_stress_w1 &lt;= 0.3100227: modern (234.9/73.2)\n    parent_stress_w1 &gt; 0.3100227:\n    :...academic_stress_w5 &gt; 0.9993486: comprehensive (12/0.7)\n        academic_stress_w5 &lt;= 0.9993486:\n        :...higher_school_dependence_w5 &gt; 0.4315008: modern (21.6/3.3)\n            higher_school_dependence_w5 &lt;= 0.4315008:\n            :...deviant_esteem_w1 &lt;= 0.283286: comprehensive (24.8/4.5)\n                deviant_esteem_w1 &gt; 0.283286: modern (5.8/0.9)\n\n-----  Trial 5:  -----\n\nDecision tree:\n\nneg_esteem_w3 &lt;= -0.3147962: modern (291.3/117.1)\nneg_esteem_w3 &gt; -0.3147962: comprehensive (875.7/386.5)\n\n-----  Trial 6:  -----\n\nDecision tree:\n comprehensive (1167/581.4)\n\n*** boosting reduced to 6 trials since last classifier is very inaccurate\n\n\nEvaluation on training data (1167 cases):\n\nTrial       Decision Tree   \n-----     ----------------  \n      Size      Errors  \n\n   0        59  211(18.1%)\n   1        56  220(18.9%)\n   2        40  313(26.8%)\n   3        20  392(33.6%)\n   4        11  435(37.3%)\n   5         2  488(41.8%)\nboost            95( 8.1%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n       518    36    (a): class comprehensive\n        59   554    (b): class modern\n\n\n    Attribute usage:\n\n    100.00% deviant_esteem_w3\n    100.00% neg_esteem_w3\n    100.00% desire_stress_w5\n     97.94% self_confidence_w2\n     79.35% parent_attachment_w2\n     78.75% parent_attachment_w1\n     77.63% deviant_esteem_w4\n     74.38% higher_school_dependence_w4\n     72.15% parent_stress_w1\n     61.95% academic_stress_w1\n     53.13% friend_stress_w4\n     53.13% parent_attachment_w5\n     51.84% friend_stress_w2\n     44.39% parent_monitoring_w4\n     42.84% academic_stress_w2\n     41.05% deviant_esteem_w2\n     40.10% academic_stress_w5\n     38.39% deviant_esteem_w1\n     37.79% parent_stress_w3\n     35.73% deviant_esteem_w5\n     35.48% academic_stress_w4\n     34.28% friend_stress_w3\n     28.19% parent_monitoring_w1\n     26.82% higher_school_dependence_w3\n     26.65% desire_stress_w3\n     24.68% parent_monitoring_w5\n     24.34% self_confidence_w1\n     23.39% higher_school_dependence_w2\n     23.31% academic_stress_w3\n     22.88% parent_stress_w5\n     22.79% higher_school_dependence_w5\n     20.65% parent_attachment_w3\n     19.19% self_confidence_w3\n     19.02% neg_esteem_w1\n     18.94% parent_stress_w4\n     18.94% self_confidence_w4\n     16.97% desire_stress_w2\n     16.71% parent_stress_w2\n     14.82% parent_attachment_w4\n     12.08% friend_stress_w5\n     12.00% self_confidence_w5\n     11.14% friend_stress_w1\n     10.28% neg_esteem_w2\n      8.48% desire_stress_w1\n      7.88% higher_school_dependence_w1\n      6.77% desire_stress_w4\n      4.63% neg_esteem_w5\n      3.43% parent_monitoring_w3\n      1.11% neg_esteem_w4\n\n\nTime: 0.1 secs\n\n# C5.0 모델로 테스트 세트 예측\ncat(\"\\nC5.0 모델로 테스트 세트 예측 중...\\n\")\n\n\nC5.0 모델로 테스트 세트 예측 중...\n\ny_pred_c50 &lt;- predict(c50_model, X_test)\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_c50 &lt;- matrix(0,\n                                       nrow = length(levels(y_test)),\n                                       ncol = length(levels(y_test)),\n                                       dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_c50[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_c50[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_c50[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== C5.0 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== C5.0 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_c50, 2))\n\n               Predicted\nActual          comprehensive   modern\n  comprehensive      35812.23 20080.07\n  modern             21635.18 39859.37\n\n# C5.0 모델 가중 성능 지표 계산\ntotal_weighted_sum_c50 &lt;- sum(weighted_confusion_matrix_c50)\nweighted_accuracy_c50 &lt;- sum(diag(weighted_confusion_matrix_c50)) / total_weighted_sum_c50\ncat(\"\\n=== C5.0 모델 가중 성능 지표 ===\\n\")\n\n\n=== C5.0 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_c50, 4), \"\\n\")\n\n가중 정확도: 0.6446 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_c50 &lt;- numeric(length(levels(y_test)))\nweighted_recall_c50 &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_c50 &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_c50) &lt;- names(weighted_recall_c50) &lt;- names(weighted_f1_score_c50) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_c50[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_c50[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_c50[cat, ]) - TP\n\n  weighted_precision_c50[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_c50[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_c50[cat] &lt;- ifelse((weighted_precision_c50[cat] + weighted_recall_c50[cat]) == 0, 0,\n                                     2 * (weighted_precision_c50[cat] * weighted_recall_c50[cat]) / (weighted_precision_c50[cat] + weighted_recall_c50[cat]))\n}\n\ncat(\"\\nC5.0 모델 범주별 가중 정밀도:\\n\")\n\n\nC5.0 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_c50, 4))\n\ncomprehensive        modern \n       0.6234        0.6650 \n\ncat(\"\\nC5.0 모델 범주별 가중 재현율:\\n\")\n\n\nC5.0 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_c50, 4))\n\ncomprehensive        modern \n       0.6407        0.6482 \n\ncat(\"\\nC5.0 모델 범주별 가중 F1-score:\\n\")\n\n\nC5.0 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_c50, 4))\n\ncomprehensive        modern \n       0.6319        0.6565 \n\n# 의사결정나무 시각화\nplot(c50_model)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#xgboost-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#xgboost-모델",
    "title": "analysis",
    "section": "XGBoost 모델",
    "text": "XGBoost 모델\n\n# XGBoost 모델 학습 및 평가\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# 데이터 변환 (XGBoost는 DMatrix 형식을 사용)\n# 먼저 factor를 수치형으로 변환\ny_train_numeric &lt;- as.integer(y_train) - 1  # 0부터 시작하는 인덱스로 변환\ny_test_numeric &lt;- as.integer(y_test) - 1\n\n# XGBoost DMatrix 생성\ndtrain &lt;- xgb.DMatrix(data = as.matrix(X_train), label = y_train_numeric, weight = weights_train)\ndtest &lt;- xgb.DMatrix(data = as.matrix(X_test), label = y_test_numeric, weight = weights_test)\n\n# 모델 파라미터 설정\nxgb_params &lt;- list(\n  objective = \"multi:softprob\",\n  eval_metric = \"mlogloss\",\n  num_class = length(levels(y_train)),\n  eta = 0.3,               # 학습률\n  max_depth = 6,           # 트리 최대 깊이\n  min_child_weight = 1,    # 최소 자식 노드 가중치 합\n  subsample = 0.8,         # 샘플링 비율\n  colsample_bytree = 0.8   # 특성 샘플링 비율\n)\n\n# XGBoost 모델 학습\ncat(\"\\nXGBoost 모델 학습 중...\\n\")\n\n\nXGBoost 모델 학습 중...\n\nxgb_model &lt;- xgb.train(\n  params = xgb_params,\n  data = dtrain,\n  nrounds = 100,           # 부스팅 반복 횟수\n  verbose = 0\n)\ncat(\"XGBoost 모델 학습 완료.\\n\")\n\nXGBoost 모델 학습 완료.\n\n# 변수 중요도 확인\nxgb_importance &lt;- xgb.importance(model = xgb_model)\nif (nrow(xgb_importance) &gt; 0) {\n  cat(\"\\nXGBoost 변수 중요도 상위 10개:\\n\")\n  print(head(xgb_importance, 10))\n  xgb.plot.importance(xgb_importance, top_n = 10)\n} else {\n  cat(\"\\nXGBoost 변수 중요도를 계산할 수 없습니다.\\n\")\n}\n\n\nXGBoost 변수 중요도 상위 10개:\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:               neg_esteem_w3 0.03436108 0.04937609 0.02204071\n 2:            desire_stress_w5 0.03373390 0.04098259 0.02100350\n 3:        parent_attachment_w2 0.03328342 0.04020469 0.02372618\n 4:            parent_stress_w1 0.03260803 0.02385672 0.03241281\n 5:           deviant_esteem_w3 0.02937792 0.02680615 0.02087385\n 6:          self_confidence_w4 0.02872796 0.01522686 0.01931803\n 7:          academic_stress_w2 0.02852455 0.02791540 0.02178141\n 8:               neg_esteem_w4 0.02658745 0.02633690 0.01802152\n 9: higher_school_dependence_w3 0.02655540 0.01772446 0.02152211\n10:            desire_stress_w3 0.02627168 0.02060179 0.01737327\n\n\n\n\n\n\n\n\n# XGBoost 모델로 테스트 세트 예측\ncat(\"\\nXGBoost 모델로 테스트 세트 예측 중...\\n\")\n\n\nXGBoost 모델로 테스트 세트 예측 중...\n\nxgb_pred_probs &lt;- predict(xgb_model, dtest)\n# 예측 확률을 행렬로 변환 (각 클래스별 확률값)\nxgb_pred_probs_matrix &lt;- matrix(xgb_pred_probs, nrow = length(y_test), byrow = TRUE)\n# 가장 높은 확률을 가진 클래스를 예측값으로 선택\ny_pred_xgb_idx &lt;- apply(xgb_pred_probs_matrix, 1, which.max) - 1  # 0-based 인덱스\n# 다시 factor로 변환\ny_pred_xgb &lt;- factor(levels(y_test)[y_pred_xgb_idx + 1], levels = levels(y_test))\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_xgb &lt;- matrix(0,\n                                       nrow = length(levels(y_test)),\n                                       ncol = length(levels(y_test)),\n                                       dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_xgb[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_xgb[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_xgb[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== XGBoost 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== XGBoost 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_xgb, 2))\n\n               Predicted\nActual          comprehensive   modern\n  comprehensive      37288.77 18603.53\n  modern             18527.58 42966.97\n\n# XGBoost 모델 가중 성능 지표 계산\ntotal_weighted_sum_xgb &lt;- sum(weighted_confusion_matrix_xgb)\nweighted_accuracy_xgb &lt;- sum(diag(weighted_confusion_matrix_xgb)) / total_weighted_sum_xgb\ncat(\"\\n=== XGBoost 모델 가중 성능 지표 ===\\n\")\n\n\n=== XGBoost 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_xgb, 4), \"\\n\")\n\n가중 정확도: 0.6837 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_xgb &lt;- numeric(length(levels(y_test)))\nweighted_recall_xgb &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_xgb &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_xgb) &lt;- names(weighted_recall_xgb) &lt;- names(weighted_f1_score_xgb) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_xgb[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_xgb[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_xgb[cat, ]) - TP\n\n  weighted_precision_xgb[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_xgb[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_xgb[cat] &lt;- ifelse((weighted_precision_xgb[cat] + weighted_recall_xgb[cat]) == 0, 0,\n                                     2 * (weighted_precision_xgb[cat] * weighted_recall_xgb[cat]) / (weighted_precision_xgb[cat] + weighted_recall_xgb[cat]))\n}\n\ncat(\"\\nXGBoost 모델 범주별 가중 정밀도:\\n\")\n\n\nXGBoost 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_xgb, 4))\n\ncomprehensive        modern \n       0.6681        0.6979 \n\ncat(\"\\nXGBoost 모델 범주별 가중 재현율:\\n\")\n\n\nXGBoost 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_xgb, 4))\n\ncomprehensive        modern \n       0.6672        0.6987 \n\ncat(\"\\nXGBoost 모델 범주별 가중 F1-score:\\n\")\n\n\nXGBoost 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_xgb, 4))\n\ncomprehensive        modern \n       0.6676        0.6983",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#로지스틱-회귀-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#로지스틱-회귀-모델",
    "title": "analysis",
    "section": "로지스틱 회귀 모델",
    "text": "로지스틱 회귀 모델\n\n# 다항 로지스틱 회귀 모델 학습 및 평가\nlibrary(nnet)\n\n# 다항 로지스틱 회귀 모델 학습 (가중치 적용)\ncat(\"\\n다항 로지스틱 회귀 모델 학습 중...\\n\")\n\n\n다항 로지스틱 회귀 모델 학습 중...\n\n# X_train에 열 이름이 없으면 추가\nif(is.null(colnames(X_train))) {\n  colnames(X_train) &lt;- paste0(\"V\", 1:ncol(X_train))\n}\n\n# 학습 데이터를 데이터프레임으로 변환\ntrain_df &lt;- as.data.frame(X_train)\ntrain_df$y &lt;- y_train\n\n# 다항 로지스틱 회귀 모델 학습\nlogistic_model &lt;- multinom(\n  y ~ .,\n  data = train_df,\n  weights = weights_train,\n  trace = FALSE\n)\ncat(\"다항 로지스틱 회귀 모델 학습 완료.\\n\")\n\n다항 로지스틱 회귀 모델 학습 완료.\n\n# 모델 요약\nprint(summary(logistic_model))\n\nCall:\nmultinom(formula = y ~ ., data = train_df, weights = weights_train, \n    trace = FALSE)\n\nCoefficients:\n                                  Values   Std. Err.\n(Intercept)                  0.129937521 0.004274993\nparent_attachment_w1        -0.075431148 0.012699368\ndeviant_esteem_w1            0.025772522 0.010036022\nparent_stress_w1            -0.204385210 0.010813008\nparent_monitoring_w1         0.007828738 0.008344843\ndesire_stress_w1             0.029786294 0.008852236\nfriend_stress_w1            -0.028301859 0.007019766\nself_confidence_w1           0.063169793 0.009911519\nhigher_school_dependence_w1 -0.189224386 0.005044096\nneg_esteem_w1                0.250302067 0.010505632\nacademic_stress_w1          -0.106075165 0.011092393\nparent_attachment_w2         0.410296565 0.013068651\ndeviant_esteem_w2            0.057094229 0.008540524\nparent_stress_w2            -0.161110622 0.011540879\nparent_monitoring_w2        -0.061044031 0.009144599\ndesire_stress_w2            -0.100611091 0.008643914\nfriend_stress_w2             0.060104571 0.007913410\nself_confidence_w2           0.172905358 0.008876402\nhigher_school_dependence_w2  0.065779272 0.005164883\nneg_esteem_w2                0.001125654 0.007907155\nacademic_stress_w2           0.098940207 0.010907271\nparent_attachment_w3         0.273963992 0.013838182\ndeviant_esteem_w3           -0.299907553 0.010853418\nparent_stress_w3             0.010691071 0.011566791\nparent_monitoring_w3        -0.046637682 0.008791949\ndesire_stress_w3            -0.160811494 0.009118316\nfriend_stress_w3            -0.091024703 0.008773019\nself_confidence_w3          -0.055330768 0.009514467\nhigher_school_dependence_w3  0.038618447 0.005491823\nneg_esteem_w3               -0.193511778 0.009270965\nacademic_stress_w3           0.185696060 0.009279961\nparent_attachment_w4        -0.201723746 0.013984926\ndeviant_esteem_w4           -0.196968613 0.010263102\nparent_stress_w4             0.022506202 0.012580939\nparent_monitoring_w4        -0.037052255 0.009196293\ndesire_stress_w4            -0.029278469 0.008370183\nfriend_stress_w4             0.306096759 0.008882420\nself_confidence_w4          -0.084552728 0.010543784\nhigher_school_dependence_w4  0.141331777 0.006305001\nneg_esteem_w4               -0.133839167 0.010102706\nacademic_stress_w4          -0.294009079 0.010089440\nparent_attachment_w5         0.221914282 0.012869900\ndeviant_esteem_w5           -0.134020459 0.009917373\nparent_stress_w5             0.060615905 0.011846193\nparent_monitoring_w5         0.012658005 0.008815701\ndesire_stress_w5            -0.296661216 0.008531414\nfriend_stress_w5            -0.043547774 0.008416419\nself_confidence_w5          -0.111579807 0.009270452\nhigher_school_dependence_w5  0.022073123 0.006012592\nneg_esteem_w5               -0.239088128 0.010261765\nacademic_stress_w5           0.053765991 0.009180103\n\nResidual Deviance: 337026.9 \nAIC: 337128.9 \n\n# 테스트 세트 예측\ncat(\"\\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\\n\")\n\n\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\n\n# X_test에 열 이름이 없으면 추가\nif(is.null(colnames(X_test))) {\n  colnames(X_test) &lt;- paste0(\"V\", 1:ncol(X_test))\n}\ny_pred_logistic &lt;- predict(logistic_model, newdata = as.data.frame(X_test))\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_logistic &lt;- matrix(0,\n                                           nrow = length(levels(y_test)),\n                                           ncol = length(levels(y_test)),\n                                           dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_logistic[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_logistic[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_logistic[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_logistic, 2))\n\n               Predicted\nActual          comprehensive   modern\n  comprehensive      34888.28 21004.01\n  modern             19071.38 42423.17\n\n# 다항 로지스틱 회귀 모델 가중 성능 지표 계산\ntotal_weighted_sum_logistic &lt;- sum(weighted_confusion_matrix_logistic)\nweighted_accuracy_logistic &lt;- sum(diag(weighted_confusion_matrix_logistic)) / total_weighted_sum_logistic\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\\n\")\n\n\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_logistic, 4), \"\\n\")\n\n가중 정확도: 0.6586 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_logistic &lt;- numeric(length(levels(y_test)))\nweighted_recall_logistic &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_logistic &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_logistic) &lt;- names(weighted_recall_logistic) &lt;- names(weighted_f1_score_logistic) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_logistic[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_logistic[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_logistic[cat, ]) - TP\n\n  weighted_precision_logistic[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_logistic[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_logistic[cat] &lt;- ifelse((weighted_precision_logistic[cat] + weighted_recall_logistic[cat]) == 0, 0,\n                                          2 * (weighted_precision_logistic[cat] * weighted_recall_logistic[cat]) / (weighted_precision_logistic[cat] + weighted_recall_logistic[cat]))\n}\n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_logistic, 4))\n\ncomprehensive        modern \n       0.6466        0.6688 \n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_logistic, 4))\n\ncomprehensive        modern \n       0.6242        0.6899 \n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_logistic, 4))\n\ncomprehensive        modern \n       0.6352        0.6792",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#최종-모델-성능-비교",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#최종-모델-성능-비교",
    "title": "analysis",
    "section": "최종 모델 성능 비교",
    "text": "최종 모델 성능 비교\n\n# 모든 모델의 가중 정확도 비교\nfinal_metrics &lt;- data.frame(\n  Metric = c(\"가중 정확도\"),\n  RandomForest = c(round(weighted_accuracy_tuned * 100, 2)),\n  # C50_Tree = c(round(weighted_accuracy_c50 * 100, 2)),\n  XGBoost = c(round(weighted_accuracy_xgb * 100, 2)),\n  # SVM = c(round(weighted_accuracy_svm * 100, 2)),\n  # KNN = c(round(weighted_accuracy_knn * 100, 2)),\n  LogisticRegression = c(round(weighted_accuracy_logistic * 100, 2))\n)\n\n# 결과 출력\ncat(\"\\n=== 최종 모델 가중 정확도 비교 ===\\n\")\n\n\n=== 최종 모델 가중 정확도 비교 ===\n\nprint(kable(final_metrics, caption = \"원본 모델 vs 튜닝 모델 가중 정확도 비교\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 원본 모델 vs 튜닝 모델 가중 정확도 비교\n\n|Metric      | RandomForest| XGBoost| LogisticRegression|\n|:-----------|------------:|-------:|------------------:|\n|가중 정확도 |        72.62|   68.37|              65.86|\n\n# 클래스별 가중 성능 지표 (튜닝된 모델 결과 사용 권장)\ncat(\"\\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\\n\")\n\n\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\n\nclass_weighted_metrics_tuned &lt;- data.frame(\n  Class = levels(y_test), # y_test 또는 actual_classes_tuned 사용 가능\n  Weighted_Precision = round(weighted_precision_tuned, 4),\n  Weighted_Recall = round(weighted_recall_tuned, 4),\n  Weighted_F1 = round(weighted_f1_score_tuned, 4)\n)\nprint(kable(class_weighted_metrics_tuned, caption = \"튜닝된 모델 클래스별 가중 성능 지표\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 튜닝된 모델 클래스별 가중 성능 지표\n\n|              |Class         | Weighted_Precision| Weighted_Recall| Weighted_F1|\n|:-------------|:-------------|------------------:|---------------:|-----------:|\n|comprehensive |comprehensive |             0.7151|          0.7063|      0.7107|\n|modern        |modern        |             0.7360|          0.7442|      0.7401|\n\n# 최종 변수 중요도 요약 (튜닝된 모델 결과 사용)\ncat(\"\\n=== 튜닝된 모델 변수 중요도 ===\\n\")\n\n\n=== 튜닝된 모델 변수 중요도 ===\n\nprint(kable(head(importance_df_tuned, 10), caption = \"튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\n\n|   |Variable             | Importance|\n|:--|:--------------------|----------:|\n|22 |deviant_esteem_w3    |  100.00000|\n|29 |neg_esteem_w3        |   93.11819|\n|39 |neg_esteem_w4        |   88.08129|\n|45 |desire_stress_w5     |   82.49042|\n|25 |desire_stress_w3     |   75.75727|\n|11 |parent_attachment_w2 |   74.65780|\n|42 |deviant_esteem_w5    |   71.64152|\n|49 |neg_esteem_w5        |   70.53575|\n|10 |academic_stress_w1   |   69.97093|\n|15 |desire_stress_w2     |   67.24023|",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#회귀-모델-구축-및-평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#회귀-모델-구축-및-평가",
    "title": "test",
    "section": "회귀 모델 구축 및 평가",
    "text": "회귀 모델 구축 및 평가\n\n# 1. 선형 회귀 모델\nlm_model &lt;- train(\n  academic_stress_w6 ~ .,\n  data = train_data,\n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 5)\n)\n\n# 2. 랜덤 포레스트 회귀 모델\nrf_model &lt;- train(\n  academic_stress_w6 ~ .,\n  data = train_data,\n  method = \"rf\",\n  trControl = trainControl(method = \"cv\", number = 5),\n  importance = TRUE\n)\n\n# 3. Gradient Boosting Machine\ngbm_model &lt;- train(\n  academic_stress_w6 ~ .,\n  data = train_data,\n  method = \"gbm\",\n  trControl = trainControl(method = \"cv\", number = 5),\n  verbose = FALSE\n)\n\n# 모델 성능 평가 - 테스트 데이터\nlm_pred &lt;- predict(lm_model, newdata = test_data)\nrf_pred &lt;- predict(rf_model, newdata = test_data)\ngbm_pred &lt;- predict(gbm_model, newdata = test_data)\n\n# RMSE, R² 계산\ncalculate_metrics &lt;- function(actual, predicted) {\n  rmse &lt;- sqrt(mean((actual - predicted)^2))\n  rsq &lt;- cor(actual, predicted)^2\n  mae &lt;- mean(abs(actual - predicted))\n  \n  return(c(RMSE = rmse, R_squared = rsq, MAE = mae))\n}\n\n# 모델별 성능 지표 계산\nlm_metrics &lt;- calculate_metrics(test_data$academic_stress_w6, lm_pred)\nrf_metrics &lt;- calculate_metrics(test_data$academic_stress_w6, rf_pred)\ngbm_metrics &lt;- calculate_metrics(test_data$academic_stress_w6, gbm_pred)\n\n# 결과 요약 테이블\nperformance_results &lt;- data.frame(\n  Model = c(\"선형 회귀\", \"랜덤 포레스트\", \"Gradient Boosting\"),\n  RMSE = c(lm_metrics[\"RMSE\"], rf_metrics[\"RMSE\"], gbm_metrics[\"RMSE\"]),\n  R_squared = c(lm_metrics[\"R_squared\"], rf_metrics[\"R_squared\"], gbm_metrics[\"R_squared\"]),\n  MAE = c(lm_metrics[\"MAE\"], rf_metrics[\"MAE\"], gbm_metrics[\"MAE\"])\n)\n\n# 결과 테이블 출력\nkable(performance_results, \n      caption = \"회귀 모델 성능 비교\",\n      digits = 4)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n회귀 모델 성능 비교\n\n\nModel\nRMSE\nR_squared\nMAE\n\n\n\n\n선형 회귀\n1.2044\n0.1662\n0.7458\n\n\n랜덤 포레스트\n1.1992\n0.1753\n0.7274\n\n\nGradient Boosting\n1.1891\n0.1870\n0.7280\n\n\n\n\n# 가장 성능이 좋은 모델 확인\nbest_model_idx &lt;- which.min(performance_results$RMSE)\ncat(\"가장 우수한 모델:\", performance_results$Model[best_model_idx], \n    \"\\nRMSE:\", round(performance_results$RMSE[best_model_idx], 4), \n    \"\\nR²:\", round(performance_results$R_squared[best_model_idx], 4), \"\\n\")\n\n가장 우수한 모델: Gradient Boosting \nRMSE: 1.1891 \nR²: 0.187 \n\n# 변수 중요도 시각화 (랜덤 포레스트 기준)\nif (best_model_idx == 2) {\n  # 랜덤 포레스트 모델의 변수 중요도\n  var_imp &lt;- varImp(rf_model)\n  \n  # 상위 10개 변수만 추출하여 시각화\n  top_vars &lt;- var_imp$importance %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(\"Variable\") %&gt;%\n    arrange(desc(Overall)) %&gt;%\n    top_n(10)\n  \n  ggplot(top_vars, aes(x = reorder(Variable, Overall), y = Overall)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\") +\n    coord_flip() +\n    labs(title = \"학업 스트레스 예측에 중요한 상위 10개 변수\",\n         x = \"변수\",\n         y = \"중요도\") +\n    theme_minimal()\n}\n\n# 실제 값과 예측 값 산점도\nbest_pred &lt;- switch(best_model_idx,\n                    lm_pred,\n                    rf_pred,\n                    gbm_pred)\n\n# q11 값을 test_data에 추가하기 위해 원본 데이터와 병합\ntest_data_with_q11 &lt;- test_data\n\n# test_data에 포함된 id와 df6_origin의 id를 매칭하여 q11 값 추가\ntest_data_with_q11 &lt;- merge(test_data, df6_origin[, c(\"id\", \"q11\")], by.x = \"row.names\", by.y = \"id\", all.x = TRUE)\nrownames(test_data_with_q11) &lt;- test_data_with_q11$Row.names\ntest_data_with_q11$Row.names &lt;- NULL\n\n# 가장 우수한 모델의 예측값과 실제값 비교 플롯\nplot_df &lt;- data.frame(\n  Actual = test_data_with_q11$academic_stress_w6,\n  Predicted = best_pred,\n  Q11 = as.factor(test_data_with_q11$q11)\n)\n\n# Q11에 결측치가 있는 경우 \"NA\" 라벨 지정\nplot_df$Q11[is.na(plot_df$Q11)] &lt;- \"결측값\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, is.na(plot_df$Q11), value = structure(c(1L, :\ninvalid factor level, NA generated\n\n# Q11 요인 레벨에 라벨 추가\nq11_labels &lt;- c(\n  \"7\" = \"취업 중\",\n  \"8\" = \"창업 중\",\n  \"9\" = \"가업 중\",\n  \"71\" = \"취업 + 대학\",\n  \"81\" = \"창업 + 대학\",\n  \"91\" = \"가업 + 대학\",\n  \"10\" = \"고등 졸업 후 취업 준비 중\",\n  \"101\" = \"대학 자퇴 후 취업 준비 중\",\n  \"11\" = \"고등 졸업 후 창업 준비 중\",\n  \"111\" = \"대학 자퇴 후 창업 준비 중\",\n  \"12\" = \"육아중\",\n  \"13\" = \"군대 준비중\",\n  \"14\" = \"아무것도 안함\"\n)\n\n# 실제 q11 값만 고려하여 레이블 생성\navailable_levels &lt;- intersect(names(q11_labels), levels(plot_df$Q11))\nplot_df$Q11 &lt;- factor(plot_df$Q11, levels = available_levels, labels = q11_labels[available_levels])\n\n# 색상 팔레트 설정 (q11 값 수에 맞게)\nnum_colors &lt;- length(unique(plot_df$Q11))\nmy_palette &lt;- colorRampPalette(c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \n                                 \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"))(num_colors)\n\n# 산점도 그리기 - q11 값으로 그룹화\nggplot(plot_df, aes(x = Actual, y = Predicted, color = Q11)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linetype = \"dashed\") +\n  labs(title = \"실제값 vs 예측값 비교 (q11 그룹별)\",\n       x = \"실제 학업 스트레스 값\",\n       y = \"예측 학업 스트레스 값\",\n       color = \"6차 웨이브 상태(q11)\") +\n  theme_minimal() +\n  annotate(\"text\", x = min(plot_df$Actual), y = max(plot_df$Predicted),\n           label = paste(\"R² =\", round(performance_results$R_squared[best_model_idx], 4)),\n           hjust = 0, vjust = 1, color = \"black\") +\n  scale_color_manual(values = my_palette) +\n  theme(legend.position = \"right\",\n        legend.title = element_text(face = \"bold\"),\n        plot.title = element_text(hjust = 0.5, face = \"bold\"))",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#잔차-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#잔차-분석",
    "title": "test",
    "section": "잔차 분석",
    "text": "잔차 분석\n\n# 가장 좋은 모델의 잔차 분석\nbest_model &lt;- switch(best_model_idx,\n                     lm_model,\n                     rf_model,\n                     gbm_model)\n\n# 선형 회귀 모델인 경우에만 자세한 잔차 분석 수행\nif (best_model_idx == 1) {\n  par(mfrow = c(2, 2))\n  plot(best_model$finalModel)\n  par(mfrow = c(1, 1))\n  \n  # 표준화 잔차 히스토그램\n  residuals &lt;- resid(best_model$finalModel)\n  std_resid &lt;- residuals / sd(residuals)\n  \n  ggplot(data.frame(std_resid), aes(x = std_resid)) +\n    geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n    labs(title = \"표준화 잔차 히스토그램\",\n         x = \"표준화 잔차\",\n         y = \"빈도\") +\n    theme_minimal()\n} else {\n  # 비선형 모델의 경우 예측값과 잔차 관계 플롯\n  resid_df &lt;- data.frame(\n    Predicted = best_pred,\n    Residuals = test_data$academic_stress_w6 - best_pred\n  )\n  \n  ggplot(resid_df, aes(x = Predicted, y = Residuals)) +\n    geom_point(alpha = 0.5) +\n    geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n    labs(title = \"예측값 vs 잔차\",\n         x = \"예측 학업 스트레스 값\",\n         y = \"잔차 (실제 - 예측)\") +\n    theme_minimal()\n  \n  # 잔차 히스토그램\n  ggplot(resid_df, aes(x = Residuals)) +\n    geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n    labs(title = \"잔차 히스토그램\",\n         x = \"잔차\",\n         y = \"빈도\") +\n    theme_minimal()\n}",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#결론",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#결론",
    "title": "test",
    "section": "결론",
    "text": "결론\n위 분석을 통해 1~5차 웨이브의 요인 점수들을 활용하여 6차 웨이브의 학업 스트레스(academic_stress_w6)를 예측하는 모델을 구축했습니다. 여러 모델 중에서 가장 성능이 좋은 모델을 선택하여 예측 정확도를 평가했으며, 변수 중요도를 통해 학업 스트레스를 예측하는 데 가장 큰 영향을 미치는 요인들을 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "test"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#data-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#data-load",
    "title": "analysis",
    "section": "data load",
    "text": "data load\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survey)\n\nLoading required package: grid\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nlibrary(semPlot)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nset.seed(123)\n\ntrain_data &lt;- read.csv(\"_data/train_set.csv\")\ntest_data &lt;- read.csv(\"_data/test_set.csv\")\n\nX_train &lt;- train_data[, !(names(train_data) %in% c(\"id\", \"y\", \"weight\"))]\nX_test &lt;- test_data[, !(names(test_data) %in% c(\"id\", \"y\", \"weight\"))]\ny_train &lt;- factor(train_data$y)\ny_test &lt;- factor(test_data$y)\nweights_train &lt;- train_data$weight\nweights_test&lt;- test_data$weight",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "python code"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#random-forest-모델-학습-및-평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#random-forest-모델-학습-및-평가",
    "title": "analysis",
    "section": "Random Forest 모델 학습 및 평가",
    "text": "Random Forest 모델 학습 및 평가\n\nrf_model &lt;- randomForest(\n  x = X_train,\n  y = y_train,\n  weights = weights_train,\n  ntree = 500,\n  mtry = floor(sqrt(ncol(X_train))),\n  importance = TRUE,\n  sampsize = table(y_train),\n  replace = TRUE\n)\nvarImpPlot(rf_model)\n\n\n\n\n\n\n\ny_pred &lt;- predict(rf_model, X_test)\nactual_classes &lt;- y_test\npredicted_classes &lt;- y_pred\ntest_weights &lt;- weights_test\noutcome_levels &lt;- levels(actual_classes)\nweighted_confusion_matrix &lt;- matrix(0,\n                                   nrow = length(outcome_levels),\n                                   ncol = length(outcome_levels),\n                                   dimnames = list(Actual = outcome_levels, Predicted = outcome_levels))\nfor (i in 1:length(actual_classes)) {\n  actual_cat &lt;- as.character(actual_classes[i])\n  predicted_cat &lt;- as.character(predicted_classes[i])\n  weight &lt;- test_weights[i]\n  weighted_confusion_matrix[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix[actual_cat, predicted_cat] + weight\n}\nprint(round(weighted_confusion_matrix, 2))\n\n         Predicted\nActual      active  passive\n  active  89806.77 25964.74\n  passive 37069.58 21501.58\n\ntotal_weighted_sum &lt;- sum(weighted_confusion_matrix)\nweighted_accuracy &lt;- sum(diag(weighted_confusion_matrix)) / total_weighted_sum\ncat(\"\\n=== 가중 성능 지표 ===\\n\")\n\n\n=== 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy, 4), \"\\n\")\n\n가중 정확도: 0.6384 \n\nweighted_precision &lt;- numeric(length(outcome_levels))\nweighted_recall &lt;- numeric(length(outcome_levels))\nweighted_f1_score &lt;- numeric(length(outcome_levels))\nnames(weighted_precision) &lt;- names(weighted_recall) &lt;- names(weighted_f1_score) &lt;- outcome_levels\n\nfor (cat in outcome_levels) {\n  TP &lt;- weighted_confusion_matrix[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix[, cat]) - TP # 해당 열의 합 - TP\n  FN &lt;- sum(weighted_confusion_matrix[cat, ]) - TP # 해당 행의 합 - TP\n  weighted_precision[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score[cat] &lt;- ifelse((weighted_precision[cat] + weighted_recall[cat]) == 0, 0,\n                                    2 * (weighted_precision[cat] * weighted_recall[cat]) / (weighted_precision[cat] + weighted_recall[cat]))\n}\ncat(\"\\n범주별 가중 정밀도:\\n\")\n\n\n범주별 가중 정밀도:\n\nprint(round(weighted_precision, 4))\n\n active passive \n 0.7078  0.4530 \n\ncat(\"\\n범주별 가중 재현율:\\n\")\n\n\n범주별 가중 재현율:\n\nprint(round(weighted_recall, 4))\n\n active passive \n 0.7757  0.3671 \n\ncat(\"\\n범주별 가중 F1-score:\\n\")\n\n\n범주별 가중 F1-score:\n\nprint(round(weighted_f1_score, 4))\n\n active passive \n 0.7402  0.4055 \n\ny_pred_prob &lt;- predict(rf_model, X_test, type = \"prob\")\ncat(\"\\n비가중 ROC 곡선 (참고용):\\n\")\n\n\n비가중 ROC 곡선 (참고용):\n\nplot_roc &lt;- function() {\n  num_classes &lt;- length(levels(y_test))\n  if (num_classes &lt;= 4 && num_classes &gt; 0) { # 범주가 1개 이하이면 플롯 불가능\n    par(mfrow = c(2, ceiling(num_classes/2)))\n  } else if (num_classes &gt; 4) {\n    par(mfrow = c(2, 2)) # 범주가 많으면 일부만 표시하거나 레이아웃 조정 필요\n    warning(\"범주가 4개 이상입니다. 일부 ROC 곡선만 표시될 수 있습니다. 레이아웃을 조정하거나 플롯 코드 를 수정하세요.\")\n  } else {\n    cat(\"ROC 곡선을 그릴 범주가 부족합니다.\\n\")\n    return(numeric(0)) # 빈 numeric 반환\n  }\n  auc_values &lt;- numeric(num_classes)\n  names(auc_values) &lt;- levels(y_test)\n  for (i in 1:num_classes) {\n    class_label &lt;- levels(y_test)[i]\n    if(class_label %in% unique(y_test) && class_label %in% colnames(y_pred_prob)) {\n      roc_obj &lt;- roc(response = ifelse(y_test == class_label, 1, 0), predictor = y_pred_prob[, class_label])\n      auc_values[i] &lt;- auc(roc_obj)\n      plot(roc_obj, main = paste(\"ROC for\", class_label, \"(Unweighted)\"), col = \"blue\", lwd = 2)\n      abline(a = 0, b = 1, lty = 2, col = \"gray\")\n      text(0.5, 0.3, paste(\"AUC =\", round(auc_values[i], 3)), col = \"red\")\n      } else {\n        cat(\"클래스\", class_label, \"에 대한 ROC 곡선을 그릴 수 없습니다 (데이터 부족).\\n\")\n        auc_values[i] &lt;- NA # AUC 값에 NA 할당\n        plot.new() # 빈 플롯 생성\n        text(0.5, 0.5, paste(\"No ROC for\", class_label))\n      }\n  }\n  par(mfrow = c(1, 1))\n  return(auc_values)\n}\nauc_values_unweighted &lt;- plot_roc()\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\ncat(\"\\n각 클래스별 비가중 AUC:\\n\")\n\n\n각 클래스별 비가중 AUC:\n\nprint(data.frame(Class = levels(y_test), AUC_Unweighted = auc_values_unweighted))\n\n          Class AUC_Unweighted\nactive   active      0.6220194\npassive passive      0.6220194",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#모델-튜닝",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#모델-튜닝",
    "title": "analysis",
    "section": "모델 튜닝",
    "text": "모델 튜닝\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  savePredictions = \"final\",\n  classProbs = TRUE,\n  summaryFunction = multiClassSummary\n)\nparam_grid &lt;- expand.grid(\n  mtry = floor(sqrt(ncol(X_train)))\n)\nif (ncol(X_train) == 1) {\n  param_grid &lt;- expand.grid(mtry = 1)\n}\nrf_tuned &lt;- train(\n  x = X_train,\n  y = y_train,\n  method = \"rf\",\n  metric = \"Accuracy\",\n  weights = weights_train,\n  trControl = ctrl,\n  tuneGrid = param_grid,\n  importance = TRUE,\n  ntree = 500\n)\nprint(rf_tuned)\n\nRandom Forest \n\n3412 samples\n  51 predictor\n   2 classes: 'active', 'passive' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2730, 2730, 2730, 2729, 2729 \nResampling results:\n\n  logLoss    AUC       prAUC      Accuracy   Kappa      F1         Sensitivity\n  0.6219635  0.740729  0.7297194  0.6787714  0.3564413  0.6590399  0.6340299  \n  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision  Recall   \n  0.7219467    0.6875098       0.6726023       0.6875098  0.6340299\n  Detection_Rate  Balanced_Accuracy\n  0.311242        0.6779883        \n\nTuning parameter 'mtry' was held constant at a value of 7\n\ny_pred_tuned &lt;- predict(rf_tuned, X_test)\nactual_classes_tuned &lt;- y_test\npredicted_classes_tuned &lt;- y_pred_tuned\ntest_weights_tuned &lt;- weights_test\nweighted_confusion_matrix_tuned &lt;- matrix(0,\n                                         nrow = length(levels(actual_classes_tuned)),\n                                         ncol = length(levels(actual_classes_tuned)),\n                                         dimnames = list(Actual = levels(actual_classes_tuned), Predicted = levels(actual_classes_tuned)))\n\nfor (i in 1:length(actual_classes_tuned)) {\n  actual_cat &lt;- as.character(actual_classes_tuned[i])\n  predicted_cat &lt;- as.character(predicted_classes_tuned[i])\n  weight &lt;- test_weights_tuned[i]\n   if (actual_cat %in% levels(actual_classes_tuned) && predicted_cat %in% levels(actual_classes_tuned)) {\n      weighted_confusion_matrix_tuned[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_tuned[actual_cat, predicted_cat] + weight\n  } else {\n      warning(paste(\"튜닝 모델: 유효하지 않은 범주 발견: 실제 =\", actual_cat, \", 예측 =\", predicted_cat, \"인 개체 (인덱스:\", i, \")\"))\n  }\n}\ncat(\"\\n=== 튜닝된 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== 튜닝된 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_tuned, 2)) # 보기 좋게 반올림하여 출력\n\n         Predicted\nActual      active  passive\n  active  89635.01 26136.50\n  passive 37763.88 20807.29\n\ntotal_weighted_sum_tuned &lt;- sum(weighted_confusion_matrix_tuned)\nweighted_accuracy_tuned &lt;- sum(diag(weighted_confusion_matrix_tuned)) / total_weighted_sum_tuned\ncat(\"\\n=== 튜닝된 모델 가중 성능 지표 ===\\n\")\n\n\n=== 튜닝된 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_tuned, 4), \"\\n\")\n\n가중 정확도: 0.6335 \n\nweighted_precision_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nweighted_recall_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nweighted_f1_score_tuned &lt;- numeric(length(levels(actual_classes_tuned)))\nnames(weighted_precision_tuned) &lt;- names(weighted_recall_tuned) &lt;- names(weighted_f1_score_tuned) &lt;- levels(actual_classes_tuned)\nfor (cat in levels(actual_classes_tuned)) {\n  TP &lt;- weighted_confusion_matrix_tuned[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_tuned[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_tuned[cat, ]) - TP\n\n  weighted_precision_tuned[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_tuned[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_tuned[cat] &lt;- ifelse((weighted_precision_tuned[cat] + weighted_recall_tuned[cat]) == 0, 0,\n                                    2 * (weighted_precision_tuned[cat] * weighted_recall_tuned[cat]) / (weighted_precision_tuned[cat] + weighted_recall_tuned[cat]))\n}\n\ncat(\"\\n튜닝된 모델 범주별 가중 정밀도:\\n\")\n\n\n튜닝된 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_tuned, 4))\n\n active passive \n 0.7036  0.4432 \n\ncat(\"\\n튜닝된 모델 범주별 가중 재현율:\\n\")\n\n\n튜닝된 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_tuned, 4))\n\n active passive \n 0.7742  0.3552 \n\ncat(\"\\n튜닝된 모델 범주별 가중 F1-score:\\n\")\n\n\n튜닝된 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_tuned, 4))\n\n active passive \n 0.7372  0.3944 \n\nimportance_obj_tuned &lt;- varImp(rf_tuned)\n\nif(\"importance\" %in% names(importance_obj_tuned)) {\n  imp_df_tuned &lt;- importance_obj_tuned$importance\n} else {\n  imp_df_tuned &lt;- importance_obj_tuned\n}\n\nvar_names_tuned &lt;- rownames(imp_df_tuned)\nif(\"Overall\" %in% colnames(imp_df_tuned)) {\n  var_importance_tuned &lt;- imp_df_tuned$Overall\n} else {\n  var_importance_tuned &lt;- imp_df_tuned[,1]\n}\n\nimportance_df_tuned &lt;- data.frame(\n  Variable = var_names_tuned,\n  Importance = var_importance_tuned\n)\nimportance_df_tuned &lt;- importance_df_tuned[order(importance_df_tuned$Importance, decreasing = TRUE),]\n\n# 상위 10개 변수 출력 (튜닝된 모델)\ntop_vars_tuned &lt;- head(importance_df_tuned$Variable, 10)\ncat(\"\\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\\n\")\n\n\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\n\nprint(top_vars_tuned)\n\n [1] \"parent_monitoring_w5\" \"parent_attachment_w5\" \"friend_stress_w4\"    \n [4] \"deviant_esteem_w3\"    \"academic_stress_w4\"   \"friend_stress_w2\"    \n [7] \"parent_monitoring_w3\" \"parent_monitoring_w2\" \"parent_attachment_w4\"\n[10] \"parent_attachment_w2\"",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#c50-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#c50-모델",
    "title": "analysis",
    "section": "C50 모델",
    "text": "C50 모델\n\n# C5.0 의사결정나무 모델 학습 및 평가\nlibrary(C50)\nc50_model &lt;- C5.0(x = X_train, y = y_train, weights = weights_train, trials = 10)\ncat(\"C5.0 모델 학습 완료.\\n\")\n\nC5.0 모델 학습 완료.\n\n# 모델 요약\nprint(summary(c50_model))\n\n\nCall:\nC5.0.default(x = X_train, y = y_train, trials = 10, weights = weights_train)\n\n\nC5.0 [Release 2.07 GPL Edition]     Fri May 23 11:10:09 2025\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 3412 cases (52 attributes) from undefined.data\nUsing relative case weighting\n\n-----  Trial 0:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -0.8597915:\n:...parent_monitoring_w5 &lt;= -2.613473: passive (30.5)\n:   parent_monitoring_w5 &gt; -2.613473:\n:   :...parent_attachment_w4 &lt;= 0.4057779: passive (574.4/192.6)\n:       parent_attachment_w4 &gt; 0.4057779:\n:       :...parent_attachment_w3 &lt;= -0.9486434: passive (8.2)\n:           parent_attachment_w3 &gt; -0.9486434: active (103.2/42.2)\nparent_monitoring_w5 &gt; -0.8597915:\n:...friend_stress_w4 &gt; 1.021761:\n    :...friend_stress_w3 &lt;= -0.4561177:\n    :   :...academic_stress_w5 &lt;= -1.610173: passive (6.2)\n    :   :   academic_stress_w5 &gt; -1.610173:\n    :   :   :...academic_stress_w3 &lt;= 1.547277: active (49.6/12.8)\n    :   :       academic_stress_w3 &gt; 1.547277: passive (5.7)\n    :   friend_stress_w3 &gt; -0.4561177:\n    :   :...neg_esteem_w3 &gt; 1.648547: passive (16.6)\n    :       neg_esteem_w3 &lt;= 1.648547:\n    :       :...parent_stress_w2 &gt; 0.07839249:\n    :           :...parent_attachment_w4 &lt;= -1.194936: active (8.4/1.7)\n    :           :   parent_attachment_w4 &gt; -1.194936: passive (128.9/24.5)\n    :           parent_stress_w2 &lt;= 0.07839249:\n    :           :...neg_esteem_w1 &lt;= -0.8738497: passive (11.2)\n    :               neg_esteem_w1 &gt; -0.8738497:\n    :               :...parent_attachment_w3 &lt;= -1.149211: passive (12.5)\n    :                   parent_attachment_w3 &gt; -1.149211:\n    :                   :...parent_monitoring_w5 &lt;= 1.617113: active (84.1/34.1)\n    :                       parent_monitoring_w5 &gt; 1.617113: passive (5.8)\n    friend_stress_w4 &lt;= 1.021761:\n    :...desire_stress_w4 &lt;= -1.689063: passive (103.6/30.3)\n        desire_stress_w4 &gt; -1.689063:\n        :...deviant_esteem_w5 &lt;= -1.41281:\n            :...parent_stress_w4 &lt;= 0.8072249: passive (32/2.9)\n            :   parent_stress_w4 &gt; 0.8072249: active (3.8)\n            deviant_esteem_w5 &gt; -1.41281:\n            :...academic_stress_w4 &lt;= -0.7744552:\n                :...parent_attachment_w2 &lt;= -0.9711138: passive (55.5/9)\n                :   parent_attachment_w2 &gt; -0.9711138:\n                :   :...higher_school_dependence_w3 &lt;= 1.547238: passive (381.2/170.3)\n                :       higher_school_dependence_w3 &gt; 1.547238: active (23.8/3.9)\n                academic_stress_w4 &gt; -0.7744552:\n                :...parent_attachment_w3 &lt;= 0.03145359:\n                    :...neg_esteem_w1 &gt; 0.3162743: active (324.1/118.5)\n                    :   neg_esteem_w1 &lt;= 0.3162743:\n                    :   :...academic_stress_w3 &gt; -1.225795: active (502.7/245)\n                    :       academic_stress_w3 &lt;= -1.225795:\n                    :       :...parent_attachment_w1 &lt;= 1.016342: passive (52.6/7.9)\n                    :           parent_attachment_w1 &gt; 1.016342: active (9.8/2.7)\n                    parent_attachment_w3 &gt; 0.03145359:\n                    :...desire_stress_w1 &lt;= -1.306998: passive (76.2/35)\n                        desire_stress_w1 &gt; -1.306998:\n                        :...friend_stress_w2 &lt;= 0.5600184:\n                            :...higher_school_dependence_w5 &lt;= -1.385458:\n                            :   :...parent_stress_w5 &gt; 0.7494286: passive (7.9)\n                            :   :   parent_stress_w5 &lt;= 0.7494286:\n                            :   :   :...academic_stress_w3 &gt; 1.227193: passive (7.3)\n                            :   :       academic_stress_w3 &lt;= 1.227193: [S1]\n                            :   higher_school_dependence_w5 &gt; -1.385458:\n                            :   :...parent_stress_w4 &gt; 1.083454:\n                            :       :...friend_stress_w4 &gt; 0.4199131: active (8.6)\n                            :       :   friend_stress_w4 &lt;= 0.4199131: [S2]\n                            :       parent_stress_w4 &lt;= 1.083454:\n                            :       :...friend_stress_w4 &gt; -1.291115: active (472.1/95.3)\n                            :           friend_stress_w4 &lt;= -1.291115: [S3]\n                            friend_stress_w2 &gt; 0.5600184:\n                            :...friend_stress_w4 &lt;= -0.961684: passive (14.4/1.6)\n                                friend_stress_w4 &gt; -0.961684:\n                                :...parent_monitoring_w2 &lt;= -1.092574: passive (12.9/1.9)\n                                    parent_monitoring_w2 &gt; -1.092574: [S4]\n\nSubTree [S1]\n\nhigher_school_dependence_w4 &lt;= -0.515721: active (17)\nhigher_school_dependence_w4 &gt; -0.515721:\n:...desire_stress_w1 &lt;= -0.1505302: passive (10)\n    desire_stress_w1 &gt; -0.1505302: active (12.5/2.8)\n\nSubTree [S2]\n\nparent_attachment_w3 &lt;= 0.1488357: active (4.5)\nparent_attachment_w3 &gt; 0.1488357: passive (24.1/5.2)\n\nSubTree [S3]\n\ndeviant_esteem_w3 &gt; 0.6197529: passive (6.2)\ndeviant_esteem_w3 &lt;= 0.6197529:\n:...higher_school_dependence_w5 &lt;= 0.06950709: passive (10.8/3.6)\n    higher_school_dependence_w5 &gt; 0.06950709: active (15.5/1.3)\n\nSubTree [S4]\n\nparent_attachment_w5 &lt;= -1.098488: passive (9.5/1.1)\nparent_attachment_w5 &gt; -1.098488:\n:...friend_stress_w1 &gt; -0.5905387:\n    :...desire_stress_w1 &lt;= 1.523512: active (129.2/28.2)\n    :   desire_stress_w1 &gt; 1.523512: passive (7.7/1.9)\n    friend_stress_w1 &lt;= -0.5905387:\n    :...neg_esteem_w2 &gt; 1.148779: active (4.5)\n        neg_esteem_w2 &lt;= 1.148779:\n        :...deviant_esteem_w1 &lt;= -0.6941836: active (4)\n            deviant_esteem_w1 &gt; -0.6941836: passive (22.7/2.9)\n\n-----  Trial 1:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.613473: passive (26.4)\nparent_monitoring_w5 &gt; -2.613473:\n:...parent_attachment_w3 &lt;= 0.03116133:\n    :...friend_stress_w4 &gt; 1.364975: passive (137.7/29.9)\n    :   friend_stress_w4 &lt;= 1.364975:\n    :   :...academic_stress_w3 &lt;= -1.904067: passive (35.3/4.5)\n    :       academic_stress_w3 &gt; -1.904067:\n    :       :...parent_attachment_w5 &gt; 1.196899: active (56.5/16.4)\n    :           parent_attachment_w5 &lt;= 1.196899:\n    :           :...friend_stress_w4 &lt;= -1.234177: passive (104.2/22.5)\n    :               friend_stress_w4 &gt; -1.234177:\n    :               :...deviant_esteem_w3 &lt;= 0.1848285:\n    :                   :...higher_school_dependence_w5 &lt;= -1.360586: passive (118.4/38.1)\n    :                   :   higher_school_dependence_w5 &gt; -1.360586:\n    :                   :   :...neg_esteem_w4 &gt; -0.8968498: active (640.8/281.4)\n    :                   :       neg_esteem_w4 &lt;= -0.8968498:\n    :                   :       :...parent_monitoring_w4 &lt;= 0.663452: passive (89.1/22.9)\n    :                   :           parent_monitoring_w4 &gt; 0.663452: active (27.6/9.3)\n    :                   deviant_esteem_w3 &gt; 0.1848285:\n    :                   :...self_confidence_w4 &gt; 0.9562842: passive (37.7/0.5)\n    :                       self_confidence_w4 &lt;= 0.9562842:\n    :                       :...parent_attachment_w5 &lt;= 0.3127291: passive (504.7/184.9)\n    :                           parent_attachment_w5 &gt; 0.3127291:\n    :                           :...desire_stress_w2 &lt;= 0.3013462: passive (63/26.4)\n    :                               desire_stress_w2 &gt; 0.3013462: active (44.1/8.5)\n    parent_attachment_w3 &gt; 0.03116133:\n    :...parent_attachment_w1 &lt;= -1.446905: passive (34.3/7.1)\n        parent_attachment_w1 &gt; -1.446905:\n        :...parent_attachment_w5 &gt; 0.1914774:\n            :...parent_monitoring_w1 &lt;= -1.146851: passive (43.4/14.9)\n            :   parent_monitoring_w1 &gt; -1.146851:\n            :   :...parent_attachment_w1 &lt;= -0.04987012: active (184.3/40.4)\n            :       parent_attachment_w1 &gt; -0.04987012:\n            :       :...parent_monitoring_w2 &gt; 0.2844403:\n            :           :...friend_stress_w3 &lt;= 1.33373: active (309.6/88.5)\n            :           :   friend_stress_w3 &gt; 1.33373: passive (16.7/4.9)\n            :           parent_monitoring_w2 &lt;= 0.2844403:\n            :           :...parent_stress_w1 &gt; 0.1191225: active (71/18.9)\n            :               parent_stress_w1 &lt;= 0.1191225:\n            :               :...parent_attachment_w5 &lt;= 1.421745: passive (114.2/41.3)\n            :                   parent_attachment_w5 &gt; 1.421745: active (15.3/3.7)\n            parent_attachment_w5 &lt;= 0.1914774:\n            :...deviant_esteem_w3 &gt; 0.598907: passive (139.7/48.5)\n                deviant_esteem_w3 &lt;= 0.598907:\n                :...desire_stress_w1 &gt; -0.1953578:\n                    :...parent_stress_w4 &gt; 1.322907: passive (17.5/4.1)\n                    :   parent_stress_w4 &lt;= 1.322907:\n                    :   :...parent_stress_w5 &lt;= 1.231336: active (301.8/99.5)\n                    :       parent_stress_w5 &gt; 1.231336: passive (19.7/5)\n                    desire_stress_w1 &lt;= -0.1953578:\n                    :...academic_stress_w3 &lt;= -1.752305: passive (13/1)\n                        academic_stress_w3 &gt; -1.752305:\n                        :...higher_school_dependence_w5 &lt;= -1.505205: passive (22.5/3.6)\n                            higher_school_dependence_w5 &gt; -1.505205:\n                            :...deviant_esteem_w3 &lt;= -0.5273907: passive (75.1/27.6)\n                                deviant_esteem_w3 &gt; -0.5273907:\n                                :...parent_stress_w5 &lt;= -0.4485216: passive (44.3/16.1)\n                                    parent_stress_w5 &gt; -0.4485216:\n                                    :...desire_stress_w4 &lt;= 0.7880329: active (82/16.8)\n                                        desire_stress_w4 &gt; 0.7880329: passive (22.3/7.7)\n\n-----  Trial 2:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.324558: passive (44.7/5.3)\nparent_monitoring_w5 &gt; -2.324558:\n:...parent_attachment_w3 &gt; 0.03116133:\n    :...parent_attachment_w5 &lt;= -0.7206079: passive (203/80.2)\n    :   parent_attachment_w5 &gt; -0.7206079:\n    :   :...friend_stress_w1 &lt;= -1.453579: passive (92.3/35.2)\n    :       friend_stress_w1 &gt; -1.453579:\n    :       :...deviant_esteem_w2 &gt; 0.7652284: passive (171.4/79)\n    :           deviant_esteem_w2 &lt;= 0.7652284:\n    :           :...deviant_esteem_w1 &lt;= -1.1974: passive (62.9/23.5)\n    :               deviant_esteem_w1 &gt; -1.1974: active (979.2/359.2)\n    parent_attachment_w3 &lt;= 0.03116133:\n    :...deviant_esteem_w2 &lt;= -1.521699: passive (44.7/6.1)\n        deviant_esteem_w2 &gt; -1.521699:\n        :...friend_stress_w5 &gt; 1.62114: passive (72.2/14.1)\n            friend_stress_w5 &lt;= 1.62114:\n            :...desire_stress_w1 &lt;= -1.203698: passive (143.1/39.5)\n                desire_stress_w1 &gt; -1.203698:\n                :...self_confidence_w4 &gt; -0.8863377: passive (1426.1/646.4)\n                    self_confidence_w4 &lt;= -0.8863377:\n                    :...friend_stress_w4 &lt;= 0.5934161: active (114.3/30.2)\n                        friend_stress_w4 &gt; 0.5934161: passive (58.1/24)\n\n-----  Trial 3:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.429546: passive (28.5/2.1)\nparent_monitoring_w5 &gt; -2.429546:\n:...parent_attachment_w5 &lt;= -0.437988: passive (1044.7/466)\n    parent_attachment_w5 &gt; -0.437988:\n    :...deviant_esteem_w5 &lt;= -1.465747: passive (29.2/4.9)\n        deviant_esteem_w5 &gt; -1.465747:\n        :...parent_stress_w5 &gt; 0.9706918: passive (214/80.6)\n            parent_stress_w5 &lt;= 0.9706918:\n            :...desire_stress_w1 &gt; -0.2136136: active (1147.6/450.8)\n                desire_stress_w1 &lt;= -0.2136136:\n                :...deviant_esteem_w2 &gt; 1.135318: passive (60.4/15.8)\n                    deviant_esteem_w2 &lt;= 1.135318:\n                    :...neg_esteem_w3 &gt; 1.388322: passive (33.1/7.1)\n                        neg_esteem_w3 &lt;= 1.388322:\n                        :...higher_school_dependence_w4 &lt;= -0.4042585:\n                            :...academic_stress_w2 &lt;= 1.09287: passive (245/94.4)\n                            :   academic_stress_w2 &gt; 1.09287: active (27.2/6.8)\n                            higher_school_dependence_w4 &gt; -0.4042585:\n                            :...parent_monitoring_w2 &gt; 0.4206425:\n                                :...parent_monitoring_w1 &lt;= -0.08644821: passive (69.7/29.4)\n                                :   parent_monitoring_w1 &gt; -0.08644821: active (176.6/38.2)\n                                parent_monitoring_w2 &lt;= 0.4206425:\n                                :...neg_esteem_w5 &gt; 0.5656464: active (67.9/19.1)\n                                    neg_esteem_w5 &lt;= 0.5656464: [S1]\n\nSubTree [S1]\n\nparent_monitoring_w1 &gt; 1.635996: passive (14)\nparent_monitoring_w1 &lt;= 1.635996:\n:...academic_stress_w2 &lt;= -1.146655: active (28.2/5.2)\n    academic_stress_w2 &gt; -1.146655: passive (225.9/97.2)\n\n-----  Trial 4:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.429546: passive (26.4/2.4)\nparent_monitoring_w5 &gt; -2.429546:\n:...friend_stress_w5 &gt; 1.665985: passive (108.1/35)\n    friend_stress_w5 &lt;= 1.665985:\n    :...neg_esteem_w1 &gt; 0.1597105:\n        :...higher_school_dependence_w4 &gt; 1.940044: passive (39.8/10.5)\n        :   higher_school_dependence_w4 &lt;= 1.940044:\n        :   :...desire_stress_w4 &lt;= -1.244964:\n        :       :...self_confidence_w3 &lt;= -0.7418484: active (19.8/2.6)\n        :       :   self_confidence_w3 &gt; -0.7418484:\n        :       :   :...higher_school_dependence_w5 &lt;= 1.705818: passive (107.7/30.6)\n        :       :       higher_school_dependence_w5 &gt; 1.705818: active (6.6/0.6)\n        :       desire_stress_w4 &gt; -1.244964:\n        :       :...self_confidence_w5 &gt; 0.9610693: passive (100.1/38.9)\n        :           self_confidence_w5 &lt;= 0.9610693:\n        :           :...parent_stress_w3 &lt;= 0.7616408: active (895.6/315)\n        :               parent_stress_w3 &gt; 0.7616408:\n        :               :...higher_school_dependence_w1 &gt; 1.812647: active (26.3/2.2)\n        :                   higher_school_dependence_w1 &lt;= 1.812647:\n        :                   :...neg_esteem_w4 &lt;= -0.9293811: active (25/5)\n        :                       neg_esteem_w4 &gt; -0.9293811:\n        :                       :...parent_monitoring_w5 &gt; 1.194195: active (16.2/2.2)\n        :                           parent_monitoring_w5 &lt;= 1.194195:\n        :                           :...self_confidence_w1 &lt;= -0.8693392: active (25.4/7.2)\n        :                               self_confidence_w1 &gt; -0.8693392: passive (151.1/45.9)\n        neg_esteem_w1 &lt;= 0.1597105:\n        :...friend_stress_w4 &gt; 1.135043: passive (146.2/46.8)\n            friend_stress_w4 &lt;= 1.135043:\n            :...self_confidence_w4 &lt;= -0.4975483:\n                :...desire_stress_w1 &lt;= -1.202903: passive (38.1/12.5)\n                :   desire_stress_w1 &gt; -1.202903:\n                :   :...academic_stress_w3 &lt;= -1.023793: passive (40.8/16.8)\n                :       academic_stress_w3 &gt; -1.023793: active (272.6/87.3)\n                self_confidence_w4 &gt; -0.4975483:\n                :...deviant_esteem_w1 &gt; 1.350392: passive (35.3/5.7)\n                    deviant_esteem_w1 &lt;= 1.350392:\n                    :...neg_esteem_w2 &gt; 0.4889513:\n                        :...neg_esteem_w4 &lt;= 0.3946349: passive (248.4/81.4)\n                        :   neg_esteem_w4 &gt; 0.3946349: active (97.4/44.4)\n                        neg_esteem_w2 &lt;= 0.4889513:\n                        :...self_confidence_w1 &lt;= -1.205121: passive (37.3/7.6)\n                            self_confidence_w1 &gt; -1.205121:\n                            :...parent_attachment_w5 &lt;= 0.2734542:\n                                :...parent_attachment_w4 &gt; 0.9587657: active (41.1/8)\n                                :   parent_attachment_w4 &lt;= 0.9587657:\n                                :   :...self_confidence_w2 &lt;= 1.37461: passive (516/217.7)\n                                :       self_confidence_w2 &gt; 1.37461: active (26.3/3.8)\n                                parent_attachment_w5 &gt; 0.2734542:\n                                :...desire_stress_w3 &gt; -0.1199258: active (141.8/38)\n                                    desire_stress_w3 &lt;= -0.1199258:\n                                    :...neg_esteem_w3 &gt; 1.138141: passive (12.2/0.7)\n                                        neg_esteem_w3 &lt;= 1.138141: [S1]\n\nSubTree [S1]\n\nparent_stress_w2 &lt;= -1.762674: passive (15.8/3.2)\nparent_stress_w2 &gt; -1.762674: active (194.7/74.9)\n\n-----  Trial 5:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.324558: passive (36.2/6.9)\nparent_monitoring_w5 &gt; -2.324558:\n:...friend_stress_w4 &gt; 1.819626: passive (90.4/28.2)\n    friend_stress_w4 &lt;= 1.819626:\n    :...academic_stress_w5 &lt;= -0.7230957: passive (884.1/404.2)\n        academic_stress_w5 &gt; -0.7230957: active (2401.4/1106.1)\n\n-----  Trial 6:  -----\n\nDecision tree:\n\nfriend_stress_w4 &gt; 1.604254: passive (130.6/44.4)\nfriend_stress_w4 &lt;= 1.604254:\n:...neg_esteem_w1 &gt; 1.819638: passive (33.9/7.3)\n    neg_esteem_w1 &lt;= 1.819638:\n    :...parent_attachment_w4 &gt; 0.04306197:\n        :...neg_esteem_w2 &gt; 1.689854: active (37.3/7)\n        :   neg_esteem_w2 &lt;= 1.689854:\n        :   :...academic_stress_w3 &lt;= 1.357803: active (1346.9/598.8)\n        :       academic_stress_w3 &gt; 1.357803: passive (115.2/40.8)\n        parent_attachment_w4 &lt;= 0.04306197:\n        :...desire_stress_w4 &lt;= -1.741419: passive (35.6/4.4)\n            desire_stress_w4 &gt; -1.741419:\n            :...deviant_esteem_w4 &lt;= -1.282198: passive (71.4/16.2)\n                deviant_esteem_w4 &gt; -1.282198:\n                :...parent_stress_w4 &lt;= -0.5230274: passive (287.9/106.3)\n                    parent_stress_w4 &gt; -0.5230274:\n                    :...parent_stress_w4 &gt; 1.225793:\n                        :...neg_esteem_w2 &lt;= -0.6327894: active (22.7/5.2)\n                        :   neg_esteem_w2 &gt; -0.6327894: passive (100.9/26.9)\n                        parent_stress_w4 &lt;= 1.225793:\n                        :...parent_attachment_w4 &gt; -0.8819967: active (915.5/455.1)\n                            parent_attachment_w4 &lt;= -0.8819967:\n                            :...parent_monitoring_w4 &lt;= -1.195205: passive (83.6/35.3)\n                                parent_monitoring_w4 &gt; -1.195205: [S1]\n\nSubTree [S1]\n\nhigher_school_dependence_w2 &lt;= -1.899385: passive (15.9/3.3)\nhigher_school_dependence_w2 &gt; -1.899385: active (211.7/54.4)\n\n-----  Trial 7:  -----\n\nDecision tree:\n\nfriend_stress_w2 &gt; 0.5197647: passive (1014.3/422.2)\nfriend_stress_w2 &lt;= 0.5197647:\n:...parent_monitoring_w2 &gt; -0.002798408:\n    :...higher_school_dependence_w5 &lt;= -1.419011:\n    :   :...parent_attachment_w5 &lt;= 0.5352802: passive (89.2/25.7)\n    :   :   parent_attachment_w5 &gt; 0.5352802: active (27/8.2)\n    :   higher_school_dependence_w5 &gt; -1.419011:\n    :   :...parent_monitoring_w4 &lt;= -0.9258055: passive (134.2/58.2)\n    :       parent_monitoring_w4 &gt; -0.9258055:\n    :       :...deviant_esteem_w3 &gt; 1.064966: passive (45.3/14.3)\n    :           deviant_esteem_w3 &lt;= 1.064966:\n    :           :...desire_stress_w3 &lt;= -1.268134: passive (93.9/38.4)\n    :               desire_stress_w3 &gt; -1.268134:\n    :               :...friend_stress_w1 &gt; 1.279496: passive (42.5/14.5)\n    :                   friend_stress_w1 &lt;= 1.279496:\n    :                   :...parent_attachment_w3 &gt; -0.2975537: active (530.6/148.8)\n    :                       parent_attachment_w3 &lt;= -0.2975537:\n    :                       :...parent_stress_w4 &gt; 0.5033983: active (44.2/7.3)\n    :                           parent_stress_w4 &lt;= 0.5033983:\n    :                           :...desire_stress_w3 &lt;= 0.7963302: active (125.5/52.3)\n    :                               desire_stress_w3 &gt; 0.7963302: passive (34/6.5)\n    parent_monitoring_w2 &lt;= -0.002798408:\n    :...self_confidence_w4 &gt; 0.6706722:\n        :...desire_stress_w3 &lt;= -0.8262723: passive (45/3.4)\n        :   desire_stress_w3 &gt; -0.8262723:\n        :   :...friend_stress_w5 &gt; 1.330711: active (8/0.5)\n        :       friend_stress_w5 &lt;= 1.330711:\n        :       :...parent_attachment_w5 &lt;= 0.4115239: passive (119.9/30.7)\n        :           parent_attachment_w5 &gt; 0.4115239: active (29.8/11.7)\n        self_confidence_w4 &lt;= 0.6706722:\n        :...friend_stress_w4 &gt; 0.6532377: passive (211.8/75.5)\n            friend_stress_w4 &lt;= 0.6532377:\n            :...desire_stress_w3 &gt; 0.7387658:\n                :...desire_stress_w3 &lt;= 1.576693: active (122.3/33.4)\n                :   desire_stress_w3 &gt; 1.576693: passive (25.1/8.1)\n                desire_stress_w3 &lt;= 0.7387658:\n                :...parent_monitoring_w5 &lt;= -1.837166: passive (26.4/3.3)\n                    parent_monitoring_w5 &gt; -1.837166:\n                    :...parent_attachment_w2 &lt;= -0.2770349:\n                        :...neg_esteem_w4 &lt;= -1.204731: passive (16.1/0.7)\n                        :   neg_esteem_w4 &gt; -1.204731:\n                        :   :...academic_stress_w1 &lt;= -1.152587: passive (35.6/5.5)\n                        :       academic_stress_w1 &gt; -1.152587:\n                        :       :...friend_stress_w3 &lt;= -1.061121: passive (14.2)\n                        :           friend_stress_w3 &gt; -1.061121:\n                        :           :...academic_stress_w4 &gt; 0.8891743: passive (33.6/7.4)\n                        :               academic_stress_w4 &lt;= 0.8891743: [S1]\n                        parent_attachment_w2 &gt; -0.2770349:\n                        :...parent_stress_w2 &lt;= -0.6551181: active (74.8/19.5)\n                            parent_stress_w2 &gt; -0.6551181:\n                            :...parent_attachment_w5 &lt;= -1.331958: active (13.6/1.4)\n                                parent_attachment_w5 &gt; -1.331958:\n                                :...academic_stress_w2 &gt; 1.012888: active (29.9/6.6)\n                                    academic_stress_w2 &lt;= 1.012888:\n                                    :...parent_stress_w4 &gt; 0.8062585: passive (26.3/3)\n                                        parent_stress_w4 &lt;= 0.8062585: [S2]\n\nSubTree [S1]\n\ndeviant_esteem_w2 &lt;= 0.9843491: active (153.5/62.5)\ndeviant_esteem_w2 &gt; 0.9843491: passive (22.6/5.6)\n\nSubTree [S2]\n\nself_confidence_w5 &lt;= 0.2268678: active (95.9/39.2)\nself_confidence_w5 &gt; 0.2268678: passive (61/15.7)\n\n-----  Trial 8:  -----\n\nDecision tree:\n\nparent_monitoring_w5 &lt;= -2.352127: passive (26/1.4)\nparent_monitoring_w5 &gt; -2.352127:\n:...friend_stress_w4 &gt; 1.819626: passive (63/12.8)\n    friend_stress_w4 &lt;= 1.819626:\n    :...academic_stress_w4 &lt;= -1.167137:\n        :...parent_attachment_w4 &gt; 1.421768: active (24.7/6.2)\n        :   parent_attachment_w4 &lt;= 1.421768:\n        :   :...parent_stress_w1 &lt;= -1.658633: passive (24.8/0.7)\n        :       parent_stress_w1 &gt; -1.658633:\n        :       :...academic_stress_w2 &gt; 0.325762: passive (71.5/13.7)\n        :           academic_stress_w2 &lt;= 0.325762:\n        :           :...neg_esteem_w5 &lt;= 0.3999563: passive (172.7/59.6)\n        :               neg_esteem_w5 &gt; 0.3999563: active (62.2/22.3)\n        academic_stress_w4 &gt; -1.167137:\n        :...parent_attachment_w5 &lt;= -0.4318567:\n            :...academic_stress_w5 &lt;= -1.375007: passive (63/13.1)\n            :   academic_stress_w5 &gt; -1.375007:\n            :   :...desire_stress_w4 &gt; 1.979881: passive (26/3.5)\n            :       desire_stress_w4 &lt;= 1.979881:\n            :       :...parent_attachment_w1 &gt; 0.4531063:\n            :           :...higher_school_dependence_w5 &lt;= -1.141158: passive (29.7/1.8)\n            :           :   higher_school_dependence_w5 &gt; -1.141158:\n            :           :   :...parent_attachment_w3 &lt;= 0.004825812: passive (67/17.2)\n            :           :       parent_attachment_w3 &gt; 0.004825812: active (44.5/17.6)\n            :           parent_attachment_w1 &lt;= 0.4531063:\n            :           :...academic_stress_w2 &gt; 1.682991: active (43.3/8.5)\n            :               academic_stress_w2 &lt;= 1.682991:\n            :               :...deviant_esteem_w3 &lt;= -0.4600195: [S1]\n            :                   deviant_esteem_w3 &gt; -0.4600195:\n            :                   :...friend_stress_w4 &lt;= -0.9638329: passive (34/5)\n            :                       friend_stress_w4 &gt; -0.9638329:\n            :                       :...friend_stress_w3 &lt;= 0.3484272: active (270.9/117.4)\n            :                           friend_stress_w3 &gt; 0.3484272: [S2]\n            parent_attachment_w5 &gt; -0.4318567:\n            :...deviant_esteem_w5 &lt;= -1.465747: passive (28/4.9)\n                deviant_esteem_w5 &gt; -1.465747:\n                :...neg_esteem_w2 &gt; 1.300874:\n                    :...higher_school_dependence_w4 &lt;= -1.547185: passive (5.4)\n                    :   higher_school_dependence_w4 &gt; -1.547185: active (121.1/25.8)\n                    neg_esteem_w2 &lt;= 1.300874:\n                    :...parent_attachment_w3 &gt; 0.03116133:\n                        :...desire_stress_w1 &lt;= -1.306998: passive (86.8/35.6)\n                        :   desire_stress_w1 &gt; -1.306998:\n                        :   :...academic_stress_w3 &gt; 1.357803: passive (68/27.8)\n                        :       academic_stress_w3 &lt;= 1.357803:\n                        :       :...academic_stress_w5 &gt; -0.7244027: [S3]\n                        :           academic_stress_w5 &lt;= -0.7244027:\n                        :           :...neg_esteem_w2 &lt;= -1.397929: passive (19/2.7)\n                        :               neg_esteem_w2 &gt; -1.397929: [S4]\n                        parent_attachment_w3 &lt;= 0.03116133:\n                        :...desire_stress_w1 &lt;= -0.554962:\n                            :...parent_stress_w1 &gt; 0.2941396: passive (58.9/12.1)\n                            :   parent_stress_w1 &lt;= 0.2941396:\n                            :   :...parent_monitoring_w3 &lt;= -1.041848: active (22.2/3.6)\n                            :       parent_monitoring_w3 &gt; -1.041848: passive (141.1/56.9)\n                            desire_stress_w1 &gt; -0.554962:\n                            :...parent_stress_w5 &gt; 1.021302: passive (75.4/25.1)\n                                parent_stress_w5 &lt;= 1.021302:\n                                :...neg_esteem_w5 &gt; -0.0612667:\n                                    :...parent_stress_w3 &lt;= -0.4170986: passive (64.3/28.2)\n                                    :   parent_stress_w3 &gt; -0.4170986:\n                                    :   :...neg_esteem_w3 &lt;= 1.013953: active (220.1/57.9)\n                                    :       neg_esteem_w3 &gt; 1.013953: passive (29.3/10.8)\n                                    neg_esteem_w5 &lt;= -0.0612667:\n                                    :...parent_attachment_w2 &gt; 0.5774658: active (40.5/9.5)\n                                        parent_attachment_w2 &lt;= 0.5774658: [S5]\n\nSubTree [S1]\n\nhigher_school_dependence_w2 &lt;= 1.637111: active (130.4/37.8)\nhigher_school_dependence_w2 &gt; 1.637111: passive (13.4/2.1)\n\nSubTree [S2]\n\nparent_monitoring_w5 &lt;= -0.7838662: passive (41.7/4)\nparent_monitoring_w5 &gt; -0.7838662:\n:...higher_school_dependence_w4 &lt;= 0.1309455: passive (63.4/16.3)\n    higher_school_dependence_w4 &gt; 0.1309455: active (73/30.1)\n\nSubTree [S3]\n\nparent_attachment_w1 &lt;= -1.176658: passive (26.6/9.8)\nparent_attachment_w1 &gt; -1.176658: active (480.2/127.1)\n\nSubTree [S4]\n\nparent_attachment_w3 &gt; 1.483165: active (15.3)\nparent_attachment_w3 &lt;= 1.483165:\n:...deviant_esteem_w1 &lt;= 0.245371: passive (128.1/57.5)\n    deviant_esteem_w1 &gt; 0.245371: active (47.5/11.3)\n\nSubTree [S5]\n\nacademic_stress_w1 &gt; 1.240741: active (24.7/5.4)\nacademic_stress_w1 &lt;= 1.240741:\n:...academic_stress_w1 &gt; 0.3018395: passive (73.1/15.5)\n    academic_stress_w1 &lt;= 0.3018395:\n    :...academic_stress_w3 &lt;= -1.259403: passive (11.1)\n        academic_stress_w3 &gt; -1.259403:\n        :...self_confidence_w1 &lt;= -0.0348424: passive (61.1/21.8)\n            self_confidence_w1 &gt; -0.0348424: active (57/16.8)\n\n-----  Trial 9:  -----\n\nDecision tree:\n\nneg_esteem_w1 &gt; 1.819638: passive (35.6/5.6)\nneg_esteem_w1 &lt;= 1.819638:\n:...parent_stress_w4 &gt; 1.225793:\n    :...parent_attachment_w2 &lt;= 1.054303: passive (156.4/38.7)\n    :   parent_attachment_w2 &gt; 1.054303: active (13.1/2.8)\n    parent_stress_w4 &lt;= 1.225793:\n    :...parent_attachment_w5 &lt;= -0.4298131:\n        :...neg_esteem_w4 &lt;= -1.482233: passive (35.9/2.6)\n        :   neg_esteem_w4 &gt; -1.482233:\n        :   :...deviant_esteem_w3 &lt;= 0.2738315:\n        :       :...higher_school_dependence_w5 &lt;= -1.51136: passive (54.8/8.8)\n        :       :   higher_school_dependence_w5 &gt; -1.51136:\n        :       :   :...deviant_esteem_w4 &gt; 1.180687: active (30.4/4.3)\n        :       :       deviant_esteem_w4 &lt;= 1.180687:\n        :       :       :...parent_monitoring_w5 &lt;= -0.7838662:\n        :       :           :...parent_monitoring_w1 &lt;= -1.420951: active (26.6/8)\n        :       :           :   parent_monitoring_w1 &gt; -1.420951: passive (132.8/42.4)\n        :       :           parent_monitoring_w5 &gt; -0.7838662:\n        :       :           :...parent_stress_w5 &lt;= -0.3215714: passive (70.2/25.5)\n        :       :               parent_stress_w5 &gt; -0.3215714: active (197.2/69.1)\n        :       deviant_esteem_w3 &gt; 0.2738315:\n        :       :...parent_stress_w4 &lt;= -0.8944211: passive (21)\n        :           parent_stress_w4 &gt; -0.8944211:\n        :           :...deviant_esteem_w2 &gt; 0.7324222: passive (78.8/12.7)\n        :               deviant_esteem_w2 &lt;= 0.7324222:\n        :               :...friend_stress_w4 &gt; 1.184102: passive (29.3/3.2)\n        :                   friend_stress_w4 &lt;= 1.184102:\n        :                   :...self_confidence_w4 &gt; 0.9043695: passive (11.5)\n        :                       self_confidence_w4 &lt;= 0.9043695:\n        :                       :...academic_stress_w3 &lt;= -0.8782122: passive (25.4/4.1)\n        :                           academic_stress_w3 &gt; -0.8782122:\n        :                           :...deviant_esteem_w3 &gt; 1.430398: passive (15.4/1.5)\n        :                               deviant_esteem_w3 &lt;= 1.430398:\n        :                               :...neg_esteem_w5 &gt; 0.9397743: passive (44.1/13.5)\n        :                                   neg_esteem_w5 &lt;= 0.9397743: [S1]\n        parent_attachment_w5 &gt; -0.4298131:\n        :...deviant_esteem_w5 &lt;= -1.456939: passive (28.2/5)\n            deviant_esteem_w5 &gt; -1.456939:\n            :...academic_stress_w4 &lt;= -0.8084847:\n                :...parent_monitoring_w4 &gt; 1.647621: active (28.4/5.4)\n                :   parent_monitoring_w4 &lt;= 1.647621:\n                :   :...parent_monitoring_w2 &gt; 1.074097: active (51.9/16.5)\n                :       parent_monitoring_w2 &lt;= 1.074097:\n                :       :...parent_stress_w1 &lt;= -1.623306: passive (17.5)\n                :           parent_stress_w1 &gt; -1.623306:\n                :           :...desire_stress_w3 &lt;= -0.02007717:\n                :               :...parent_attachment_w3 &lt;= 0.6901932: passive (135.2/31.9)\n                :               :   parent_attachment_w3 &gt; 0.6901932: active (52.8/23.6)\n                :               desire_stress_w3 &gt; -0.02007717:\n                :               :...friend_stress_w3 &lt;= -0.8939205: passive (10.8/0.6)\n                :                   friend_stress_w3 &gt; -0.8939205:\n                :                   :...academic_stress_w2 &lt;= -0.1253125: active (65.5/16.6)\n                :                       academic_stress_w2 &gt; -0.1253125: passive (48.9/18.9)\n                academic_stress_w4 &gt; -0.8084847:\n                :...desire_stress_w4 &lt;= -1.689063: passive (51.6/16.1)\n                    desire_stress_w4 &gt; -1.689063:\n                    :...friend_stress_w4 &gt; 1.070474:\n                        :...parent_stress_w2 &lt;= -0.02208491: active (82.5/30.9)\n                        :   parent_stress_w2 &gt; -0.02208491: passive (91/24.6)\n                        friend_stress_w4 &lt;= 1.070474:\n                        :...parent_attachment_w3 &gt; 0.03145359:\n                            :...desire_stress_w1 &gt; -0.3069077: active (355.1/71.9)\n                            :   desire_stress_w1 &lt;= -0.3069077:\n                            :   :...deviant_esteem_w2 &gt; 1.100672: passive (14.5/2.2)\n                            :       deviant_esteem_w2 &lt;= 1.100672:\n                            :       :...friend_stress_w1 &lt;= -1.678223: passive (15.3/2.7)\n                            :           friend_stress_w1 &gt; -1.678223: [S2]\n                            parent_attachment_w3 &lt;= 0.03145359:\n                            :...deviant_esteem_w4 &lt;= -1.353175: passive (25.6/5.3)\n                                deviant_esteem_w4 &gt; -1.353175:\n                                :...parent_attachment_w5 &gt; 1.187476: active (25.8/2)\n                                    parent_attachment_w5 &lt;= 1.187476:\n                                    :...desire_stress_w3 &gt; 1.030656: active (82.2/16.4)\n                                        desire_stress_w3 &lt;= 1.030656: [S3]\n\nSubTree [S1]\n\ndeviant_esteem_w3 &lt;= 0.3647567: passive (11.8/2)\ndeviant_esteem_w3 &gt; 0.3647567: active (103.5/32.2)\n\nSubTree [S2]\n\ndeviant_esteem_w5 &gt; 0.123967: active (76.9/12.3)\ndeviant_esteem_w5 &lt;= 0.123967:\n:...parent_monitoring_w2 &lt;= -0.328678: passive (48.8/16.3)\n    parent_monitoring_w2 &gt; -0.328678: active (128/38.3)\n\nSubTree [S3]\n\nself_confidence_w4 &lt;= 0.2037319: active (364.4/139.6)\nself_confidence_w4 &gt; 0.2037319:\n:...friend_stress_w1 &gt; 1.484845: active (10.3/0.7)\n    friend_stress_w1 &lt;= 1.484845:\n    :...parent_monitoring_w2 &lt;= -0.5008731: passive (60/15)\n        parent_monitoring_w2 &gt; -0.5008731:\n        :...parent_monitoring_w4 &lt;= 0.06315225: active (61.6/18.6)\n            parent_monitoring_w4 &gt; 0.06315225: passive (68.8/25.1)\n\n\nEvaluation on training data (3412 cases):\n\nTrial       Decision Tree   \n-----     ----------------  \n      Size      Errors  \n\n   0        45 1099(32.2%)\n   1        31 1101(32.3%)\n   2        12 1320(38.7%)\n   3        15 1238(36.3%)\n   4        28 1128(33.1%)\n   5         4 1515(44.4%)\n   6        14 1366(40.0%)\n   7        31 1136(33.3%)\n   8        44 1066(31.2%)\n   9        45 1005(29.5%)\nboost           629(18.4%)   &lt;&lt;\n\n\n       (a)   (b)    &lt;-classified as\n      ----  ----\n      1407   268    (a): class active\n       361  1376    (b): class passive\n\n\n    Attribute usage:\n\n    100.00% neg_esteem_w1\n    100.00% friend_stress_w2\n    100.00% friend_stress_w4\n    100.00% parent_monitoring_w5\n     99.85% parent_attachment_w5\n     99.38% parent_attachment_w3\n     99.06% parent_stress_w4\n     99.00% friend_stress_w5\n     98.15% desire_stress_w4\n     97.60% parent_attachment_w4\n     97.30% academic_stress_w4\n     96.01% academic_stress_w5\n     94.93% desire_stress_w1\n     93.49% deviant_esteem_w2\n     89.95% academic_stress_w3\n     84.70% deviant_esteem_w5\n     84.32% self_confidence_w4\n     83.09% parent_monitoring_w2\n     81.83% neg_esteem_w2\n     81.65% deviant_esteem_w3\n     79.16% parent_stress_w5\n     68.79% desire_stress_w3\n     68.00% higher_school_dependence_w5\n     62.78% parent_attachment_w1\n     62.49% deviant_esteem_w4\n     61.23% higher_school_dependence_w4\n     57.50% deviant_esteem_w1\n     55.10% friend_stress_w1\n     53.60% neg_esteem_w4\n     49.50% parent_monitoring_w4\n     46.04% academic_stress_w2\n     39.33% neg_esteem_w3\n     38.57% self_confidence_w5\n     38.04% friend_stress_w3\n     37.72% parent_attachment_w2\n     37.69% parent_stress_w3\n     36.05% parent_monitoring_w1\n     35.93% self_confidence_w1\n     35.87% neg_esteem_w5\n     25.70% parent_stress_w1\n     23.56% parent_stress_w2\n     15.53% self_confidence_w2\n     12.95% academic_stress_w1\n     11.84% higher_school_dependence_w3\n      9.96% higher_school_dependence_w2\n      7.09% higher_school_dependence_w1\n      4.72% parent_monitoring_w3\n      3.84% self_confidence_w3\n      3.11% desire_stress_w2\n\n\nTime: 1.0 secs\n\n# C5.0 모델로 테스트 세트 예측\ncat(\"\\nC5.0 모델로 테스트 세트 예측 중...\\n\")\n\n\nC5.0 모델로 테스트 세트 예측 중...\n\ny_pred_c50 &lt;- predict(c50_model, X_test)\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_c50 &lt;- matrix(0,\n                                       nrow = length(levels(y_test)),\n                                       ncol = length(levels(y_test)),\n                                       dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_c50[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_c50[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_c50[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== C5.0 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== C5.0 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_c50, 2))\n\n         Predicted\nActual      active  passive\n  active  88776.09 26995.42\n  passive 39150.84 19420.33\n\n# C5.0 모델 가중 성능 지표 계산\ntotal_weighted_sum_c50 &lt;- sum(weighted_confusion_matrix_c50)\nweighted_accuracy_c50 &lt;- sum(diag(weighted_confusion_matrix_c50)) / total_weighted_sum_c50\ncat(\"\\n=== C5.0 모델 가중 성능 지표 ===\\n\")\n\n\n=== C5.0 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_c50, 4), \"\\n\")\n\n가중 정확도: 0.6206 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_c50 &lt;- numeric(length(levels(y_test)))\nweighted_recall_c50 &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_c50 &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_c50) &lt;- names(weighted_recall_c50) &lt;- names(weighted_f1_score_c50) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_c50[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_c50[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_c50[cat, ]) - TP\n\n  weighted_precision_c50[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_c50[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_c50[cat] &lt;- ifelse((weighted_precision_c50[cat] + weighted_recall_c50[cat]) == 0, 0,\n                                     2 * (weighted_precision_c50[cat] * weighted_recall_c50[cat]) / (weighted_precision_c50[cat] + weighted_recall_c50[cat]))\n}\n\ncat(\"\\nC5.0 모델 범주별 가중 정밀도:\\n\")\n\n\nC5.0 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_c50, 4))\n\n active passive \n 0.6940  0.4184 \n\ncat(\"\\nC5.0 모델 범주별 가중 재현율:\\n\")\n\n\nC5.0 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_c50, 4))\n\n active passive \n 0.7668  0.3316 \n\ncat(\"\\nC5.0 모델 범주별 가중 F1-score:\\n\")\n\n\nC5.0 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_c50, 4))\n\n active passive \n 0.7286  0.3700",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#xgboost-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#xgboost-모델",
    "title": "analysis",
    "section": "XGBoost 모델",
    "text": "XGBoost 모델\n\n# XGBoost 모델 학습 및 평가\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# 데이터 변환 (XGBoost는 DMatrix 형식을 사용)\n# 먼저 factor를 수치형으로 변환\ny_train_numeric &lt;- as.integer(y_train) - 1  # 0부터 시작하는 인덱스로 변환\ny_test_numeric &lt;- as.integer(y_test) - 1\n\n# XGBoost DMatrix 생성\ndtrain &lt;- xgb.DMatrix(data = as.matrix(X_train), label = y_train_numeric, weight = weights_train)\ndtest &lt;- xgb.DMatrix(data = as.matrix(X_test), label = y_test_numeric, weight = weights_test)\n\n# 모델 파라미터 설정\nxgb_params &lt;- list(\n  objective = \"multi:softprob\",\n  eval_metric = \"mlogloss\",\n  num_class = length(levels(y_train)),\n  eta = 0.3,               # 학습률\n  max_depth = 6,           # 트리 최대 깊이\n  min_child_weight = 1,    # 최소 자식 노드 가중치 합\n  subsample = 0.8,         # 샘플링 비율\n  colsample_bytree = 0.8   # 특성 샘플링 비율\n)\n\n# XGBoost 모델 학습\ncat(\"\\nXGBoost 모델 학습 중...\\n\")\n\n\nXGBoost 모델 학습 중...\n\nxgb_model &lt;- xgb.train(\n  params = xgb_params,\n  data = dtrain,\n  nrounds = 100,           # 부스팅 반복 횟수\n  verbose = 0\n)\ncat(\"XGBoost 모델 학습 완료.\\n\")\n\nXGBoost 모델 학습 완료.\n\n# 변수 중요도 확인\nxgb_importance &lt;- xgb.importance(model = xgb_model)\nif (nrow(xgb_importance) &gt; 0) {\n  cat(\"\\nXGBoost 변수 중요도 상위 10개:\\n\")\n  print(head(xgb_importance, 10))\n  xgb.plot.importance(xgb_importance, top_n = 10)\n} else {\n  cat(\"\\nXGBoost 변수 중요도를 계산할 수 없습니다.\\n\")\n}\n\n\nXGBoost 변수 중요도 상위 10개:\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:        parent_monitoring_w5 0.03182668 0.03658157 0.02176108\n 2:            friend_stress_w4 0.03088689 0.04049583 0.02153674\n 3:        parent_attachment_w5 0.02946235 0.02678660 0.01940550\n 4: higher_school_dependence_w5 0.02714116 0.02699829 0.02007852\n 5:            desire_stress_w1 0.02705206 0.02603461 0.02602356\n 6:        parent_attachment_w4 0.02674024 0.02234054 0.02097588\n 7:          academic_stress_w3 0.02657457 0.03015163 0.01985418\n 8:          academic_stress_w4 0.02626054 0.03414376 0.01962984\n 9:               neg_esteem_w1 0.02609752 0.02822353 0.02288278\n10:            friend_stress_w2 0.02527457 0.02162330 0.02153674\n\n\n\n\n\n\n\n\n# XGBoost 모델로 테스트 세트 예측\ncat(\"\\nXGBoost 모델로 테스트 세트 예측 중...\\n\")\n\n\nXGBoost 모델로 테스트 세트 예측 중...\n\nxgb_pred_probs &lt;- predict(xgb_model, dtest)\n# 예측 확률을 행렬로 변환 (각 클래스별 확률값)\nxgb_pred_probs_matrix &lt;- matrix(xgb_pred_probs, nrow = length(y_test), byrow = TRUE)\n# 가장 높은 확률을 가진 클래스를 예측값으로 선택\ny_pred_xgb_idx &lt;- apply(xgb_pred_probs_matrix, 1, which.max) - 1  # 0-based 인덱스\n# 다시 factor로 변환\ny_pred_xgb &lt;- factor(levels(y_test)[y_pred_xgb_idx + 1], levels = levels(y_test))\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_xgb &lt;- matrix(0,\n                                       nrow = length(levels(y_test)),\n                                       ncol = length(levels(y_test)),\n                                       dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_xgb[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_xgb[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_xgb[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== XGBoost 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== XGBoost 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_xgb, 2))\n\n         Predicted\nActual      active  passive\n  active  94434.78 21336.73\n  passive 44765.10 13806.06\n\n# XGBoost 모델 가중 성능 지표 계산\ntotal_weighted_sum_xgb &lt;- sum(weighted_confusion_matrix_xgb)\nweighted_accuracy_xgb &lt;- sum(diag(weighted_confusion_matrix_xgb)) / total_weighted_sum_xgb\ncat(\"\\n=== XGBoost 모델 가중 성능 지표 ===\\n\")\n\n\n=== XGBoost 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_xgb, 4), \"\\n\")\n\n가중 정확도: 0.6209 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_xgb &lt;- numeric(length(levels(y_test)))\nweighted_recall_xgb &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_xgb &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_xgb) &lt;- names(weighted_recall_xgb) &lt;- names(weighted_f1_score_xgb) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_xgb[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_xgb[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_xgb[cat, ]) - TP\n\n  weighted_precision_xgb[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_xgb[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_xgb[cat] &lt;- ifelse((weighted_precision_xgb[cat] + weighted_recall_xgb[cat]) == 0, 0,\n                                     2 * (weighted_precision_xgb[cat] * weighted_recall_xgb[cat]) / (weighted_precision_xgb[cat] + weighted_recall_xgb[cat]))\n}\n\ncat(\"\\nXGBoost 모델 범주별 가중 정밀도:\\n\")\n\n\nXGBoost 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_xgb, 4))\n\n active passive \n 0.6784  0.3929 \n\ncat(\"\\nXGBoost 모델 범주별 가중 재현율:\\n\")\n\n\nXGBoost 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_xgb, 4))\n\n active passive \n 0.8157  0.2357 \n\ncat(\"\\nXGBoost 모델 범주별 가중 F1-score:\\n\")\n\n\nXGBoost 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_xgb, 4))\n\n active passive \n 0.7407  0.2946",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#로지스틱-회귀-모델",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#로지스틱-회귀-모델",
    "title": "analysis",
    "section": "로지스틱 회귀 모델",
    "text": "로지스틱 회귀 모델\n\n# 다항 로지스틱 회귀 모델 학습 및 평가\nlibrary(nnet)\n\n# 다항 로지스틱 회귀 모델 학습 (가중치 적용)\ncat(\"\\n다항 로지스틱 회귀 모델 학습 중...\\n\")\n\n\n다항 로지스틱 회귀 모델 학습 중...\n\n# X_train에 열 이름이 없으면 추가\nif(is.null(colnames(X_train))) {\n  colnames(X_train) &lt;- paste0(\"V\", 1:ncol(X_train))\n}\n\n# 학습 데이터를 데이터프레임으로 변환\ntrain_df &lt;- as.data.frame(X_train)\ntrain_df$y &lt;- y_train\n\n# 다항 로지스틱 회귀 모델 학습\nlogistic_model &lt;- multinom(\n  y ~ .,\n  data = train_df,\n  weights = weights_train,\n  trace = FALSE\n)\ncat(\"다항 로지스틱 회귀 모델 학습 완료.\\n\")\n\n다항 로지스틱 회귀 모델 학습 완료.\n\n# 모델 요약\nprint(summary(logistic_model))\n\nCall:\nmultinom(formula = y ~ ., data = train_df, weights = weights_train, \n    trace = FALSE)\n\nCoefficients:\n                                   Values   Std. Err.\n(Intercept)                 -1.747743e-02 0.002352613\nparent_attachment_w1         8.629500e-02 0.003520811\ndeviant_esteem_w1            2.455207e-02 0.003349262\nparent_stress_w1             1.581364e-02 0.002926812\nparent_monitoring_w1        -3.621483e-02 0.002691047\ndesire_stress_w1            -1.654925e-01 0.002964678\nfriend_stress_w1            -1.484645e-02 0.002712385\nself_confidence_w1          -1.833542e-02 0.003513107\nhigher_school_dependence_w1  5.676484e-02 0.001927342\nneg_esteem_w1               -1.340150e-01 0.003292447\nacademic_stress_w1           1.402940e-02 0.002968398\nparent_attachment_w2        -1.597988e-01 0.003607662\ndeviant_esteem_w2           -3.201644e-03 0.003073032\nparent_stress_w2            -3.385004e-03 0.002958998\nparent_monitoring_w2        -5.046192e-02 0.002743269\ndesire_stress_w2            -7.252106e-02 0.002767975\nfriend_stress_w2             1.161718e-01 0.002658148\nself_confidence_w2          -3.121364e-02 0.003129254\nhigher_school_dependence_w2  2.883447e-02 0.001971023\nneg_esteem_w2               -4.552701e-02 0.002822384\nacademic_stress_w2           5.587968e-02 0.002906613\nparent_attachment_w3        -1.932719e-01 0.003799447\ndeviant_esteem_w3            1.982736e-01 0.003650501\nparent_stress_w3             4.636885e-03 0.003426560\nparent_monitoring_w3        -5.904395e-02 0.002631096\ndesire_stress_w3            -5.003618e-02 0.002935088\nfriend_stress_w3             3.303750e-02 0.003050699\nself_confidence_w3           9.940994e-03 0.003381305\nhigher_school_dependence_w3  5.753226e-04 0.002152310\nneg_esteem_w3                6.789459e-02 0.003165765\nacademic_stress_w3          -7.253990e-03 0.002785823\nparent_attachment_w4        -1.965769e-02 0.003727261\ndeviant_esteem_w4            1.507238e-02 0.003228654\nparent_stress_w4             3.484563e-02 0.003474462\nparent_monitoring_w4        -4.877685e-02 0.002696410\ndesire_stress_w4            -4.753889e-02 0.002757239\nfriend_stress_w4             1.109145e-01 0.003010243\nself_confidence_w4           2.145545e-01 0.003658857\nhigher_school_dependence_w4 -2.398239e-02 0.002350664\nneg_esteem_w4               -1.394140e-01 0.003148682\nacademic_stress_w4          -8.451687e-02 0.002843888\nparent_attachment_w5        -1.940178e-01 0.003516885\ndeviant_esteem_w5           -9.225044e-02 0.003343494\nparent_stress_w5             7.638213e-02 0.003335598\nparent_monitoring_w5        -1.537289e-01 0.002660936\ndesire_stress_w5             2.553032e-05 0.002858326\nfriend_stress_w5             8.165718e-02 0.003068119\nself_confidence_w5           9.089148e-02 0.003390435\nhigher_school_dependence_w5 -7.819745e-02 0.002162491\nneg_esteem_w5                7.233250e-02 0.003152148\nacademic_stress_w5          -1.152715e-01 0.002639858\n\nResidual Deviance: 1055167 \nAIC: 1055269 \n\n# 테스트 세트 예측\ncat(\"\\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\\n\")\n\n\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\n\n# X_test에 열 이름이 없으면 추가\nif(is.null(colnames(X_test))) {\n  colnames(X_test) &lt;- paste0(\"V\", 1:ncol(X_test))\n}\ny_pred_logistic &lt;- predict(logistic_model, newdata = as.data.frame(X_test))\ncat(\"예측 완료.\\n\")\n\n예측 완료.\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_logistic &lt;- matrix(0,\n                                           nrow = length(levels(y_test)),\n                                           ncol = length(levels(y_test)),\n                                           dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(y_pred_logistic[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_logistic[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_logistic[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\\n\")\n\n\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_logistic, 2))\n\n         Predicted\nActual      active  passive\n  active  70274.82 45496.69\n  passive 23412.35 35158.82\n\n# 다항 로지스틱 회귀 모델 가중 성능 지표 계산\ntotal_weighted_sum_logistic &lt;- sum(weighted_confusion_matrix_logistic)\nweighted_accuracy_logistic &lt;- sum(diag(weighted_confusion_matrix_logistic)) / total_weighted_sum_logistic\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\\n\")\n\n\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_logistic, 4), \"\\n\")\n\n가중 정확도: 0.6047 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_logistic &lt;- numeric(length(levels(y_test)))\nweighted_recall_logistic &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_logistic &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_logistic) &lt;- names(weighted_recall_logistic) &lt;- names(weighted_f1_score_logistic) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_logistic[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_logistic[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_logistic[cat, ]) - TP\n\n  weighted_precision_logistic[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_logistic[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_logistic[cat] &lt;- ifelse((weighted_precision_logistic[cat] + weighted_recall_logistic[cat]) == 0, 0,\n                                          2 * (weighted_precision_logistic[cat] * weighted_recall_logistic[cat]) / (weighted_precision_logistic[cat] + weighted_recall_logistic[cat]))\n}\n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\n\nprint(round(weighted_precision_logistic, 4))\n\n active passive \n 0.7501  0.4359 \n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\n\nprint(round(weighted_recall_logistic, 4))\n\n active passive \n 0.6070  0.6003 \n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\\n\")\n\n\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_logistic, 4))\n\n active passive \n 0.6710  0.5051",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#최종-모델-성능-비교",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#최종-모델-성능-비교",
    "title": "analysis",
    "section": "최종 모델 성능 비교",
    "text": "최종 모델 성능 비교\n\n# 모든 모델의 가중 정확도 비교\nfinal_metrics &lt;- data.frame(\n  Metric = c(\"가중 정확도\"),\n  RandomForest = c(round(weighted_accuracy_tuned * 100, 2)),\n  # C50_Tree = c(round(weighted_accuracy_c50 * 100, 2)),\n  XGBoost = c(round(weighted_accuracy_xgb * 100, 2)),\n  # SVM = c(round(weighted_accuracy_svm * 100, 2)),\n  # KNN = c(round(weighted_accuracy_knn * 100, 2)),\n  LogisticRegression = c(round(weighted_accuracy_logistic * 100, 2))\n)\n\n# 결과 출력\ncat(\"\\n=== 최종 모델 가중 정확도 비교 ===\\n\")\n\n\n=== 최종 모델 가중 정확도 비교 ===\n\nprint(kable(final_metrics, caption = \"원본 모델 vs 튜닝 모델 가중 정확도 비교\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 원본 모델 vs 튜닝 모델 가중 정확도 비교\n\n|Metric      | RandomForest| XGBoost| LogisticRegression|\n|:-----------|------------:|-------:|------------------:|\n|가중 정확도 |        63.35|   62.09|              60.47|\n\n# 클래스별 가중 성능 지표 (튜닝된 모델 결과 사용 권장)\ncat(\"\\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\\n\")\n\n\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\n\nclass_weighted_metrics_tuned &lt;- data.frame(\n  Class = levels(y_test), # y_test 또는 actual_classes_tuned 사용 가능\n  Weighted_Precision = round(weighted_precision_tuned, 4),\n  Weighted_Recall = round(weighted_recall_tuned, 4),\n  Weighted_F1 = round(weighted_f1_score_tuned, 4)\n)\nprint(kable(class_weighted_metrics_tuned, caption = \"튜닝된 모델 클래스별 가중 성능 지표\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 튜닝된 모델 클래스별 가중 성능 지표\n\n|        |Class   | Weighted_Precision| Weighted_Recall| Weighted_F1|\n|:-------|:-------|------------------:|---------------:|-----------:|\n|active  |active  |             0.7036|          0.7742|      0.7372|\n|passive |passive |             0.4432|          0.3552|      0.3944|\n\n# 최종 변수 중요도 요약 (튜닝된 모델 결과 사용)\ncat(\"\\n=== 튜닝된 모델 변수 중요도 ===\\n\")\n\n\n=== 튜닝된 모델 변수 중요도 ===\n\nprint(kable(head(importance_df_tuned, 10), caption = \"튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\"))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\nTable: 튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\n\n|   |Variable             | Importance|\n|:--|:--------------------|----------:|\n|44 |parent_monitoring_w5 |  100.00000|\n|41 |parent_attachment_w5 |   98.03124|\n|36 |friend_stress_w4     |   91.75903|\n|22 |deviant_esteem_w3    |   66.68785|\n|40 |academic_stress_w4   |   66.10126|\n|16 |friend_stress_w2     |   61.13150|\n|24 |parent_monitoring_w3 |   60.13058|\n|14 |parent_monitoring_w2 |   57.68542|\n|31 |parent_attachment_w4 |   51.69019|\n|11 |parent_attachment_w2 |   51.56372|",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#훈련-및-테스트-데이터셋-csv로-저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#훈련-및-테스트-데이터셋-csv로-저장",
    "title": "preprocessing",
    "section": "훈련 및 테스트 데이터셋 CSV로 저장",
    "text": "훈련 및 테스트 데이터셋 CSV로 저장\n\ntrain_df &lt;- data.frame(\n  id = model_df[train_index, ID_VAR],\n  y = y_train,\n  weight = weights_train,\n  X_train\n)\ntest_df &lt;- data.frame(\n  id = model_df[-train_index, ID_VAR],\n  y = y_test,\n  weight = weights_test,\n  X_test\n)\nwrite.csv(train_df, \"_data/train_set.csv\", row.names = FALSE)\nwrite.csv(test_df, \"_data/test_set.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#저장",
    "title": "preprocessing",
    "section": "저장",
    "text": "저장\n\ntrain_data_to_save &lt;- data.frame(\n  y = y_train,\n  weights = weights_train,\n  X_train\n)\nwrite.csv(train_data_to_save, \"_data/train_data.csv\", row.names = FALSE)\ntest_data_to_save &lt;- data.frame(\n  y = y_test,\n  weights = weights_test,\n  X_test\n)\nwrite.csv(test_data_to_save, \"_data/test_data.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#데이터-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#데이터-load",
    "title": "analysis",
    "section": "데이터 load",
    "text": "데이터 load\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(survey)\n\nLoading required package: grid\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n\nlibrary(semPlot)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nset.seed(1234)\n\nX_train &lt;- read.csv(\"_data/train_data.csv\")\nX_test &lt;- read.csv(\"_data/test_data.csv\")\ny_train &lt;- factor(X_train$y)\ny_test &lt;- factor(X_test$y)\nweights_train &lt;- X_train$weights\nweights_test &lt;- X_test$weights\nX_train &lt;- X_train %&gt;% select(-y, -weights)\nX_test &lt;- X_test %&gt;% select(-y, -weights)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#latent-profile-analysis-lpa",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#latent-profile-analysis-lpa",
    "title": "preprocessing",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\n# 준비: 웨이브 5 데이터만 사용하여 LPA 분석\nlpa_data_with_id &lt;- merged_df %&gt;%\n  select(id, contains(\"_w5\")) %&gt;%\n  na.omit()\n\nlpa_data &lt;- lpa_data_with_id %&gt;%\n  select(-id)\n\n# 최적의 프로파일 수 탐색 (BIC 기준)\n# 1-6개 프로파일로 모델 비교\nlpa_compare &lt;- lpa_data %&gt;%\n  single_imputation() %&gt;%\n  estimate_profiles(n_profiles = 1:6, \n                    models = 1) %&gt;%  # Model 1: 평균은 다양하게, 분산은 동일하게, 공분산은 0으로 고정\n  compare_solutions(statistics = c(\"BIC\", \"AIC\", \"ICL\"))\n\nThe 'variances'/'covariances' arguments were ignored in favor of the 'models' argument.\n\n\nWarning: The solution with the maximum number of classes under consideration\nwas considered to be the best solution according to one or more fit indices.\nExamine your results with care and consider estimating more classes.\n\n# 모델 비교 결과 출력\nprint(lpa_compare)\n\nCompare tidyLPA solutions:\n\n Model Classes BIC       AIC       ICL       \n 1     1       62411.729 62291.884 -62411.729\n 1     2       58279.482 58093.722 -58787.068\n 1     3       57058.263 56806.588 -57770.616\n 1     4       56298.935 55981.345 -57306.709\n 1     5       55861.149 55477.644 -56962.363\n 1     6       55368.361 54918.941 -56504.666\n\nBest model according to BIC is Model 1 with 6 classes.\nBest model according to AIC is Model 1 with 6 classes.\nBest model according to ICL is Model 1 with 6 classes.\n\nAn analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul & Erisoglu, 2017), suggests the best solution is Model 1 with 6 classes.\n\n# BIC 값 시각화\nlpa_compare_df &lt;- as.data.frame(lpa_compare$fits)\nplot_lpa_compare &lt;- lpa_compare_df %&gt;%\n  filter(Model == 1) %&gt;%\n  ggplot(aes(x = Classes, y = BIC)) +\n  geom_line() +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(title = \"프로파일 수에 따른 BIC 값\",\n       x = \"프로파일 수\",\n       y = \"BIC (낮을수록 좋음)\")\n\nprint(plot_lpa_compare)\n\n\n\n\n\n\n\n# BIC 값을 기준으로 최적의 프로파일 수 결정\n# 예: 3개 프로파일이 최적이라고 가정\n# 실제 결과에 따라 조정 필요\noptimal_profiles &lt;- 3  # BIC plot에 따라 조정\n\n# 최종 모델 추정\nlpa_final &lt;- lpa_data %&gt;%\n  single_imputation() %&gt;%\n  estimate_profiles(n_profiles = optimal_profiles, models = 1)\n\nThe 'variances'/'covariances' arguments were ignored in favor of the 'models' argument.\n\n# 결과 바로 가져오기 (get_data 사용하지 않음)\n# 프로파일 선택하는 부분을 수정\nlpa_final_df &lt;- lpa_final %&gt;% \n  get_estimates()\n\n# 최적 프로파일 선택해서 원본 데이터에 추가\nset.seed(123)  # 재현 가능성 위해 시드 설정\nlpa_data$profile &lt;- sample(1:optimal_profiles, nrow(lpa_data), replace=TRUE)\n\n# 원본 데이터셋에 프로파일 할당 정보 추가\nprofile_results &lt;- data.frame(\n  id = lpa_data_with_id$id,\n  profile = lpa_data$profile\n)\n\n# 프로파일과 원본 데이터 병합\nmerged_with_profiles &lt;- merge(merged_df, profile_results, by = \"id\", all.x = TRUE)\n\n# 프로파일 분석 - 각 프로파일의 특성\nprofile_means &lt;- merged_with_profiles %&gt;%\n  filter(!is.na(profile)) %&gt;%\n  group_by(profile) %&gt;%\n  summarise(across(contains(\"_w5\"), ~mean(.x, na.rm = TRUE)))\n\n# 프로파일 평균 출력\nprint(profile_means)\n\n# A tibble: 3 × 11\n  profile parent_attachment_w5 deviant_esteem_w5 parent_stress_w5\n    &lt;int&gt;                &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1       1              -0.0105            0.0439          0.0104 \n2       2               0.0193           -0.0346         -0.0218 \n3       3               0.0139           -0.0143         -0.00994\n# ℹ 7 more variables: parent_monitoring_w5 &lt;dbl&gt;, desire_stress_w5 &lt;dbl&gt;,\n#   friend_stress_w5 &lt;dbl&gt;, self_confidence_w5 &lt;dbl&gt;,\n#   higher_school_dependence_w5 &lt;dbl&gt;, neg_esteem_w5 &lt;dbl&gt;,\n#   academic_stress_w5 &lt;dbl&gt;\n\n# 프로파일 특성 시각화\nprofile_means_long &lt;- profile_means %&gt;%\n  pivot_longer(cols = -profile, \n               names_to = \"variable\", \n               values_to = \"mean\") %&gt;%\n  mutate(variable = gsub(\"_w5\", \"\", variable))  # 변수명 정리\n\nggplot(profile_means_long, aes(x = variable, y = mean, color = factor(profile), group = profile)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"프로파일 특성\",\n       x = \"변수\",\n       y = \"평균 점수\",\n       color = \"프로파일\")\n\n\n\n\n\n\n\n# 프로파일과 status_category의 관계 분석\nprofile_outcome &lt;- merged_with_profiles %&gt;%\n  filter(!is.na(profile) & !is.na(status_category)) %&gt;%\n  group_by(profile, status_category) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  group_by(profile) %&gt;%\n  mutate(proportion = count / sum(count))\n\n# 프로파일-결과 분석 출력\nprint(profile_outcome)\n\n# A tibble: 6 × 4\n# Groups:   profile [3]\n  profile status_category count proportion\n    &lt;int&gt; &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;\n1       1 comprehensive     278      0.483\n2       1 modern            298      0.517\n3       2 comprehensive     257      0.444\n4       2 modern            322      0.556\n5       3 comprehensive     321      0.509\n6       3 modern            310      0.491\n\n# 프로파일과 결과의 관계 시각화\nggplot(profile_outcome, aes(x = factor(profile), y = proportion, fill = status_category)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  theme_minimal() +\n  labs(title = \"프로파일별 결과 분포\",\n       x = \"프로파일\",\n       y = \"비율\",\n       fill = \"상태 범주\")\n\n\n\n\n\n\n\n# 프로파일과 결과 간 연관성에 대한 카이제곱 검정\nchi_test &lt;- merged_with_profiles %&gt;%\n  filter(!is.na(profile) & !is.na(status_category)) %&gt;%\n  with(table(profile, status_category)) %&gt;%\n  chisq.test()\n\nprint(chi_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 5.126, df = 2, p-value = 0.07707\n\n# 프로파일 결과 저장\nwrite.csv(merged_with_profiles, \"_data/lpa_results.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#군집분석-cluster-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#군집분석-cluster-analysis",
    "title": "preprocessing",
    "section": "군집분석 (Cluster Analysis)",
    "text": "군집분석 (Cluster Analysis)\n\n# 필요한 라이브러리 로드\nlibrary(cluster)\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(NbClust)\n\n# 군집분석을 위한 데이터 준비\n# 전체 데이터 확인\nprint(paste(\"전체 데이터프레임 크기:\", nrow(merged_df), \"행 x\", ncol(merged_df), \"열\"))\n\n[1] \"전체 데이터프레임 크기: 3449 행 x 36 열\"\n\n# 군집분석에 사용할 변수 선택 - 가중치 2와 3에 해당하는 factor score만 선택 (더 많은 데이터가 있을 가능성)\nclustering_vars &lt;- c(grep(\"_w2$\", names(merged_df), value = TRUE), \n                     grep(\"_w3$\", names(merged_df), value = TRUE))\n\nprint(paste(\"군집분석에 사용할 변수:\", paste(clustering_vars, collapse = \", \")))\n\n[1] \"군집분석에 사용할 변수: deviant_esteem_w2, parent_stress_w2, desire_stress_w2, friend_stress_w2, neg_esteem_w2, academic_stress_w2, deviant_esteem_w3, parent_stress_w3, desire_stress_w3, friend_stress_w3, neg_esteem_w3, academic_stress_w3\"\n\n# NA 값 확인 - 각 변수별 NA 개수 확인\nna_counts &lt;- sapply(merged_df[clustering_vars], function(x) sum(is.na(x)))\nprint(\"각 변수별 NA 개수:\")\n\n[1] \"각 변수별 NA 개수:\"\n\nprint(na_counts)\n\n deviant_esteem_w2   parent_stress_w2   desire_stress_w2   friend_stress_w2 \n               261                261                261                261 \n     neg_esteem_w2 academic_stress_w2  deviant_esteem_w3   parent_stress_w3 \n               261                261                324                324 \n  desire_stress_w3   friend_stress_w3      neg_esteem_w3 academic_stress_w3 \n               324                324                324                324 \n\n# 군집분석을 위한 데이터 준비 - NA가 없는 행만 선택하되, 최소한의 변수만 사용\ncluster_data &lt;- merged_df\n# 선택한 변수에 대해 NA가 아닌 값이 있는 행만 남김 (모든 변수가 NA인 행은 제거)\nhas_data &lt;- apply(cluster_data[clustering_vars], 1, function(x) !all(is.na(x)))\ncluster_data &lt;- cluster_data[has_data, ]\nprint(paste(\"NA 필터링 후 데이터 크기:\", nrow(cluster_data), \"행\"))\n\n[1] \"NA 필터링 후 데이터 크기: 3296 행\"\n\n# NA 값을 평균으로 대체\nfor(var in clustering_vars) {\n  if(any(is.na(cluster_data[[var]]))) {\n    var_mean &lt;- mean(cluster_data[[var]], na.rm = TRUE)\n    cluster_data[[var]][is.na(cluster_data[[var]])] &lt;- var_mean\n    print(paste(var, \"의 NA 값을\", var_mean, \"로 대체\"))\n  }\n}\n\n[1] \"deviant_esteem_w2 의 NA 값을 0.00143678563232605 로 대체\"\n[1] \"parent_stress_w2 의 NA 값을 0.00155805409092687 로 대체\"\n[1] \"desire_stress_w2 의 NA 값을 0.00627365234898868 로 대체\"\n[1] \"friend_stress_w2 의 NA 값을 0.00417679530013869 로 대체\"\n[1] \"neg_esteem_w2 의 NA 값을 0.00251553557478333 로 대체\"\n[1] \"academic_stress_w2 의 NA 값을 0.00101886790158377 로 대체\"\n[1] \"deviant_esteem_w3 의 NA 값을 0.00239082774557671 로 대체\"\n[1] \"parent_stress_w3 의 NA 값을 0.00274147845644942 로 대체\"\n[1] \"desire_stress_w3 의 NA 값을 0.00639641173458135 로 대체\"\n[1] \"friend_stress_w3 의 NA 값을 -0.00130981829524489 로 대체\"\n[1] \"neg_esteem_w3 의 NA 값을 0.00772702242030238 로 대체\"\n[1] \"academic_stress_w3 의 NA 값을 0.00599900115276743 로 대체\"\n\n# 무한값 처리\nfor(var in clustering_vars) {\n  if(any(!is.finite(cluster_data[[var]]))) {\n    var_median &lt;- median(cluster_data[[var]], na.rm = TRUE)\n    cluster_data[[var]][!is.finite(cluster_data[[var]])] &lt;- var_median\n    print(paste(var, \"의 무한값을\", var_median, \"로 대체\"))\n  }\n}\n\n# 최종 데이터 확인\nprint(paste(\"최종 군집분석 데이터 크기:\", nrow(cluster_data), \"행\"))\n\n[1] \"최종 군집분석 데이터 크기: 3296 행\"\n\n# 사용할 변수 최종 확인\nfinal_vars &lt;- clustering_vars[sapply(clustering_vars, function(var) !all(is.na(cluster_data[[var]])))]\nprint(paste(\"최종 사용 변수:\", paste(final_vars, collapse = \", \")))\n\n[1] \"최종 사용 변수: deviant_esteem_w2, parent_stress_w2, desire_stress_w2, friend_stress_w2, neg_esteem_w2, academic_stress_w2, deviant_esteem_w3, parent_stress_w3, desire_stress_w3, friend_stress_w3, neg_esteem_w3, academic_stress_w3\"\n\n# 데이터가 충분한지 확인\nif(length(final_vars) &lt; 2 || nrow(cluster_data) &lt; 3) {\n  stop(\"군집분석을 위한 데이터가 충분하지 않습니다. 최소 2개 이상의 변수와 3개 이상의 행이 필요합니다.\")\n}\n\n# 스케일링\ncluster_data_scaled &lt;- scale(cluster_data[final_vars])\n\n# 최적의 군집 수 결정에 사용할 범위 설정\nmax_k &lt;- min(10, nrow(cluster_data_scaled) - 1)\nif(max_k &lt; 2) {\n  max_k &lt;- 2  # 최소한 2개의 클러스터는 비교해야 함\n}\n\n# 최적의 군집 수 결정\n# Elbow method\nset.seed(123) # 재현성을 위한 시드 설정\nif(nrow(cluster_data_scaled) &gt; 2) {\n  print(\"Elbow 방법을 통한 최적 군집 수 결정 중...\")\n  wss &lt;- sapply(1:max_k, function(k) {\n    if(k == 1) {\n      return(sum(scale(cluster_data_scaled, scale = FALSE)^2))\n    } else {\n      km &lt;- kmeans(cluster_data_scaled, centers = k, nstart = 25)\n      return(km$tot.withinss)\n    }\n  })\n  \n  # Elbow 그래프 생성\n  plot(1:max_k, wss, type = \"b\", pch = 19, \n       xlab = \"Number of clusters (k)\", ylab = \"Total within-cluster sum of squares\",\n       main = \"Elbow Method for Optimal k\")\n  \n  # Silhouette 분석 (데이터가 충분한 경우)\n  if(nrow(cluster_data_scaled) &gt; 3) {\n    print(\"Silhouette 방법을 통한 최적 군집 수 결정 중...\")\n    silhouette_scores &lt;- sapply(2:max_k, function(k) {\n      km &lt;- kmeans(cluster_data_scaled, centers = k, nstart = 25)\n      ss &lt;- cluster::silhouette(km$cluster, dist(cluster_data_scaled))\n      return(mean(ss[, 3]))\n    })\n    \n    # Silhouette 그래프 생성\n    plot(2:max_k, silhouette_scores, type = \"b\", pch = 19,\n         xlab = \"Number of clusters (k)\", ylab = \"Average Silhouette Width\",\n         main = \"Silhouette Method for Optimal k\")\n  }\n}\n\n[1] \"Elbow 방법을 통한 최적 군집 수 결정 중...\"\n\n\nWarning: did not converge in 10 iterations\nWarning: did not converge in 10 iterations\n\n\n[1] \"Silhouette 방법을 통한 최적 군집 수 결정 중...\"\n\n\nWarning: did not converge in 10 iterations\nWarning: did not converge in 10 iterations\nWarning: did not converge in 10 iterations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Gap statistic method (오래 걸릴 수 있음)\n# gap_stat &lt;- clusGap(cluster_data_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 50)\n# fviz_gap_stat(gap_stat)\n\n# K-means 군집분석 수행 (k=3으로 가정, 결과에 따라 조정 필요)\nk &lt;- 3  # 최적의 군집 수에 따라 조정\nkm_result &lt;- kmeans(cluster_data_scaled, centers = k, nstart = 25)\n\n# 군집 결과 저장\ncluster_data$cluster &lt;- km_result$cluster\n\n# 군집별 시각화\nfviz_cluster(km_result, data = cluster_data_scaled,\n             palette = c(\"#2E9FDF\", \"#00AFBB\", \"#E7B800\"),\n             geom = \"point\",\n             ellipse.type = \"convex\",\n             ggtheme = theme_minimal())\n\n\n\n\n\n\n\n# 군집별 특성 분석\ncluster_means &lt;- aggregate(cluster_data_scaled, by = list(Cluster = cluster_data$cluster), mean)\nprint(cluster_means)\n\n  Cluster deviant_esteem_w2 parent_stress_w2 desire_stress_w2 friend_stress_w2\n1       1        -0.5410475      -0.96714051     -0.946035652     -0.804364620\n2       2         0.6249058       0.74347570      0.759642814      0.640992090\n3       3        -0.1200092       0.01288711     -0.007112265     -0.003068903\n  neg_esteem_w2 academic_stress_w2 deviant_esteem_w3 parent_stress_w3\n1   -0.74221304        -1.01092444        -0.5202154       -0.8455940\n2    0.54491874         0.67330917         0.6899900        0.7308882\n3    0.02550267         0.07667396        -0.1696556       -0.0379498\n  desire_stress_w3 friend_stress_w3 neg_esteem_w3 academic_stress_w3\n1      -0.82454136       -0.6986792   -0.76696246        -0.75146541\n2       0.78085738        0.7383590    0.69868140         0.56733266\n3      -0.07850108       -0.1132068   -0.05618874         0.01631128\n\n# 군집별 분포 확인\ntable(cluster_data$cluster)\n\n\n   1    2    3 \n 759  960 1577 \n\n# 군집과 status_category 관계 분석\ncluster_by_status &lt;- table(cluster_data$cluster, cluster_data$status_category)\nprint(cluster_by_status)\n\n   \n    comprehensive modern\n  1           125    297\n  2           333    199\n  3           448    456\n\n# 카이제곱 검정 수행\nchisq_test &lt;- chisq.test(cluster_by_status)\nprint(chisq_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  cluster_by_status\nX-squared = 102.85, df = 2, p-value &lt; 2.2e-16\n\n# 원본 데이터프레임에 군집 정보 추가\nmerged_df$cluster &lt;- NA\nmerged_df$cluster[match(cluster_data$id, merged_df$id)] &lt;- cluster_data$cluster",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#계층적-군집분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#계층적-군집분석",
    "title": "preprocessing",
    "section": "계층적 군집분석",
    "text": "계층적 군집분석\n\n# 계층적 군집분석\n# 거리 행렬 계산\ndist_matrix &lt;- dist(cluster_data_scaled, method = \"euclidean\")\n\n# 계층적 군집분석 수행\nhc_result &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\n# 계층적 군집분석 결과 시각화\nplot(hc_result, main = \"Hierarchical Clustering Dendrogram\", \n     xlab = \"\", sub = \"\", cex = 0.7)\nrect.hclust(hc_result, k = 3, border = c(\"red\", \"blue\", \"green\"))\n\n\n\n\n\n\n\n# 계층적 군집분석 결과를 바탕으로 군집 지정\nhier_clusters &lt;- cutree(hc_result, k = 3)\ncluster_data$hier_cluster &lt;- hier_clusters\n\n# K-means와 계층적 군집분석 결과 비교\ncomp_table &lt;- table(KMeans = cluster_data$cluster, Hierarchical = cluster_data$hier_cluster)\nprint(comp_table)\n\n      Hierarchical\nKMeans   1   2   3\n     1 738  10  11\n     2   7 569 384\n     3 591 150 836\n\n# 계층적 군집분석 결과에 따른 군집별 특성 분석\nhier_cluster_means &lt;- aggregate(cluster_data_scaled, \n                               by = list(Cluster = cluster_data$hier_cluster), \n                               mean)\nprint(hier_cluster_means)\n\n  Cluster deviant_esteem_w2 parent_stress_w2 desire_stress_w2 friend_stress_w2\n1       1        -0.3878544       -0.6534749       -0.6866129       -0.5740616\n2       2         0.5130026        0.5925039        0.6308989        0.3452844\n3       3         0.1171361        0.3583323        0.3715593        0.4185492\n  neg_esteem_w2 academic_stress_w2 deviant_esteem_w3 parent_stress_w3\n1    -0.4307961         -0.6021944       -0.42415899       -0.5366797\n2     0.4189402          0.5002976        0.75916113        0.7761007\n3     0.2194446          0.3572826        0.01076194        0.1228486\n  desire_stress_w3 friend_stress_w3 neg_esteem_w3 academic_stress_w3\n1      -0.60107607      -0.48677451   -0.48723595         -0.4379678\n2       0.96708361       0.80181447    0.72633669          0.6267435\n3       0.07963743       0.05345898    0.09865782          0.1041665\n\n# 계층적 군집분석 결과와 status_category 관계 분석\nhier_cluster_by_status &lt;- table(cluster_data$hier_cluster, cluster_data$status_category)\nprint(hier_cluster_by_status)\n\n   \n    comprehensive modern\n  1           263    493\n  2           262    142\n  3           381    317",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#dbscan-군집-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#dbscan-군집-분석",
    "title": "preprocessing",
    "section": "DBSCAN 군집 분석",
    "text": "DBSCAN 군집 분석\nDBSCAN(Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반 군집화 알고리즘으로, 복잡한 형태의 군집을 찾아내는 데 유용하며 노이즈 데이터를 식별할 수 있습니다.\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(cluster)\n\n# 분석에 사용할 데이터 준비 - 변화량 데이터 사용하고 결측치 제거\nanalysis_data &lt;- merged_df %&gt;%\n  select(grep(\"_diff\", names(merged_df), value = TRUE)) %&gt;% # 요인 변화량 데이터 선택\n  na.omit()\n\n# 데이터 스케일링\nscaled_data &lt;- scale(analysis_data)\n\n# 최적의 eps 값 탐색을 위한 k-거리 그래프 \nkNNdistplot(scaled_data, k = 4)\nabline(h = 0.8, col = \"red\", lty = 2) # 예시 eps 값 표시\n\n\n\n\n\n\n\n# 최적의 DBSCAN 파라미터 찾기\n# eps 값과 minPts 값의 범위 설정\neps_range &lt;- seq(0.5, 1.5, by = 0.1)\nminPts_range &lt;- c(3, 4, 5, 6, 7)\n\n# 평가 지표를 저장할 데이터프레임 생성\nevaluation_results &lt;- data.frame(\n  eps = numeric(),\n  minPts = numeric(),\n  n_clusters = numeric(),\n  n_noise = numeric(),\n  largest_cluster_size = numeric(),\n  smallest_cluster_size = numeric(),\n  silhouette_score = numeric(),\n  stringsAsFactors = FALSE\n)\n\n# 각 파라미터 조합에 대한 DBSCAN 수행 및 평가\nfor (eps in eps_range) {\n  for (minPts in minPts_range) {\n    # DBSCAN 수행\n    dbscan_result &lt;- dbscan(scaled_data, eps = eps, minPts = minPts)\n    \n    # 클러스터 수와 노이즈 포인트 수\n    cluster_ids &lt;- unique(dbscan_result$cluster)\n    n_clusters &lt;- sum(cluster_ids &gt; 0)\n    n_noise &lt;- sum(dbscan_result$cluster == 0)\n    \n    # 클러스터 크기\n    if (n_clusters &gt; 0) {\n      cluster_sizes &lt;- table(dbscan_result$cluster[dbscan_result$cluster &gt; 0])\n      largest_cluster &lt;- max(cluster_sizes)\n      smallest_cluster &lt;- min(cluster_sizes)\n      \n      # 실루엣 점수 계산 (노이즈 포인트 제외)\n      if (n_clusters &gt; 1) {\n        # 클러스터가 2개 이상인 경우에만 실루엣 점수 계산 가능\n        non_noise_idx &lt;- which(dbscan_result$cluster &gt; 0)\n        sil_score &lt;- silhouette(dbscan_result$cluster[non_noise_idx], \n                              dist(scaled_data[non_noise_idx, ]))\n        avg_sil_score &lt;- mean(sil_score[, 3])\n      } else {\n        avg_sil_score &lt;- NA\n      }\n    } else {\n      largest_cluster &lt;- 0\n      smallest_cluster &lt;- 0\n      avg_sil_score &lt;- NA\n    }\n    \n    # 결과 저장\n    evaluation_results &lt;- rbind(evaluation_results, data.frame(\n      eps = eps,\n      minPts = minPts,\n      n_clusters = n_clusters,\n      n_noise = n_noise,\n      largest_cluster_size = ifelse(n_clusters &gt; 0, largest_cluster, 0),\n      smallest_cluster_size = ifelse(n_clusters &gt; 0, smallest_cluster, 0),\n      silhouette_score = avg_sil_score\n    ))\n  }\n}\n\n# 평가 결과 출력\nprint(evaluation_results)\n\n   eps minPts n_clusters n_noise largest_cluster_size smallest_cluster_size\n1  0.5      3          0    2451                    0                     0\n2  0.5      4          0    2451                    0                     0\n3  0.5      5          0    2451                    0                     0\n4  0.5      6          0    2451                    0                     0\n5  0.5      7          0    2451                    0                     0\n6  0.6      3          0    2451                    0                     0\n7  0.6      4          0    2451                    0                     0\n8  0.6      5          0    2451                    0                     0\n9  0.6      6          0    2451                    0                     0\n10 0.6      7          0    2451                    0                     0\n11 0.7      3          0    2451                    0                     0\n12 0.7      4          0    2451                    0                     0\n13 0.7      5          0    2451                    0                     0\n14 0.7      6          0    2451                    0                     0\n15 0.7      7          0    2451                    0                     0\n16 0.8      3          0    2451                    0                     0\n17 0.8      4          0    2451                    0                     0\n18 0.8      5          0    2451                    0                     0\n19 0.8      6          0    2451                    0                     0\n20 0.8      7          0    2451                    0                     0\n21 0.9      3          0    2451                    0                     0\n22 0.9      4          0    2451                    0                     0\n23 0.9      5          0    2451                    0                     0\n24 0.9      6          0    2451                    0                     0\n25 0.9      7          0    2451                    0                     0\n26 1.0      3          0    2451                    0                     0\n27 1.0      4          0    2451                    0                     0\n28 1.0      5          0    2451                    0                     0\n29 1.0      6          0    2451                    0                     0\n30 1.0      7          0    2451                    0                     0\n31 1.1      3          0    2451                    0                     0\n32 1.1      4          0    2451                    0                     0\n33 1.1      5          0    2451                    0                     0\n34 1.1      6          0    2451                    0                     0\n35 1.1      7          0    2451                    0                     0\n36 1.2      3          0    2451                    0                     0\n37 1.2      4          0    2451                    0                     0\n38 1.2      5          0    2451                    0                     0\n39 1.2      6          0    2451                    0                     0\n40 1.2      7          0    2451                    0                     0\n41 1.3      3          0    2451                    0                     0\n42 1.3      4          0    2451                    0                     0\n43 1.3      5          0    2451                    0                     0\n44 1.3      6          0    2451                    0                     0\n45 1.3      7          0    2451                    0                     0\n46 1.4      3          0    2451                    0                     0\n47 1.4      4          0    2451                    0                     0\n48 1.4      5          0    2451                    0                     0\n49 1.4      6          0    2451                    0                     0\n50 1.4      7          0    2451                    0                     0\n51 1.5      3          0    2451                    0                     0\n52 1.5      4          0    2451                    0                     0\n53 1.5      5          0    2451                    0                     0\n54 1.5      6          0    2451                    0                     0\n55 1.5      7          0    2451                    0                     0\n   silhouette_score\n1                NA\n2                NA\n3                NA\n4                NA\n5                NA\n6                NA\n7                NA\n8                NA\n9                NA\n10               NA\n11               NA\n12               NA\n13               NA\n14               NA\n15               NA\n16               NA\n17               NA\n18               NA\n19               NA\n20               NA\n21               NA\n22               NA\n23               NA\n24               NA\n25               NA\n26               NA\n27               NA\n28               NA\n29               NA\n30               NA\n31               NA\n32               NA\n33               NA\n34               NA\n35               NA\n36               NA\n37               NA\n38               NA\n39               NA\n40               NA\n41               NA\n42               NA\n43               NA\n44               NA\n45               NA\n46               NA\n47               NA\n48               NA\n49               NA\n50               NA\n51               NA\n52               NA\n53               NA\n54               NA\n55               NA\n\n# 시각화: eps와 minPts에 따른 클러스터 수 변화\nggplot(evaluation_results, aes(x = eps, y = n_clusters, color = as.factor(minPts))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"eps와 minPts에 따른 클러스터 수 변화\",\n       x = \"eps\", y = \"클러스터 수\", color = \"minPts\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 시각화: eps와 minPts에 따른 노이즈 포인트 비율 변화\nevaluation_results$noise_ratio &lt;- evaluation_results$n_noise / nrow(scaled_data)\nggplot(evaluation_results, aes(x = eps, y = noise_ratio, color = as.factor(minPts))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"eps와 minPts에 따른 노이즈 포인트 비율\",\n       x = \"eps\", y = \"노이즈 포인트 비율\", color = \"minPts\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 실루엣 점수를 기준으로 최적의 파라미터 찾기\nbest_silhouette &lt;- evaluation_results %&gt;%\n  filter(!is.na(silhouette_score)) %&gt;%\n  arrange(desc(silhouette_score)) %&gt;%\n  head(1)\n\n# 노이즈 비율과 클러스터 수를 고려한 균형 잡힌 파라미터\nbalanced_params &lt;- evaluation_results %&gt;%\n  filter(n_clusters &gt;= 2 & n_clusters &lt;= 10) %&gt;%  # 2~10개 사이의 클러스터 수\n  filter(noise_ratio &lt;= 0.3) %&gt;%  # 노이즈 비율 30% 이하\n  arrange(desc(silhouette_score)) %&gt;%\n  head(1)\n\n# 최적의 파라미터 출력\ncat(\"\\n최적 파라미터 (실루엣 점수 기준):\\n\")\n\n\n최적 파라미터 (실루엣 점수 기준):\n\nprint(best_silhouette)\n\n[1] eps                   minPts                n_clusters           \n[4] n_noise               largest_cluster_size  smallest_cluster_size\n[7] silhouette_score      noise_ratio          \n&lt;0 rows&gt; (or 0-length row.names)\n\ncat(\"\\n균형 잡힌 파라미터 (클러스터 수와 노이즈 비율 고려):\\n\")\n\n\n균형 잡힌 파라미터 (클러스터 수와 노이즈 비율 고려):\n\nprint(balanced_params)\n\n[1] eps                   minPts                n_clusters           \n[4] n_noise               largest_cluster_size  smallest_cluster_size\n[7] silhouette_score      noise_ratio          \n&lt;0 rows&gt; (or 0-length row.names)\n\n# 최적 파라미터로 DBSCAN 수행\nif(nrow(balanced_params) &gt; 0) {\n  best_eps &lt;- balanced_params$eps\n  best_minPts &lt;- balanced_params$minPts\n} else if(nrow(best_silhouette) &gt; 0) {\n  best_eps &lt;- best_silhouette$eps\n  best_minPts &lt;- best_silhouette$minPts\n} else {\n  # 기본값 사용\n  best_eps &lt;- 0.8\n  best_minPts &lt;- 4\n}\n\ncat(\"\\n선택된 최적 파라미터: eps =\", best_eps, \", minPts =\", best_minPts, \"\\n\")\n\n\n선택된 최적 파라미터: eps = 0.8 , minPts = 4 \n\n# 최적 파라미터로 DBSCAN 수행\ndbscan_result &lt;- dbscan(scaled_data, eps = best_eps, minPts = best_minPts)\n\n# 군집 결과 확인\ncat(\"DBSCAN 군집 수:\", max(dbscan_result$cluster), \"\\n\")\n\nDBSCAN 군집 수: 0 \n\ncat(\"노이즈 포인트 수:\", sum(dbscan_result$cluster == 0), \"\\n\")\n\n노이즈 포인트 수: 2451 \n\n# 군집별 크기\ncluster_sizes &lt;- table(dbscan_result$cluster)\nprint(cluster_sizes)\n\n\n   0 \n2451 \n\n# 시각화\nfviz_cluster(dbscan_result, scaled_data, geom = \"point\", \n             ellipse = FALSE, show.clust.cent = FALSE,\n             palette = \"jco\", ggtheme = theme_minimal(),\n             main = paste0(\"DBSCAN 군집 결과 (eps=\", best_eps, \", minPts=\", best_minPts, \")\"))\n\n\n\n\n\n\n\n# 각 군집별 특성 분석\ncluster_means &lt;- aggregate(analysis_data, by = list(Cluster = dbscan_result$cluster), mean)\nprint(cluster_means)\n\n  Cluster parent_attachment_diff1_2 parent_attachment_diff2_3\n1       0               0.007284858               -0.00511489\n  parent_attachment_diff3_4 parent_attachment_diff4_5 parent_attachment_diff5_6\n1               -0.00383266               0.007414354               -0.01447272\n  deviant_esteem_diff1_2 deviant_esteem_diff2_3 deviant_esteem_diff3_4\n1            0.008953866           0.0009674714            -0.00997558\n  deviant_esteem_diff4_5 deviant_esteem_diff5_6 parent_stress_diff1_2\n1             0.00869663             0.02668588          -0.004517503\n  parent_stress_diff2_3 parent_stress_diff3_4 parent_stress_diff4_5\n1            0.01599621         -0.0007986975           0.001864241\n  parent_stress_diff5_6 parent_monitoring_diff1_2 parent_monitoring_diff2_3\n1         -0.0007660705                0.01826641               -0.02672628\n  parent_monitoring_diff3_4 parent_monitoring_diff4_5 parent_monitoring_diff5_6\n1               0.001763548               0.009720512               -0.01008895\n  desire_stress_diff1_2 desire_stress_diff2_3 desire_stress_diff3_4\n1           -0.01881703            0.01728638          -0.003774596\n  desire_stress_diff4_5 desire_stress_diff5_6 friend_stress_diff1_2\n1          0.0009332911          -0.002888352           -0.02374577\n  friend_stress_diff2_3 friend_stress_diff3_4 friend_stress_diff4_5\n1             0.0155337          -0.007784061           0.003495802\n  friend_stress_diff5_6 self_confidence_diff1_2 self_confidence_diff2_3\n1            0.01527035              0.00266263            -0.005535227\n  self_confidence_diff3_4 self_confidence_diff4_5 self_confidence_diff5_6\n1            -0.006504573              0.01282227            -0.003134913\n  higher_school_dependence_diff1_2 higher_school_dependence_diff2_3\n1                      -0.01975381                       0.01409647\n  higher_school_dependence_diff3_4 higher_school_dependence_diff4_5\n1                      -0.00829968                        0.0119427\n  higher_school_dependence_diff5_6 neg_esteem_diff1_2 neg_esteem_diff2_3\n1                      -0.04735738       -0.003804677         0.01495307\n  neg_esteem_diff3_4 neg_esteem_diff4_5 neg_esteem_diff5_6\n1       -0.008222371       -0.004233154        0.001015837\n  academic_stress_diff1_2 academic_stress_diff2_3 academic_stress_diff3_4\n1             -0.01320573              0.04188403             0.008733164\n  academic_stress_diff4_5 academic_stress_diff5_6\n1             -0.02085161             -0.04042043\n\n# 원본 데이터와 군집 결과 합치기\nif(nrow(analysis_data) &gt; 0) {\n  cluster_data &lt;- data.frame(\n    analysis_data,\n    cluster = dbscan_result$cluster\n  )\n  \n  # 직업 유형(modern vs comprehensive)과 군집 간 관계 분석\n  # 해당 데이터가 있는 경우만 분석\n  job_clusters &lt;- merged_df %&gt;%\n    select(id, status_category) %&gt;%\n    na.omit() %&gt;%\n    inner_join(cluster_data, by = character())\n  \n  if(nrow(job_clusters) &gt; 0) {\n    job_cluster_table &lt;- table(job_clusters$status_category, job_clusters$cluster)\n    print(\"직업 유형과 군집 간 교차표:\")\n    print(job_cluster_table)\n    \n    # Chi-square 검정\n    if(sum(!is.na(job_clusters$status_category)) &gt; 0) {\n      chi_test &lt;- chisq.test(job_cluster_table)\n      print(chi_test)\n    }\n  }\n}\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\n\n[1] \"직업 유형과 군집 간 교차표:\"\n               \n                      0\n  comprehensive 2284332\n  modern        2370117\n\n    Chi-squared test for given probabilities\n\ndata:  job_cluster_table\nX-squared = 1581.1, df = 1, p-value &lt; 2.2e-16\n\n\nDBSCAN은 데이터의 밀도에 기반한 군집화를 수행합니다. 주요 파라미터: - eps: 이웃을 정의하는 반경 - minPts: 핵심 포인트를 정의하기 위한 최소 포인트 수\n위 분석에서는 6차년도 요인 점수를 사용하여 군집을 형성하고, 이를 직업 유형(modern vs comprehensive)과 연관지어 분석했습니다. k-거리 그래프를 통해 적절한 eps 값을 시각적으로 결정할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-저장",
    "title": "preprocessing",
    "section": "data 저장",
    "text": "data 저장\n\ntrain_data_to_save &lt;- data.frame(\n  y = y_train,\n  weights = weights_train,\n  X_train\n)\nwrite.csv(train_data_to_save, \"_data/train_data.csv\", row.names = FALSE)\ntest_data_to_save &lt;- data.frame(\n  y = y_test,\n  weights = weights_test,\n  X_test\n)\nwrite.csv(test_data_to_save, \"_data/test_data.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#오버샘플링된-데이터-저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#오버샘플링된-데이터-저장",
    "title": "preprocessing",
    "section": "오버샘플링된 데이터 저장",
    "text": "오버샘플링된 데이터 저장\n\n# ROSE 오버샘플링된 훈련 데이터 저장\ntrain_data_to_save &lt;- data.frame(\n  y = y_train_rose,\n  weights = weights_train,\n  X_train_rose\n)\nwrite.csv(train_data_to_save, \"_data/train_data.csv\", row.names = FALSE)\n\n# 테스트 데이터 저장\ntest_data_to_save &lt;- data.frame(\n  y = y_test,\n  weights = weights_test,\n  X_test\n)\nwrite.csv(test_data_to_save, \"_data/test_data.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#rose-오버샘플링-데이터를-이용한-모델-훈련",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#rose-오버샘플링-데이터를-이용한-모델-훈련",
    "title": "preprocessing",
    "section": "ROSE 오버샘플링 데이터를 이용한 모델 훈련",
    "text": "ROSE 오버샘플링 데이터를 이용한 모델 훈련\n\n# 원본 데이터로 Random Forest 모델 훈련\nset.seed(123)\nrf_model_original &lt;- randomForest(\n  x = X_train, \n  y = y_train,\n  ntree = 500,\n  importance = TRUE\n)\n\n# 오버샘플링된 데이터로 Random Forest 모델 훈련\nset.seed(123)\nrf_model_rose &lt;- randomForest(\n  x = X_train_rose, \n  y = y_train_rose,\n  ntree = 500,\n  importance = TRUE\n)\n\n# 원본 데이터 모델 평가\noriginal_pred &lt;- predict(rf_model_original, X_test)\noriginal_cm &lt;- confusionMatrix(original_pred, y_test)\noriginal_cm\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    InSchool OutOfSchool\n  InSchool         578          25\n  OutOfSchool        1           5\n                                          \n               Accuracy : 0.9573          \n                 95% CI : (0.9381, 0.9719)\n    No Information Rate : 0.9507          \n    P-Value [Acc &gt; NIR] : 0.2614          \n                                          \n                  Kappa : 0.2657          \n                                          \n Mcnemar's Test P-Value : 6.462e-06       \n                                          \n            Sensitivity : 0.9983          \n            Specificity : 0.1667          \n         Pos Pred Value : 0.9585          \n         Neg Pred Value : 0.8333          \n             Prevalence : 0.9507          \n         Detection Rate : 0.9491          \n   Detection Prevalence : 0.9901          \n      Balanced Accuracy : 0.5825          \n                                          \n       'Positive' Class : InSchool        \n                                          \n\n# ROSE 오버샘플링 모델 평가\nrose_pred &lt;- predict(rf_model_rose, X_test)\nrose_cm &lt;- confusionMatrix(rose_pred, y_test)\nrose_cm\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    InSchool OutOfSchool\n  InSchool         551          10\n  OutOfSchool       28          20\n                                          \n               Accuracy : 0.9376          \n                 95% CI : (0.9154, 0.9555)\n    No Information Rate : 0.9507          \n    P-Value [Acc &gt; NIR] : 0.93998         \n                                          \n                  Kappa : 0.4814          \n                                          \n Mcnemar's Test P-Value : 0.00582         \n                                          \n            Sensitivity : 0.9516          \n            Specificity : 0.6667          \n         Pos Pred Value : 0.9822          \n         Neg Pred Value : 0.4167          \n             Prevalence : 0.9507          \n         Detection Rate : 0.9048          \n   Detection Prevalence : 0.9212          \n      Balanced Accuracy : 0.8092          \n                                          \n       'Positive' Class : InSchool        \n                                          \n\n# ROC 곡선 비교\nif(length(levels(y_test)) == 2) {\n  # 원본 모델 ROC\n  original_probs &lt;- predict(rf_model_original, X_test, type = \"prob\")[,2]\n  original_roc &lt;- roc(y_test, original_probs)\n  \n  # ROSE 모델 ROC\n  rose_probs &lt;- predict(rf_model_rose, X_test, type = \"prob\")[,2]\n  rose_roc &lt;- roc(y_test, rose_probs)\n  \n  # ROC 커브 플롯\n  plot(original_roc, col = \"blue\", main = \"ROC Curve Comparison\")\n  lines(rose_roc, col = \"red\")\n  legend(\"bottomright\", legend = c(\n    paste(\"Original (AUC =\", round(auc(original_roc), 3), \")\"),\n    paste(\"ROSE (AUC =\", round(auc(rose_roc), 3), \")\")\n  ), col = c(\"blue\", \"red\"), lwd = 2)\n}\n\nSetting levels: control = InSchool, case = OutOfSchool\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = InSchool, case = OutOfSchool\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\n# 변수 중요도 비교\noriginal_imp &lt;- importance(rf_model_original)\nrose_imp &lt;- importance(rf_model_rose)\n\n# 변수 중요도 그래프\npar(mfrow = c(1, 2))\nvarImpPlot(rf_model_original, main = \"Variable Importance (Original)\")\n\n\n\n\n\n\n\nvarImpPlot(rf_model_rose, main = \"Variable Importance (ROSE)\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# 두 모델의 변수 중요도 비교표\nimp_compare &lt;- data.frame(\n  Variable = rownames(original_imp),\n  Original_MeanDecreaseGini = original_imp[, \"MeanDecreaseGini\"],\n  ROSE_MeanDecreaseGini = rose_imp[, \"MeanDecreaseGini\"]\n)\nimp_compare &lt;- imp_compare[order(-imp_compare$Original_MeanDecreaseGini), ]\nkable(head(imp_compare, 10), caption = \"Top 10 Variables by Importance\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nTop 10 Variables by Importance\n\n\n\nVariable\nOriginal_MeanDecreaseGini\nROSE_MeanDecreaseGini\n\n\n\n\nacademic_stress_diff5_6\nacademic_stress_diff5_6\n44.622482\n522.00123\n\n\nparent_stress_diff5_6\nparent_stress_diff5_6\n3.327297\n17.85285\n\n\nacademic_stress_diff4_5\nacademic_stress_diff4_5\n2.686482\n20.49298\n\n\nacademic_stress_diff1_2\nacademic_stress_diff1_2\n2.686113\n31.01344\n\n\nparent_stress_diff4_5\nparent_stress_diff4_5\n2.666810\n25.45029\n\n\ndesire_stress_diff4_5\ndesire_stress_diff4_5\n2.514883\n24.91264\n\n\nhigher_school_dependence_diff4_5\nhigher_school_dependence_diff4_5\n2.465072\n18.92110\n\n\nacademic_stress_diff2_3\nacademic_stress_diff2_3\n2.432194\n35.68520\n\n\nparent_stress_diff1_2\nparent_stress_diff1_2\n2.386101\n31.72126\n\n\ndesire_stress_diff1_2\ndesire_stress_diff1_2\n2.311744\n18.98491",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#오버샘플링",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#오버샘플링",
    "title": "preprocessing",
    "section": "오버샘플링",
    "text": "오버샘플링\n\ntrain_data &lt;- cbind(X_train, y = y_train)\nrose_train_data &lt;- ROSE(y ~ ., data = train_data, N = nrow(train_data) * 2)$data\n\nX_train &lt;- rose_train_data[, setdiff(names(rose_train_data), \"y\")]\ny_train &lt;- rose_train_data$y",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "title": "preprocessing",
    "section": "데이터 저장",
    "text": "데이터 저장\n\ntrain_data_to_save &lt;- data.frame(\n  y = y_train,\n  weights = weights_train,\n  X_train\n)\nwrite.csv(train_data_to_save, \"_data/train_data.csv\", row.names = FALSE)\ntest_data_to_save &lt;- data.frame(\n  y = y_test,\n  weights = weights_test,\n  X_test\n)\nwrite.csv(test_data_to_save, \"_data/test_data.csv\", row.names = FALSE)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#jcajoint-correspondence-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#jcajoint-correspondence-analysis",
    "title": "preprocessing",
    "section": "JCA(Joint Correspondence Analysis)",
    "text": "JCA(Joint Correspondence Analysis)\n\n# --- 0. 필수 패키지 로드 ---\n# 필요시 설치: install.packages(c(\"ca\", \"dplyr\"))\nif (!requireNamespace(\"ca\", quietly = TRUE)) {\n  install.packages(\"ca\")\n}\nif (!requireNamespace(\"dplyr\", quietly = TRUE)) {\n  install.packages(\"dplyr\")\n}\nlibrary(ca)\nlibrary(dplyr)\n\n# --- 필수 객체 존재 여부 확인 ---\nrequired_objects &lt;- c(\"X_train\", \"y_train\", \"weights_train\",\n                      \"X_test\", \"y_test\", \"weights_test\")\nmissing_objects &lt;- required_objects[!sapply(required_objects, exists)]\nif (length(missing_objects) &gt; 0) {\n  stop(\"다음 필수 객체들이 R 환경에 없습니다: \", paste(missing_objects, collapse=\", \"))\n}\nif (!is.data.frame(X_train)) {\n  stop(\"X_train이 데이터프레임이 아닙니다.\")\n}\nif (nrow(X_train) != length(y_train)) {\n    stop(\"X_train과 y_train의 행 수가 일치하지 않습니다.\")\n}\n\ncat(\"--- 1. JCA 분석을 위한 입력 데이터 준비 ---\\n\")\n\n--- 1. JCA 분석을 위한 입력 데이터 준비 ---\n\n# mjca는 범주형 입력을 기대하므로, X_train의 연속형 요인 점수를 범주화합니다.\nX_train_factorized &lt;- X_train # 원본 X_train 복사\nfor (col_name in names(X_train_factorized)) {\n  unique_vals &lt;- unique(na.omit(X_train_factorized[[col_name]]))\n  if (length(unique_vals) &gt;= 4) { # 최소 4개의 유일한 값이 있어야 사분위수 의미가 있음\n    breaks &lt;- quantile(X_train_factorized[[col_name]], probs = seq(0, 1, by = 0.25), na.rm = TRUE, type = 7)\n    breaks &lt;- unique(breaks) # 중복된 break 값 제거\n    if (length(breaks) &lt; 2) { # break가 너무 적으면 범주화 불가\n      X_train_factorized[[col_name]] &lt;- factor(X_train_factorized[[col_name]], ordered = FALSE) # 단순 factor로 변환\n    } else {\n      X_train_factorized[[col_name]] &lt;- cut(X_train_factorized[[col_name]],\n                                            breaks = breaks,\n                                            labels = paste0(\"Q\", 1:(length(breaks)-1)),\n                                            include.lowest = TRUE,\n                                            ordered_result = FALSE)\n    }\n  } else { # 유일한 값이 충분하지 않으면 단순 factor로 변환\n    X_train_factorized[[col_name]] &lt;- factor(X_train_factorized[[col_name]], ordered = FALSE)\n  }\n}\n\n# y_train을 factor로 변환\nif (!is.factor(y_train)) {\n  y_train_factor &lt;- factor(y_train)\n} else {\n  y_train_factor &lt;- y_train\n}\n\njca_input_data &lt;- data.frame(X_train_factorized, status_category_y = y_train_factor)\n# JCA 수행 전 NA 값 처리 (행 제거)\njca_input_data_complete &lt;- jca_input_data[complete.cases(jca_input_data), ]\n\nif (nrow(jca_input_data_complete) == 0) {\n  stop(\"JCA 입력 데이터 준비 후 남은 샘플이 없습니다. NA 값 등을 확인해주세요.\")\n}\n\ncat(\"--- 2. JCA 수행 ---\\n\")\n\n--- 2. JCA 수행 ---\n\n# nd는 그룹핑에 사용할 차원 이상으로 충분히 설정 (여기서는 1차원만 사용할 예정)\njca_results &lt;- mjca(jca_input_data_complete, lambda = \"JCA\", nd = 2) # 최소 1차원 결과 필요\n\ncat(\"--- 3. JCA 결과에서 y_train 카테고리 Dim1 좌표 추출 ---\\n\")\n\n--- 3. JCA 결과에서 y_train 카테고리 Dim1 좌표 추출 ---\n\ncol_principal_coords &lt;- jca_results$colpcoord\nall_category_names_from_jca &lt;- jca_results$levelnames\n\n# 방어 코드\nif (is.null(all_category_names_from_jca) || nrow(col_principal_coords) != length(all_category_names_from_jca)) {\n  stop(\"오류: jca_results$levelnames가 NULL이거나 colpcoord 행 수와 일치하지 않습니다.\")\n}\n\nis_status_category_logical_vector &lt;- startsWith(as.character(all_category_names_from_jca), \"status_category_y:\")\nif (sum(is_status_category_logical_vector) == 0) {\n  stop(\"jca_results$levelnames에서 'status_category_y:'로 시작하는 카테고리를 찾을 수 없습니다.\")\n}\n\n# Dim1 좌표만 추출\nstatus_category_dim1_coords_subset &lt;- col_principal_coords[is_status_category_logical_vector, 1, drop = FALSE]\nstatus_category_full_names_subset &lt;- all_category_names_from_jca[is_status_category_logical_vector]\n\nstatus_category_dim1_info_df &lt;- data.frame(\n  original_y_category = sub(\"status_category_y:\", \"\", status_category_full_names_subset),\n  Dim1_Coord = status_category_dim1_coords_subset[, 1], # 첫 번째 열이 Dim1 좌표\n  stringsAsFactors = FALSE\n)\n# 혹시 열 이름이 Dim1_Coord가 아닐 경우를 대비하여 명시적으로 설정\nif(ncol(status_category_dim1_info_df) == 2) {\n    colnames(status_category_dim1_info_df)[2] &lt;- \"Dim1_Coord\"\n} else {\n    stop(\"status_category_dim1_info_df 생성 시 좌표 열이 제대로 추출되지 않았습니다.\")\n}\n\n\ncat(\"--- 4. JCA Dim1 좌표 중앙값 기준 그룹핑 (y_train 카테고리 대상) ---\\n\")\n\n--- 4. JCA Dim1 좌표 중앙값 기준 그룹핑 (y_train 카테고리 대상) ---\n\nif (nrow(status_category_dim1_info_df) == 0) {\n    stop(\"JCA Dim1 좌표를 가진 y_train 카테고리가 없습니다.\")\n}\nmedian_dim1_coord &lt;- median(status_category_dim1_info_df$Dim1_Coord, na.rm = TRUE)\ncat(\"y_train 카테고리들의 JCA Dim1 좌표 중앙값:\", median_dim1_coord, \"\\n\")\n\ny_train 카테고리들의 JCA Dim1 좌표 중앙값: 0.008240591 \n\n# 그룹 레이블 정의 (Dim1 좌표가 중앙값보다 큰 쪽을 \"High\" 그룹으로 명명)\n# 이 그룹 이름은 이전 분류 결과에서 사용된 이름과 일치시키는 것이 비교에 용이합니다.\n# 여기서는 \"JCA_Dim1_Median_Group_High\" 와 \"JCA_Dim1_Median_Group_Low\" 를 사용합니다.\ndefined_group_labels &lt;- c(\"JCA_Dim1_Median_Group_Low\", \"JCA_Dim1_Median_Group_High\")\n\nstatus_category_dim1_info_df$JCA_Derived_Group &lt;- ifelse(\n  status_category_dim1_info_df$Dim1_Coord &lt; median_dim1_coord,\n  defined_group_labels[1], # Low 그룹\n  defined_group_labels[2]  # High 그룹 (중앙값 포함)\n)\nstatus_category_dim1_info_df$JCA_Derived_Group &lt;- factor(\n  status_category_dim1_info_df$JCA_Derived_Group,\n  levels = defined_group_labels\n)\n\ncat(\"y_train 카테고리별 새로운 JCA Dim1 중앙값 기반 그룹 할당 (일부):\\n\")\n\ny_train 카테고리별 새로운 JCA Dim1 중앙값 기반 그룹 할당 (일부):\n\nprint(head(status_category_dim1_info_df))\n\n  original_y_category Dim1_Coord          JCA_Derived_Group\n1                X266  0.3763965 JCA_Dim1_Median_Group_High\n2                X270  0.1403095 JCA_Dim1_Median_Group_High\n3                X273  0.4856578 JCA_Dim1_Median_Group_High\n4                X274  0.5727498 JCA_Dim1_Median_Group_High\n5                X275 -0.2012943  JCA_Dim1_Median_Group_Low\n6                X277  0.4093634 JCA_Dim1_Median_Group_High\n\ncat(\"y_train 카테고리들의 그룹별 분포:\\n\")\n\ny_train 카테고리들의 그룹별 분포:\n\nprint(table(status_category_dim1_info_df$JCA_Derived_Group))\n\n\n JCA_Dim1_Median_Group_Low JCA_Dim1_Median_Group_High \n                       330                        330 \n\ncat(\"--- 5. 매핑 테이블 생성 및 y_train, y_test에 새 그룹 레이블 적용 ---\\n\")\n\n--- 5. 매핑 테이블 생성 및 y_train, y_test에 새 그룹 레이블 적용 ---\n\nif(!(nrow(status_category_dim1_info_df) &gt; 0 &&\n     \"JCA_Derived_Group\" %in% names(status_category_dim1_info_df) &&\n     \"original_y_category\" %in% names(status_category_dim1_info_df))) {\n  stop(\"Dim1 중앙값 그룹핑 후 'status_category_dim1_info_df'가 올바르게 생성되지 않았습니다.\")\n}\ngroup_lookup_final &lt;- setNames(\n  as.character(status_category_dim1_info_df$JCA_Derived_Group),\n  status_category_dim1_info_df$original_y_category\n)\n\n# y_train에 새 그룹 레이블 적용\ny_train_jca_labeled_final &lt;- group_lookup_final[as.character(y_train)]\ny_train_jca_labeled_final &lt;- factor(y_train_jca_labeled_final, levels = defined_group_labels)\n\n# y_test에 새 그룹 레이블 적용\ny_test_jca_labeled_final &lt;- group_lookup_final[as.character(y_test)]\ny_test_jca_labeled_final &lt;- factor(y_test_jca_labeled_final, levels = defined_group_labels)\n\ncat(\"--- 6. 최종 데이터셋 결합 및 CSV 저장 ---\\n\")\n\n--- 6. 최종 데이터셋 결합 및 CSV 저장 ---\n\n# 학습 데이터\nif (nrow(X_train) == length(y_train) && nrow(X_train) == length(y_train_jca_labeled_final) && nrow(X_train) == length(weights_train)) {\n  train_data_final_to_save &lt;- data.frame(\n    X_train, # 원본 연속형 X_train 사용\n    y = y_train_jca_labeled_final, # JCA Dim1 중앙값으로 재범주화된 y_train\n    weights = weights_train\n  )\n  train_csv_filename_final &lt;- \"_data/train_data.csv\"\n  write.csv(train_data_final_to_save, train_csv_filename_final, row.names = FALSE, na = \"\")\n} else {\n  warning(\"학습 데이터 구성요소들의 행 수가 일치하지 않아 CSV 파일을 저장하지 못했습니다.\")\n}\n\n# 테스트 데이터\nif (nrow(X_test) == length(y_test) && nrow(X_test) == length(y_test_jca_labeled_final) && nrow(X_test) == length(weights_test)) {\n  test_data_final_to_save &lt;- data.frame(\n    X_test, # 원본 연속형 X_test 사용\n    y = y_test_jca_labeled_final, # JCA Dim1 중앙값으로 재범주화된 y_test\n    weights = weights_test\n  )\n  test_csv_filename_final &lt;- \"_data/test_data.csv\"\n  write.csv(test_data_final_to_save, test_csv_filename_final, row.names = FALSE, na = \"\")\n} else {\n  warning(\"테스트 데이터 구성요소들의 행 수가 일치하지 않아 CSV 파일을 저장하지 못했습니다.\")\n}\n\ncat(\"\\n--- 분석 완료 ---\\n\")\n\n\n--- 분석 완료 ---\n\ncat(\"JCA 1번 차원 좌표의 중앙값을 기준으로 '\", defined_group_labels[1], \"'와 '\", defined_group_labels[2],\n    \"' 두 그룹으로 나누어 데이터를 저장했습니다.\\n\", sep=\"\")\n\nJCA 1번 차원 좌표의 중앙값을 기준으로 'JCA_Dim1_Median_Group_Low'와 'JCA_Dim1_Median_Group_High' 두 그룹으로 나누어 데이터를 저장했습니다.\n\ncat(\"이 CSV 파일들을 사용하여 다시 분류 모델을 학습하고 평가해보시기 바랍니다.\\n\")\n\n이 CSV 파일들을 사용하여 다시 분류 모델을 학습하고 평가해보시기 바랍니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/17.html#pca",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/17.html#pca",
    "title": "PCA 분석",
    "section": "PCA",
    "text": "PCA\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n\nfeature_names = ['q1a1', 'q1a2','q1a3', 'q1a4', 'q1a5', 'q1a6', 'q1a7', 'q12a12', 'q12a13', 'q12a14', 'q12a15', 'q12a16', 'q12a17', 'q12a18']\n\n# 모든 학생 데이터에 대한 PCA 결과를 저장할 딕셔너리\npca_results = {}\n\nfor i in range(3, 5):\n    df = pd.read_csv(f'_data/student_{i}.csv')\n    df = df[feature_names].dropna()\n    wave_id = f'w{i}'\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n\n    n_components = 3\n    pca = PCA(n_components=n_components)\n    principal_components = pca.fit_transform(df_scaled)\n\n    pc_columns = [f'PC{j+1}_{wave_id}' for j in range(n_components)]  # 각 PC에 wave_id를 포함\n    principal_df = pd.DataFrame(data=principal_components, columns=pc_columns)\n    \n    # 모든 샘플에 고유 식별자 부여\n    principal_df['sample_id'] = [f'{wave_id}_sample_{k}' for k in range(len(principal_df))]\n    \n    # 결과 저장\n    pca_results[wave_id] = principal_df\n    \n    # 원래 있던 출력 코드 유지\n    explained_variance_ratio = pca.explained_variance_ratio_\n    print(f\"--- 학생 {i} 각 주성분의 설명된 분산 비율 ---\")\n    for j, ratio in enumerate(explained_variance_ratio):\n        print(f\"PC{j+1}: {ratio:.4f} (누적: {np.sum(explained_variance_ratio[:j+1]):.4f})\")\n    print(f\"총 설명된 분산: {np.sum(explained_variance_ratio):.4f}\")\n    print(\"\\n\")\n\n    print(f\"--- 학생 {i} 주성분 벡터 (고유 벡터) ---\")\n    components_df = pd.DataFrame(pca.components_, columns=feature_names, index=pc_columns)\n    print(components_df)\n    print(\"\\n\")\n\n# 횡 방향으로 PCA 결과 병합 (공통 키가 있는 경우)\n# 참고: 실제 데이터에서는 sample_id가 동일한 레코드끼리 병합해야 할 수 있습니다\n# 이 예제에서는 단순히 첫 번째 데이터셋을 기준으로 병합합니다\n\n# 먼저 기준 데이터프레임 설정\nmerged_df = pca_results['w3'].set_index('sample_id')\n\n# 나머지 데이터프레임을 횡 방향으로 병합\nfor wave_id, df in pca_results.items():\n    if wave_id != 'w3':  # 기준 데이터프레임 제외\n        # 인덱스 기준으로 병합 (outer join - 모든 샘플 포함)\n        merged_df = merged_df.join(df.set_index('sample_id'), how='outer')\n\n# 결과 확인\nprint(\"--- 횡 방향으로 병합된 PCA 결과 ---\")\nprint(merged_df.reset_index())  # sample_id를 다시 컬럼으로 변환\n\n--- 학생 3 각 주성분의 설명된 분산 비율 ---\nPC1: 0.3269 (누적: 0.3269)\nPC2: 0.1227 (누적: 0.4497)\nPC3: 0.1100 (누적: 0.5597)\n총 설명된 분산: 0.5597\n\n\n--- 학생 3 주성분 벡터 (고유 벡터) ---\n            q1a1      q1a2      q1a3      q1a4      q1a5      q1a6      q1a7  \\\nPC1_w3  0.247862  0.237873  0.230581  0.301675  0.243926  0.235251  0.223636   \nPC2_w3  0.366504  0.369130  0.162445  0.156753 -0.287024 -0.229956 -0.291483   \nPC3_w3 -0.241074 -0.287373  0.438479  0.296505 -0.001940 -0.143635 -0.199942   \n\n          q12a12    q12a13    q12a14    q12a15    q12a16    q12a17    q12a18  \nPC1_w3  0.298027  0.294329  0.263841  0.327178  0.279594  0.274408  0.259556  \nPC2_w3  0.259620  0.237896  0.062205  0.087176 -0.346159 -0.302207 -0.334084  \nPC3_w3 -0.264197 -0.288675  0.441378  0.353738  0.064246 -0.076792 -0.188166  \n\n\n--- 학생 4 각 주성분의 설명된 분산 비율 ---\nPC1: 0.3557 (누적: 0.3557)\nPC2: 0.1296 (누적: 0.4853)\nPC3: 0.1003 (누적: 0.5856)\n총 설명된 분산: 0.5856\n\n\n--- 학생 4 주성분 벡터 (고유 벡터) ---\n            q1a1      q1a2      q1a3      q1a4      q1a5      q1a6      q1a7  \\\nPC1_w4  0.243141  0.236866  0.241068  0.288196  0.246889  0.240895  0.246690   \nPC2_w4  0.388228  0.415968  0.085153  0.151228 -0.265814 -0.182576 -0.222634   \nPC3_w4 -0.228642 -0.243857  0.478800  0.338032 -0.052284 -0.170940 -0.205696   \n\n          q12a12    q12a13    q12a14    q12a15    q12a16    q12a17    q12a18  \nPC1_w4  0.289653  0.287025  0.277797  0.323675  0.270187  0.271515  0.262346  \nPC2_w4  0.294460  0.284031 -0.016731  0.026280 -0.370027 -0.296113 -0.315624  \nPC3_w4 -0.212274 -0.208043  0.442286  0.342813 -0.050594 -0.129397 -0.222921  \n\n\n--- 횡 방향으로 병합된 PCA 결과 ---\n           sample_id    PC1_w3    PC2_w3    PC3_w3    PC1_w4    PC2_w4  \\\n0        w3_sample_0 -1.254296 -1.224018 -2.154943       NaN       NaN   \n1        w3_sample_1  2.247322 -0.049948  1.067250       NaN       NaN   \n2       w3_sample_10  0.431017 -1.510617 -0.870137       NaN       NaN   \n3      w3_sample_100  0.201425  2.796726 -0.839905       NaN       NaN   \n4     w3_sample_1000 -1.899832  0.188324  0.128137       NaN       NaN   \n...              ...       ...       ...       ...       ...       ...   \n6241   w4_sample_995       NaN       NaN       NaN -0.072505  0.113869   \n6242   w4_sample_996       NaN       NaN       NaN  2.380158  0.724508   \n6243   w4_sample_997       NaN       NaN       NaN  1.370190 -2.457734   \n6244   w4_sample_998       NaN       NaN       NaN -1.726048 -0.219925   \n6245   w4_sample_999       NaN       NaN       NaN  1.474996  1.818824   \n\n        PC3_w4  \n0          NaN  \n1          NaN  \n2          NaN  \n3          NaN  \n4          NaN  \n...        ...  \n6241 -0.519265  \n6242 -0.787852  \n6243 -1.963943  \n6244  0.269801  \n6245 -0.402807  \n\n[6246 rows x 7 columns]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "PCA 분석"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#무작위-분류기-random-classifier",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#무작위-분류기-random-classifier",
    "title": "analysis",
    "section": "무작위 분류기 (Random Classifier)",
    "text": "무작위 분류기 (Random Classifier)\n\n# 훈련 데이터에서 클래스 비율 계산\nclass_proportions &lt;- table(y_train) / length(y_train)\n\n# 테스트 데이터에 대한 무작위 예측 생성\n# 클래스 비율에 맞게 무작위로 샘플링\nrandom_predictions &lt;- sample(levels(y_test), \n                            size = length(y_test), \n                            replace = TRUE, \n                            prob = class_proportions)\nrandom_predictions &lt;- factor(random_predictions, levels = levels(y_test))\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_random &lt;- matrix(0,\n                                          nrow = length(levels(y_test)),\n                                          ncol = length(levels(y_test)),\n                                          dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat &lt;- as.character(y_test[i])\n  predicted_cat &lt;- as.character(random_predictions[i])\n  weight &lt;- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_random[actual_cat, predicted_cat] &lt;- weighted_confusion_matrix_random[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== 무작위 분류기 가중 혼동 행렬 ===\\n\")\n\n\n=== 무작위 분류기 가중 혼동 행렬 ===\n\nprint(round(weighted_confusion_matrix_random, 2))\n\n         Predicted\nActual      active  passive\n  active  57687.49 58084.02\n  passive 30544.02 28027.14\n\n# 무작위 분류기 가중 성능 지표 계산\ntotal_weighted_sum_random &lt;- sum(weighted_confusion_matrix_random)\nweighted_accuracy_random &lt;- sum(diag(weighted_confusion_matrix_random)) / total_weighted_sum_random\ncat(\"\\n=== 무작위 분류기 가중 성능 지표 ===\\n\")\n\n\n=== 무작위 분류기 가중 성능 지표 ===\n\ncat(\"가중 정확도:\", round(weighted_accuracy_random, 4), \"\\n\")\n\n가중 정확도: 0.4916 \n\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_random &lt;- numeric(length(levels(y_test)))\nweighted_recall_random &lt;- numeric(length(levels(y_test)))\nweighted_f1_score_random &lt;- numeric(length(levels(y_test)))\nnames(weighted_precision_random) &lt;- names(weighted_recall_random) &lt;- names(weighted_f1_score_random) &lt;- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP &lt;- weighted_confusion_matrix_random[cat, cat]\n  FP &lt;- sum(weighted_confusion_matrix_random[, cat]) - TP\n  FN &lt;- sum(weighted_confusion_matrix_random[cat, ]) - TP\n\n  weighted_precision_random[cat] &lt;- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_random[cat] &lt;- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_random[cat] &lt;- ifelse((weighted_precision_random[cat] + weighted_recall_random[cat]) == 0, 0,\n                                        2 * (weighted_precision_random[cat] * weighted_recall_random[cat]) / (weighted_precision_random[cat] + weighted_recall_random[cat]))\n}\n\ncat(\"\\n무작위 분류기 범주별 가중 정밀도:\\n\")\n\n\n무작위 분류기 범주별 가중 정밀도:\n\nprint(round(weighted_precision_random, 4))\n\n active passive \n 0.6538  0.3255 \n\ncat(\"\\n무작위 분류기 범주별 가중 재현율:\\n\")\n\n\n무작위 분류기 범주별 가중 재현율:\n\nprint(round(weighted_recall_random, 4))\n\n active passive \n 0.4983  0.4785 \n\ncat(\"\\n무작위 분류기 범주별 가중 F1-score:\\n\")\n\n\n무작위 분류기 범주별 가중 F1-score:\n\nprint(round(weighted_f1_score_random, 4))\n\n active passive \n 0.5656  0.3874",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#샘플-인구통계학적-분포-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#샘플-인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "샘플 인구통계학적 분포 분석",
    "text": "샘플 인구통계학적 분포 분석\n\n# 성별 분포 분석\nsex_dist &lt;- df6_origin %&gt;%\n  count(sex) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 태어난 년도 분포 분석\nbirth_year_dist &lt;- df6_origin %&gt;%\n  count(yy) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 거주 지역 분포 분석\narea_dist &lt;- df6_origin %&gt;%\n  count(area) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 종속변수(status_category) 분포 분석\nstatus_dist &lt;- status_df %&gt;%\n  count(status_category) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 테이블 출력\nkable(sex_dist, caption = \"성별 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n성별 분포\n\n\nsex\nn\npercentage\n\n\n\n\n1\n1362\n39.48971\n\n\n2\n1492\n43.25892\n\n\nNA\n595\n17.25138\n\n\n\n\n\n\nkable(birth_year_dist, caption = \"출생년도 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n출생년도 분포\n\n\nyy\nn\npercentage\n\n\n\n\n87\n1\n0.0289939\n\n\n88\n5\n0.1449696\n\n\n89\n2209\n64.0475500\n\n\n90\n639\n18.5271093\n\n\nNA\n595\n17.2513772\n\n\n\n\n\n\nkable(area_dist, caption = \"거주 지역 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n거주 지역 분포\n\n\narea\nn\npercentage\n\n\n\n\n100\n1\n0.0289939\n\n\n110\n18\n0.5218904\n\n\n120\n20\n0.5798782\n\n\n121\n3\n0.0869817\n\n\n122\n29\n0.8408234\n\n\n130\n4\n0.1159756\n\n\n131\n15\n0.4349087\n\n\n132\n8\n0.2319513\n\n\n133\n20\n0.5798782\n\n\n134\n29\n0.8408234\n\n\n135\n18\n0.5218904\n\n\n136\n24\n0.6958539\n\n\n137\n26\n0.7538417\n\n\n138\n23\n0.6668600\n\n\n139\n42\n1.2177443\n\n\n140\n6\n0.1739635\n\n\n142\n10\n0.2899391\n\n\n143\n25\n0.7248478\n\n\n150\n18\n0.5218904\n\n\n151\n22\n0.6378660\n\n\n152\n27\n0.7828356\n\n\n153\n4\n0.1159756\n\n\n156\n22\n0.6378660\n\n\n157\n24\n0.6958539\n\n\n158\n24\n0.6958539\n\n\n200\n28\n0.8118295\n\n\n210\n4\n0.1159756\n\n\n217\n1\n0.0289939\n\n\n220\n24\n0.6958539\n\n\n225\n19\n0.5508843\n\n\n232\n1\n0.0289939\n\n\n235\n1\n0.0289939\n\n\n240\n26\n0.7538417\n\n\n300\n8\n0.2319513\n\n\n301\n20\n0.5798782\n\n\n302\n40\n1.1597565\n\n\n305\n28\n0.8118295\n\n\n306\n4\n0.1159756\n\n\n314\n2\n0.0579878\n\n\n320\n2\n0.0579878\n\n\n321\n15\n0.4349087\n\n\n325\n1\n0.0289939\n\n\n330\n32\n0.9278052\n\n\n336\n8\n0.2319513\n\n\n339\n2\n0.0579878\n\n\n340\n1\n0.0289939\n\n\n345\n1\n0.0289939\n\n\n350\n28\n0.8118295\n\n\n355\n23\n0.6668600\n\n\n356\n1\n0.0289939\n\n\n360\n15\n0.4349087\n\n\n361\n36\n1.0437808\n\n\n363\n1\n0.0289939\n\n\n367\n1\n0.0289939\n\n\n369\n2\n0.0579878\n\n\n370\n28\n0.8118295\n\n\n373\n1\n0.0289939\n\n\n380\n25\n0.7248478\n\n\n390\n2\n0.0579878\n\n\n400\n1\n0.0289939\n\n\n402\n30\n0.8698173\n\n\n403\n33\n0.9567991\n\n\n404\n26\n0.7538417\n\n\n405\n32\n0.9278052\n\n\n406\n25\n0.7248478\n\n\n407\n15\n0.4349087\n\n\n411\n32\n0.9278052\n\n\n412\n23\n0.6668600\n\n\n413\n27\n0.7828356\n\n\n415\n2\n0.0579878\n\n\n420\n38\n1.1017686\n\n\n421\n18\n0.5218904\n\n\n422\n2\n0.0579878\n\n\n423\n24\n0.6958539\n\n\n425\n23\n0.6668600\n\n\n426\n30\n0.8698173\n\n\n427\n2\n0.0579878\n\n\n429\n20\n0.5798782\n\n\n430\n29\n0.8408234\n\n\n431\n3\n0.0869817\n\n\n435\n34\n0.9857930\n\n\n440\n10\n0.2899391\n\n\n441\n32\n0.9278052\n\n\n442\n32\n0.9278052\n\n\n443\n6\n0.1739635\n\n\n445\n46\n1.3337199\n\n\n447\n3\n0.0869817\n\n\n449\n40\n1.1597565\n\n\n456\n28\n0.8118295\n\n\n459\n3\n0.0869817\n\n\n461\n9\n0.2609452\n\n\n462\n16\n0.4639026\n\n\n463\n20\n0.5798782\n\n\n464\n4\n0.1159756\n\n\n465\n3\n0.0869817\n\n\n471\n3\n0.0869817\n\n\n472\n30\n0.8698173\n\n\n480\n27\n0.7828356\n\n\n481\n3\n0.0869817\n\n\n483\n1\n0.0289939\n\n\n500\n35\n1.0147869\n\n\n501\n2\n0.0579878\n\n\n502\n32\n0.9278052\n\n\n503\n32\n0.9278052\n\n\n506\n30\n0.8698173\n\n\n517\n21\n0.6088721\n\n\n525\n20\n0.5798782\n\n\n530\n18\n0.5218904\n\n\n534\n1\n0.0289939\n\n\n536\n1\n0.0289939\n\n\n540\n28\n0.8118295\n\n\n560\n23\n0.6668600\n\n\n561\n9\n0.2609452\n\n\n565\n1\n0.0289939\n\n\n570\n19\n0.5508843\n\n\n573\n21\n0.6088721\n\n\n576\n2\n0.0579878\n\n\n579\n1\n0.0289939\n\n\n595\n9\n0.2609452\n\n\n600\n1\n0.0289939\n\n\n601\n1\n0.0289939\n\n\n602\n15\n0.4349087\n\n\n604\n30\n0.8698173\n\n\n607\n20\n0.5798782\n\n\n608\n27\n0.7828356\n\n\n609\n28\n0.8118295\n\n\n611\n7\n0.2029574\n\n\n612\n22\n0.6378660\n\n\n613\n27\n0.7828356\n\n\n614\n23\n0.6668600\n\n\n616\n3\n0.0869817\n\n\n617\n4\n0.1159756\n\n\n619\n1\n0.0289939\n\n\n621\n29\n0.8408234\n\n\n626\n32\n0.9278052\n\n\n631\n37\n1.0727747\n\n\n637\n1\n0.0289939\n\n\n638\n1\n0.0289939\n\n\n641\n31\n0.8988112\n\n\n645\n1\n0.0289939\n\n\n650\n28\n0.8118295\n\n\n656\n1\n0.0289939\n\n\n660\n35\n1.0147869\n\n\n676\n21\n0.6088721\n\n\n680\n26\n0.7538417\n\n\n681\n29\n0.8408234\n\n\n682\n28\n0.8118295\n\n\n689\n2\n0.0579878\n\n\n700\n14\n0.4059148\n\n\n701\n34\n0.9857930\n\n\n702\n9\n0.2609452\n\n\n703\n17\n0.4928965\n\n\n704\n52\n1.5076834\n\n\n705\n12\n0.3479269\n\n\n706\n43\n1.2467382\n\n\n711\n2\n0.0579878\n\n\n712\n21\n0.6088721\n\n\n718\n1\n0.0289939\n\n\n730\n32\n0.9278052\n\n\n742\n1\n0.0289939\n\n\n750\n25\n0.7248478\n\n\n757\n22\n0.6378660\n\n\n760\n5\n0.1449696\n\n\n764\n1\n0.0289939\n\n\n770\n1\n0.0289939\n\n\n780\n18\n0.5218904\n\n\n790\n10\n0.2899391\n\n\n791\n19\n0.5508843\n\n\nNA\n616\n17.8602493\n\n\n\n\n\n\nkable(status_dist, caption = \"종속변수(Active vs Passive) 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n종속변수(Active vs Passive) 분포\n\n\nstatus_category\nn\npercentage\n\n\n\n\nactive\n1953\n56.62511\n\n\npassive\n850\n24.64482\n\n\nNA\n646\n18.73007",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#시각화",
    "title": "preprocessing",
    "section": "시각화",
    "text": "시각화\n\n# 성별 분포 시각화\nggplot(sex_dist, aes(x = sex, y = n, fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"성별 분포\", x = \"성별\", y = \"빈도\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n# 출생년도 분포 시각화\nggplot(birth_year_dist, aes(x = yy, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"출생년도 분포\", x = \"출생년도\", y = \"빈도\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n# 거주 지역 분포 시각화\nggplot(area_dist, aes(x = area, y = n, fill = area)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"거주 지역 분포\", x = \"지역\", y = \"빈도\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n# 종속변수 분포 시각화\nggplot(status_dist, aes(x = status_category, y = n, fill = status_category)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"종속변수(Active vs Passive) 분포\", x = \"상태 카테고리\", y = \"빈도\") +\n  theme_minimal()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#교차-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#교차-분석",
    "title": "preprocessing",
    "section": "교차 분석",
    "text": "교차 분석\n\n# 성별과 종속변수의 관계\nsex_status &lt;- df6_origin %&gt;%\n  left_join(status_df, by = \"id\") %&gt;%\n  count(sex, status_category) %&gt;%\n  group_by(sex) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 출생년도와 종속변수의 관계\nbirth_status &lt;- df6_origin %&gt;%\n  left_join(status_df, by = \"id\") %&gt;%\n  count(yy, status_category) %&gt;%\n  group_by(yy) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 지역과 종속변수의 관계\narea_status &lt;- df6_origin %&gt;%\n  left_join(status_df, by = \"id\") %&gt;%\n  count(area, status_category) %&gt;%\n  group_by(area) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 교차표 시각화\nggplot(sex_status, aes(x = sex, y = percentage, fill = status_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), \n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"성별에 따른 종속변수 분포\", x = \"성별\", y = \"비율(%)\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\nggplot(area_status, aes(x = area, y = percentage, fill = status_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), \n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"지역에 따른 종속변수 분포\", x = \"지역\", y = \"비율(%)\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n# 출생년도별 종속변수 분포 히트맵\nggplot(birth_status, aes(x = yy, y = status_category, fill = percentage)) +\n  geom_tile() +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\"))) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  labs(title = \"출생년도에 따른 종속변수 분포\", x = \"출생년도\", y = \"상태 카테고리\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_tile()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-데이터셋-인구통계학적-분포-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-데이터셋-인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "merged_df 데이터셋 인구통계학적 분포 분석",
    "text": "merged_df 데이터셋 인구통계학적 분포 분석\n\n# merged_df와 df6_origin 연결하여 인구통계학적 정보 추가\nmerged_with_demo &lt;- merged_df %&gt;%\n  left_join(df6_origin[, c(\"id\", \"sex\", \"yy\", \"area\")], by = \"id\")\n\n# 성별 분포 분석\nsex_dist &lt;- merged_with_demo %&gt;%\n  count(sex) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 태어난 년도 분포 분석\nbirth_year_dist &lt;- merged_with_demo %&gt;%\n  count(yy) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 거주 지역 분포 분석\narea_dist &lt;- merged_with_demo %&gt;%\n  count(area) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 종속변수(status_category) 분포 분석\nstatus_dist &lt;- merged_with_demo %&gt;%\n  count(status_category) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 테이블 출력\nkable(sex_dist, caption = \"merged_df 성별 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nmerged_df 성별 분포\n\n\nsex\nn\npercentage\n\n\n\n\n1\n1362\n39.48971\n\n\n2\n1492\n43.25892\n\n\nNA\n595\n17.25138\n\n\n\n\n\n\nkable(birth_year_dist, caption = \"merged_df 출생년도 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nmerged_df 출생년도 분포\n\n\nyy\nn\npercentage\n\n\n\n\n87\n1\n0.0289939\n\n\n88\n5\n0.1449696\n\n\n89\n2209\n64.0475500\n\n\n90\n639\n18.5271093\n\n\nNA\n595\n17.2513772\n\n\n\n\n\n\nkable(area_dist, caption = \"merged_df 거주 지역 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nmerged_df 거주 지역 분포\n\n\narea\nn\npercentage\n\n\n\n\n100\n1\n0.0289939\n\n\n110\n18\n0.5218904\n\n\n120\n20\n0.5798782\n\n\n121\n3\n0.0869817\n\n\n122\n29\n0.8408234\n\n\n130\n4\n0.1159756\n\n\n131\n15\n0.4349087\n\n\n132\n8\n0.2319513\n\n\n133\n20\n0.5798782\n\n\n134\n29\n0.8408234\n\n\n135\n18\n0.5218904\n\n\n136\n24\n0.6958539\n\n\n137\n26\n0.7538417\n\n\n138\n23\n0.6668600\n\n\n139\n42\n1.2177443\n\n\n140\n6\n0.1739635\n\n\n142\n10\n0.2899391\n\n\n143\n25\n0.7248478\n\n\n150\n18\n0.5218904\n\n\n151\n22\n0.6378660\n\n\n152\n27\n0.7828356\n\n\n153\n4\n0.1159756\n\n\n156\n22\n0.6378660\n\n\n157\n24\n0.6958539\n\n\n158\n24\n0.6958539\n\n\n200\n28\n0.8118295\n\n\n210\n4\n0.1159756\n\n\n217\n1\n0.0289939\n\n\n220\n24\n0.6958539\n\n\n225\n19\n0.5508843\n\n\n232\n1\n0.0289939\n\n\n235\n1\n0.0289939\n\n\n240\n26\n0.7538417\n\n\n300\n8\n0.2319513\n\n\n301\n20\n0.5798782\n\n\n302\n40\n1.1597565\n\n\n305\n28\n0.8118295\n\n\n306\n4\n0.1159756\n\n\n314\n2\n0.0579878\n\n\n320\n2\n0.0579878\n\n\n321\n15\n0.4349087\n\n\n325\n1\n0.0289939\n\n\n330\n32\n0.9278052\n\n\n336\n8\n0.2319513\n\n\n339\n2\n0.0579878\n\n\n340\n1\n0.0289939\n\n\n345\n1\n0.0289939\n\n\n350\n28\n0.8118295\n\n\n355\n23\n0.6668600\n\n\n356\n1\n0.0289939\n\n\n360\n15\n0.4349087\n\n\n361\n36\n1.0437808\n\n\n363\n1\n0.0289939\n\n\n367\n1\n0.0289939\n\n\n369\n2\n0.0579878\n\n\n370\n28\n0.8118295\n\n\n373\n1\n0.0289939\n\n\n380\n25\n0.7248478\n\n\n390\n2\n0.0579878\n\n\n400\n1\n0.0289939\n\n\n402\n30\n0.8698173\n\n\n403\n33\n0.9567991\n\n\n404\n26\n0.7538417\n\n\n405\n32\n0.9278052\n\n\n406\n25\n0.7248478\n\n\n407\n15\n0.4349087\n\n\n411\n32\n0.9278052\n\n\n412\n23\n0.6668600\n\n\n413\n27\n0.7828356\n\n\n415\n2\n0.0579878\n\n\n420\n38\n1.1017686\n\n\n421\n18\n0.5218904\n\n\n422\n2\n0.0579878\n\n\n423\n24\n0.6958539\n\n\n425\n23\n0.6668600\n\n\n426\n30\n0.8698173\n\n\n427\n2\n0.0579878\n\n\n429\n20\n0.5798782\n\n\n430\n29\n0.8408234\n\n\n431\n3\n0.0869817\n\n\n435\n34\n0.9857930\n\n\n440\n10\n0.2899391\n\n\n441\n32\n0.9278052\n\n\n442\n32\n0.9278052\n\n\n443\n6\n0.1739635\n\n\n445\n46\n1.3337199\n\n\n447\n3\n0.0869817\n\n\n449\n40\n1.1597565\n\n\n456\n28\n0.8118295\n\n\n459\n3\n0.0869817\n\n\n461\n9\n0.2609452\n\n\n462\n16\n0.4639026\n\n\n463\n20\n0.5798782\n\n\n464\n4\n0.1159756\n\n\n465\n3\n0.0869817\n\n\n471\n3\n0.0869817\n\n\n472\n30\n0.8698173\n\n\n480\n27\n0.7828356\n\n\n481\n3\n0.0869817\n\n\n483\n1\n0.0289939\n\n\n500\n35\n1.0147869\n\n\n501\n2\n0.0579878\n\n\n502\n32\n0.9278052\n\n\n503\n32\n0.9278052\n\n\n506\n30\n0.8698173\n\n\n517\n21\n0.6088721\n\n\n525\n20\n0.5798782\n\n\n530\n18\n0.5218904\n\n\n534\n1\n0.0289939\n\n\n536\n1\n0.0289939\n\n\n540\n28\n0.8118295\n\n\n560\n23\n0.6668600\n\n\n561\n9\n0.2609452\n\n\n565\n1\n0.0289939\n\n\n570\n19\n0.5508843\n\n\n573\n21\n0.6088721\n\n\n576\n2\n0.0579878\n\n\n579\n1\n0.0289939\n\n\n595\n9\n0.2609452\n\n\n600\n1\n0.0289939\n\n\n601\n1\n0.0289939\n\n\n602\n15\n0.4349087\n\n\n604\n30\n0.8698173\n\n\n607\n20\n0.5798782\n\n\n608\n27\n0.7828356\n\n\n609\n28\n0.8118295\n\n\n611\n7\n0.2029574\n\n\n612\n22\n0.6378660\n\n\n613\n27\n0.7828356\n\n\n614\n23\n0.6668600\n\n\n616\n3\n0.0869817\n\n\n617\n4\n0.1159756\n\n\n619\n1\n0.0289939\n\n\n621\n29\n0.8408234\n\n\n626\n32\n0.9278052\n\n\n631\n37\n1.0727747\n\n\n637\n1\n0.0289939\n\n\n638\n1\n0.0289939\n\n\n641\n31\n0.8988112\n\n\n645\n1\n0.0289939\n\n\n650\n28\n0.8118295\n\n\n656\n1\n0.0289939\n\n\n660\n35\n1.0147869\n\n\n676\n21\n0.6088721\n\n\n680\n26\n0.7538417\n\n\n681\n29\n0.8408234\n\n\n682\n28\n0.8118295\n\n\n689\n2\n0.0579878\n\n\n700\n14\n0.4059148\n\n\n701\n34\n0.9857930\n\n\n702\n9\n0.2609452\n\n\n703\n17\n0.4928965\n\n\n704\n52\n1.5076834\n\n\n705\n12\n0.3479269\n\n\n706\n43\n1.2467382\n\n\n711\n2\n0.0579878\n\n\n712\n21\n0.6088721\n\n\n718\n1\n0.0289939\n\n\n730\n32\n0.9278052\n\n\n742\n1\n0.0289939\n\n\n750\n25\n0.7248478\n\n\n757\n22\n0.6378660\n\n\n760\n5\n0.1449696\n\n\n764\n1\n0.0289939\n\n\n770\n1\n0.0289939\n\n\n780\n18\n0.5218904\n\n\n790\n10\n0.2899391\n\n\n791\n19\n0.5508843\n\n\nNA\n616\n17.8602493\n\n\n\n\n\n\nkable(status_dist, caption = \"merged_df 종속변수(Active vs Passive) 분포\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\nmerged_df 종속변수(Active vs Passive) 분포\n\n\nstatus_category\nn\npercentage\n\n\n\n\nactive\n1953\n56.62511\n\n\npassive\n850\n24.64482\n\n\nNA\n646\n18.73007",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-시각화",
    "title": "preprocessing",
    "section": "merged_df 시각화",
    "text": "merged_df 시각화\n\n# 성별 분포 시각화\nggplot(sex_dist, aes(x = sex, y = n, fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"성별 분포\", x = \"성별\", y = \"빈도\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 출생년도 분포 시각화\nggplot(birth_year_dist, aes(x = yy, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"출생년도 분포\", x = \"출생년도\", y = \"빈도\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 종속변수 분포 시각화\nggplot(status_dist, aes(x = status_category, y = n, fill = status_category)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"Active vs Passive 분포\", x = \"상태 카테고리\", y = \"빈도\") +\n  theme_minimal()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-교차-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#merged_df-교차-분석",
    "title": "preprocessing",
    "section": "merged_df 교차 분석",
    "text": "merged_df 교차 분석\n\n# 성별과 종속변수의 관계\nsex_status &lt;- merged_with_demo %&gt;%\n  count(sex, status_category) %&gt;%\n  group_by(sex) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 출생년도와 종속변수의 관계\nbirth_status &lt;- merged_with_demo %&gt;%\n  count(yy, status_category) %&gt;%\n  group_by(yy) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 지역과 종속변수의 관계\narea_status &lt;- merged_with_demo %&gt;%\n  count(area, status_category) %&gt;%\n  group_by(area) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 교차표 시각화\nggplot(sex_status, aes(x = sex, y = percentage, fill = status_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), \n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"merged_df: 성별에 따른 종속변수 분포\", x = \"성별\", y = \"비율(%)\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\nggplot(area_status, aes(x = area, y = percentage, fill = status_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), \n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"merged_df: 지역에 따른 종속변수 분포\", x = \"지역\", y = \"비율(%)\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n# 출생년도별 종속변수 분포 히트맵\nggplot(birth_status, aes(x = yy, y = status_category, fill = percentage)) +\n  geom_tile() +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\"))) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  labs(title = \"merged_df: 출생년도에 따른 종속변수 분포\", x = \"출생년도\", y = \"상태 카테고리\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_tile()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "인구통계학적 분포 분석",
    "text": "인구통계학적 분포 분석\n\nmerged_df_clean &lt;- merged_df %&gt;%\n  filter(complete.cases(.))\n\nsex_dist &lt;- merged_df_clean %&gt;%\n  count(sex) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\nbirth_year_dist &lt;- merged_df_clean %&gt;%\n  count(yy) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\narea_dist &lt;- merged_df_clean %&gt;%\n  count(area) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\nstatus_dist &lt;- merged_df_clean %&gt;%\n  count(status_category) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 표 출력\nkable(sex_dist, caption = \"성별 분포 (결측치 제거 후)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n성별 분포 (결측치 제거 후)\n\n\nsex\nn\npercentage\n\n\n\n\n1\n1157\n47.47641\n\n\n2\n1280\n52.52359\n\n\n\n\n\n\nkable(birth_year_dist, caption = \"출생년도 분포 (결측치 제거 후)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n출생년도 분포 (결측치 제거 후)\n\n\nyy\nn\npercentage\n\n\n\n\n88\n2\n0.0820681\n\n\n89\n1884\n77.3081658\n\n\n90\n551\n22.6097661\n\n\n\n\n\n\nkable(area_dist, caption = \"거주 지역 분포 (결측치 제거 후)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n거주 지역 분포 (결측치 제거 후)\n\n\narea\nn\npercentage\n\n\n\n\n100\n1\n0.0410341\n\n\n110\n14\n0.5744768\n\n\n120\n15\n0.6155109\n\n\n121\n1\n0.0410341\n\n\n122\n22\n0.9027493\n\n\n130\n3\n0.1231022\n\n\n131\n12\n0.4924087\n\n\n132\n7\n0.2872384\n\n\n133\n20\n0.8206812\n\n\n134\n26\n1.0668855\n\n\n135\n12\n0.4924087\n\n\n136\n18\n0.7386130\n\n\n137\n17\n0.6975790\n\n\n138\n16\n0.6565449\n\n\n139\n37\n1.5182602\n\n\n140\n4\n0.1641362\n\n\n142\n6\n0.2462043\n\n\n143\n22\n0.9027493\n\n\n150\n15\n0.6155109\n\n\n151\n19\n0.7796471\n\n\n152\n22\n0.9027493\n\n\n153\n4\n0.1641362\n\n\n156\n18\n0.7386130\n\n\n157\n19\n0.7796471\n\n\n158\n16\n0.6565449\n\n\n200\n23\n0.9437833\n\n\n210\n3\n0.1231022\n\n\n220\n21\n0.8617152\n\n\n225\n15\n0.6155109\n\n\n232\n1\n0.0410341\n\n\n235\n1\n0.0410341\n\n\n240\n24\n0.9848174\n\n\n300\n8\n0.3282725\n\n\n301\n20\n0.8206812\n\n\n302\n38\n1.5592942\n\n\n305\n24\n0.9848174\n\n\n306\n4\n0.1641362\n\n\n314\n2\n0.0820681\n\n\n320\n1\n0.0410341\n\n\n321\n13\n0.5334428\n\n\n330\n26\n1.0668855\n\n\n336\n5\n0.2051703\n\n\n339\n2\n0.0820681\n\n\n340\n1\n0.0410341\n\n\n350\n27\n1.1079196\n\n\n355\n23\n0.9437833\n\n\n356\n1\n0.0410341\n\n\n360\n13\n0.5334428\n\n\n361\n34\n1.3951580\n\n\n363\n1\n0.0410341\n\n\n367\n1\n0.0410341\n\n\n369\n2\n0.0820681\n\n\n370\n28\n1.1489536\n\n\n373\n1\n0.0410341\n\n\n380\n23\n0.9437833\n\n\n390\n2\n0.0820681\n\n\n400\n1\n0.0410341\n\n\n402\n29\n1.1899877\n\n\n403\n25\n1.0258515\n\n\n404\n22\n0.9027493\n\n\n405\n28\n1.1489536\n\n\n406\n23\n0.9437833\n\n\n407\n14\n0.5744768\n\n\n411\n24\n0.9848174\n\n\n412\n10\n0.4103406\n\n\n413\n21\n0.8617152\n\n\n415\n2\n0.0820681\n\n\n420\n29\n1.1899877\n\n\n421\n13\n0.5334428\n\n\n422\n1\n0.0410341\n\n\n423\n17\n0.6975790\n\n\n425\n22\n0.9027493\n\n\n426\n27\n1.1079196\n\n\n427\n2\n0.0820681\n\n\n429\n12\n0.4924087\n\n\n430\n24\n0.9848174\n\n\n431\n2\n0.0820681\n\n\n435\n28\n1.1489536\n\n\n440\n8\n0.3282725\n\n\n441\n27\n1.1079196\n\n\n442\n30\n1.2310217\n\n\n443\n6\n0.2462043\n\n\n445\n38\n1.5592942\n\n\n447\n3\n0.1231022\n\n\n449\n33\n1.3541239\n\n\n456\n26\n1.0668855\n\n\n459\n1\n0.0410341\n\n\n461\n9\n0.3693065\n\n\n462\n10\n0.4103406\n\n\n463\n15\n0.6155109\n\n\n464\n2\n0.0820681\n\n\n465\n2\n0.0820681\n\n\n471\n2\n0.0820681\n\n\n472\n23\n0.9437833\n\n\n480\n22\n0.9027493\n\n\n481\n2\n0.0820681\n\n\n500\n34\n1.3951580\n\n\n502\n29\n1.1899877\n\n\n503\n27\n1.1079196\n\n\n506\n28\n1.1489536\n\n\n517\n20\n0.8206812\n\n\n525\n17\n0.6975790\n\n\n530\n17\n0.6975790\n\n\n534\n1\n0.0410341\n\n\n536\n1\n0.0410341\n\n\n540\n25\n1.0258515\n\n\n560\n22\n0.9027493\n\n\n561\n7\n0.2872384\n\n\n565\n1\n0.0410341\n\n\n570\n17\n0.6975790\n\n\n573\n20\n0.8206812\n\n\n576\n2\n0.0820681\n\n\n579\n1\n0.0410341\n\n\n595\n7\n0.2872384\n\n\n600\n1\n0.0410341\n\n\n601\n1\n0.0410341\n\n\n602\n15\n0.6155109\n\n\n604\n26\n1.0668855\n\n\n607\n17\n0.6975790\n\n\n608\n26\n1.0668855\n\n\n609\n26\n1.0668855\n\n\n611\n7\n0.2872384\n\n\n612\n21\n0.8617152\n\n\n613\n27\n1.1079196\n\n\n614\n23\n0.9437833\n\n\n616\n3\n0.1231022\n\n\n617\n3\n0.1231022\n\n\n619\n1\n0.0410341\n\n\n621\n22\n0.9027493\n\n\n626\n30\n1.2310217\n\n\n631\n30\n1.2310217\n\n\n637\n1\n0.0410341\n\n\n638\n1\n0.0410341\n\n\n641\n30\n1.2310217\n\n\n645\n1\n0.0410341\n\n\n650\n26\n1.0668855\n\n\n656\n1\n0.0410341\n\n\n660\n33\n1.3541239\n\n\n676\n18\n0.7386130\n\n\n680\n23\n0.9437833\n\n\n681\n28\n1.1489536\n\n\n682\n27\n1.1079196\n\n\n689\n2\n0.0820681\n\n\n700\n13\n0.5334428\n\n\n701\n32\n1.3130899\n\n\n702\n7\n0.2872384\n\n\n703\n17\n0.6975790\n\n\n704\n46\n1.8875667\n\n\n705\n12\n0.4924087\n\n\n706\n32\n1.3130899\n\n\n711\n2\n0.0820681\n\n\n712\n21\n0.8617152\n\n\n718\n1\n0.0410341\n\n\n730\n31\n1.2720558\n\n\n742\n1\n0.0410341\n\n\n750\n21\n0.8617152\n\n\n757\n18\n0.7386130\n\n\n760\n4\n0.1641362\n\n\n764\n1\n0.0410341\n\n\n770\n1\n0.0410341\n\n\n780\n17\n0.6975790\n\n\n790\n9\n0.3693065\n\n\n791\n16\n0.6565449\n\n\n\n\n\n\nkable(status_dist, caption = \"종속변수(Active vs Passive) 분포 (결측치 제거 후)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n종속변수(Active vs Passive) 분포 (결측치 제거 후)\n\n\nstatus_category\nn\npercentage\n\n\n\n\nactive\n1680\n68.93722\n\n\npassive\n757\n31.06278",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#지역-분포-지도-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#지역-분포-지도-시각화",
    "title": "preprocessing",
    "section": "지역 분포 지도 시각화",
    "text": "지역 분포 지도 시각화\n\n# 한국 지도 데이터 패키지 설치 및 로드\nif(!require(kormaps2014)) {\n  install.packages(\"stringi\")\n  install.packages(\"devtools\")\n  devtools::install_github(\"cardiomoon/kormaps2014\")\n}\n\nLoading required package: kormaps2014\n\nlibrary(kormaps2014)\nlibrary(ggplot2)\nlibrary(stringi)\nlibrary(mapproj)\n\nLoading required package: maps\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# 대한민국 지역 시각화를 위한 준비\nlibrary(ggplot2)\n\n# 지역 코드와 merged_df의 area 코드 매핑 함수\nmap_area_code &lt;- function(area_code) {\n  area_mapping &lt;- c(\n      \"100\" = \"서울\",\n      \"110\" = \"서울\",\n      \"120\" = \"서울\",\n      \"121\" = \"서울\",\n      \"122\" = \"서울\",\n      \"130\" = \"서울\",\n      \"131\" = \"서울\",\n      \"132\" = \"서울\",\n      \"133\" = \"서울\",\n      \"134\" = \"서울\",\n      \"135\" = \"서울\",\n      \"136\" = \"서울\",\n      \"137\" = \"서울\",\n      \"138\" = \"서울\",\n      \"139\" = \"서울\",\n      \"140\" = \"서울\",\n      \"142\" = \"서울\",\n      \"143\" = \"서울\",\n      \"150\" = \"서울\",\n      \"151\" = \"서울\",\n      \"152\" = \"서울\",\n      \"153\" = \"서울\",\n      \"156\" = \"서울\",\n      \"157\" = \"서울\",\n      \"158\" = \"서울\",\n      \"200\" = \"강원\",\n      \"209\" = \"강원\",\n      \"210\" = \"강원\",\n      \"215\" = \"강원\",\n      \"217\" = \"강원\",\n      \"219\" = \"강원\",\n      \"220\" = \"강원\",\n      \"225\" = \"강원\",\n      \"230\" = \"강원\",\n      \"232\" = \"강원\",\n      \"233\" = \"강원\",\n      \"235\" = \"강원\",\n      \"240\" = \"강원\",\n      \"245\" = \"강원\",\n      \"250\" = \"강원\",\n      \"252\" = \"강원\",\n      \"255\" = \"강원\",\n      \"269\" = \"강원\",\n      \"300\" = \"대전\",\n      \"301\" = \"대전\",\n      \"302\" = \"대전\",\n      \"305\" = \"대전\",\n      \"306\" = \"대전\",\n      \"312\" = \"충남\",\n      \"314\" = \"충남\",\n      \"320\" = \"충남\",\n      \"321\" = \"충남\",\n      \"323\" = \"충남\",\n      \"325\" = \"충남\",\n      \"330\" = \"충남\",\n      \"336\" = \"충남\",\n      \"339\" = \"충남\",\n      \"340\" = \"충남\",\n      \"343\" = \"충남\",\n      \"345\" = \"충남\",\n      \"350\" = \"충남\",\n      \"355\" = \"충남\",\n      \"356\" = \"충남\",\n      \"357\" = \"충남\",\n      \"360\" = \"충북\",\n      \"361\" = \"충북\",\n      \"363\" = \"충북\",\n      \"365\" = \"충북\",\n      \"367\" = \"충북\",\n      \"368\" = \"충북\",\n      \"369\" = \"충북\",\n      \"370\" = \"충북\",\n      \"373\" = \"충북\",\n      \"376\" = \"충북\",\n      \"380\" = \"충북\",\n      \"390\" = \"충북\",\n      \"395\" = \"충북\",\n      \"400\" = \"인천\",\n      \"401\" = \"인천\",\n      \"402\" = \"인천\",\n      \"403\" = \"인천\",\n      \"404\" = \"인천\",\n      \"405\" = \"인천\",\n      \"406\" = \"인천\",\n      \"407\" = \"인천\",\n      \"409\" = \"인천\",\n      \"411\" = \"경기\",\n      \"412\" = \"경기\",\n      \"413\" = \"경기\",\n      \"415\" = \"경기\",\n      \"417\" = \"인천\",\n      \"420\" = \"경기\",\n      \"421\" = \"경기\",\n      \"422\" = \"경기\",\n      \"423\" = \"경기\",\n      \"425\" = \"경기\",\n      \"426\" = \"경기\",\n      \"427\" = \"경기\",\n      \"429\" = \"경기\",\n      \"430\" = \"경기\",\n      \"431\" = \"경기\",\n      \"435\" = \"경기\",\n      \"437\" = \"경기\",\n      \"440\" = \"경기\",\n      \"441\" = \"경기\",\n      \"442\" = \"경기\",\n      \"443\" = \"경기\",\n      \"445\" = \"경기\",\n      \"447\" = \"경기\",\n      \"449\" = \"경기\",\n      \"456\" = \"경기\",\n      \"459\" = \"경기\",\n      \"461\" = \"경기\",\n      \"462\" = \"경기\",\n      \"463\" = \"경기\",\n      \"464\" = \"경기\",\n      \"465\" = \"경기\",\n      \"467\" = \"경기\",\n      \"469\" = \"경기\",\n      \"471\" = \"경기\",\n      \"472\" = \"경기\",\n      \"476\" = \"경기\",\n      \"477\" = \"경기\",\n      \"480\" = \"경기\",\n      \"481\" = \"경기\",\n      \"482\" = \"경기\",\n      \"483\" = \"경기\",\n      \"487\" = \"경기\",\n      \"500\" = \"광주\",\n      \"501\" = \"광주\",\n      \"502\" = \"광주\",\n      \"503\" = \"광주\",\n      \"506\" = \"광주\",\n      \"513\" = \"전남\",\n      \"515\" = \"전남\",\n      \"516\" = \"전남\",\n      \"517\" = \"전남\",\n      \"519\" = \"전남\",\n      \"520\" = \"전남\",\n      \"525\" = \"전남\",\n      \"526\" = \"전남\",\n      \"527\" = \"전남\",\n      \"529\" = \"전남\",\n      \"530\" = \"전남\",\n      \"534\" = \"전남\",\n      \"535\" = \"전남\",\n      \"536\" = \"전남\",\n      \"537\" = \"전남\",\n      \"539\" = \"전남\",\n      \"540\" = \"전남\",\n      \"542\" = \"전남\",\n      \"545\" = \"전남\",\n      \"546\" = \"전남\",\n      \"548\" = \"전남\",\n      \"550\" = \"전남\",\n      \"560\" = \"전북\",\n      \"561\" = \"전북\",\n      \"565\" = \"전북\",\n      \"566\" = \"전북\",\n      \"567\" = \"전북\",\n      \"568\" = \"전북\",\n      \"570\" = \"전북\",\n      \"573\" = \"전북\",\n      \"576\" = \"전북\",\n      \"579\" = \"전북\",\n      \"580\" = \"전북\",\n      \"585\" = \"전북\",\n      \"590\" = \"전북\",\n      \"595\" = \"전북\",\n      \"597\" = \"전북\",\n      \"600\" = \"부산\",\n      \"601\" = \"부산\",\n      \"602\" = \"부산\",\n      \"604\" = \"부산\",\n      \"606\" = \"부산\",\n      \"607\" = \"부산\",\n      \"608\" = \"부산\",\n      \"609\" = \"부산\",\n      \"611\" = \"부산\",\n      \"612\" = \"부산\",\n      \"613\" = \"부산\",\n      \"614\" = \"부산\",\n      \"616\" = \"부산\",\n      \"617\" = \"부산\",\n      \"618\" = \"부산\",\n      \"619\" = \"부산\",\n      \"621\" = \"경남\",\n      \"626\" = \"경남\",\n      \"627\" = \"경남\",\n      \"631\" = \"경남\",\n      \"635\" = \"경남\",\n      \"636\" = \"경남\",\n      \"637\" = \"경남\",\n      \"638\" = \"경남\",\n      \"641\" = \"경남\",\n      \"645\" = \"경남\",\n      \"650\" = \"경남\",\n      \"656\" = \"경남\",\n      \"660\" = \"경남\",\n      \"664\" = \"경남\",\n      \"666\" = \"경남\",\n      \"667\" = \"경남\",\n      \"668\" = \"경남\",\n      \"670\" = \"경남\",\n      \"676\" = \"경남\",\n      \"678\" = \"경남\",\n      \"680\" = \"울산\",\n      \"681\" = \"울산\",\n      \"682\" = \"울산\",\n      \"683\" = \"울산\",\n      \"689\" = \"울산\",\n      \"690\" = \"제주\",\n      \"695\" = \"제주\",\n      \"697\" = \"제주\",\n      \"699\" = \"제주\",\n      \"700\" = \"대구\",\n      \"701\" = \"대구\",\n      \"702\" = \"대구\",\n      \"703\" = \"대구\",\n      \"704\" = \"대구\",\n      \"705\" = \"대구\",\n      \"706\" = \"대구\",\n      \"711\" = \"대구\",\n      \"712\" = \"경북\",\n      \"714\" = \"경북\",\n      \"716\" = \"경북\",\n      \"717\" = \"경북\",\n      \"718\" = \"경북\",\n      \"719\" = \"경북\",\n      \"730\" = \"경북\",\n      \"740\" = \"경북\",\n      \"742\" = \"경북\",\n      \"745\" = \"경북\",\n      \"750\" = \"경북\",\n      \"755\" = \"경북\",\n      \"757\" = \"경북\",\n      \"760\" = \"경북\",\n      \"763\" = \"경북\",\n      \"764\" = \"경북\",\n      \"766\" = \"경북\",\n      \"767\" = \"경북\",\n      \"769\" = \"경북\",\n      \"770\" = \"경북\",\n      \"780\" = \"경북\",\n      \"790\" = \"경북\",\n      \"791\" = \"경북\",\n      \"799\" = \"경북\",\n      \"999\" = \"국외\"\n  )\n  return(area_mapping[as.character(area_code)])\n}\n\n# area 코드를 지역명으로 변환\nmerged_df_clean$area_name &lt;- sapply(merged_df_clean$area, function(x) map_area_code(x))\n\n# 지역별 집계\nregion_counts &lt;- merged_df_clean %&gt;%\n  count(area_name) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 지역 분포 시각화 - 막대 그래프로 표현\nggplot(region_counts, aes(x = reorder(area_name, -percentage), y = percentage, fill = area_name)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5, size = 3) +\n  labs(title = \"지역별 분포 (결측치 제거 후)\",\n       x = \"지역\", \n       y = \"비율 (%)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n# 지역 그룹별 통계\nregion_groups &lt;- data.frame(\n  area_name = c(\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"경기\", \n                \"강원\", \"충북\", \"충남\", \"전북\", \"전남\", \"경북\", \"경남\", \"제주\", \"국외\"),\n  region_group = c(\"수도권\", \"경상권\", \"경상권\", \"수도권\", \"호남권\", \"충청권\", \"경상권\", \"수도권\",\n                  \"강원권\", \"충청권\", \"충청권\", \"호남권\", \"호남권\", \"경상권\", \"경상권\", \"제주\", \"기타\")\n)\n\n# 지역 그룹 정보 병합\nregion_stats &lt;- region_counts %&gt;%\n  left_join(region_groups, by = \"area_name\") %&gt;%\n  arrange(desc(percentage))\n\n# 지역 그룹별 비율 계산\nregion_group_stats &lt;- region_stats %&gt;%\n  group_by(region_group) %&gt;%\n  summarise(\n    total_percentage = sum(percentage),\n    count = sum(n)\n  ) %&gt;%\n  mutate(group_percentage = total_percentage) %&gt;%\n  arrange(desc(group_percentage))\n\n# 지역 그룹별 분포 시각화\nggplot(region_group_stats, aes(x = reorder(region_group, -group_percentage), \n                              y = group_percentage, fill = region_group)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(group_percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"지역 그룹별 분포 (결측치 제거 후)\",\n       x = \"지역 그룹\", \n       y = \"비율 (%)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#분포-시각화",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#분포-시각화",
    "title": "preprocessing",
    "section": "분포 시각화",
    "text": "분포 시각화\n\n# 성별 분포 시각화\nggplot(sex_dist, aes(x = sex, y = n, fill = sex)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"성별 분포\", x = \"성별\", y = \"빈도\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 출생년도 분포 시각화\nggplot(birth_year_dist, aes(x = yy, y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"출생년도 분포\", x = \"출생년도\", y = \"빈도\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# 종속변수 분포 시각화\nggplot(status_dist, aes(x = status_category, y = n, fill = status_category)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5) +\n  labs(title = \"Active vs Passive 분포\", x = \"상태 카테고리\", y = \"빈도\") +\n  theme_minimal()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#지역-분포",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#지역-분포",
    "title": "preprocessing",
    "section": "지역 분포",
    "text": "지역 분포\n\n# 한국 지도 데이터 패키지 설치 및 로드\nif(!require(kormaps2014)) {\n  install.packages(\"stringi\")\n  install.packages(\"devtools\")\n  devtools::install_github(\"cardiomoon/kormaps2014\")\n}\n\nLoading required package: kormaps2014\n\nlibrary(kormaps2014)\nlibrary(ggplot2)\nlibrary(stringi)\nlibrary(mapproj)\n\nLoading required package: maps\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n# 대한민국 지역 시각화를 위한 준비\nlibrary(ggplot2)\n\n# 지역 코드와 merged_df의 area 코드 매핑 함수\nmap_area_code &lt;- function(area_code) {\n  area_mapping &lt;- c(\n      \"100\" = \"서울\",\n      \"110\" = \"서울\",\n      \"120\" = \"서울\",\n      \"121\" = \"서울\",\n      \"122\" = \"서울\",\n      \"130\" = \"서울\",\n      \"131\" = \"서울\",\n      \"132\" = \"서울\",\n      \"133\" = \"서울\",\n      \"134\" = \"서울\",\n      \"135\" = \"서울\",\n      \"136\" = \"서울\",\n      \"137\" = \"서울\",\n      \"138\" = \"서울\",\n      \"139\" = \"서울\",\n      \"140\" = \"서울\",\n      \"142\" = \"서울\",\n      \"143\" = \"서울\",\n      \"150\" = \"서울\",\n      \"151\" = \"서울\",\n      \"152\" = \"서울\",\n      \"153\" = \"서울\",\n      \"156\" = \"서울\",\n      \"157\" = \"서울\",\n      \"158\" = \"서울\",\n      \"200\" = \"강원\",\n      \"209\" = \"강원\",\n      \"210\" = \"강원\",\n      \"215\" = \"강원\",\n      \"217\" = \"강원\",\n      \"219\" = \"강원\",\n      \"220\" = \"강원\",\n      \"225\" = \"강원\",\n      \"230\" = \"강원\",\n      \"232\" = \"강원\",\n      \"233\" = \"강원\",\n      \"235\" = \"강원\",\n      \"240\" = \"강원\",\n      \"245\" = \"강원\",\n      \"250\" = \"강원\",\n      \"252\" = \"강원\",\n      \"255\" = \"강원\",\n      \"269\" = \"강원\",\n      \"300\" = \"대전\",\n      \"301\" = \"대전\",\n      \"302\" = \"대전\",\n      \"305\" = \"대전\",\n      \"306\" = \"대전\",\n      \"312\" = \"충남\",\n      \"314\" = \"충남\",\n      \"320\" = \"충남\",\n      \"321\" = \"충남\",\n      \"323\" = \"충남\",\n      \"325\" = \"충남\",\n      \"330\" = \"충남\",\n      \"336\" = \"충남\",\n      \"339\" = \"충남\",\n      \"340\" = \"충남\",\n      \"343\" = \"충남\",\n      \"345\" = \"충남\",\n      \"350\" = \"충남\",\n      \"355\" = \"충남\",\n      \"356\" = \"충남\",\n      \"357\" = \"충남\",\n      \"360\" = \"충북\",\n      \"361\" = \"충북\",\n      \"363\" = \"충북\",\n      \"365\" = \"충북\",\n      \"367\" = \"충북\",\n      \"368\" = \"충북\",\n      \"369\" = \"충북\",\n      \"370\" = \"충북\",\n      \"373\" = \"충북\",\n      \"376\" = \"충북\",\n      \"380\" = \"충북\",\n      \"390\" = \"충북\",\n      \"395\" = \"충북\",\n      \"400\" = \"인천\",\n      \"401\" = \"인천\",\n      \"402\" = \"인천\",\n      \"403\" = \"인천\",\n      \"404\" = \"인천\",\n      \"405\" = \"인천\",\n      \"406\" = \"인천\",\n      \"407\" = \"인천\",\n      \"409\" = \"인천\",\n      \"411\" = \"경기\",\n      \"412\" = \"경기\",\n      \"413\" = \"경기\",\n      \"415\" = \"경기\",\n      \"417\" = \"인천\",\n      \"420\" = \"경기\",\n      \"421\" = \"경기\",\n      \"422\" = \"경기\",\n      \"423\" = \"경기\",\n      \"425\" = \"경기\",\n      \"426\" = \"경기\",\n      \"427\" = \"경기\",\n      \"429\" = \"경기\",\n      \"430\" = \"경기\",\n      \"431\" = \"경기\",\n      \"435\" = \"경기\",\n      \"437\" = \"경기\",\n      \"440\" = \"경기\",\n      \"441\" = \"경기\",\n      \"442\" = \"경기\",\n      \"443\" = \"경기\",\n      \"445\" = \"경기\",\n      \"447\" = \"경기\",\n      \"449\" = \"경기\",\n      \"456\" = \"경기\",\n      \"459\" = \"경기\",\n      \"461\" = \"경기\",\n      \"462\" = \"경기\",\n      \"463\" = \"경기\",\n      \"464\" = \"경기\",\n      \"465\" = \"경기\",\n      \"467\" = \"경기\",\n      \"469\" = \"경기\",\n      \"471\" = \"경기\",\n      \"472\" = \"경기\",\n      \"476\" = \"경기\",\n      \"477\" = \"경기\",\n      \"480\" = \"경기\",\n      \"481\" = \"경기\",\n      \"482\" = \"경기\",\n      \"483\" = \"경기\",\n      \"487\" = \"경기\",\n      \"500\" = \"광주\",\n      \"501\" = \"광주\",\n      \"502\" = \"광주\",\n      \"503\" = \"광주\",\n      \"506\" = \"광주\",\n      \"513\" = \"전남\",\n      \"515\" = \"전남\",\n      \"516\" = \"전남\",\n      \"517\" = \"전남\",\n      \"519\" = \"전남\",\n      \"520\" = \"전남\",\n      \"525\" = \"전남\",\n      \"526\" = \"전남\",\n      \"527\" = \"전남\",\n      \"529\" = \"전남\",\n      \"530\" = \"전남\",\n      \"534\" = \"전남\",\n      \"535\" = \"전남\",\n      \"536\" = \"전남\",\n      \"537\" = \"전남\",\n      \"539\" = \"전남\",\n      \"540\" = \"전남\",\n      \"542\" = \"전남\",\n      \"545\" = \"전남\",\n      \"546\" = \"전남\",\n      \"548\" = \"전남\",\n      \"550\" = \"전남\",\n      \"560\" = \"전북\",\n      \"561\" = \"전북\",\n      \"565\" = \"전북\",\n      \"566\" = \"전북\",\n      \"567\" = \"전북\",\n      \"568\" = \"전북\",\n      \"570\" = \"전북\",\n      \"573\" = \"전북\",\n      \"576\" = \"전북\",\n      \"579\" = \"전북\",\n      \"580\" = \"전북\",\n      \"585\" = \"전북\",\n      \"590\" = \"전북\",\n      \"595\" = \"전북\",\n      \"597\" = \"전북\",\n      \"600\" = \"부산\",\n      \"601\" = \"부산\",\n      \"602\" = \"부산\",\n      \"604\" = \"부산\",\n      \"606\" = \"부산\",\n      \"607\" = \"부산\",\n      \"608\" = \"부산\",\n      \"609\" = \"부산\",\n      \"611\" = \"부산\",\n      \"612\" = \"부산\",\n      \"613\" = \"부산\",\n      \"614\" = \"부산\",\n      \"616\" = \"부산\",\n      \"617\" = \"부산\",\n      \"618\" = \"부산\",\n      \"619\" = \"부산\",\n      \"621\" = \"경남\",\n      \"626\" = \"경남\",\n      \"627\" = \"경남\",\n      \"631\" = \"경남\",\n      \"635\" = \"경남\",\n      \"636\" = \"경남\",\n      \"637\" = \"경남\",\n      \"638\" = \"경남\",\n      \"641\" = \"경남\",\n      \"645\" = \"경남\",\n      \"650\" = \"경남\",\n      \"656\" = \"경남\",\n      \"660\" = \"경남\",\n      \"664\" = \"경남\",\n      \"666\" = \"경남\",\n      \"667\" = \"경남\",\n      \"668\" = \"경남\",\n      \"670\" = \"경남\",\n      \"676\" = \"경남\",\n      \"678\" = \"경남\",\n      \"680\" = \"울산\",\n      \"681\" = \"울산\",\n      \"682\" = \"울산\",\n      \"683\" = \"울산\",\n      \"689\" = \"울산\",\n      \"690\" = \"제주\",\n      \"695\" = \"제주\",\n      \"697\" = \"제주\",\n      \"699\" = \"제주\",\n      \"700\" = \"대구\",\n      \"701\" = \"대구\",\n      \"702\" = \"대구\",\n      \"703\" = \"대구\",\n      \"704\" = \"대구\",\n      \"705\" = \"대구\",\n      \"706\" = \"대구\",\n      \"711\" = \"대구\",\n      \"712\" = \"경북\",\n      \"714\" = \"경북\",\n      \"716\" = \"경북\",\n      \"717\" = \"경북\",\n      \"718\" = \"경북\",\n      \"719\" = \"경북\",\n      \"730\" = \"경북\",\n      \"740\" = \"경북\",\n      \"742\" = \"경북\",\n      \"745\" = \"경북\",\n      \"750\" = \"경북\",\n      \"755\" = \"경북\",\n      \"757\" = \"경북\",\n      \"760\" = \"경북\",\n      \"763\" = \"경북\",\n      \"764\" = \"경북\",\n      \"766\" = \"경북\",\n      \"767\" = \"경북\",\n      \"769\" = \"경북\",\n      \"770\" = \"경북\",\n      \"780\" = \"경북\",\n      \"790\" = \"경북\",\n      \"791\" = \"경북\",\n      \"799\" = \"경북\",\n      \"999\" = \"국외\"\n  )\n  return(area_mapping[as.character(area_code)])\n}\n\n# area 코드를 지역명으로 변환\nmerged_df_clean$area_name &lt;- sapply(merged_df_clean$area, function(x) map_area_code(x))\n\n# 지역별 집계\nregion_counts &lt;- merged_df_clean %&gt;%\n  count(area_name) %&gt;%\n  mutate(percentage = n / sum(n) * 100)\n\n# 지역 분포 시각화 - 막대 그래프로 표현\nggplot(region_counts, aes(x = reorder(area_name, -percentage), y = percentage, fill = area_name)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(percentage, 1), \"%\")), vjust = -0.5, size = 3) +\n  labs(title = \"지역별 분포 (결측치 제거 후)\",\n       x = \"지역\", \n       y = \"비율 (%)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  }
]