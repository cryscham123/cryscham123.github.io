<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/04_archives/adp_실기/</link>
<atom:link href="https://cryscham123.github.io/posts/04_archives/adp_실기/index.xml" rel="self" type="application/rss+xml"/>
<description>ADP 실기를 준비해 봅시다.</description>
<image>
<url>https://cryscham123.github.io/profile.jpg</url>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/04_archives/adp_실기/</link>
</image>
<generator>quarto-1.5.56</generator>
<lastBuildDate>Thu, 18 Dec 2025 15:00:00 GMT</lastBuildDate>
<item>
  <title>결과</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/12.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-12-19-18-58-07.png" class="img-fluid figure-img"></p>
<figcaption>시험 결과(75점 이상 합격)</figcaption>
</figure>
</div>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>결과가 나오고 1달 정도 지나서 후기를 작성한다. 이번 학기가 너무 바빴다.</p>
<p>뭐 어쨌든 결과는...불합격! 예상은 하고 있었으니 뭐 크게 실망스럽진 않지만 한 문제 차이로 떨어질거라곤 생각도 못했다.</p>
</section>
<section id="틀린-문제" class="level2">
<h2 class="anchored" data-anchor-id="틀린-문제">틀린 문제</h2>
<p>기계 학습 분야에서 21점이 감점되었는데 이건 오히려 잘한 것이다. 왜냐하면 20점 짜리 파트 하나를 통으로 날렸기 때문에, 내가 푼 문제에서는 1점만 까였다는 뜻이니까.</p>
<p>통계 분석 파트에서는 아마도 로지스틱 회귀분석 문제에서 점수가 까인것 같은데...사실 이 문제는 틀릴만한 문제는 아니였다. 심지어...시험 전날 adp 커뮤니티에서 누군가 이 문제가 나온다고 예상 글을 올렸을 때도 ‘이런걸 누가 틀려’ 하고 대수롭지 않게 여겼는데 막상 시험에 나와서 풀어보니 내가 예상한대로 동작하지 않았다.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>저 문제를 맞았다면 아마 합격했을테지만 뭐 이제와서 그게 중요한가? 어쨌든 이번 시험 준비 과정에서 많이 성장했다는 것을 체감하고 있고, 떨어진거야 뭐 아쉬운거지.</p>
<p>다음 시험을 준비할지는 아직 잘 모르겠다. 시험을 보긴 하겠지만 이번에 준비한만큼 하진 않을 것 같다.</p>
<p>들리는 소문에 의하면 이번 회차 합격자는 10명이라고 하던데 정말 양심이 없는 시험이 아닐 수 없다. 하지만 이런 시험에서 합격한다면 실력 증명 하나는 확실히 하는게 되는 것 아닐까? 하는 욕심은 남는다. 그래도 멈춰야할 때를 확실히 아는 것도 필요하다고 본다. 여기에만 온전히 집중할 수는 없으니 말이다.</p>
<p>앞으로는 인공지능 쪽 공부에 더 집중해보려 한다.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>adp 실기</category>
  <category>데이터 분석</category>
  <category>후기</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/12.html</guid>
  <pubDate>Thu, 18 Dec 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>try 1 후기</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/11.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>개같이 망했다.</p>
<p>사실 조금 방심했다.</p>
<p>통계파트가 다 아는 문제가 나와서 음..이거 잘 하면 합격하겠는데? 라는 생각이 들었다.</p>
<p>그래서 여유롭게 1시간동안 풀고 화장실도 다녀왔다.</p>
</section>
<section id="머신러닝" class="level2">
<h2 class="anchored" data-anchor-id="머신러닝">머신러닝</h2>
<p>이런 젠장. 머신러닝 파트 왜 이렇게 오래 걸리는거야?</p>
<p>분명 풀 수 있는 문제지만, 시간을 보니 10분밖에 안 남아 있었다.</p>
<p>결국 2번 문제는 전처리조차 하지 못하고 통으로 버릴 수 밖에 없었다.</p>
<p>물론 통으로 버려도 합격은 할 수 있다. 그런 사람이 실제로 있는지는 잘 모르겠지만.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>ADP 실기 시험까지 오는 여정은 의미가 깊었다. 진심으로 그렇게 생각한다.</p>
<p>그래서 결과가 좋지 않더라도 실망은 10% 정도만 할 것 같다.</p>
<p>하지만 계속 시험을 준비하는 것은 다른 이야기다. 내가 6개월을 더 준비할 가치가 아직 남아있을까?</p>
<p>솔직히 잘 모르겠다. 일단은 다른 것들을 병행해보면서 생각을 정리해보려 한다.</p>
<p>당장 눈 앞에 닥친 중간고사나 제대로 준비해보자.</p>
<p>아아! 한 번에 붙을 수 있었는데! (결과는 아직 안나오긴 했다.)</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>adp 실기</category>
  <category>데이터 분석</category>
  <category>후기</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/11.html</guid>
  <pubDate>Fri, 17 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>확률과 통계</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/10.html</link>
  <description><![CDATA[ 




<section id="통계학" class="level2">
<h2 class="anchored" data-anchor-id="통계학">통계학</h2>
<ul>
<li><strong>불확실한 상황</strong> 하에서 데이터에 근거하여 <strong>과학적인 의사결정</strong>을 도출하기 위한 이론과 방법의 체계</li>
<li><strong>모집단</strong>으로 부터 수집된 <strong>데이터</strong>(sample)를 기반으로 모집단의 <strong>특성을 추론</strong>하는 것을 목표로 한다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-03-08-12-28-23.png" class="img-fluid figure-img"></p>
<figcaption>통계적 의사결정 과정</figcaption>
</figure>
</div>
</section>
<section id="확률" class="level2">
<h2 class="anchored" data-anchor-id="확률">확률</h2>
<ul>
<li>고전적 의미: 표본공간에서 특정 사건이 차지하는 비율</li>
<li>통계적 의미: 특정 사건이 발생하는 <strong>상대도수의 극한</strong>
<ul>
<li>각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.</li>
</ul></li>
</ul>
</section>
<section id="확률-분포-정의-단계" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포-정의-단계">확률 분포 정의 단계</h2>
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-03-08-13-00-48.png" class="img-fluid"></p>
<ul>
<li><strong>Experiment(확률실험)</strong>: 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측</li>
<li><strong>Sample space(표본공간)</strong>: 모든 simple event의 집합</li>
<li><strong>Event(사건)</strong>: 실험에서 발생하는 결과 (부분 집합)</li>
<li><strong>Simple event(단순사건)</strong>: 원소가 하나인 사건</li>
<li><strong>확률 변수</strong>: 확률실험의 결과를 수치로 나타낸 변수</li>
</ul>
</section>
<section id="확률-분포" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포">확률 분포</h2>
<section id="이산-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="이산-확률-분포">이산 확률 분포</h3>
<p>이산 표본 공간, 연속 표본공간에서 정의 가능포</p>
<ul>
<li><strong>베르누이 시행</strong>: 각 시행은 서로 <strong>독립적</strong>이고, 실패와 성공 <strong>두 가지 결과만 존재</strong>.
<ul>
<li>단 <strong>모집단의 크기가 충분히 크고</strong>, <strong>표본(시행)의 크기가 충분히 작다면</strong> <strong>비복원 추출</strong>에서도 <strong>유효</strong></li>
<li>평균: p</li>
<li>분산: p(1-p)</li>
</ul></li>
<li><strong>이항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 성공 횟수를 측정
<ul>
<li>X ~ B(n, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bn%7D%7Bx%7D%20p%5Ex%20(1-p)%5E%7Bn-x%7D"></li>
<li>평균: np</li>
<li>분산: np(1-p)</li>
<li>n이 매우 크고, p가 매우 작을 때, <strong>포아송 분포로 근사</strong>할 수 있다. (λ = np)</li>
</ul></li>
<li><strong>음이항 분포</strong>
<ul>
<li>정의: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k번 성공하고, r번 실패한 경우 (n = k + r)
<ol type="1">
<li>r번의 실패가 나오기 전까지, 성공한 횟수 x
<ul>
<li>X ~ NB(r, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx+r-1%7D%7Bx%7D%20p%5Ex%20(1-p)%5Er"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B1-p%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B(1-p)%5E2%7D"></li>
</ul></li>
<li>r번의 실패가 나오기 전까지, 시행한 횟수 x
<ul>
<li>4번에서 성공을 실패로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 실패한 횟수 x
<ul>
<li>1번에서 실패를 성공으로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 시행한 횟수 x
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx-1%7D%7Bk-1%7D%20p%5Ek%20(1-p)%5E%7Bx-k%7D"></li>
<li>k가 1일 때 기하분포와 동일</li>
</ul></li>
<li>n번의 시행 횟수에서, k번 성공 또는 r번 실패한 경우: 이항분포</li>
</ol></li>
</ul></li>
<li><strong>기하 분포</strong>:
<ul>
<li>정의:
<ol type="1">
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 시행 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5E%7Bx-1%7D%20p,%20x%20=%201,%202,%203,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 실패 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5Ex%20p,%20x%20=%200,%201,%202,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
</ol></li>
<li><strong>비기억 특성</strong>: <img src="https://latex.codecogs.com/png.latex?P(X%20%3E%20n+k%20%7C%20X%20%3E%20n)%20=%20P(X%20%3E%20k)"></li>
</ul></li>
<li><strong>초기하 분포</strong>: <strong>베르누이 시행이 아닌 시행</strong>에서 성공하는 횟수
<ul>
<li>X ~ H(n, N, k), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B%5Cbinom%7BK%7D%7Bx%7D%20%5Cbinom%7BN-K%7D%7Bn-x%7D%7D%7B%5Cbinom%7BN%7D%7Bn%7D%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK%7D%7BN%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK(N-K)(N-n)%7D%7BN%5E2(N-1)%7D"></li>
</ul></li>
<li><strong>포아송 분포</strong>: <strong>임의의 기간</strong>동안 <strong>어떤 사건이 간헐적</strong>으로 발생할 때, 동일한 길이의 기간동안 실제 사건이 발생하는 횟수
<ul>
<li>X ~ Poisson(λ), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7Be%5E%7B-%CE%BB%7D%20%CE%BB%5Ex%7D%7Bx!%7D,%20%CE%BB%20%3E%200"></li>
<li>평균: λ</li>
<li>분산: λ</li>
</ul></li>
</ul>
</section>
<section id="연속-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="연속-확률-분포">연속 확률 분포</h3>
<p>연속 표본 공간에서 정의 가능</p>
<ul>
<li><strong>균일 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7Bb-a%7D,%20a%20%E2%89%A4%20x%20%E2%89%A4%20b"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Ba+b%7D%7B2%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(b-a)%5E2%7D%7B12%7D"></li>
</ul></li>
<li><strong>정규 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20+%20Y%20%5Csim%20N(%CE%BC_1%20+%20%CE%BC_2,%20%CF%83_1%5E2%20+%20%CF%83_2%5E2)"></li>
<li>선형 변환: <img src="https://latex.codecogs.com/png.latex?Y%20=%20aX%20+%20b%20%5Csim%20N(a%CE%BC%20+%20b,%20a%5E2%CF%83%5E2)"></li>
</ul></li>
<li><strong>t 분포</strong>
<ul>
<li>자유도가 커질수록 표준 정규분포에 근사함.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BZ%7D%7B%5Csqrt%7BV/n%7D%7D%20%5Csim%20t(n)">, Z: 표준정규분포, V: 자유도가 n인 카이제곱분포</li>
</ul></li>
<li><strong>f 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?F%20=%20%5Cfrac%7BX_1/%CE%BD_1%7D%7BX_2/%CE%BD_2%7D">, <img src="https://latex.codecogs.com/png.latex?X_1%20%5Csim%20%CF%87%5E2(%CE%BD_1)">, <img src="https://latex.codecogs.com/png.latex?X_2%20%5Csim%20%CF%87%5E2(%CE%BD_2)">, X1과 X2는 서로 독립</li>
</ul></li>
<li><strong>감마 분포</strong>
<ul>
<li>α: 분포의 형태 결정, θ: 분포의 크기 결정</li>
<li>평균: αθ</li>
<li>분산: αθ²</li>
<li><strong>카이제곱 분포</strong>: α = v/2, θ = 2 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z_i%20%5Csim%20N(0,1)">일 때, <img src="https://latex.codecogs.com/png.latex?Z_1%5E2%20+%20Z_2%5E2%20+%20...%20%20+%20Z_n%5E2%20%5Csim%20%CF%87%5E2(n)"></li>
<li><img src="https://latex.codecogs.com/png.latex?X_i">가 서로 독립이고, 자유도가 <img src="https://latex.codecogs.com/png.latex?%CE%BD_i">인 카이제곱분포를 따른다면, <img src="https://latex.codecogs.com/png.latex?X_1%20+%20X_2%20+%20...%20+%20X_n%20%5Csim%20x%5E2(%CE%BD_1%20+%20%CE%BD_2%20+%20...%20+%20%CE%BD_n)"></li>
<li>자유도가 커질수록 기댓값을 중심으로 모이고, 대칭에 가까워진다.</li>
</ul></li>
<li><strong>지수 분포</strong>: α = 1, θ = 1/λ 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20~%20Exp(%CE%BB%20=%20%5Cfrac%7B1%7D%7B%CE%B8%7D)">, f(x) = <img src="https://latex.codecogs.com/png.latex?%CE%BBe%5E%7B-%CE%BBx%7D,%20x%20%3E%200"></li>
<li>θ: 평균 사건 발생 간격, λ: 단위 시간당 사건 발생 횟수</li>
<li>포아송 분포에서 사건 발생 간격의 분포</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7Bn%7D%20X_i%20%5Csim%20%CE%93(n,%20%CE%B8)">, <img src="https://latex.codecogs.com/png.latex?%CE%B8%20=%201/%CE%BB"></li>
<li>비기억 특성을 가진다: <img src="https://latex.codecogs.com/png.latex?p(X%20%3E%20s%20+%20t%20%7C%20X%20%3E%20s)%20=%20p(X%20%3E%20t)%20=%20e%5E%7B-%CE%BBt%7D"></li>
<li>독립적으로 동일한 지수분포를 따르는 확률변수 n개의 합은 <img src="https://latex.codecogs.com/png.latex?%CE%B1%20=%20n,%20%CE%B8%20=%20%5Cfrac%7B1%7D%7B%CE%BB%7D">인 감마분포를 따른다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="다변량-분포" class="level3">
<h3 class="anchored" data-anchor-id="다변량-분포">다변량 분포</h3>
<ul>
<li><strong>다항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k개의 범주로 분류
<ul>
<li>X ~ M(n, p1, p2, …, pk), <img src="https://latex.codecogs.com/png.latex?f(x_1,%20x_2,%20...,%20x_k)%20=%20%5Cfrac%7Bn!%7D%7Bx_1!%20x_2!%20...%20x_k!%7D%20p_1%5E%7Bx_1%7D%20p_2%5E%7Bx_2%7D%20...%20p_k%5E%7Bx_k%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1,%20np_2,%20...,%20np_k%5D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1(1-p_1),%20np_2(1-p_2),%20...,%20np_k(1-p_k)%5D"></li>
<li>공분산: <img src="https://latex.codecogs.com/png.latex?-np_ip_j%20(i%20%E2%89%A0%20j)"></li>
<li>독립인 변수의 갯수는 k-1개 (k개의 사건)</li>
</ul></li>
</ul>
</section>
<section id="샘플링" class="level3">
<h3 class="anchored" data-anchor-id="샘플링">샘플링</h3>
</section>
<section id="분포의-동질성-검정" class="level3">
<h3 class="anchored" data-anchor-id="분포의-동질성-검정">분포의 동질성 검정</h3>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="표본의-분포" class="level2">
<h2 class="anchored" data-anchor-id="표본의-분포">표본의 분포</h2>
<ul>
<li><p>샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.</p></li>
<li><p>통계량: 표본의 특성을 나타내는 값</p></li>
<li><p>추정량: 아래의 조건을 만족하는 통계량</p>
<ul>
<li>불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.</li>
<li>효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.</li>
</ul></li>
</ul>
<section id="표본-평균의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-평균의-분포">표본 평균의 분포</h3>
<ul>
<li>모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2">이면, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D">의 평균은 μ이고, 분산은 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2/n">인 정규분포를 따른다.
<ul>
<li>단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리<sup>1</sup>)</li>
</ul></li>
<li>만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.
<ul>
<li>단 이때는 <strong>모집단이 정규분포를 따라야 한다.</strong></li>
</ul></li>
</ul>
</section>
<section id="표본-분산의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-분산의-분포">표본 분산의 분포</h3>
<ul>
<li><strong>정규 모집단으로 부터 나온 표본</strong>의 분산 S에 대하여, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(n-1)S%5E2%7D%7B%CF%83%5E2%7D">은 자유도가 n-1인 카이제곱 분포를 따른다.
<ul>
<li>모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.</li>
</ul></li>
<li>두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.</li>
</ul>
</section>
</section>
<section id="추정" class="level2">
<h2 class="anchored" data-anchor-id="추정">추정</h2>
<ul>
<li>통계적 추론: <strong>모집단에서 추출된 표본</strong>의 <strong>통계량</strong>으로부터 <strong>모수</strong>를 추론하는 것
<ul>
<li>추정
<ul>
<li>점추정</li>
<li>구간추정</li>
</ul></li>
<li>가설 검정</li>
</ul></li>
</ul>
<section id="점-추정" class="level3">
<h3 class="anchored" data-anchor-id="점-추정">점 추정</h3>
<ul>
<li>불편성
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20=%20%CE%B8"></li>
<li>bias = <img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20-%20%5Ctheta">
<ul>
<li>보통 sample size가 커질수록 bias는 0에 수렴</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D,%20X_n">은 μ의 불편추정량이다.</li>
</ul></li>
<li>최소분산
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Var(%5Cbar%7BX%7D)">가 <img src="https://latex.codecogs.com/png.latex?Var(X_n)">보다 분산이 작아서 더 좋은 추정량</li>
<li><img src="https://latex.codecogs.com/png.latex?MSE(%5Chat%7B%5Ctheta%7D)%20=%20E%5B(%5Chat%7B%5Ctheta%7D%20-%20%5Ctheta)%5E2%5D%20=%20Var(%5Chat%7B%5Ctheta%7D)%20+%20bias%5E2">
<ul>
<li>큰 오차에 더 큰 페널티를 주기 위해 제곱</li>
</ul></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-09-06-20-37-05.png" class="img-fluid figure-img"></p>
<figcaption>대표적인 불편추정량</figcaption>
</figure>
</div>
<ul>
<li><strong>전부 중심극한의정리를 적용</strong>할 수 있다. (비율은 0과 1의 평균이므로)</li>
<li>모평균, 모비율의 차이는 서로 독립이라는 가정이 필요하다.</li>
</ul>
</section>
<section id="구간-추정" class="level3">
<h3 class="anchored" data-anchor-id="구간-추정">구간 추정</h3>
<ul>
<li>α: 유의수준</li>
<li>1 - α: 신뢰수준<sup>2</sup></li>
<li>(θ_L, θ_U) = (1 - α) × 100% 신뢰구간</li>
</ul>
<ol type="1">
<li>(<img src="https://latex.codecogs.com/png.latex?%CE%B8_L,%20%CE%B8_U">) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다</li>
<li>구간이 충분히 좁아야 한다
<ul>
<li>표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.</li>
<li>고로 신뢰구간이 대칭임</li>
</ul></li>
</ol>
</section>
<section id="표본의-크기-결정" class="level3">
<h3 class="anchored" data-anchor-id="표본의-크기-결정">표본의 크기 결정</h3>
<p>특정 오차 아래로 하는 표본의 수 구하는 법</p>
<ul>
<li>그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.</li>
<li><strong>모비율</strong>을 모를 때는 일단 <strong>0.5로 보수적으로 놓고 계산</strong></li>
</ul>
</section>
</section>
<section id="모분산-추정" class="level2">
<h2 class="anchored" data-anchor-id="모분산-추정">모분산 추정</h2>
<ul>
<li>카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음
<ul>
<li>그냥 쉽게 구하기 위해 <img src="https://latex.codecogs.com/png.latex?(x%5E2_%7B%CE%B1/2%7D,%20x%5E2_%7B1-%CE%B1/2%7D)">를 사용</li>
</ul></li>
<li>모분산의 신뢰구간: <img src="https://latex.codecogs.com/png.latex?(%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B(1-%5Calpha)/2%7D(n-1)%7D,%20%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B%5Calpha/2%7D(n-1)%7D)"></li>
<li>표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.</li>
</ul>


</section>


<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">각주</h2>

<ol>
<li id="fn1"><p>모집단의 분포와 상관 없이, 표본의 평균은 정규분포에 수렴한다는 정리. 이항분포의 경우, P(X=c) ~ P(c - 0.5 &lt; X &lt; c + 0.5)로 근사 가능하다는 라플라스의 정리를 일반화한 것↩︎</p></li>
<li id="fn2"><p>샘플링을 무한히 반복했을 때, <strong>이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함</strong>한다. 즉, 구간이 확률 변수이다.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/10.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>기타</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/09.html</link>
  <description><![CDATA[ 




<section id="or" class="level2">

<ul>
<li>pulp 이용해서 푼다.</li>
<li>제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pulp example</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="pulp example" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pulp</span>
<span id="cb1-2"></span>
<span id="cb1-3">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpProblem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Problem_name"</span>, pulp.LpMinimize) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문제에 맞게 설정</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)</span></span>
<span id="cb1-6">x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-7">x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-8">x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-9">x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-10">x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-11">x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 목표 함수 정의 (총 운송 비용)</span></span>
<span id="cb1-14">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B3, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"obj_name"</span></span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 제약 조건 정의</span></span>
<span id="cb1-17">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_A_Supply"</span></span>
<span id="cb1-18">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_B_Supply"</span></span>
<span id="cb1-19">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_1_Demand"</span></span>
<span id="cb1-20">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_2_Demand"</span></span>
<span id="cb1-21">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">130</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_3_Demand"</span></span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5. 문제 풀이</span></span>
<span id="cb1-24">prob.solve()</span>
<span id="cb1-25"></span>
<span id="cb1-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 6. 결과 확인</span></span>
<span id="cb1-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Status:"</span>, pulp.LpStatus[prob.status])</span>
<span id="cb1-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Optimal Transportation Plan:"</span>)</span>
<span id="cb1-29"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> prob.variables():</span>
<span id="cb1-30">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(v.name, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"="</span>, v.varValue)</span>
<span id="cb1-31"></span>
<span id="cb1-32"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Total Minimum Cost = "</span>, pulp.value(prob.objective))</span></code></pre></div>
</div>
<ul>
<li>그 외 자주 안나오는 문제 예상 관련 글이 올라올 경우 추가 예정</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/09.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>시계열 분석</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/07.html</link>
  <description><![CDATA[ 




<section id="구성-요소" class="level2">
<h2 class="anchored" data-anchor-id="구성-요소">구성 요소</h2>
<ul>
<li><code>추세</code>(level)</li>
<li><code>계절, 순한</code>: 추세에서 벗어나는 변화의 정도</li>
<li><code>잔차</code>(white noise)</li>
</ul>
</section>
<section id="eda" class="level2">
<h2 class="anchored" data-anchor-id="eda">EDA</h2>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">df_isna <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.asfreq(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'H'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간 기준 결측치 탐색. index가 datetime이어야 함</span></span>
<span id="cb1-2">df_isna[df_isna.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.seasonal <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> STL</span>
<span id="cb2-2"></span>
<span id="cb2-3">decomposition <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> STL(df_final[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], period<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>).fit() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># period는 계절성 주기</span></span>
<span id="cb2-4">fig, (ax1, ax2, ax3, ax4) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sharex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb2-5">ax1.plot(decomposition.observed)</span>
<span id="cb2-6">ax1.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Obseved'</span>)</span>
<span id="cb2-7"></span>
<span id="cb2-8">ax2.plot(decomposition.trend)</span>
<span id="cb2-9">ax2.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Trend'</span>)</span>
<span id="cb2-10"></span>
<span id="cb2-11">ax3.plot(decomposition.seasonal)</span>
<span id="cb2-12">ax3.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Seasonal'</span>)</span>
<span id="cb2-13"></span>
<span id="cb2-14">ax4.plot(decomposition.resid)</span>
<span id="cb2-15">ax4.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Residuals'</span>)</span>
<span id="cb2-16"></span>
<span id="cb2-17">plt.show()</span></code></pre></div>
</section>
<section id="예측-방법" class="level2">
<h2 class="anchored" data-anchor-id="예측-방법">예측 방법</h2>
<p><strong>Rolling Forecast (동적 예측)</strong>: - 각 시점에서 예측을 수행한 후, 실제 값이 관측되면 이를 학습 데이터에 추가하여 다음 시점을 예측 - 실제 운영 환경을 시뮬레이션하는 방법으로, 새로운 정보가 지속적으로 업데이트됨 - 장점: 최신 정보를 반영하여 더 정확한 예측 가능 - 단점: 계산 비용이 높고, 모델 성능 평가 시 과적합 위험</p>
<p><strong>Static Forecast (정적 예측)</strong>: - 초기 학습 데이터로만 모델을 한 번 학습하고, 이후 테스트 기간 전체에 대해 연속적으로 예측 - 고정된 모델로 여러 시점을 예측하므로 모델의 일반화 성능을 더 엄격하게 평가 - 장점: 계산 비용이 낮고, 공정한 모델 평가 가능 - 단점: 시간이 지날수록 예측 정확도가 떨어질 수 있음</p>
</section>
<section id="단순-기법" class="level2">
<h2 class="anchored" data-anchor-id="단순-기법">단순 기법</h2>
<ul>
<li>단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t"></li>
<li>추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t%20+%20p_t%20-%20p_%7Bt-1%7D"></li>
<li>단순 이동평균법: time window를 계속 이동하면서 평균 구하는거
<ul>
<li>time window ↑: 먼 과거까지 보겠다</li>
</ul></li>
<li>가중 이동평균법: 가중치를 다르게 부여한 단순이동평균법</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>단순 기법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="단순 기법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_simple(train_data, test_data, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>):</span>
<span id="cb3-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb3-4">    </span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb3-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>:</span>
<span id="cb3-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 단순예측법: 마지막 값</span></span>
<span id="cb3-8">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-9">            </span>
<span id="cb3-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>:</span>
<span id="cb3-11">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추세분석법</span></span>
<span id="cb3-12">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-13">            </span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>:</span>
<span id="cb3-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4기간 이동평균</span></span>
<span id="cb3-16">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-17">            </span>
<span id="cb3-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>:</span>
<span id="cb3-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12기간 이동평균</span></span>
<span id="cb3-20">            window <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb3-21">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>window:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-22">            </span>
<span id="cb3-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>:</span>
<span id="cb3-24">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 단순예측법 (4분기 전과 동일)</span></span>
<span id="cb3-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:</span>
<span id="cb3-26">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-28">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-29">        </span>
<span id="cb3-30">        predictions.append(pred)</span>
<span id="cb3-31">        </span>
<span id="cb3-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb3-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb3-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-35">    </span>
<span id="cb3-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb3-37"></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 방법별 rolling forecast 수행</span></span>
<span id="cb3-39">methods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>]</span>
<span id="cb3-40">rolling_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb3-41"></span>
<span id="cb3-42"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> methods:</span>
<span id="cb3-43">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_simple(train, test, method)</span>
<span id="cb3-44">    rolling_results[method] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> predictions</span>
<span id="cb3-45">    </span>
<span id="cb3-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결과를 DataFrame으로 정리</span></span>
<span id="cb3-47">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(rolling_results)</span>
<span id="cb3-48">results_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'actual'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>].values</span>
<span id="cb3-49">results_df.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test.index</span>
<span id="cb3-50"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results_df)</span></code></pre></div>
</div>
</section>
<section id="지수-평활법" class="level2">
<h2 class="anchored" data-anchor-id="지수-평활법">지수 평활법</h2>
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-10-14-19-44-17.png" class="img-fluid"></p>
<ul>
<li>α -&gt; 1: 최근 자료에 비중을 둠. α -&gt; 0: 기존 예측을 따름</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>지수 평활법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="지수 평활법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.holtwinters <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SimpleExpSmoothing</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_exponential_smoothing(train_data, test_data, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>):</span>
<span id="cb4-4">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-5">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb4-6">    </span>
<span id="cb4-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb4-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb4-9">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb4-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SimpleExpSmoothing(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-11">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(smoothing_level<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>a, optimized<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-12"></span>
<span id="cb4-13">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb4-14">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.predict(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb4-15">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb4-16"></span>
<span id="cb4-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb4-18">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb4-20">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-21">        </span>
<span id="cb4-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb4-23">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb4-24">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-25">    </span>
<span id="cb4-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb4-27"></span>
<span id="cb4-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 지수 평활법 rolling forecast 실행 예시</span></span>
<span id="cb4-29">exp_smooth_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_exponential_smoothing(train, test, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
</section>
<section id="sarimax-계열" class="level2">
<h2 class="anchored" data-anchor-id="sarimax-계열">SARIMAX 계열</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-10-14-08-07-42.png" class="img-fluid figure-img"></p>
<figcaption>SARIMAX 과정</figcaption>
</figure>
</div>
<section id="정상성-확인" class="level3">
<h3 class="anchored" data-anchor-id="정상성-확인">정상성 확인</h3>
<ul>
<li>정상시계열은 분산과 평균, 자기 상관이 시간에 따라 변하지 않는 시계열</li>
<li>분산이 일정하지 않은 경우: 로그 변환
<ul>
<li>그래프로 확인</li>
</ul></li>
<li>평균이 일정하지 않은 경우: 차분</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>stationarity test</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="stationarity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.stattools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> adfuller</span>
<span id="cb5-2"></span>
<span id="cb5-3">ad_fuller_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> adfuller(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ADF Statistic:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'p-value:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<ul>
<li>p-value &lt; 0.05: 귀무가설 기각, 정상시계열</li>
<li>주로 평균이 일정하지 않은 것을 찾아낼 수 있다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>difference</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff()</span></code></pre></div>
</div>
<ul>
<li>만약 차분 후에도 정상성이 만족되지 않는다면, 계절 차분을 고려</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>seasonal difference</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="seasonal difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12개월 주기</span></span></code></pre></div>
</div>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.graphics.tsaplots <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_acf, plot_pacf</span>
<span id="cb8-2"></span>
<span id="cb8-3">plot_acf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-4">plot_pacf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-5">plt.show()</span></code></pre></div>
<ul>
<li>자기 상관이 존재하는 경우
<ul>
<li>ACF, PACF 그래프로 확인</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-10-14-08-43-44.png" class="img-fluid figure-img"></p>
<figcaption>모델 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>하지만 매우 주관적일 수 있으므로, 직접 여러 모델을 돌려본 후 AIC 기준으로 선택.
<ul>
<li>그리고 다시 검정 진행</li>
</ul></li>
</ul>
</section>
<section id="sarimax-모델-적합" class="level3">
<h3 class="anchored" data-anchor-id="sarimax-모델-적합">SARIMAX 모델 적합</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="SARIMAX" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.statespace.sarimax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SARIMAX</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> product</span>
<span id="cb9-3"></span>
<span id="cb9-4">p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-5">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 차분 횟수</span></span>
<span id="cb9-6">q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-7">P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-8">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 차분 횟수</span></span>
<span id="cb9-9">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-10">s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 주기</span></span>
<span id="cb9-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> product(p, q, P, Q)</span>
<span id="cb9-12">order_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(parameters)</span>
<span id="cb9-13"></span>
<span id="cb9-14">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-15"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> order <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> order_list:</span>
<span id="cb9-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb9-17">        model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb9-18">                        exog,</span>
<span id="cb9-19">                        order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], d, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), </span>
<span id="cb9-20">                        seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], D, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], s),</span>
<span id="cb9-21">                        simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span>:</span>
<span id="cb9-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span></span>
<span id="cb9-24">    aic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.aic</span>
<span id="cb9-25">    results.append([order, aic])</span>
<span id="cb9-26">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(p, q, P, Q)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>])</span>
<span id="cb9-27">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-28">display(results_df)</span></code></pre></div>
</div>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">best_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb10-2">                    exog,</span>
<span id="cb10-3">                    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [2, 3], 0, 1 이런 식으로도 가능(y_t-2, y_t-3 사용하는 방법)</span></span>
<span id="cb10-4">                    simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">best_model.summary()</span></code></pre></div>
<ul>
<li>결과 확인</li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">best_model.plot_diagnostics(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb12-2">plt.show()</span></code></pre></div>
<ul>
<li>등분산성, 정규성 만족하는지 육안으로 확인</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>ljung-box</strong></pre>
</div>
<div class="sourceCode" id="cb13" data-filename="ljung-box" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.diagnostic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> acorr_ljungbox</span>
<span id="cb13-2"></span>
<span id="cb13-3">ljung_box_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> acorr_ljungbox(best_model.resid, lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>], return_df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb13-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(ljung_box_result)</span></code></pre></div>
</div>
<ul>
<li>p-value가 0.05보다 크면 잔차가 백색잡음</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb14" data-filename="SARIMAX rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_sarimax(train_data, test_data, order, seasonal_order, exog_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, exog_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb14-4">    exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exog_train.copy() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_train <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb14-5">    </span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb14-8">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb14-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-10">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-11">                               exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_extended,</span>
<span id="cb14-12">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-13">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-14">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-16">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-17">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-18">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-19">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-20">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-21">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb14-22">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-23">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb14-24">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-25">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-26">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb14-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb14-28">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb14-29">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb14-30">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb14-31">        </span>
<span id="cb14-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb14-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb14-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-35">        </span>
<span id="cb14-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-37">            exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([exog_extended, exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-38">    </span>
<span id="cb14-39">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb14-40"></span>
<span id="cb14-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SARIMAX rolling forecast 실행 예시</span></span>
<span id="cb14-42">sarimax_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_sarimax(</span>
<span id="cb14-43">    train, test, </span>
<span id="cb14-44">    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), </span>
<span id="cb14-45">    seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절성이 없는 경우</span></span>
<span id="cb14-46">)</span></code></pre></div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/07.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>비지도 학습 템플릿</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/06.html</link>
  <description><![CDATA[ 




<section id="군집-분석" class="level2">
<h2 class="anchored" data-anchor-id="군집-분석">군집 분석</h2>
<section id="distance-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-methods">Distance-based methods</h3>
<ul>
<li>Partitioning methods</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>k-means</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="k-means" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-3"></span>
<span id="cb1-4">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># or RobustScaler</span></span>
<span id="cb1-5">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Elbow method</span></span>
<span id="cb1-8">I <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>):</span>
<span id="cb1-10">    kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>i) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sklearn은 기본적으로 k-means++</span></span>
<span id="cb1-11">    kmeans.fit(df_scaled)</span>
<span id="cb1-12">    I.append(kmeans.inertia_)</span>
<span id="cb1-13">plt.plot(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>), I, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k 선택 후 군집화</span></span>
<span id="cb1-16">kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-17">kmeans.fit(df_scaled)</span>
<span id="cb1-18">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cluster'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans.labels_</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 군집 중심값 정보</span></span>
<span id="cb1-21">centers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.inverse_transform(kmeans.cluster_centers_)</span>
<span id="cb1-22">centers_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(centers, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'cluster_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(centers.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])])</span>
<span id="cb1-23">display(centers_df)</span></code></pre></div>
</div>
<ol type="1">
<li>Kmeans
<ul>
<li>polinominal 시간 안에 해결 가능</li>
<li>noise, outlier에 민감함</li>
<li>수치형만 처리 가능</li>
</ul></li>
<li>k-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함</li>
<li>k-prototype: 범주형, 수치형 섞인거 처리 가능</li>
<li>k-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함
<ul>
<li>PAM: Partitioning Around Medoids
<ul>
<li>scalability 문제 있음</li>
</ul></li>
<li>CLARA: sampling을 통해서 PAM의 scalability 문제를 해결
<ul>
<li>샘플링 과정에서 biased될 수 있음</li>
</ul></li>
<li>CLARANS: medoid 후보를 랜덤하게 선택함</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p>k-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함 ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)</p>
</blockquote>
<ul>
<li>Hierarchical methods</li>
</ul>
<ol type="1">
<li>top-down: divisive, dia</li>
<li>bottom-up: agglomerative</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>agglomerative</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="agglomerative" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.cluster.hierarchy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dendrogram, linkage. cut_tree</span>
<span id="cb2-2"></span>
<span id="cb2-3">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linkage(df_scaled, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ward'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 'single', 'complete', 'average', 'ward' 등등</span></span>
<span id="cb2-4"></span>
<span id="cb2-5">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cut_tree(z, n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).flatten()</span>
<span id="cb2-6"></span>
<span id="cb2-7">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dendogram(z, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index))</span>
<span id="cb2-8">plt.show()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>합쳐지는 거리</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="합쳐지는 거리" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">thr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dcoord'</span>])</span>
<span id="cb3-2">thr <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이</span></span>
<span id="cb3-3">thr[thr[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>].sort_values(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 70 이상의 높이에서 합쳐지는 군집들의 높이</span></span></code></pre></div>
</div>
<ul>
<li>거리기반 군집의 단점:
<ul>
<li>군집의 모양이 구형이 아닐 경우 찾기 어려움</li>
<li>군집의 갯수 결정하기 어려움</li>
<li>군집의 밀도가 높아야함</li>
</ul></li>
</ul>
</section>
<section id="density-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="density-based-methods">Density-based methods</h3>
<ul>
<li>다양한 모양의 군집을 찾을 수 있음</li>
<li>noise, outlier에 강함</li>
</ul>
<ol type="1">
<li>DBSCAN: 잡음 포인트는 군집에서 제외
<ol type="1">
<li>core point를 찾음(eps 이내에 minPts 이상 있는 점)</li>
<li>core point를 중심으로 군집을 확장
<ul>
<li>core point가 아닌 경우 확장 종료</li>
</ul></li>
</ol>
<ul>
<li>고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음</li>
<li>군집간 계층관계를 인식하기 어렵다</li>
<li>대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN eps 결정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="DBSCAN eps 결정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.neighbors <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> NearestNeighbors</span>
<span id="cb4-2"></span>
<span id="cb4-3">MIN_SAMPLES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb4-4">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb4-5"></span>
<span id="cb4-6">neigh <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NearestNeighbors(n_neighbors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES)</span>
<span id="cb4-7">neigh.fit(df_scaled)</span>
<span id="cb4-8">distances, indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> neigh.kneighbors(df_scaled)</span>
<span id="cb4-9"></span>
<span id="cb4-10">k_distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sort(distances[:, MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])[::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot 그리기</span></span>
<span id="cb4-13">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb4-14">plt.plot(k_distances)</span>
<span id="cb4-15">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data Points sorted by distance'</span>)</span>
<span id="cb4-16">plt.ylabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-th Nearest Neighbor Distance'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 자기 자신을 제외한 거리</span></span>
<span id="cb4-17">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'k-distance Graph'</span>)</span>
<span id="cb4-18">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-19">plt.show()</span></code></pre></div>
</div>
<ul>
<li>min samples 기준
<ul>
<li>2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.</li>
<li>이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기</li>
</ul></li>
<li>eps 기준
<ul>
<li>k-dist plot에서 급격히 꺾이는 지점</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="DBSCAN" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DBSCAN</span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 찾은 값으로 DBSCAN 적용</span></span>
<span id="cb5-4">eps_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">18</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 결정한 값</span></span>
<span id="cb5-5"></span>
<span id="cb5-6">db <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DBSCAN(eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eps_value, min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb5-7">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> db.labels_</span></code></pre></div>
</div>
<ol start="2" type="1">
<li>OPTICS: DBSCAN의 단점을 보완
<ul>
<li>군집의 밀도가 다를 때도 잘 처리함</li>
<li>군집의 계층 구조를 인식할 수 있음</li>
<li>minPts 파라미터가 필요함</li>
<li>얘로도 이상치 탐지 가능</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>OPTICS</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="OPTICS" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OPTICS</span>
<span id="cb6-2"></span>
<span id="cb6-3">optics <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OPTICS(min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb6-4">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optics.labels_</span></code></pre></div>
</div>
</section>
<section id="평가" class="level3">
<h3 class="anchored" data-anchor-id="평가">평가</h3>
<ul>
<li>silhuette score: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20s(i)%7D%7Bn%7D">
<ul>
<li>s(i): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bb(i)%20-%20a(i)%7D%7Bmax((a(i),%20b(i)))%7D">
<ul>
<li>a(i): 군집 내 노드간의 평균 거리</li>
<li>b(i): 가장 가까운 군집과의 노드 간 평균 거리</li>
</ul></li>
<li>1에 가까울 수록 좋음</li>
</ul></li>
<li>그 외 sklearn metrics 참고</li>
</ul>
</section>
</section>
<section id="차원-축소" class="level2">
<h2 class="anchored" data-anchor-id="차원-축소">차원 축소</h2>
<ul>
<li>PCA, LSA, t-SNE, UMAP, ICA, MDS, NMF 등등
<ul>
<li>정말 많은 방법들이 있다.</li>
</ul></li>
<li>요인분석에서 확인적 요인분석은 python에서 제공하는 library가 없는걸로 알고 있음</li>
</ul>
</section>
<section id="연관-분석" class="level2">
<h2 class="anchored" data-anchor-id="연관-분석">연관 분석</h2>
<div id="0bf5a808" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TransactionEncoder</span>
<span id="cb7-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.frequent_patterns <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> apriori, association_rules</span>
<span id="cb7-3"></span>
<span id="cb7-4">transactions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb7-5">te <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TransactionEncoder()</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 전처리</span></span>
<span id="cb7-8">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>).values</span>
<span id="cb7-9">te_ary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> te.fit_transform(data)</span>
<span id="cb7-10">transactions[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(te_ary, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>te.columns_)</span>
<span id="cb7-11"></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈발패턴 생성</span></span>
<span id="cb7-13">fset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apriori(t, min_support<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, use_colnames<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb7-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"빈발패턴이 존재하지 않습니다."</span>)</span>
<span id="cb7-16"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 연관규칙 생성</span></span>
<span id="cb7-18">    rule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association_rules(fset, metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"confidence"</span>, min_threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb7-19">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_ant'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'antecedents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-20">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'consequents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-21">    display(rule[(rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lift'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.2</span>)].reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</div>
<ul>
<li>지지도: 전체 거래에서 특정 항목 집합이 나타나는 비율</li>
<li>신뢰도: 특정 항목 집합이 주어졌을 때 다른 항목</li>
<li>향상도: 두 항목 집합이 독립적인 경우에 비해 함께 나타날 가능성</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/06.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분산 분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/05.html</link>
  <description><![CDATA[ 




<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-09-07-10-39-35.png" class="img-fluid figure-img"></p>
<figcaption>검정 방법 선택 기준</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-09-07-10-45-15.png" class="img-fluid figure-img"></p>
<figcaption>모수 검정 방법 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, …), 각 케이스에 맞는 모형을 사용해야 함.</li>
</ul>
<section id="정규성-검정" class="level3">
<h3 class="anchored" data-anchor-id="정규성-검정">정규성 검정</h3>
<ul>
<li><p>표본이 정규분포를 따르는지 검정.</p></li>
<li><p>따르지 않더라도 중심극한정리에 의해 <strong>표본의 크기가 충분히 크면</strong> 모수 검정을 사용할 수 있다.</p></li>
<li><p><strong>shapiro wilk 검정</strong>: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>jarque-Bera</strong>: 대표본에 사용.</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>Q-Q plot</strong>: x축이 이론적 분위수, y축이 표본 분위수</p></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>normality test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="normality test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro, jarque_bera, zscore, probplot</span>
<span id="cb1-4"></span>
<span id="cb1-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span>
<span id="cb1-6">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(data)</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jarque_bera(data)</span>
<span id="cb1-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jarque-Bera Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-15"></span>
<span id="cb1-16">zdata <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> zscore(data)</span>
<span id="cb1-17">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-18"></span>
<span id="cb1-19">(osm, odr), (slope, intercept, r) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> probplot(zdata, plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-20">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Q-Q Plot"</span>)</span>
<span id="cb1-21"></span>
<span id="cb1-22">data.hist(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'skyblue'</span>, density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-23">sns.kdeplot(data, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'KDE'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-24">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Histogram"</span>)</span>
<span id="cb1-25"></span>
<span id="cb1-26">plt.show()</span></code></pre></div>
</div>
</section>
<section id="등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="등분산성-검정">등분산성 검정</h3>
<ul>
<li><strong>Barlett 검정</strong>: <strong>정규성을 만족하는 경우에만</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
<li><strong>Levene 검정</strong>: <strong>정규성을 만족하지 않는 경우에도</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>equal variance test</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="equal variance test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> bartlett, levene</span>
<span id="cb2-4"></span>
<span id="cb2-5">group1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb2-6">group2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>]</span>
<span id="cb2-7">group3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span>
<span id="cb2-8"></span>
<span id="cb2-9">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bartlett(group1, group2, group3)</span>
<span id="cb2-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Bartlett's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-11"></span>
<span id="cb2-12">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> levene(group1, group2, group3)</span>
<span id="cb2-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Levene's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="분산-분석" class="level2">
<h2 class="anchored" data-anchor-id="분산-분석">분산 분석</h2>
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-10-06-18-54-33.png" class="img-fluid"></p>
<ul>
<li>정규성을 만족 못할 경우, 서열척도 비모수 검정 사용 가능</li>
<li>코크란 Q 검정: 이항분포를 따르는 반복 측정 자료에 사용
<ul>
<li>만약 대응표본이 아닐 경우 (실패, 성공) 변수를 만들어서 독립성 검정 사용</li>
<li>만약 이항분포가 아닐 경우 프리드만 검정 고려</li>
</ul></li>
<li>독립성 검정의 cell의 기대도수가 5 미만인 경우, Boschloo’s test 혹은 Fisher’s Exact 사용
<ul>
<li>만약 2x2을 넘어갈 경우 몬테카를로 시뮬레이션 사용(python으로 구현하기 복잡하다.)</li>
</ul></li>
<li>크루스칼, 맨휘트니: 동점이 과하게 많을 경우 permutation test, 순열 분산 분석 사용 가능
<ul>
<li>하지만 ADP 환경의 scipy에서는 버전이 낮아서 permutation test를 쓸 수 없다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>사후 검정</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="사후 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.sandbox.stats.multicomp <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MultiComparison</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ttest_ind</span>
<span id="cb3-5"></span>
<span id="cb3-6">mc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultiComparison(data, groups).allpairtest(ttest_ind, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bonf'</span>)</span>
<span id="cb3-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(mc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb3-8"></span>
<span id="cb3-9">mc.plot_simultaneous()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</div>
<section id="적합성-검정" class="level3">
<h3 class="anchored" data-anchor-id="적합성-검정">적합성 검정</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>카이제곱 적합성 검정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="카이제곱 적합성 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb4-2"></span>
<span id="cb4-3">count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].value_counts().sort_index()</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈도 수가 5 미만인 경우 합침</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb4-8">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-9">count.drop([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모수 추정</span></span>
<span id="cb4-12">lam <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].mean()</span>
<span id="cb4-13">poi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> poisson(lam)</span>
<span id="cb4-14"></span>
<span id="cb4-15">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> count.values.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-16">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([(poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), </span>
<span id="cb4-17">       <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>poi.pmf(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)), </span>
<span id="cb4-18">       poi.sf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 마지막 값은 이상 값으로</span></span>
<span id="cb4-19">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> n</span>
<span id="cb4-20"></span>
<span id="cb4-21"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chisquare</span>
<span id="cb4-22"></span>
<span id="cb4-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추정한 모수 갯수 만큼 자유도 차감</span></span>
<span id="cb4-24">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chisquare(count.values, exp, ddof<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-25">p</span></code></pre></div>
</div>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="다변량-분산분석" class="level2">
<h2 class="anchored" data-anchor-id="다변량-분산분석">다변량 분산분석</h2>
<section id="way-anova" class="level3">
<h3 class="anchored" data-anchor-id="way-anova">2-way ANOVA</h3>
<ul>
<li>반복이 없는 경우 교효작용 검정은 불가능</li>
<li>정규성, 등분산성 검정은 잔차에 대해 수행
<ul>
<li>모든 집단이 정규성과 등분산성을 만족하면 오차는 N(0, σ²)를 따른다.</li>
<li>고로 오차의 추정값인 잔차가 정규성과 등분산성을 만족하는지 검정으로 대체 가능</li>
</ul></li>
</ul>
<div id="ad8f925f" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb5-3"></span>
<span id="cb5-4"></span>
<span id="cb5-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)+C(성별):C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data).fit()</span>
<span id="cb5-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb5-7">atab</span></code></pre></div>
</div>
<div id="3d7e74f6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro</span>
<span id="cb6-2"></span>
<span id="cb6-3">residuals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb6-4"></span>
<span id="cb6-5">shapiro_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(residuals)</span>
<span id="cb6-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test on Residuals: Statistic=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>statistic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pvalue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div id="86f2de49" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>residuals, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-2">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb7-3">plt.show()</span></code></pre></div>
</div>
<ul>
<li>교효작용이 유의하지 않을 경우 오차항에 pooling을 한다.</li>
</ul>
<div id="958391d3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb8-3"></span>
<span id="cb8-4"></span>
<span id="cb8-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tmp).fit()</span>
<span id="cb8-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb8-7">atab</span></code></pre></div>
</div>
<ul>
<li>main 효과의 사후검정은 anova와 동일
<ul>
<li>하지만 interation 효과가 유의할 경우, 주효과는 무의미하다.</li>
</ul></li>
<li>interaction 효과는 시각적으로 보여주는게 좋다.</li>
</ul>
<div id="1d9244c4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">grouped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tmp.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>].mean().reset_index()</span>
<span id="cb9-2">pivot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grouped.pivot(index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>, values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>)</span>
<span id="cb9-3">pivot.plot(marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb9-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'평균 성적'</span>)</span>
<span id="cb9-5">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별-교육방법 교호작용 효과'</span>)</span>
<span id="cb9-6">plt.legend(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>)</span>
<span id="cb9-7">plt.tight_layout()</span>
<span id="cb9-8">plt.show()</span></code></pre></div>
</div>
<ul>
<li>2way 이상은 해석이 어려워서 잘 사용하지 않는다.</li>
</ul>
</section>
<section id="반복측정-분산분석" class="level3">
<h3 class="anchored" data-anchor-id="반복측정-분산분석">반복측정 분산분석</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>repeated measures anova</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="repeated measures anova" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pingouin <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pg</span>
<span id="cb10-2"></span>
<span id="cb10-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>],</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">78</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">85</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">82</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">86</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">88</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">83</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>]</span>
<span id="cb10-7">})</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 반복측정 ANOVA 수행 및 구형성 검정, 보정 포함</span></span>
<span id="cb10-10">aov <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg.rm_anova(dv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>, within<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>, subject<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, correction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-11">display(aov)</span></code></pre></div>
</div>
<ul>
<li>sphericity가 true이면 구형성 만족</li>
<li>p-unc: 구형성 보정 전 p-value</li>
<li>p-GG-corr: Greenhouse-Geisser 보정 후 p-value
<ul>
<li>구형성 위반 시 사용</li>
</ul></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/05.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/04.html</link>
  <description><![CDATA[ 




<section id="가정" class="level2">
<h2 class="anchored" data-anchor-id="가정">가정</h2>
<ul>
<li><strong>선형성</strong>: 종속변수와 독립변수 간의 관계는 선형이다.</li>
<li><strong>정규성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 정규분포이다.</li>
<li><strong>등분산성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 동일한 분산을 갖는다.</li>
<li><strong>독립성</strong>: 모든 <strong>잔차값</strong>은 서로 독립이다.</li>
</ul>
</section>
<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<ul>
<li>먼저 모델링을 진행한 후, 잔차를 통해 가정을 검정한다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>linear regression test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="linear regression test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb1-2"></span>
<span id="cb1-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb1-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, Xc).fit()</span>
<span id="cb1-5">resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<section id="이상치-영향치-처리" class="level3">
<h3 class="anchored" data-anchor-id="이상치-영향치-처리">이상치, 영향치 처리</h3>
<ul>
<li>레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차
<ul>
<li>Cook’s distance</li>
<li>Leverage</li>
<li>그 외 DFFITS, DFBETAS 등</li>
</ul></li>
</ul>
<div id="71ac02f0" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid.mean()</span>
<span id="cb2-2">std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid.std()</span>
<span id="cb2-3">outlier_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> (resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std)</span>
<span id="cb2-4"></span>
<span id="cb2-5">influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_influence()</span>
<span id="cb2-6">cooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> influence.cooks_distance[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-7">influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(resid) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> model.df_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-8"></span>
<span id="cb2-9">outlier <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid[outlier_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> influence].index</span>
<span id="cb2-10"></span>
<span id="cb2-11">y_log <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.drop(outlier)</span>
<span id="cb2-12">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X.drop(outlier))</span>
<span id="cb2-13">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, Xc).fit()</span>
<span id="cb2-14">resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
</section>
<section id="다중공선성-검정" class="level3">
<h3 class="anchored" data-anchor-id="다중공선성-검정">다중공선성 검정</h3>
<ol type="1">
<li>전 변수 집합 대상</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>multicollinearity test</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="multicollinearity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">cor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.corr()</span>
<span id="cb3-2">cond_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.cond(cor)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Condition Number:"</span>, cond_num)</span></code></pre></div>
</div>
<ul>
<li><strong>30을 초과</strong>하면 다중공선성이 높다고 판단한다.</li>
<li>선형 종속 가능성을 봄.</li>
<li>statsmodels의 ols를 사용해도 볼 수 있음</li>
<li>scaling이 선행되어야 함.</li>
<li>초과 시 해석:
<ul>
<li>독립변수의 전체 차원이 부족한 경우</li>
<li>표본을 더 모으거나 새로운 변수를 도입</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>VIF test</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="VIF test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.outliers_influence <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> variance_inflation_factor</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> check_vif(X, y):</span>
<span id="cb4-5">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb4-6">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, X)</span>
<span id="cb4-7">    model.fit()</span>
<span id="cb4-8">    vif_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>])</span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(model.exog_names)):</span>
<span id="cb4-10">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.exog_names[i]</span>
<span id="cb4-11">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> variance_inflation_factor(model.exog, i)</span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> vif_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-13"></span>
<span id="cb4-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(check_vif(X, y))</span></code></pre></div>
</div>
<ul>
<li>특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우</li>
<li><strong>VIF가 10</strong>을 넘을 경우</li>
<li>덜 중요하다면 제거</li>
<li>중요하다면 변수에 대한 독립적 정보 보강(세분화, …)</li>
</ul>
<ol start="3" type="1">
<li>두 변수의 상관계수가 높은 경우</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>correlation heatmap</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="correlation heatmap" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb5-2"></span>
<span id="cb5-3">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>))</span>
<span id="cb5-4">sns.heatmap(cor, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span></code></pre></div>
</div>
<ul>
<li>둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)</li>
<li>둘 중 하나를 제거</li>
</ul>
</section>
<section id="정규성" class="level3">
<h3 class="anchored" data-anchor-id="정규성">정규성</h3>
<ul>
<li>ols의 summary를 통해 확인 가능</li>
<li>혹은 EDA 과정에서 사용한 정규성 검정 참조</li>
</ul>
</section>
<section id="선형성-등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="선형성-등분산성-검정">선형성, 등분산성 검정</h3>
<div id="50e74334" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb6-3"></span>
<span id="cb6-4">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.resid, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb6-5">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb6-6">plt.show()</span></code></pre></div>
</div>
<ul>
<li>선형성:
<ul>
<li>잔차도가 어떠한 패턴도 보이지 않아야 한다.</li>
</ul></li>
<li>등분산성:
<ul>
<li>잔차도가 일정한 폭을 가져야 한다.</li>
</ul></li>
</ul>
<div id="d9f59650" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.diagnostic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> het_breuschpagan</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Breusch-Pagan 검정</span></span>
<span id="cb7-4">lm, lm_pvalue, fvalue, f_pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> het_breuschpagan(resid, Xc)</span>
<span id="cb7-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Breusch-Pagan p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lm_pvalue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 잔차의 정규성 만족시 lm_pvalue, 그 외 f_pvalue 사용</span></span>
<span id="cb7-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> lm_pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>:</span>
<span id="cb7-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"등분산성 가정을 만족합니다."</span>)</span>
<span id="cb7-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"등분산성 가정을 위반합니다."</span>)</span></code></pre></div>
</div>
</section>
<section id="독립성" class="level3">
<h3 class="anchored" data-anchor-id="독립성">독립성</h3>
<ul>
<li>독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다.
<ul>
<li>더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.</li>
<li>2에 가까울수록 독립성 만족</li>
<li>statsmodels의 ols로 확인 가능</li>
</ul></li>
</ul>
</section>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ol type="1">
<li>범주형 변수 처리:
<ul>
<li>더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현
<ul>
<li>각 범주의 회귀계수는 기준 범주와의 차이를 의미</li>
</ul></li>
</ul></li>
<li>이상치 / 영향점: 관측값 제거</li>
<li>선형성 위반: 독립변수 변환, GAM</li>
<li>정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM</li>
<li>다중공산성 위반: 다중공산성 파트 참고
<ul>
<li>혹은 변수 선택법을 사용</li>
</ul></li>
</ol>
<ul>
<li>가정 만족할 때까지 검정, 전처리 계속 반복</li>
</ul>
<section id="변수-변환" class="level3">
<h3 class="anchored" data-anchor-id="변수-변환">변수 변환</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-09-20-18-38-36.png" class="img-fluid figure-img"></p>
<figcaption>회귀 모델 수정 λ</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/img/2025-09-20-18-55-55.png" class="img-fluid figure-img"></p>
<figcaption>경험적인 적절한 λ</figcaption>
</figure>
</div>
<ul>
<li>최적의 λ는 최대 우도 추정법으로 구할 수 있다.</li>
<li>변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.</li>
<li>일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.
<ul>
<li>아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.</li>
<li>통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>box cox transformation</strong></pre>
</div>
<div class="sourceCode" id="cb8" data-filename="box cox transformation" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> boxcox</span>
<span id="cb8-2"></span>
<span id="cb8-3">y_transformed, best_lambda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boxcox(y)</span>
<span id="cb8-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Best lambda: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_lambda<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
<section id="변수-선택법" class="level3">
<h3 class="anchored" data-anchor-id="변수-선택법">변수 선택법</h3>
<ul>
<li>전진 선택법</li>
<li>후진 선택법</li>
<li>단계적 선택법</li>
<li>최적조합 선택법: 모든 조합 다 해봄</li>
<li>기준
<ul>
<li>R2, Adj R2</li>
<li>AIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법</li>
<li>BIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌</li>
<li>Mallows’ Cp</li>
</ul></li>
</ul>
</section>
</section>
<section id="규제-선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="규제-선형-회귀">규제 선형 회귀</h2>
<ul>
<li>지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦</li>
<li>독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)
<ul>
<li>일반적으로는 scale을 하든 안하든 r square에 차이가 없다.</li>
</ul></li>
</ul>
<ol type="1">
<li>릿지회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%CE%B2_j%5E2"></li>
<li>회귀계수 절댓값을 0에 가깝게 함</li>
<li>하지만 0으로 만들지는 않음</li>
<li>작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.</li>
<li>회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음</li>
</ul></li>
<li>라쏘회귀:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%7C%CE%B2_j%7C"></li>
<li>회귀계수를 0으로 만들 수 있음</li>
<li>변수 선택 효과</li>
<li>릿지보다 해석이 쉬움</li>
<li>일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음</li>
</ul></li>
<li>엘라스틱넷 회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB_1%5CSigma%20%7C%CE%B2_j%7C%20+%20%CE%BB_2%5CSigma%20%CE%B2_j%5E2"></li>
<li>릿지와 라쏘의 장점을 모두 가짐</li>
<li>변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦</li>
<li>독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음</li>
</ul></li>
</ol>
</section>
<section id="일반화-선형-회귀glm" class="level2">
<h2 class="anchored" data-anchor-id="일반화-선형-회귀glm">일반화 선형 회귀(GLM)</h2>
<ul>
<li>종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우
<ul>
<li>이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.</li>
<li>포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.</li>
<li>따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.</li>
</ul></li>
</ul>
<section id="logistic-회귀" class="level3">
<h3 class="anchored" data-anchor-id="logistic-회귀">Logistic 회귀</h3>
<ul>
<li>종속변수가 범주형일 경우</li>
<li><img src="https://latex.codecogs.com/png.latex?z%20=%20%CE%B2_0%20+%20%CE%B2_1%20x_1%20+%20%CE%B2_2%20x_2%20+%20...%20+%20%CE%B2_n%20x_n"></li>
<li><img src="https://latex.codecogs.com/png.latex?p%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-z%7D%7D"></li>
<li>오즈: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7B1-p%7D"> = <img src="https://latex.codecogs.com/png.latex?e%5Ez"></li>
<li>오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(e%5E%7B%CE%B2_k%7D)%5Ek"></li>
</ul></li>
<li>LLR의 p-value가 낮다면, 모델이 통계적으로 유의미함을 의미</li>
<li>잔차 검정은 안함</li>
</ul>
</section>
<section id="포아송-회귀" class="level3">
<h3 class="anchored" data-anchor-id="포아송-회귀">포아송 회귀</h3>
<ul>
<li>종속변수가 count 데이터일 경우</li>
</ul>
<div id="ba8572d0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb9-2"></span>
<span id="cb9-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb9-4"></span>
<span id="cb9-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, X, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.Poisson())</span>
<span id="cb9-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb9-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x_k">가 한 단위 증가할 때, 빈도 수가 <img src="https://latex.codecogs.com/png.latex?exp(%CE%B2_k)">배 증가</li>
<li>만약 관찰 시간이 다를 경우, offset로 np.log(df[관찰시간])을 넣어줘야 함</li>
<li>Deviance / DF_resid 가 1보다 크면 과산포, 작으면 과소산포
<ul>
<li>과산포: 사건발생 확률이 일정하지 않음</li>
<li>과산포 시 음이항 회귀 사용</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>음이항 회귀</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="음이항 회귀" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb10-2"></span>
<span id="cb10-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb10-4"></span>
<span id="cb10-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, Xc, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.NegativeBinomial())</span>
<span id="cb10-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb10-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li>Quasi-Poisson도 있지만, statsmodels에서는 제공하지 않음</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/04.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>데이터 전처리</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/03.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="데이터-전처리의-의미" class="level2">
<h2 class="anchored" data-anchor-id="데이터-전처리의-의미">데이터 전처리의 의미</h2>
<ol type="1">
<li>데이터 클리닝</li>
<li>데이터 통합</li>
<li>데이터 변환</li>
<li>데이터 축소</li>
<li>불균형 데이터 처리</li>
<li>데이터 분할</li>
</ol>
</section>
<section id="이상치-확인-및-정제" class="level2">
<h2 class="anchored" data-anchor-id="이상치-확인-및-정제">이상치 확인 및 정제</h2>
<section id="이상치-확인" class="level3">
<h3 class="anchored" data-anchor-id="이상치-확인">이상치 확인</h3>
<div id="4f457b88" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pandas.core.common <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random_state</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_wine</span>
<span id="cb1-5"></span>
<span id="cb1-6">wine_load <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_wine()</span>
<span id="cb1-7">wine <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(wine_load.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>wine_load.feature_names)</span>
<span id="cb1-8">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine_load.target</span>
<span id="cb1-9">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_0'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_1'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_2'</span>})</span>
<span id="cb1-10"></span>
<span id="cb1-11">plt.boxplot(wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>], whis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>)</span>
<span id="cb1-12">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Boxplot of color_intensity'</span>)</span>
<span id="cb1-13">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/03_files/figure-html/cell-2-output-1.png" width="566" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="4a714c6b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> outliers_iqr(dt, col):</span>
<span id="cb2-4">    q1, q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.percentile(dt[col], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>])</span>
<span id="cb2-5">    iqr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> q1</span>
<span id="cb2-6">    lower_bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> (iqr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>)</span>
<span id="cb2-7">    upper_bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (iqr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>)</span>
<span id="cb2-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dt[(dt[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> lower_bound) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> (dt[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> upper_bound)]</span>
<span id="cb2-9"></span>
<span id="cb2-10">outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outliers_iqr(wine, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>)</span>
<span id="cb2-11">outliers</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">alcohol</th>
<th data-quarto-table-cell-role="th">malic_acid</th>
<th data-quarto-table-cell-role="th">ash</th>
<th data-quarto-table-cell-role="th">alcalinity_of_ash</th>
<th data-quarto-table-cell-role="th">magnesium</th>
<th data-quarto-table-cell-role="th">total_phenols</th>
<th data-quarto-table-cell-role="th">flavanoids</th>
<th data-quarto-table-cell-role="th">nonflavanoid_phenols</th>
<th data-quarto-table-cell-role="th">proanthocyanins</th>
<th data-quarto-table-cell-role="th">color_intensity</th>
<th data-quarto-table-cell-role="th">hue</th>
<th data-quarto-table-cell-role="th">od280/od315_of_diluted_wines</th>
<th data-quarto-table-cell-role="th">proline</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">151</td>
<td>12.79</td>
<td>2.67</td>
<td>2.48</td>
<td>22.0</td>
<td>112.0</td>
<td>1.48</td>
<td>1.36</td>
<td>0.24</td>
<td>1.26</td>
<td>10.80</td>
<td>0.48</td>
<td>1.47</td>
<td>480.0</td>
<td>class_2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">158</td>
<td>14.34</td>
<td>1.68</td>
<td>2.70</td>
<td>25.0</td>
<td>98.0</td>
<td>2.80</td>
<td>1.31</td>
<td>0.53</td>
<td>2.70</td>
<td>13.00</td>
<td>0.57</td>
<td>1.96</td>
<td>660.0</td>
<td>class_2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">159</td>
<td>13.48</td>
<td>1.67</td>
<td>2.64</td>
<td>22.5</td>
<td>89.0</td>
<td>2.60</td>
<td>1.10</td>
<td>0.52</td>
<td>2.29</td>
<td>11.75</td>
<td>0.57</td>
<td>1.78</td>
<td>620.0</td>
<td>class_2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">166</td>
<td>13.45</td>
<td>3.70</td>
<td>2.60</td>
<td>23.0</td>
<td>111.0</td>
<td>1.70</td>
<td>0.92</td>
<td>0.43</td>
<td>1.46</td>
<td>10.68</td>
<td>0.85</td>
<td>1.56</td>
<td>695.0</td>
<td>class_2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="이상치-정제" class="level3">
<h3 class="anchored" data-anchor-id="이상치-정제">이상치 정제</h3>
<ol type="1">
<li>이상치 제거</li>
</ol>
<div id="1e5358e9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">drop_outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine.drop(index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>outliers.index)</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Original:"</span>, wine.shape)</span>
<span id="cb3-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Drop outliers:"</span>, drop_outliers.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original: (178, 14)
Drop outliers: (174, 14)</code></pre>
</div>
</div>
<ol start="2" type="1">
<li>이상치 대체</li>
</ol>
<p>이상치를 NULL로 만든 후, 결측치와 함께 대체</p>
<div id="ea78d11a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">wine.loc[outliers.index, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.NaN</span>
<span id="cb5-2"></span>
<span id="cb5-3">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>].fillna(wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>].mean(), inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-4">wine.loc[outliers.index, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color_intensity'</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_13054/3568685677.py:3: FutureWarning:

A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>151    4.908678
158    4.908678
159    4.908678
166    4.908678
Name: color_intensity, dtype: float64</code></pre>
</div>
</div>
</section>
</section>
<section id="범주형-데이터-처리" class="level2">
<h2 class="anchored" data-anchor-id="범주형-데이터-처리">범주형 데이터 처리</h2>
<div id="1a11037f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb8-2"></span>
<span id="cb8-3">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb8-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.feature_names)</span>
<span id="cb8-5">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris().target</span>
<span id="cb8-6">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Setosa'</span>, </span>
<span id="cb8-7">                                   <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Versicolour'</span>, </span>
<span id="cb8-8">                                   <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Virginica'</span>})</span></code></pre></div>
</div>
</section>
<section id="데이터-분할" class="level2">
<h2 class="anchored" data-anchor-id="데이터-분할">데이터 분할</h2>
<div id="f19e897c" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb9-2"></span>
<span id="cb9-3">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(iris.drop(</span>
<span id="cb9-4">  columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>), iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>], test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1004</span>)</span>
<span id="cb9-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'X_train: '</span>, X_train.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'X_test: '</span>, X_test.shape)</span>
<span id="cb9-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y_train: '</span>, y_train.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y_test: '</span>, y_test.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X_train:  (120, 4) X_test:  (30, 4)
y_train:  (120,) y_test:  (30,)</code></pre>
</div>
</div>
<div id="394f46f6" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">X_train.head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">87</td>
<td>6.3</td>
<td>2.3</td>
<td>4.4</td>
<td>1.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">67</td>
<td>5.8</td>
<td>2.7</td>
<td>4.1</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">131</td>
<td>7.9</td>
<td>3.8</td>
<td>6.4</td>
<td>2.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="0caf27f7" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">y_train.head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>87     Versicolour
67     Versicolour
131      Virginica
Name: Class, dtype: object</code></pre>
</div>
</div>
<div id="cbad7763" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].value_counts()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>Class
Setosa         50
Versicolour    50
Virginica      50
Name: count, dtype: int64</code></pre>
</div>
</div>
<div id="98872734" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">y_train.value_counts()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>Class
Versicolour    41
Setosa         40
Virginica      39
Name: count, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="데이터-스케일링" class="level2">
<h2 class="anchored" data-anchor-id="데이터-스케일링">데이터 스케일링</h2>
<section id="standard-scaler" class="level3">
<h3 class="anchored" data-anchor-id="standard-scaler">Standard Scaler</h3>
<ul>
<li>평균이 0, 분산이 1이 되도록 변환</li>
<li>이상치에 민감하다.</li>
<li>회귀분석보다는 분류분석에 적합</li>
</ul>
<div id="815e234c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RobustScaler, StandardScaler</span>
<span id="cb18-2"></span>
<span id="cb18-3">StdScaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb18-4"></span>
<span id="cb18-5">StdScaler.fit(X_train)</span>
<span id="cb18-6">X_train_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StdScaler.transform(X_train)</span>
<span id="cb18-7">X_test_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StdScaler.transform(X_test)</span></code></pre></div>
</div>
</section>
<section id="min-max-scaler" class="level3">
<h3 class="anchored" data-anchor-id="min-max-scaler">Min-Max Scaler</h3>
<ul>
<li>0 ~ 1 사이의 값으로 변환</li>
<li>이상치에 민감하다.</li>
<li>회귀분석에 적합</li>
</ul>
<div id="4b8cae33" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MinMaxScaler</span>
<span id="cb19-2"></span>
<span id="cb19-3">MinMaxScaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MinMaxScaler()</span>
<span id="cb19-4"></span>
<span id="cb19-5">MinMaxScaler.fit(X_train)</span>
<span id="cb19-6">X_train_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MinMaxScaler.transform(X_train)</span>
<span id="cb19-7"></span>
<span id="cb19-8">X_test_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MinMaxScaler.transform(X_test)</span></code></pre></div>
</div>
</section>
<section id="max-abs-scaler" class="level3">
<h3 class="anchored" data-anchor-id="max-abs-scaler">Max Abs Scaler</h3>
<ul>
<li>-1 ~ 1 사이의 값으로 변환</li>
<li>이상치에 민감하다.</li>
<li>회귀분석에 적합</li>
</ul>
<div id="c3c62c1c" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MaxAbsScaler</span>
<span id="cb20-2"></span>
<span id="cb20-3">MaxAbsScaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MaxAbsScaler()</span>
<span id="cb20-4"></span>
<span id="cb20-5">MaxAbsScaler.fit(X_train)</span>
<span id="cb20-6">X_train_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MaxAbsScaler.transform(X_train)</span>
<span id="cb20-7"></span>
<span id="cb20-8">X_test_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MaxAbsScaler.transform(X_test)</span></code></pre></div>
</div>
</section>
<section id="robust-scaler" class="level3">
<h3 class="anchored" data-anchor-id="robust-scaler">Robust Scaler</h3>
<ul>
<li>중앙값을 0으로 설정하고, IQR을 사용하여 잉상치 영향을 최소화함</li>
</ul>
<div id="06c29bbb" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RobustScaler</span>
<span id="cb21-2"></span>
<span id="cb21-3">RobustScaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RobustScaler()</span>
<span id="cb21-4"></span>
<span id="cb21-5">RobustScaler.fit(X_train)</span>
<span id="cb21-6">X_train_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RobustScaler.transform(X_train)</span>
<span id="cb21-7"></span>
<span id="cb21-8">X_test_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RobustScaler.transform(X_test)</span></code></pre></div>
</div>
</section>
<section id="다시-완본으로-변경" class="level3">
<h3 class="anchored" data-anchor-id="다시-완본으로-변경">다시 완본으로 변경</h3>
<ul>
<li><code>scaler.inverse_transform()</code></li>
</ul>
<div id="dcc3e633" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">pd.DataFrame(X_train_sc).head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.384615</td>
<td>-1.4</td>
<td>0.028369</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.000000</td>
<td>-0.6</td>
<td>-0.056738</td>
<td>-0.200000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1.615385</td>
<td>1.6</td>
<td>0.595745</td>
<td>0.466667</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="4a09735a" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">X_original <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RobustScaler.inverse_transform(X_train_sc)</span>
<span id="cb23-2"></span>
<span id="cb23-3">pd.DataFrame(X_original).head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>6.3</td>
<td>2.3</td>
<td>4.4</td>
<td>1.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5.8</td>
<td>2.7</td>
<td>4.1</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>7.9</td>
<td>3.8</td>
<td>6.4</td>
<td>2.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="차원-축소" class="level2">
<h2 class="anchored" data-anchor-id="차원-축소">차원 축소</h2>
<div id="df55cca4" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb24-2">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>)</span>
<span id="cb24-3"></span>
<span id="cb24-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(x)</span>
<span id="cb24-5"></span>
<span id="cb24-6">pd.DataFrame(x).head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-0.900681</td>
<td>1.019004</td>
<td>-1.340227</td>
<td>-1.315444</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-1.143017</td>
<td>-0.131979</td>
<td>-1.340227</td>
<td>-1.315444</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-1.385353</td>
<td>0.328414</td>
<td>-1.397064</td>
<td>-1.315444</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="3fadf900" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.decomposition <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PCA</span>
<span id="cb25-2"></span>
<span id="cb25-3">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb25-4">pca_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pca.fit(x)</span>
<span id="cb25-5"></span>
<span id="cb25-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(pca.singular_values_)</span>
<span id="cb25-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(pca.explained_variance_ratio_.cumsum())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[20.92306556 11.7091661   4.69185798  1.76273239]
[0.72962445 0.95813207 0.99482129 1.        ]</code></pre>
</div>
</div>
<div id="6f525d73" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scree Plot'</span>)</span>
<span id="cb27-2">plt.plot(pca.explained_variance_ratio_, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o-'</span>)</span>
<span id="cb27-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/03_files/figure-html/cell-20-output-1.png" width="571" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="데이터-불균형-문제-처리" class="level2">
<h2 class="anchored" data-anchor-id="데이터-불균형-문제-처리">데이터 불균형 문제 처리</h2>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/03.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA와 시각화</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="edaexploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="edaexploratory-data-analysis">EDA(Exploratory Data Analysis)</h2>
<p>: 데이터의 특징과 데이터에 내재된 관계를 알아내기 위해 그래프와 통계적 분석 방법을 활용하여 탐구하는 것</p>
<section id="주제" class="level3">
<h3 class="anchored" data-anchor-id="주제">주제</h3>
<ol type="1">
<li>저항성 강조: 부분적 변동(이상치 등)에 대한 민감성 확인</li>
<li>잔차 계산</li>
<li>자료변수의 재표현: 변수를 적당한 척도로 바꾸는 것</li>
<li>그래프를 통한 현시성</li>
</ol>
</section>
</section>
<section id="막대-그래프" class="level2">
<h2 class="anchored" data-anchor-id="막대-그래프">막대 그래프</h2>
<p><code>범주형 데이터</code>를 요약하고 시각적으로 비교하는 데 활용</p>
<div id="5800ac0c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_wine</span>
<span id="cb1-4"></span>
<span id="cb1-5">wine_load <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_wine()</span>
<span id="cb1-6">wine <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(wine_load.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>wine_load.feature_names)</span>
<span id="cb1-7">wine_load</span>
<span id="cb1-8">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine_load.target</span>
<span id="cb1-9">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_0'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_1'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_2'</span>})</span>
<span id="cb1-10"></span>
<span id="cb1-11">wine_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].value_counts()</span>
<span id="cb1-12">wine_type</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Class
class_1    71
class_0    59
class_2    48
Name: count, dtype: int64</code></pre>
</div>
</div>
<div id="c3e9feaa" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수직 막대</span></span>
<span id="cb3-2">plt.bar(wine_type.index, wine_type.values, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, align <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'center'</span>)</span>
<span id="cb3-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-3-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0ef77ef7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수평 막대</span></span>
<span id="cb4-2">plt.barh(wine_type.index, wine_type.values, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, left<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, align <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'center'</span>)</span>
<span id="cb4-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-4-output-1.png" width="598" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>각 범주의 값의 갯수 <code>차이가 극단적</code>인지 확인한다. 극단적일 경우, 전처리 과정에서 <code>업/다운 샘플링 등을 통해 갯수가 유사해지도록 조정</code>해야한다.</p>
</section>
<section id="히스토그램" class="level2">
<h2 class="anchored" data-anchor-id="히스토그램">히스토그램</h2>
<p><code>연속형 데이터</code>의 분포를 확인하는 데 활용</p>
<div id="34f1e1be" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Wine alcohol histogram'</span>)</span>
<span id="cb5-2">plt.hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alcohol'</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'purple'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>wine)</span>
<span id="cb5-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-5-output-1.png" width="566" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="box-plot" class="level2">
<h2 class="anchored" data-anchor-id="box-plot">box plot</h2>
<p><code>수치형 변수</code>의 분포를 확인하는 그래프</p>
<div id="195202ef" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb6-2"></span>
<span id="cb6-3">iris_load <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb6-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris_load.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_load.feature_names)</span>
<span id="cb6-5">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris_load.target</span>
<span id="cb6-6">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'setosa'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'versicolor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'virginica'</span>})</span>
<span id="cb6-7"></span>
<span id="cb6-8">plt.boxplot(iris.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>))</span>
<span id="cb6-9">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-6-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="3f9f5e77" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb7-2"></span>
<span id="cb7-3">sns.boxplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"class"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sepal width (cm)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris)</span>
<span id="cb7-4">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-7-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="산점도" class="level2">
<h2 class="anchored" data-anchor-id="산점도">산점도</h2>
<p>두 개의 <code>수치형 변수</code>의 <code>분포와 관계</code>를 확인하는 그래프</p>
<div id="4d43f52b" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'iris scatter'</span>)</span>
<span id="cb8-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>)</span>
<span id="cb8-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>)</span>
<span id="cb8-4"></span>
<span id="cb8-5">plt.scatter(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb8-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-8-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b2d82b85" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">sns.scatterplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>, hue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris, style<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb9-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-9-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="선그래프" class="level2">
<h2 class="anchored" data-anchor-id="선그래프">선그래프</h2>
<section id="수평-수직-선" class="level3">
<h3 class="anchored" data-anchor-id="수평-수직-선">수평 / 수직 선</h3>
<div id="1244e0d2" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">plt.hlines(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, xmin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, xmax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linestyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'solid'</span>)</span>
<span id="cb10-2">plt.vlines(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, ymin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, ymax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, linestyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dashed'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-10-output-1.png" width="590" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="함수식" class="level3">
<h3 class="anchored" data-anchor-id="함수식">함수식</h3>
<div id="3dbfcac4" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> linear_func(x):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb11-3"></span>
<span id="cb11-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>]</span>
<span id="cb11-5">plt.plot(X, linear_func(X), c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb11-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-11-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="회귀선" class="level3">
<h3 class="anchored" data-anchor-id="회귀선">회귀선</h3>
<div id="fa5bad03" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb12-2"></span>
<span id="cb12-3">X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>], iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>]</span>
<span id="cb12-4">plt.scatter(X, Y, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb12-5">a, b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.polyfit(X, Y, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-6">plt.plot(X, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb12-7">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-12-output-1.png" width="571" height="413" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>2차 이상의 그래프는 X값에 대하여 정렬해야 한다.</p>
<div id="9dfc8eb9" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">iris2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>)</span>
<span id="cb13-2">X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris2[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>], iris2[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length (cm)'</span>]</span>
<span id="cb13-3">b2, b1, b0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.polyfit(X, Y, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb13-4">plt.scatter(X, Y, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb13-5">plt.plot(X, b0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb13-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-13-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="꺾은선" class="level3">
<h3 class="anchored" data-anchor-id="꺾은선">꺾은선</h3>
<div id="7d2b23a4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">plt.plot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length (cm)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris2)</span>
<span id="cb14-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-14-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="상관관계-시각화" class="level2">
<h2 class="anchored" data-anchor-id="상관관계-시각화">상관관계 시각화</h2>
<section id="산점도-행렬" class="level3">
<h3 class="anchored" data-anchor-id="산점도-행렬">산점도 행렬</h3>
<div id="886974fd" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pandas.plotting <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scatter_matrix</span>
<span id="cb15-2"></span>
<span id="cb15-3">scatter_matrix(iris, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>), diagonal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>)</span>
<span id="cb15-4">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-15-output-1.png" width="649" height="655" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="fcde566f" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">sns.pairplot(iris, diag_kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>, hue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb16-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-16-output-1.png" width="1069" height="947" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="상관계수-행렬-그래프" class="level3">
<h3 class="anchored" data-anchor-id="상관계수-행렬-그래프">상관계수 행렬 그래프</h3>
<div id="6748649c" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">iris_corr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>).corr(method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pearson'</span>)</span>
<span id="cb17-2">sns.heatmap(iris_corr, xticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_corr.columns, yticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_corr.columns, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"RdBu_r"</span>, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02_files/figure-html/cell-17-output-1.png" width="648" height="520" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="pandas-profiling" class="level2">
<h2 class="anchored" data-anchor-id="pandas-profiling">Pandas Profiling</h2>
<div id="ff8ad8b8" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># from pandas_profiling import ProfileReport</span></span>
<span id="cb18-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#</span></span>
<span id="cb18-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ProfileReport(iris)</span></span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/02.html</guid>
  <pubDate>Sun, 28 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>pandas data 구조</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/01.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<p>pandas: numpy를 라벨링한거</p>
<section id="series" class="level1">
<h1>Series</h1>
<p>1차원 배열 구조, 이름과 형식을 가지고 모든 값에 고유한 인덱스를 가짐</p>
<div id="7c26af7d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"></span>
<span id="cb1-3">data1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'haha'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>}, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb1-4">data1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>ulala    1
haha     2
Name: class, dtype: int64</code></pre>
</div>
</div>
<div id="3a179798" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">data1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>}, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb3-2">data1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>ulala    2
Name: class, dtype: int64</code></pre>
</div>
</div>
<div id="e0ba7a29" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">data2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb5-2">data2</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>0    1
1    2
Name: class, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="dataframe" class="level1">
<h1>DataFrame</h1>
<p>2차원 배열 구조, 각 행은 인덱스를 가지고, 각 열은 이름과 형식을 가짐</p>
<section id="before" class="level2">
<h2 class="anchored" data-anchor-id="before">Before</h2>
<p>데이터를 호출하고, 데이터 내용과 요약 / 통계 정보를 확인해야함</p>
<p>칼럼명이 칼럼 타입을 변경해야할 때도 있음</p>
<section id="pandas-사용-준비" class="level3">
<h3 class="anchored" data-anchor-id="pandas-사용-준비">Pandas 사용 준비</h3>
<ol type="1">
<li>라이브러리 설치</li>
<li>라이브러리 호출</li>
</ol>
<div id="9e010b25" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb7-2"></span>
<span id="cb7-3">pd.set_option(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'display.max_rows'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
</div>
</section>
<section id="dataframe-선언" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-선언">DataFrame 선언</h3>
<div id="275add4f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb8-2">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>], [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]])</span>
<span id="cb8-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 1</span></span>
<span id="cb8-4">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(dataset, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>])</span>
<span id="cb8-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 2</span></span>
<span id="cb8-6">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>], [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]], columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>])</span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 3</span></span>
<span id="cb8-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]})</span>
<span id="cb8-9">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="dataframe-읽고-저장" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-읽고-저장">DataFrame 읽고 저장</h3>
<div id="90994a4b" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># filepath = '../book/data/data.csv'</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data = pd.read_csv(filepath, na_values='NA', encoding='utf8')</span></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data.to_csv('result.csv', header=True, index=True, encoding='utf8')</span></span></code></pre></div>
</div>
</section>
<section id="dataframe-출력" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-출력">DataFrame 출력</h3>
<div id="ba4ef5fc" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb10-2"></span>
<span id="cb10-3">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb10-4">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>{'data': array([[5.1, 3.5, 1.4, 0.2],
        [4.9, 3. , 1.4, 0.2],
        [4.7, 3.2, 1.3, 0.2],
        [4.6, 3.1, 1.5, 0.2],
        [5. , 3.6, 1.4, 0.2],
        [5.4, 3.9, 1.7, 0.4],
        [4.6, 3.4, 1.4, 0.3],
        [5. , 3.4, 1.5, 0.2],
        [4.4, 2.9, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.1],
        [5.4, 3.7, 1.5, 0.2],
        [4.8, 3.4, 1.6, 0.2],
        [4.8, 3. , 1.4, 0.1],
        [4.3, 3. , 1.1, 0.1],
        [5.8, 4. , 1.2, 0.2],
        [5.7, 4.4, 1.5, 0.4],
        [5.4, 3.9, 1.3, 0.4],
        [5.1, 3.5, 1.4, 0.3],
        [5.7, 3.8, 1.7, 0.3],
        [5.1, 3.8, 1.5, 0.3],
        [5.4, 3.4, 1.7, 0.2],
        [5.1, 3.7, 1.5, 0.4],
        [4.6, 3.6, 1. , 0.2],
        [5.1, 3.3, 1.7, 0.5],
        [4.8, 3.4, 1.9, 0.2],
        [5. , 3. , 1.6, 0.2],
        [5. , 3.4, 1.6, 0.4],
        [5.2, 3.5, 1.5, 0.2],
        [5.2, 3.4, 1.4, 0.2],
        [4.7, 3.2, 1.6, 0.2],
        [4.8, 3.1, 1.6, 0.2],
        [5.4, 3.4, 1.5, 0.4],
        [5.2, 4.1, 1.5, 0.1],
        [5.5, 4.2, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.2],
        [5. , 3.2, 1.2, 0.2],
        [5.5, 3.5, 1.3, 0.2],
        [4.9, 3.6, 1.4, 0.1],
        [4.4, 3. , 1.3, 0.2],
        [5.1, 3.4, 1.5, 0.2],
        [5. , 3.5, 1.3, 0.3],
        [4.5, 2.3, 1.3, 0.3],
        [4.4, 3.2, 1.3, 0.2],
        [5. , 3.5, 1.6, 0.6],
        [5.1, 3.8, 1.9, 0.4],
        [4.8, 3. , 1.4, 0.3],
        [5.1, 3.8, 1.6, 0.2],
        [4.6, 3.2, 1.4, 0.2],
        [5.3, 3.7, 1.5, 0.2],
        [5. , 3.3, 1.4, 0.2],
        [7. , 3.2, 4.7, 1.4],
        [6.4, 3.2, 4.5, 1.5],
        [6.9, 3.1, 4.9, 1.5],
        [5.5, 2.3, 4. , 1.3],
        [6.5, 2.8, 4.6, 1.5],
        [5.7, 2.8, 4.5, 1.3],
        [6.3, 3.3, 4.7, 1.6],
        [4.9, 2.4, 3.3, 1. ],
        [6.6, 2.9, 4.6, 1.3],
        [5.2, 2.7, 3.9, 1.4],
        [5. , 2. , 3.5, 1. ],
        [5.9, 3. , 4.2, 1.5],
        [6. , 2.2, 4. , 1. ],
        [6.1, 2.9, 4.7, 1.4],
        [5.6, 2.9, 3.6, 1.3],
        [6.7, 3.1, 4.4, 1.4],
        [5.6, 3. , 4.5, 1.5],
        [5.8, 2.7, 4.1, 1. ],
        [6.2, 2.2, 4.5, 1.5],
        [5.6, 2.5, 3.9, 1.1],
        [5.9, 3.2, 4.8, 1.8],
        [6.1, 2.8, 4. , 1.3],
        [6.3, 2.5, 4.9, 1.5],
        [6.1, 2.8, 4.7, 1.2],
        [6.4, 2.9, 4.3, 1.3],
        [6.6, 3. , 4.4, 1.4],
        [6.8, 2.8, 4.8, 1.4],
        [6.7, 3. , 5. , 1.7],
        [6. , 2.9, 4.5, 1.5],
        [5.7, 2.6, 3.5, 1. ],
        [5.5, 2.4, 3.8, 1.1],
        [5.5, 2.4, 3.7, 1. ],
        [5.8, 2.7, 3.9, 1.2],
        [6. , 2.7, 5.1, 1.6],
        [5.4, 3. , 4.5, 1.5],
        [6. , 3.4, 4.5, 1.6],
        [6.7, 3.1, 4.7, 1.5],
        [6.3, 2.3, 4.4, 1.3],
        [5.6, 3. , 4.1, 1.3],
        [5.5, 2.5, 4. , 1.3],
        [5.5, 2.6, 4.4, 1.2],
        [6.1, 3. , 4.6, 1.4],
        [5.8, 2.6, 4. , 1.2],
        [5. , 2.3, 3.3, 1. ],
        [5.6, 2.7, 4.2, 1.3],
        [5.7, 3. , 4.2, 1.2],
        [5.7, 2.9, 4.2, 1.3],
        [6.2, 2.9, 4.3, 1.3],
        [5.1, 2.5, 3. , 1.1],
        [5.7, 2.8, 4.1, 1.3],
        [6.3, 3.3, 6. , 2.5],
        [5.8, 2.7, 5.1, 1.9],
        [7.1, 3. , 5.9, 2.1],
        [6.3, 2.9, 5.6, 1.8],
        [6.5, 3. , 5.8, 2.2],
        [7.6, 3. , 6.6, 2.1],
        [4.9, 2.5, 4.5, 1.7],
        [7.3, 2.9, 6.3, 1.8],
        [6.7, 2.5, 5.8, 1.8],
        [7.2, 3.6, 6.1, 2.5],
        [6.5, 3.2, 5.1, 2. ],
        [6.4, 2.7, 5.3, 1.9],
        [6.8, 3. , 5.5, 2.1],
        [5.7, 2.5, 5. , 2. ],
        [5.8, 2.8, 5.1, 2.4],
        [6.4, 3.2, 5.3, 2.3],
        [6.5, 3. , 5.5, 1.8],
        [7.7, 3.8, 6.7, 2.2],
        [7.7, 2.6, 6.9, 2.3],
        [6. , 2.2, 5. , 1.5],
        [6.9, 3.2, 5.7, 2.3],
        [5.6, 2.8, 4.9, 2. ],
        [7.7, 2.8, 6.7, 2. ],
        [6.3, 2.7, 4.9, 1.8],
        [6.7, 3.3, 5.7, 2.1],
        [7.2, 3.2, 6. , 1.8],
        [6.2, 2.8, 4.8, 1.8],
        [6.1, 3. , 4.9, 1.8],
        [6.4, 2.8, 5.6, 2.1],
        [7.2, 3. , 5.8, 1.6],
        [7.4, 2.8, 6.1, 1.9],
        [7.9, 3.8, 6.4, 2. ],
        [6.4, 2.8, 5.6, 2.2],
        [6.3, 2.8, 5.1, 1.5],
        [6.1, 2.6, 5.6, 1.4],
        [7.7, 3. , 6.1, 2.3],
        [6.3, 3.4, 5.6, 2.4],
        [6.4, 3.1, 5.5, 1.8],
        [6. , 3. , 4.8, 1.8],
        [6.9, 3.1, 5.4, 2.1],
        [6.7, 3.1, 5.6, 2.4],
        [6.9, 3.1, 5.1, 2.3],
        [5.8, 2.7, 5.1, 1.9],
        [6.8, 3.2, 5.9, 2.3],
        [6.7, 3.3, 5.7, 2.5],
        [6.7, 3. , 5.2, 2.3],
        [6.3, 2.5, 5. , 1.9],
        [6.5, 3. , 5.2, 2. ],
        [6.2, 3.4, 5.4, 2.3],
        [5.9, 3. , 5.1, 1.8]]),
 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),
 'frame': None,
 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),
 'DESCR': '.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n    - class:\n            - Iris-Setosa\n            - Iris-Versicolour\n            - Iris-Virginica\n\n:Summary Statistics:\n\n============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD   Class Correlation\n============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n============== ==== ==== ======= ===== ====================\n\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n:Creator: R.A. Fisher\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n:Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. dropdown:: References\n\n  - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n    Mathematical Statistics" (John Wiley, NY, 1950).\n  - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n    (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n  - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n    Structure and Classification Rule for Recognition in Partially Exposed\n    Environments".  IEEE Transactions on Pattern Analysis and Machine\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n  - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n    on Information Theory, May 1972, 431-433.\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n    conceptual clustering system finds 3 classes in the data.\n  - Many, many more ...\n',
 'feature_names': ['sepal length (cm)',
  'sepal width (cm)',
  'petal length (cm)',
  'petal width (cm)'],
 'filename': 'iris.csv',
 'data_module': 'sklearn.datasets.data'}</code></pre>
</div>
</div>
<div id="23cdc8fa" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.feature_names)</span>
<span id="cb12-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
<div id="556d971f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">iris.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 4 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   sepal length (cm)  150 non-null    float64
 1   sepal width (cm)   150 non-null    float64
 2   petal length (cm)  150 non-null    float64
 3   petal width (cm)   150 non-null    float64
dtypes: float64(4)
memory usage: 4.8 KB</code></pre>
</div>
</div>
<div id="f79fa00e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">iris.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>5.843333</td>
<td>3.057333</td>
<td>3.758000</td>
<td>1.199333</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>0.828066</td>
<td>0.435866</td>
<td>1.765298</td>
<td>0.762238</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>4.300000</td>
<td>2.000000</td>
<td>1.000000</td>
<td>0.100000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>5.100000</td>
<td>2.800000</td>
<td>1.600000</td>
<td>0.300000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>5.800000</td>
<td>3.000000</td>
<td>4.350000</td>
<td>1.300000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>6.400000</td>
<td>3.300000</td>
<td>5.100000</td>
<td>1.800000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>7.900000</td>
<td>4.400000</td>
<td>6.900000</td>
<td>2.500000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>sepal length와 petal width의 값의 차이가 크다.</p>
<p>전처리 과정에서 변수 정규화 수행의 근거가 된다.</p>
</section>
<section id="index-column-명-변경" class="level3">
<h3 class="anchored" data-anchor-id="index-column-명-변경">index / column 명 변경</h3>
<div id="70b9e7a6" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">df.index</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>RangeIndex(start=0, stop=2, step=1)</code></pre>
</div>
</div>
<div id="5c860c41" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[0, 1]</code></pre>
</div>
</div>
<div id="768b86b5" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">df.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>]</span>
<span id="cb20-2">df.index</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>Index(['A', 'B'], dtype='object')</code></pre>
</div>
</div>
<div id="81935398" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">A</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">B</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="324b711a" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">df.set_index(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, append<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb23-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="f05d4cc9" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df.reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb24-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="43969090" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">iris.columns</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',
       'petal width (cm)'],
      dtype='object')</code></pre>
</div>
</div>
<div id="8b732b2d" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">iris.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal width'</span>]</span>
<span id="cb27-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length</th>
<th data-quarto-table-cell-role="th">sepal width</th>
<th data-quarto-table-cell-role="th">petal length</th>
<th data-quarto-table-cell-role="th">petal width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
<div id="c3d7a877" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">iris.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.columns.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_'</span>)</span>
<span id="cb28-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
</section>
<section id="데이터-타입-변경" class="level3">
<h3 class="anchored" data-anchor-id="데이터-타입-변경">데이터 타입 변경</h3>
<p>사용 가능한 타입</p>
<ul>
<li>int</li>
<li>float</li>
<li>bool</li>
<li>datetime</li>
<li>category</li>
<li>object</li>
</ul>
<div id="c16cedfa" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">iris.dtypes</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>sepal_length    float64
sepal_width     float64
petal_length    float64
petal_width     float64
dtype: object</code></pre>
</div>
</div>
<div id="4d0cf636" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'int'</span>)</span>
<span id="cb31-2">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb31-3">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>]].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'int'</span>)</span>
<span id="cb31-4">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5</td>
<td>3</td>
<td>5</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
</section>
</section>
<section id="row-coumn-선택-추가-삭제" class="level2">
<h2 class="anchored" data-anchor-id="row-coumn-선택-추가-삭제">row / coumn 선택 추가 삭제</h2>
<section id="row-선택" class="level3">
<h3 class="anchored" data-anchor-id="row-선택">row 선택</h3>
<div id="dd375a41" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">iris[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-선택" class="level3">
<h3 class="anchored" data-anchor-id="column-선택">column 선택</h3>
<p>Series 형식으로 출력</p>
<div id="b00b4008" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0      5
1      4
2      4
3      4
4      5
      ..
145    6
146    6
147    6
148    6
149    5
Name: sepal_length, Length: 150, dtype: int64</code></pre>
</div>
</div>
<p>DataFrame 형식으로 출력</p>
<div id="b79b9f35" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>

<p>150 rows × 2 columns</p>
</div>
</div>
</div>
</section>
<section id="column-row-선택" class="level3">
<h3 class="anchored" data-anchor-id="column-row-선택">column, row 선택</h3>
<div id="d72c0520" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">iris.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="001ceb0a" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">iris.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="row-추가" class="level3">
<h3 class="anchored" data-anchor-id="row-추가">row 추가</h3>
<div id="17589b2e" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 방법 1: concat 사용</span></span>
<span id="cb38-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># df = pd.concat([df, pd.DataFrame([{'class': 'eng', 'score': 90}])], ignore_index=True)</span></span>
<span id="cb38-3"></span>
<span id="cb38-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 방법 2: loc 사용 </span></span>
<span id="cb38-5">df.loc[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(df)] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eng'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>}</span>
<span id="cb38-6">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>eng</td>
<td>90</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-추가" class="level3">
<h3 class="anchored" data-anchor-id="column-추가">column 추가</h3>
<div id="582c3bca" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'yo'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb39-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">yo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>80</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>90</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>eng</td>
<td>90</td>
<td>100</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="row-삭제" class="level3">
<h3 class="anchored" data-anchor-id="row-삭제">row 삭제</h3>
<div id="83dd2228" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">df.drop(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb40-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">yo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>80</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>90</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-삭제" class="level3">
<h3 class="anchored" data-anchor-id="column-삭제">column 삭제</h3>
<div id="f9688ca8" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">df.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'yo'</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb41-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="조건-선택" class="level2">
<h2 class="anchored" data-anchor-id="조건-선택">조건 선택</h2>
<div id="b3996427" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">iris[(iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">54</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">58</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">62</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">63</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">68</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">130</td>
<td>7</td>
<td>2</td>
<td>6</td>
<td>1.9</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">132</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>2.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">133</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">134</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.9</td>
</tr>
</tbody>
</table>

<p>29 rows × 4 columns</p>
</div>
</div>
</div>
<div id="a6815611" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">df.loc[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pass'</span></span>
<span id="cb43-2">df.loc[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pass'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fail'</span></span>
<span id="cb43-3">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="31b99779" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb44-2"></span>
<span id="cb44-3">condition_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>), </span>
<span id="cb44-4">                  (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>),</span>
<span id="cb44-5">                  (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)]</span>
<span id="cb44-6">grade_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]</span>
<span id="cb44-7">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'grade'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.select(condition_list, grade_list, default<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'F'</span>)</span>
<span id="cb44-8">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
<th data-quarto-table-cell-role="th">grade</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
<td>A</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
<td>A</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<section id="결측치-탐색" class="level3">
<h3 class="anchored" data-anchor-id="결측치-탐색">결측치 탐색</h3>
<div id="f00dbe80" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">df.isna().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>class    0
score    0
합격       0
grade    0
dtype: int64</code></pre>
</div>
</div>
<div id="1f62b957" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">df.notna().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 행 기준</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0    4
1    4
dtype: int64</code></pre>
</div>
</div>
</section>
<section id="결측치-제거" class="level3">
<h3 class="anchored" data-anchor-id="결측치-제거">결측치 제거</h3>
<div id="b5c8d0b7" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># dropna(axis=0, how='any' or 'all', thresh=None, subset=None, inplace=False)</span></span>
<span id="cb49-2">df.dropna()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
<th data-quarto-table-cell-role="th">grade</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
<td>A</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
<td>A</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="결측치-대체" class="level3">
<h3 class="anchored" data-anchor-id="결측치-대체">결측치 대체</h3>
<div id="c6ae0eb1" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># fillna(value=None, method=None ('pad', 'ffill', 'backfill', 'bfill'), axis=None, inplace=False, limit=None)</span></span></code></pre></div>
</div>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/01.html</guid>
  <pubDate>Sat, 20 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>전처리</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/core/01.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="결측치-처리" class="level2">
<h2 class="anchored" data-anchor-id="결측치-처리">결측치 처리</h2>
<ul>
<li>대푯값으로 대체</li>
<li>단순확률대치법</li>
<li>다른 모델로 예측</li>
<li>보간법: 시계열에서 주로 사용.</li>
</ul>
</section>
<section id="이상치-처리" class="level2">
<h2 class="anchored" data-anchor-id="이상치-처리">이상치 처리</h2>
<ul>
<li>ESD</li>
<li>IQR</li>
<li>DBSCAN</li>
</ul>
</section>
<section id="클래스-불균형" class="level2">
<h2 class="anchored" data-anchor-id="클래스-불균형">클래스 불균형</h2>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/core/01.html</guid>
  <pubDate>Fri, 15 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/core/00.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="질적-변수" class="level2">
<h2 class="anchored" data-anchor-id="질적-변수">질적 변수</h2>
<section id="상관분석" class="level3">
<h3 class="anchored" data-anchor-id="상관분석">상관분석</h3>
<div id="f164d83b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> spearmanr, kendalltau</span>
<span id="cb1-2"></span>
<span id="cb1-3">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spearmanr(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-4"></span>
<span id="cb1-5">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kendalltau(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-6"></span>
<span id="cb1-7">df.corr(method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kendall'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># kendal, spearman</span></span></code></pre></div>
</div>
<div id="6b9b11ac" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.contingeny <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> association</span>
<span id="cb2-2"></span>
<span id="cb2-3">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tschuprow"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># phi 계수</span></span>
<span id="cb2-4">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cramer'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 크래머 v</span></span>
<span id="cb2-5">v2</span></code></pre></div>
</div>
<ul>
<li>상관계수: 공분산을 각 변수의 표준편차로 나눈 것</li>
<li>스피어만 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.</li>
<li>켄달의 타우: 서열척도 vs 서열척도.
<ul>
<li>둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.</li>
<li>샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.</li>
<li>두 변수의 크기는 같아야함.</li>
</ul></li>
<li>phi 계수: 명목척도 vs 명목척도
<ul>
<li>두 변인 모두 level이 2개일 때 사용</li>
<li>두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산</li>
</ul></li>
<li>크래머 v: 명목척도 vs 명목척도.
<ul>
<li>적어도 하나의 변수가 3개 이상의 level을 가지면 사용</li>
<li>범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.</li>
</ul></li>
<li>Point-biserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 2개일 때</li>
</ul></li>
<li>Polyserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 3개 이상일 때</li>
</ul></li>
<li>명목과 순서의 경우
<ul>
<li>level이 2개: Mann-Whitney U검정</li>
<li>3개 이상: Kruskal-Wallis H test</li>
</ul></li>
</ul>
</section>
<section id="시각화" class="level3">
<h3 class="anchored" data-anchor-id="시각화">시각화</h3>
<div id="127dbbb5" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-2"></span>
<span id="cb3-3">cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'your'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>, ...]</span>
<span id="cb3-4">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df[cols].value_counts()) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 도수분포표</span></span>
<span id="cb3-5">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'proportion'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cols].value_counts(normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상대도수분포표</span></span></code></pre></div>
</div>
<div id="0abe98f7" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].plot.bar(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), subplots<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb4-2">plt.tight_layout()</span>
<span id="cb4-3">plt.show()</span></code></pre></div>
</div>
<div id="2de0d5b0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.pie(freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].values, </span>
<span id="cb5-2">        labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>freq.index, </span>
<span id="cb5-3">        autopct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%1.1f%%</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb5-4">        colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sns.color_palette(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pastel'</span>, n_colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(freq)))</span>
<span id="cb5-5">plt.show()</span></code></pre></div>
</div>
</section>
</section>
<section id="양적-변수" class="level2">
<h2 class="anchored" data-anchor-id="양적-변수">양적 변수</h2>
<section id="기술통계" class="level3">
<h3 class="anchored" data-anchor-id="기술통계">기술통계</h3>
<div id="f53785d4" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.mstats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gmean, hmean, tmean</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb6-3"></span>
<span id="cb6-4">np.mean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 산술평균</span></span>
<span id="cb6-5">gmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 기하평균</span></span>
<span id="cb6-6">hmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 조화평균</span></span>
<span id="cb6-7">tmean(example, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 절사평균</span></span>
<span id="cb6-8">np.sqrt(np.mean(np.array(example) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 평방평균</span></span></code></pre></div>
</div>
<ul>
<li>기하평균: 비율의 평균에 주로 사용됨. 한 값이라도 0이면 전체가 0이 됨</li>
<li>조화평균: 속도, 밀도 등의 평균에 주로 사용됨.</li>
<li>절사평균: 극단값의 영향을 줄이기 위해 상위, 하위 몇 %를 제외한 평균</li>
<li>평방평균: 신호, 파동 등에서 자주 사용</li>
</ul>
<div id="04ca6557" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.median()</span>
<span id="cb7-2">df.mode()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb7-3">df.quantile(q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span></code></pre></div>
</div>
<ul>
<li>상관계수: 피어슨</li>
</ul>
</section>
<section id="시각화-1" class="level3">
<h3 class="anchored" data-anchor-id="시각화-1">시각화</h3>
<ul>
<li>도수분포표</li>
<li>상대도수분포표</li>
<li>줄기잎그림</li>
<li>히스토그램</li>
<li>상자그림</li>
<li>산점도</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/core/00.html</guid>
  <pubDate>Mon, 04 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 신용 카드 사기 검출</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/04.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="under-over-sampling" class="level2">
<h2 class="anchored" data-anchor-id="under-over-sampling">under, over sampling</h2>
<ul>
<li>under sampling: 많은 비중을 차지하는 레이블을 작은 비중의 레이블에 맞추는것</li>
<li>over sampling: 반대
<ul>
<li>smote: k 최근접 이웃 진행 후, 이웃 간 간격을 맞추는 record를 새로 생성하는 방식</li>
</ul></li>
</ul>
<div id="a250d69a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/04.html</guid>
  <pubDate>Fri, 01 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 감성 분석</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/09.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/09.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 20 뉴스그룹 분류</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/08.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<div id="73e704c6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fetch_20newsgroups</span>
<span id="cb1-2"></span>
<span id="cb1-3">train_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-4">X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.data</span>
<span id="cb1-5">y_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.target</span>
<span id="cb1-6"></span>
<span id="cb1-7">test_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-8">X_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.data</span>
<span id="cb1-9">y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.target</span></code></pre></div>
</div>
</section>
<section id="학습" class="level2">
<h2 class="anchored" data-anchor-id="학습">학습</h2>
<section id="count-vector" class="level3">
<h3 class="anchored" data-anchor-id="count-vector">Count Vector</h3>
<div id="7ee07adb" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CountVectorizer</span>
<span id="cb2-2"></span>
<span id="cb2-3">cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CountVectorizer()</span>
<span id="cb2-4">X_train_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.fit_transform(X_train)</span>
<span id="cb2-5">X_test_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.transform(X_test)</span></code></pre></div>
</div>
<div id="fadbe7a8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb3-4"></span>
<span id="cb3-5">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb3-8">lr_clf.fit(X_train_cnt_vect, y_train)</span>
<span id="cb3-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_cnt_vect)</span>
<span id="cb3-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.731 </code></pre>
</div>
</div>
</section>
<section id="tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf">TF-IDF</h3>
<div id="a8facfda" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TfidfVectorizer</span>
<span id="cb5-2"></span>
<span id="cb5-3">tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TfidfVectorizer()</span>
<span id="cb5-4">X_train_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.fit_transform(X_train)</span>
<span id="cb5-5">X_test_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.transform(X_test)</span>
<span id="cb5-6"></span>
<span id="cb5-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb5-8">lr_clf.fit(X_train_tfidf_vect, y_train)</span>
<span id="cb5-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_tfidf_vect)</span>
<span id="cb5-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.778 </code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/08.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/07.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">overview</h2>
<section id="nlp-vs-텍스트-분석" class="level3">
<h3 class="anchored" data-anchor-id="nlp-vs-텍스트-분석">NLP vs 텍스트 분석</h3>
<ul>
<li>NLP(자연어 처리)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술을 의미</li>
<li>텍스트 분석은 주로 비정형 텍스트 데이터를 머신러닝, 통계 등의 방법으로 예측 분석이나 유용한 정보를 추출하는 데 중점을 둔다.</li>
</ul>
</section>
<section id="종류" class="level3">
<h3 class="anchored" data-anchor-id="종류">종류</h3>
<ul>
<li>텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측 (연예 / 정치 / 스포츠 같은 카테고리 분류 혹은 스팸 메일 검출). 지도 학습</li>
<li>감성 분석: 텍스트에서 주관적 요소를 분석하는 기법. 지도 혹은 비지도.</li>
<li>텍스트 요약: 텍스트 내에서 주제나 중심 사상을 추출</li>
<li>텍스트 군집화: 비슷한 유형의 문서를 군집화 하는 것. 비지도 학습</li>
</ul>
</section>
</section>
<section id="프로세스" class="level2">
<h2 class="anchored" data-anchor-id="프로세스">프로세스</h2>
<ol type="1">
<li>텍스트 전처리: 대 / 소문자 변경, 특수 문자 제거, 토큰화, 불용어 제거, 어근 추출 등의 정규화 작업</li>
<li>피처 벡터화 / 추출: 텍스트에서 피처를 추출하고 벡터 값을 할당. BOW와 Word2Vec이 대표적</li>
<li>ML 모델 수립 및 학습 / 예측 / 평가</li>
</ol>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ul>
<li>클렌징: 문자, 기호 등을 사전에 제거</li>
<li>토큰화
<ul>
<li>문장 토큰화: 마침표, 개행문자 등을 기준으로 문장을 분리. 각 문장이 가지는 의미가 중요한 경우 사용.</li>
<li>단어 토큰화: 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리.
<ul>
<li>n-gram: 단어의 연속된 n개를 묶어서 하나의 단위로 처리하는 방법. 문장이 가지는 의미를 조금이라도 보존할 수 있다.</li>
</ul></li>
</ul></li>
</ul>
<div id="269974e5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sent_tokenize</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb1-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문장을 분리하는 마침표, 개행문자 등의 데이터 셋 다운로드</span></span>
<span id="cb1-4">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt_tab'</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">text_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes."</span></span>
<span id="cb1-7">sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text_sample)</span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sentences)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The Matrix is everywhere its all around us, here even in this room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package punkt to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!</code></pre>
</div>
</div>
<div id="76165d19" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> word_tokenize</span>
<span id="cb4-2"></span>
<span id="cb4-3">sentence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room."</span></span>
<span id="cb4-4">words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word_tokenize(sentence)</span>
<span id="cb4-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(words)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']</code></pre>
</div>
</div>
<div id="ccff5c25" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tokenize_text(text):</span>
<span id="cb6-2">    sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text)</span>
<span id="cb6-3">    words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [word_tokenize(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb6-4"></span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> words</span>
<span id="cb6-6"></span>
<span id="cb6-7">word_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize_text(text_sample)</span>
<span id="cb6-8">word_tokens</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[['The',
  'Matrix',
  'is',
  'everywhere',
  'its',
  'all',
  'around',
  'us',
  ',',
  'here',
  'even',
  'in',
  'this',
  'room',
  '.'],
 ['You',
  'can',
  'see',
  'it',
  'when',
  'you',
  'look',
  'out',
  'your',
  'window',
  'or',
  'when',
  'you',
  'turn',
  'on',
  'your',
  'television',
  '.'],
 ['You',
  'can',
  'feel',
  'it',
  'when',
  'you',
  'go',
  'to',
  'work',
  ',',
  'when',
  'you',
  'go',
  'to',
  'church',
  ',',
  'when',
  'you',
  'pay',
  'your',
  'taxes',
  '.']]</code></pre>
</div>
</div>
<ul>
<li>stopword 제거: 분석에 필요하지 않은 단어를 제거하는 작업. 예) 관사, 전치사, 접속사 등</li>
</ul>
<div id="cada5991" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.corpus <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stopwords</span>
<span id="cb8-2">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'stopwords'</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># stopwords 데이터 셋 다운로드</span></span>
<span id="cb8-3"></span>
<span id="cb8-4">stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>['a',
 'about',
 'above',
 'after',
 'again',
 'against',
 'ain',
 'all',
 'am',
 'an',
 'and',
 'any',
 'are',
 'aren',
 "aren't",
 'as',
 'at',
 'be',
 'because',
 'been']</code></pre>
</div>
</div>
<div id="fb7e7b55" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)</span>
<span id="cb11-2">all_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> word_tokens:</span>
<span id="cb11-4">    filtered_words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentence:</span>
<span id="cb11-6">        word <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word.lower()</span>
<span id="cb11-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sw:</span>
<span id="cb11-8">            filtered_words.append(word)</span>
<span id="cb11-9">    all_tokens.append(filtered_words)</span>
<span id="cb11-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(all_tokens)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'look', 'window', 'turn', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', ',', 'pay', 'taxes', '.']]</code></pre>
</div>
</div>
<ul>
<li>stemming, lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것
<ul>
<li>stemming이 더 단순하고 빠르지만 lemmatization 이 더 저오학함</li>
</ul></li>
</ul>
<div id="2736b8f8" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LancasterStemmer</span>
<span id="cb13-2"></span>
<span id="cb13-3">stemmer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LancasterStemmer()</span>
<span id="cb13-4"></span>
<span id="cb13-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'working'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'works'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worked'</span>))</span>
<span id="cb13-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>))</span>
<span id="cb13-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>))</span>
<span id="cb13-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>work work work
amus amus amus
happy happiest
fant fanciest</code></pre>
</div>
</div>
<div id="e3e177a6" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> WordNetLemmatizer</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb15-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wordnet'</span>)</span>
<span id="cb15-4"></span>
<span id="cb15-5">lemma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> WordNetLemmatizer()</span>
<span id="cb15-6"></span>
<span id="cb15-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>))</span>
<span id="cb15-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span>
<span id="cb15-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package wordnet to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>amuse amuse amuse
happy happy
fancy fancy</code></pre>
</div>
</div>
</section>
<section id="bow" class="level2">
<h2 class="anchored" data-anchor-id="bow">BOW</h2>
<ul>
<li>문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 빈도 값을 부여해 피처 값을 추출하는 모델</li>
<li>count 기반 벡터화: 빈도가 높을수록 중요한 단어로 인식</li>
<li>TF-IDF(term frequency - inverse document frequency) 기반 벡터화: 빈도가 높을수록 좋으나, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 패털티를 줌
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i%20*%20log%5Cfrac%7BN%7D%7BDF_i%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i">: 개별 문서에서의 단어 i 빈도</li>
<li><img src="https://latex.codecogs.com/png.latex?DF_i">: 단어 i를 가지고 있는 문서 개수</li>
<li>N: 전체 문서 개수</li>
</ul></li>
</ul></li>
<li>희소행렬 문제: 불필요한 0 값이 많아지는 문제
<ul>
<li>COO</li>
<li>CSR</li>
<li>혹은 희소행렬을 잘 처리하는 알고리즘: 로지스틱 회귀, 선형 svm, 나이브 베이즈 등</li>
</ul></li>
</ul>
<section id="coo" class="level3">
<h3 class="anchored" data-anchor-id="coo">COO</h3>
<ul>
<li>0이 아닌 데이터만 별도의 array에 저장.</li>
</ul>
<div id="49f91716" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb18-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb18-3"></span>
<span id="cb18-4">dense <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb18-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb18-6">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-7">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-8">sparse_coo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.coo_matrix((data, (row_pos, col_pos)))</span>
<span id="cb18-9">sparse_coo</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;COOrdinate sparse matrix of dtype 'int64'
    with 3 stored elements and shape (2, 3)&gt;</code></pre>
</div>
</div>
<div id="f0ec05eb" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">sparse_coo.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([[3, 0, 1],
       [0, 2, 0]])</code></pre>
</div>
</div>
</section>
<section id="csr" class="level3">
<h3 class="anchored" data-anchor-id="csr">CSR</h3>
<ul>
<li>COO + 시작위치만 기록하는 방법</li>
</ul>
<div id="9c5709a1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb22-2"></span>
<span id="cb22-3">dense2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-4">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-5">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-6">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-7">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb22-8">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb22-9">data2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb22-10">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>])</span>
<span id="cb22-11">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb22-12">row_pos_ind <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>])</span>
<span id="cb22-13"></span>
<span id="cb22-14">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix((data2, col_pos, row_pos_ind))</span>
<span id="cb22-15">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>
<div id="32bd3d1f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix(dense2)</span>
<span id="cb24-2">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/07.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>차원 축소</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="pca" class="level2">
<h2 class="anchored" data-anchor-id="pca">PCA</h2>
<div id="8eff2c0c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"></span>
<span id="cb1-5">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb1-6">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_width'</span>]</span>
<span id="cb1-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>columns)</span>
<span id="cb1-8">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb1-9">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb1-12">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>]</span>
<span id="cb1-13">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]</span>
<span id="cb1-14">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb1-15">plt.legend()</span>
<span id="cb1-16">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length'</span>)</span>
<span id="cb1-17">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width'</span>)</span>
<span id="cb1-18">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06_files/figure-html/cell-2-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>PCA는 scaling의 영향을 받음.</li>
</ul>
<div id="8718dc6d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.decomposition <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PCA</span>
<span id="cb2-3"></span>
<span id="cb2-4">scaled_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb2-5"></span>
<span id="cb2-6">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pca.fit_transform(scaled_df)</span></code></pre></div>
</div>
<div id="1ee52674" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">pca_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb3-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pca_columns)</span>
<span id="cb3-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span></code></pre></div>
</div>
<div id="aced6683" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb4-4">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>]</span>
<span id="cb4-5">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb4-6">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb4-7">plt.legend()</span>
<span id="cb4-8">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>)</span>
<span id="cb4-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>)</span>
<span id="cb4-10">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06_files/figure-html/cell-5-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="신용카드-고객-데이터" class="level2">
<h2 class="anchored" data-anchor-id="신용카드-고객-데이터">신용카드 고객 데이터</h2>
<div id="9b9fcb7f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_excel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/creadit_card.xls'</span>, header<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sheet_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data'</span>).iloc[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb5-2">df.rename(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_0'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default payment next month'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>}, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>]</span>
<span id="cb5-4">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="dc4c0c12" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"></span>
<span id="cb6-3">corr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> features.corr()</span>
<span id="cb6-4">sns.heatmap(corr, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, fmt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.1g'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06_files/figure-html/cell-7-output-1.png" width="610" height="482" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>BILL_AMT1~6, PAY_1~6의 상관도가 높다.</li>
</ul>
<div id="6f7f199e" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">cols_bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BILL_AMT'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)]</span>
<span id="cb7-2">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb7-3">df_cols_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(features[cols_bill])</span>
<span id="cb7-4">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-5">pca.fit(df_cols_scaled)</span>
<span id="cb7-6">pca.explained_variance_ratio_</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([0.90555253, 0.0509867 ])</code></pre>
</div>
</div>
<ul>
<li>PCA 할 때 column 전부 다 안 넣어도 되나?</li>
<li>다 넣어야 하는 듯</li>
</ul>
</section>
<section id="lda" class="level2">
<h2 class="anchored" data-anchor-id="lda">LDA</h2>
<ul>
<li>클래스 분리를 최대화하는 축을 찾음</li>
<li>PCA와 다르게 지도 학습임.</li>
</ul>
<div id="d127773a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.discriminant_analysis <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearDiscriminantAnalysis</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb9-3"></span>
<span id="cb9-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb9-5">iris_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(iris.data)</span></code></pre></div>
</div>
<div id="6f9cfd97" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearDiscriminantAnalysis(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-2">iris_lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lda.fit_transform(iris_scaled, iris.target)</span></code></pre></div>
</div>
<div id="f7952d84" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">lda_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris_lda, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lda_columns)</span>
<span id="cb11-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb11-4"></span>
<span id="cb11-5">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb11-8">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>]</span>
<span id="cb11-9">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-10">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb11-11">plt.legend()</span>
<span id="cb11-12">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>)</span>
<span id="cb11-13">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>)</span>
<span id="cb11-14">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06_files/figure-html/cell-11-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/06.html</guid>
  <pubDate>Mon, 28 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/05.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="경사하강법" class="level2">
<h2 class="anchored" data-anchor-id="경사하강법">경사하강법</h2>
<div id="029d0ea5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"></span>
<span id="cb1-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-5">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-6">plt.scatter(X, y)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/05_files/figure-html/cell-2-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="354a9f59" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_cost(y, y_pred):</span>
<span id="cb2-2">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-3">    cost <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.square(y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> N</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> cost</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>):</span>
<span id="cb2-7">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-8">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w1)</span>
<span id="cb2-9">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w0)</span>
<span id="cb2-10">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(X, w1.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb2-11">    diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred</span>
<span id="cb2-12"></span>
<span id="cb2-13">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.dot(X.T, diff)</span>
<span id="cb2-14">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(diff)</span>
<span id="cb2-15"></span>
<span id="cb2-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1_update, w0_update</span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>):</span>
<span id="cb2-19">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-20">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb2-23">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb2-24">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb2-25">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb2-26"></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="83a91373" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-4">plt.scatter(X, y)</span>
<span id="cb3-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 5.452 w1: 4.276, total cost: 1.360</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/05_files/figure-html/cell-4-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>일반 경사하강법은 시간이 오래걸려서 잘 안씀</li>
</ul>
</section>
<section id="미니-배치-확률적-경사-하강법" class="level2">
<h2 class="anchored" data-anchor-id="미니-배치-확률적-경사-하강법">미니 배치 확률적 경사 하강법</h2>
<div id="9606898c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> stochastic_gradient_descent_steps(X, y, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb5-2">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-3">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ind <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb5-6">        stochastic_random_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-7">        sample_X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-8">        sample_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-9"></span>
<span id="cb5-10">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, sample_X, sample_y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb5-11">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb5-12">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb5-13"></span>
<span id="cb5-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="0c6e1cdf" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stochastic_gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb6-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb6-4">plt.scatter(X, y)</span>
<span id="cb6-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 5.464 w1: 4.263, total cost: 1.360</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/05_files/figure-html/cell-6-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀">선형 회귀</h2>
<div id="de85051e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb8-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb8-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_boston</span>
<span id="cb8-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb8-6"></span>
<span id="cb8-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb8-8"></span>
<span id="cb8-9">boston <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_boston()</span>
<span id="cb8-10"></span>
<span id="cb8-11">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(boston.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>boston.feature_names)</span>
<span id="cb8-12">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boston.target</span>
<span id="cb8-13">df.head()</span></code></pre></div>
</div>
<div id="93a28620" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">lm_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RM'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ZN'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'INDUS'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NOX'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AGE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PTRAIO'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LSTAT'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RAD'</span>]</span>
<span id="cb9-2"></span>
<span id="cb9-3">fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>), ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(lm_features) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, feature <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(lm_features):</span>
<span id="cb9-6">    row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-7">    col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-8"></span>
<span id="cb9-9">    sns.regplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feature, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[row][col])</span></code></pre></div>
</div>
<p>boston 데이터가 윤리적 문제로 사용 불가능하다고 한다.</p>
<div id="5bb65265" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb10-3"></span>
<span id="cb10-4">y_target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>]</span>
<span id="cb10-5">X_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb10-6">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression()</span>
<span id="cb10-7"></span>
<span id="cb10-8">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(lr, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb10-9">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb10-10">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
<p>cross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함</p>
</section>
<section id="다항-회귀" class="level2">
<h2 class="anchored" data-anchor-id="다항-회귀">다항 회귀</h2>
<div id="57ac78e0" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PolynomialFeatures</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> polynominal_func(X):</span>
<span id="cb11-6">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y</span>
<span id="cb11-8"></span>
<span id="cb11-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'poly'</span>, PolynomialFeatures(degree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)),</span>
<span id="cb11-10">                  (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>, LinearRegression())])</span>
<span id="cb11-11">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb11-12">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> polynominal_func(X)</span>
<span id="cb11-13"></span>
<span id="cb11-14">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(X, y)</span>
<span id="cb11-15"></span>
<span id="cb11-16">np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(model.named_steps[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>].coef_, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])</code></pre>
</div>
</div>
</section>
<section id="규제" class="level2">
<h2 class="anchored" data-anchor-id="규제">규제</h2>
<ul>
<li><p>L2 규제(Ridge): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C%5E2)"></p></li>
<li><p>L1 규제(Lasso): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C_1)"></p></li>
<li><p>λ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐</p></li>
<li><p>L1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음</p></li>
</ul>
<section id="릿지" class="level3">
<h3 class="anchored" data-anchor-id="릿지">릿지</h3>
<div id="4ace7edd" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge</span>
<span id="cb13-2"></span>
<span id="cb13-3">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb13-4">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(ridge, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb13-5">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb13-6">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
</section>
<section id="라쏘-엘라스틱넷" class="level3">
<h3 class="anchored" data-anchor-id="라쏘-엘라스틱넷">라쏘 엘라스틱넷</h3>
<div id="e6b06942" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb14-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge, Lasso, ElasticNet</span>
<span id="cb14-3"></span>
<span id="cb14-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_linear_reg_eval(model_name, params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, X_data_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, y_target_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-5">    coeff_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> params:</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ridge'</span>:</span>
<span id="cb14-8">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lasso'</span>:</span>
<span id="cb14-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Lasso(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ElasticNet'</span>:</span>
<span id="cb14-12">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ElasticNet(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param, l1_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-13">        neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(model, X_data_n, y_target_n, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb14-14">        rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb14-15">        avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span>
<span id="cb14-16">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>avg_rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb14-17"></span>
<span id="cb14-18">        model.fit(X_data_n, y_target_n)</span>
<span id="cb14-19">        coeff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.coef_, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_data_n.columns)</span>
<span id="cb14-20">        colname <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alpha:'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(param)</span>
<span id="cb14-21">        coeff_df[colname] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> coeff</span>
<span id="cb14-22"></span>
<span id="cb14-23">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> coeff_df</span></code></pre></div>
</div>
</section>
</section>
<section id="선형-회귀-모델을-위한-데이터-변환" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀-모델을-위한-데이터-변환">선형 회귀 모델을 위한 데이터 변환</h2>
<ul>
<li>로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.</li>
</ul>
<div id="fcde1ce5" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">np.log1p(data)</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/05.html</guid>
  <pubDate>Sun, 27 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 산탄데르 고객 만족 예측</title>
  <link>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/03.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<div id="7fedd07a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb1-5"></span>
<span id="cb1-6">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.family'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Noto Sans KR'</span></span>
<span id="cb1-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/train.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb1-10">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 76020 entries, 0 to 76019
Columns: 371 entries, ID to TARGET
dtypes: float64(111), int64(260)
memory usage: 215.2 MB</code></pre>
</div>
</div>
<div id="ce66c231" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ID</th>
<th data-quarto-table-cell-role="th">var3</th>
<th data-quarto-table-cell-role="th">var15</th>
<th data-quarto-table-cell-role="th">imp_ent_var16_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult3</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult3</th>
<th data-quarto-table-cell-role="th">var38</th>
<th data-quarto-table-cell-role="th">TARGET</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>...</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>7.602000e+04</td>
<td>76020.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>75964.050723</td>
<td>-1523.199277</td>
<td>33.212865</td>
<td>86.208265</td>
<td>72.363067</td>
<td>119.529632</td>
<td>3.559130</td>
<td>6.472698</td>
<td>0.412946</td>
<td>0.567352</td>
<td>...</td>
<td>7.935824</td>
<td>1.365146</td>
<td>12.215580</td>
<td>8.784074</td>
<td>31.505324</td>
<td>1.858575</td>
<td>76.026165</td>
<td>56.614351</td>
<td>1.172358e+05</td>
<td>0.039569</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>43781.947379</td>
<td>39033.462364</td>
<td>12.956486</td>
<td>1614.757313</td>
<td>339.315831</td>
<td>546.266294</td>
<td>93.155749</td>
<td>153.737066</td>
<td>30.604864</td>
<td>36.513513</td>
<td>...</td>
<td>455.887218</td>
<td>113.959637</td>
<td>783.207399</td>
<td>538.439211</td>
<td>2013.125393</td>
<td>147.786584</td>
<td>4040.337842</td>
<td>2852.579397</td>
<td>1.826646e+05</td>
<td>0.194945</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000000</td>
<td>-999999.000000</td>
<td>5.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.163750e+03</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>38104.750000</td>
<td>2.000000</td>
<td>23.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.787061e+04</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>76043.000000</td>
<td>2.000000</td>
<td>28.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.064092e+05</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>113748.750000</td>
<td>2.000000</td>
<td>40.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.187563e+05</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>151838.000000</td>
<td>238.000000</td>
<td>105.000000</td>
<td>210000.000000</td>
<td>12888.030000</td>
<td>21024.810000</td>
<td>8237.820000</td>
<td>11073.570000</td>
<td>6600.000000</td>
<td>6600.000000</td>
<td>...</td>
<td>50003.880000</td>
<td>20385.720000</td>
<td>138831.630000</td>
<td>91778.730000</td>
<td>438329.220000</td>
<td>24650.010000</td>
<td>681462.900000</td>
<td>397884.300000</td>
<td>2.203474e+07</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

<p>8 rows × 371 columns</p>
</div>
</div>
</div>
<div id="dd1f03da" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-2">df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-3"></span>
<span id="cb4-4">X_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-5">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<div id="618c0e4e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/test.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb5-2">test_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">test_df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<div id="637bf5d6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_features, labels, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span></code></pre></div>
</div>
<ul>
<li>train, test의 label의 비율이 동일한게 좋은걸까</li>
</ul>
</section>
<section id="xgboost" class="level2">
<h2 class="anchored" data-anchor-id="xgboost">XGBoost</h2>
<div id="6f409853" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">X_tr, X_val, y_tr, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_train, y_train, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
<div id="1ba15f60" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb8-3"></span>
<span id="cb8-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb8-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>, </span>
<span id="cb8-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, </span>
<span id="cb8-7">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb8-8">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb8-9">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb8-10">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb8-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
<section id="베이지안-최적화" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화">베이지안 최적화</h3>
<div id="1b31b6f9" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb9-5">    xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb9-6">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb9-7">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb9-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb9-9">                            min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb9-10">                            colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>],</span>
<span id="cb9-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb9-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb9-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb9-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb9-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb9-17"></span>
<span id="cb9-18">        xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb9-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb9-20">        roc_auc_list.append(score)</span>
<span id="cb9-21"></span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="f38f3cc3" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb10-2"></span>
<span id="cb10-3">xgb_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span>),</span>
<span id="cb10-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb10-8">}</span>
<span id="cb10-9"></span>
<span id="cb10-10">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb10-11">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb10-12">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>xgb_search_space,</span>
<span id="cb10-13">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb10-14">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb10-15">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb10-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
</div>
</section>
<section id="재-학습" class="level3">
<h3 class="anchored" data-anchor-id="재-학습">재 학습</h3>
<div id="dfcb6ac0" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb11-3"></span>
<span id="cb11-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb11-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb11-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-7">                    max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb11-8">                    min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb11-9">                    colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-10">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb11-11">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb11-12">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb11-13">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb11-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
</section>
<section id="plot-importance" class="level3">
<h3 class="anchored" data-anchor-id="plot-importance">plot importance</h3>
<div id="aaf3503d" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_importance</span>
<span id="cb12-2"></span>
<span id="cb12-3">plot_importance(xgb_clf, max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="lightgbm" class="level2">
<h2 class="anchored" data-anchor-id="lightgbm">LightGBM</h2>
<div id="f767eb16" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMClassifier</span>
<span id="cb13-3"></span>
<span id="cb13-4">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb13-5"></span>
<span id="cb13-6">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb13-7">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb13-8"></span>
<span id="cb13-9">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb13-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1680, number of negative: 40891
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007185 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 13313
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 246
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039463 -&gt; initscore=-3.192116
[LightGBM] [Info] Start training from score -3.192116
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[38]    training's binary_logloss: 0.115227 valid_1's binary_logloss: 0.136678
[LightGBM] [Warning] Unknown parameter: eval_metric
0.835</code></pre>
</div>
</div>
<section id="베이지안-최적화-1" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화-1">베이지안 최적화</h3>
<div id="2b2c2571" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb15-4">    lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb15-5">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb15-6">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb15-7">                            num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb15-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb15-9">                            min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb15-10">                            subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>],</span>
<span id="cb15-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb15-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb15-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb15-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb15-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb15-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb15-17"></span>
<span id="cb15-18">        lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb15-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb15-20">        roc_auc_list.append(score)</span>
<span id="cb15-21"></span>
<span id="cb15-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="8063226d" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb16-2"></span>
<span id="cb16-3">lgbm_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">160</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-8">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb16-9">}</span>
<span id="cb16-10"></span>
<span id="cb16-11">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb16-12">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb16-13">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lgbm_search_space,</span>
<span id="cb16-14">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb16-15">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb16-16">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb16-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009724 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12869
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.184987
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.112255 valid_1's binary_logloss: 0.135706
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008359 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12947
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.196685
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.110659 valid_1's binary_logloss: 0.138201
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12908
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.181760
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[61]    training's binary_logloss: 0.115291 valid_1's binary_logloss: 0.135127
  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006936 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12812
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.184987
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.116064 valid_1's binary_logloss: 0.136172
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008904 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12943
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.196685
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[16]    training's binary_logloss: 0.120339 valid_1's binary_logloss: 0.138716
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012688 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12908
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.181760
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[16]    training's binary_logloss: 0.122183 valid_1's binary_logloss: 0.135018
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:51,  3.51s/trial, best loss: -0.8321249357878048]  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011380 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12804
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.184987
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:05&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[25]    training's binary_logloss: 0.116716 valid_1's binary_logloss: 0.136274
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008522 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12847
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.196685
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:06&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[25]    training's binary_logloss: 0.115805 valid_1's binary_logloss: 0.137993
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010308 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12817
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.181760
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:07&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[25]    training's binary_logloss: 0.116979 valid_1's binary_logloss: 0.135074
  4%|▍         | 2/50 [00:08&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:08&lt;02:07,  2.65s/trial, best loss: -0.8321249357878048]  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009715 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12944
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.184987
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:08&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[18]    training's binary_logloss: 0.11434  valid_1's binary_logloss: 0.136843
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007691 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12993
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.196685
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[18]    training's binary_logloss: 0.113403 valid_1's binary_logloss: 0.13851
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:09&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008144 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12917
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.181760
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[13]    training's binary_logloss: 0.11959  valid_1's binary_logloss: 0.135042
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:12,  2.83s/trial, best loss: -0.8321249357878048]  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007364 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12804
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.184987
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:10&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[50]    training's binary_logloss: 0.114951 valid_1's binary_logloss: 0.135266
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009043 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12838
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.196685
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:11&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[49]    training's binary_logloss: 0.114376 valid_1's binary_logloss: 0.138019
  8%|▊         | 4/50 [00:12&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:12&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:12&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:12&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006748 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Total Bins 12817
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Info] Start training from score -3.181760
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 Early stopping, best iteration is:
[51]    training's binary_logloss: 0.114762 valid_1's binary_logloss: 0.135074
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;01:53,  2.46s/trial, best loss: -0.8321249357878048] 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007293 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Total Bins 12812
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:13&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Early stopping, best iteration is:
[28]    training's binary_logloss: 0.11413  valid_1's binary_logloss: 0.13591
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008305 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Total Bins 12943
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:14&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Early stopping, best iteration is:
[17]    training's binary_logloss: 0.120592 valid_1's binary_logloss: 0.138248
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009350 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Total Bins 12879
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 Early stopping, best iteration is:
[26]    training's binary_logloss: 0.116027 valid_1's binary_logloss: 0.134638
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:02,  2.73s/trial, best loss: -0.8321345573094822] 12%|█▏        | 6/50 [00:15&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:15&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:15&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12900
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Early stopping, best iteration is:
[33]    training's binary_logloss: 0.113658 valid_1's binary_logloss: 0.13587
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010190 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12993
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:16&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Early stopping, best iteration is:
[31]    training's binary_logloss: 0.113983 valid_1's binary_logloss: 0.138398
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009348 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12917
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:17&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 Early stopping, best iteration is:
[28]    training's binary_logloss: 0.116757 valid_1's binary_logloss: 0.135006
 12%|█▏        | 6/50 [00:18&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:18&lt;01:49,  2.49s/trial, best loss: -0.8321862965498973] 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014190 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12804
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:18&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.115034 valid_1's binary_logloss: 0.135206
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007240 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12847
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:19&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.114229 valid_1's binary_logloss: 0.137941
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008922 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Total Bins 12817
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:20&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.11512  valid_1's binary_logloss: 0.134343
 14%|█▍        | 7/50 [00:21&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:49,  2.55s/trial, best loss: -0.8321862965498973] 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009362 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Total Bins 12804
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 16%|█▌        | 8/50 [00:21&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 16%|█▌        | 8/50 [00:22&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:22&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Did not meet early stopping. Best iteration is:
[80]    training's binary_logloss: 0.116244 valid_1's binary_logloss: 0.135104
 16%|█▌        | 8/50 [00:22&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019852 seconds.
You can set `force_col_wise=true` to remove the overhead.
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Total Bins 12838
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:23&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.116122 valid_1's binary_logloss: 0.137831
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008059 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Total Bins 12817
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:24&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 Did not meet early stopping. Best iteration is:
[72]    training's binary_logloss: 0.11803  valid_1's binary_logloss: 0.134506
 16%|█▌        | 8/50 [00:25&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:25&lt;01:56,  2.77s/trial, best loss: -0.8334837969495431] 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007825 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Total Bins 12804
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:25&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Early stopping, best iteration is:
[19]    training's binary_logloss: 0.119694 valid_1's binary_logloss: 0.135681
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007929 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Total Bins 12838
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Early stopping, best iteration is:
[16]    training's binary_logloss: 0.120319 valid_1's binary_logloss: 0.138538
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009337 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Total Bins 12817
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 18%|█▊        | 9/50 [00:27&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 18%|█▊        | 9/50 [00:27&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:27&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 Early stopping, best iteration is:
[17]    training's binary_logloss: 0.120802 valid_1's binary_logloss: 0.13482
 18%|█▊        | 9/50 [00:27&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:27&lt;02:07,  3.11s/trial, best loss: -0.8335214645673078] 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008130 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Total Bins 12804
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.119408 valid_1's binary_logloss: 0.134911
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009044 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Total Bins 12838
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.119133 valid_1's binary_logloss: 0.137546
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007803 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Total Bins 12817
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:28&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  Early stopping, best iteration is:
[47]    training's binary_logloss: 0.120688 valid_1's binary_logloss: 0.134302
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:47,  2.69s/trial, best loss: -0.8335214645673078] 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011880 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12804
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:29&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.115117 valid_1's binary_logloss: 0.136209
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011556 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12838
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:30&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[27]    training's binary_logloss: 0.115512 valid_1's binary_logloss: 0.138156
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008892 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12817
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:31&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.117743 valid_1's binary_logloss: 0.134897
 22%|██▏       | 11/50 [00:32&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:32&lt;01:39,  2.54s/trial, best loss: -0.8345904314135609] 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008049 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12804
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.118268 valid_1's binary_logloss: 0.135684
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 24%|██▍       | 12/50 [00:32&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006989 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12847
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.118735 valid_1's binary_logloss: 0.137976
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006796 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12817
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[24]    training's binary_logloss: 0.119628 valid_1's binary_logloss: 0.134916
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:38,  2.59s/trial, best loss: -0.8345904314135609] 26%|██▌       | 13/50 [00:33&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010210 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12804
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.114319 valid_1's binary_logloss: 0.135995
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:34&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12838
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.113925 valid_1's binary_logloss: 0.137893
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009469 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12817
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:35&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[33]    training's binary_logloss: 0.117803 valid_1's binary_logloss: 0.135217
 26%|██▌       | 13/50 [00:36&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:36&lt;01:27,  2.36s/trial, best loss: -0.8345904314135609] 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007370 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12804
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:36&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.117141 valid_1's binary_logloss: 0.135196
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013043 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12838
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:37&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.116927 valid_1's binary_logloss: 0.137695
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010449 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12817
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:38&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.117458 valid_1's binary_logloss: 0.134804
 28%|██▊       | 14/50 [00:39&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:39&lt;01:26,  2.40s/trial, best loss: -0.8345904314135609] 30%|███       | 15/50 [00:39&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:39&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:39&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009063 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12821
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.11267  valid_1's binary_logloss: 0.136106
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:40&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009274 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12943
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.115948 valid_1's binary_logloss: 0.13866
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:41&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011803 seconds.
You can set `force_col_wise=true` to remove the overhead.
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12908
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[23]    training's binary_logloss: 0.116131 valid_1's binary_logloss: 0.135187
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:33,  2.67s/trial, best loss: -0.8345904314135609] 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008073 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12900
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:42&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Did not meet early stopping. Best iteration is:
[84]    training's binary_logloss: 0.11371  valid_1's binary_logloss: 0.135137
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007565 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12993
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 32%|███▏      | 16/50 [00:43&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[67]    training's binary_logloss: 0.116405 valid_1's binary_logloss: 0.138124
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008722 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12917
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 32%|███▏      | 16/50 [00:44&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  Did not meet early stopping. Best iteration is:
[84]    training's binary_logloss: 0.114059 valid_1's binary_logloss: 0.134317
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:33,  2.76s/trial, best loss: -0.8345904314135609] 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009001 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12804
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:45&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.116861 valid_1's binary_logloss: 0.134877
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008590 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12838
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:46&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.11687  valid_1's binary_logloss: 0.13752
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006636 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Total Bins 12817
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  Early stopping, best iteration is:
[67]    training's binary_logloss: 0.115782 valid_1's binary_logloss: 0.134209
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:32,  2.82s/trial, best loss: -0.8345904314135609] 36%|███▌      | 18/50 [00:47&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007175 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12944
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.117901 valid_1's binary_logloss: 0.135627
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008634 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12993
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:48&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[24]    training's binary_logloss: 0.114214 valid_1's binary_logloss: 0.138485
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008114 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12917
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.118579 valid_1's binary_logloss: 0.135136
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:25,  2.66s/trial, best loss: -0.8346199818249235] 38%|███▊      | 19/50 [00:49&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:49&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:49&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007573 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12869
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[64]    training's binary_logloss: 0.112009 valid_1's binary_logloss: 0.13523
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:50&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007871 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12947
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[61]    training's binary_logloss: 0.112082 valid_1's binary_logloss: 0.13837
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:51&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008356 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12908
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.113244 valid_1's binary_logloss: 0.134797
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:14,  2.42s/trial, best loss: -0.8346199818249235] 40%|████      | 20/50 [00:52&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:52&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:52&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:52&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 40%|████      | 20/50 [00:52&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007853 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12804
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.13445  valid_1's binary_logloss: 0.139692
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008540 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12838
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:53&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.133592 valid_1's binary_logloss: 0.142113
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010222 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12817
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:54&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.135205 valid_1's binary_logloss: 0.138742
 40%|████      | 20/50 [00:55&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:55&lt;01:17,  2.58s/trial, best loss: -0.8346199818249235] 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008607 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12804
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[52]    training's binary_logloss: 0.118045 valid_1's binary_logloss: 0.134995
 42%|████▏     | 21/50 [00:55&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12913
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.118325 valid_1's binary_logloss: 0.137852
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:56&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007213 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12879
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[54]    training's binary_logloss: 0.118203 valid_1's binary_logloss: 0.134179
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:13,  2.55s/trial, best loss: -0.8346199818249235] 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009051 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12804
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:57&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.118534 valid_1's binary_logloss: 0.136106
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007194 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12913
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[15]    training's binary_logloss: 0.120171 valid_1's binary_logloss: 0.137868
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007805 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12879
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:58&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.118138 valid_1's binary_logloss: 0.13535
 44%|████▍     | 22/50 [00:59&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:09,  2.47s/trial, best loss: -0.8346199818249235] 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008298 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12804
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123442 valid_1's binary_logloss: 0.135652
 46%|████▌     | 23/50 [00:59&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007983 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12838
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.122527 valid_1's binary_logloss: 0.138185
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:00&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012976 seconds.
You can set `force_col_wise=true` to remove the overhead.
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12817
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [01:01&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123812 valid_1's binary_logloss: 0.13482
 46%|████▌     | 23/50 [01:02&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:02&lt;01:00,  2.23s/trial, best loss: -0.8346199818249235] 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007683 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12804
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:02&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[55]    training's binary_logloss: 0.118913 valid_1's binary_logloss: 0.134591
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007060 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12838
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:03&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.119109 valid_1's binary_logloss: 0.137532
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007158 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Total Bins 12817
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  Early stopping, best iteration is:
[53]    training's binary_logloss: 0.119682 valid_1's binary_logloss: 0.134044
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:06,  2.56s/trial, best loss: -0.8346199818249235] 50%|█████     | 25/50 [01:04&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006965 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[41]    training's binary_logloss: 0.116789 valid_1's binary_logloss: 0.135098
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009464 seconds.
You can set `force_col_wise=true` to remove the overhead.
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12838
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:05&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.116539 valid_1's binary_logloss: 0.138054
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006983 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12817
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.117685 valid_1's binary_logloss: 0.134656
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:01,  2.47s/trial, best loss: -0.8353293081416346] 52%|█████▏    | 26/50 [01:06&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008118 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.131216 valid_1's binary_logloss: 0.139484
 52%|█████▏    | 26/50 [01:07&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017901 seconds.
You can set `force_col_wise=true` to remove the overhead.
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12903
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:08&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.130191 valid_1's binary_logloss: 0.141574
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007720 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:09&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.131799 valid_1's binary_logloss: 0.138351
 52%|█████▏    | 26/50 [01:10&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:10&lt;00:57,  2.38s/trial, best loss: -0.8353293081416346] 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007748 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.119196 valid_1's binary_logloss: 0.134697
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007410 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12838
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.118245 valid_1's binary_logloss: 0.137653
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008656 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12817
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:11&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.11982  valid_1's binary_logloss: 0.134115
 54%|█████▍    | 27/50 [01:12&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:12&lt;01:00,  2.64s/trial, best loss: -0.8353293081416346] 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008201 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:12&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[55]    training's binary_logloss: 0.118255 valid_1's binary_logloss: 0.13523
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008806 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12838
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:13&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 56%|█████▌    | 28/50 [01:14&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 56%|█████▌    | 28/50 [01:14&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:14&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.116637 valid_1's binary_logloss: 0.137971
 56%|█████▌    | 28/50 [01:14&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:14&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011744 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12817
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[57]    training's binary_logloss: 0.118083 valid_1's binary_logloss: 0.134172
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:15&lt;00:55,  2.51s/trial, best loss: -0.8353293081416346] 58%|█████▊    | 29/50 [01:15&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009021 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[69]    training's binary_logloss: 0.118755 valid_1's binary_logloss: 0.134976
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:16&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013066 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12913
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[66]    training's binary_logloss: 0.118516 valid_1's binary_logloss: 0.13759
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:17&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011270 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[69]    training's binary_logloss: 0.119213 valid_1's binary_logloss: 0.134123
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:58,  2.81s/trial, best loss: -0.8353293081416346] 60%|██████    | 30/50 [01:18&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007448 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124451 valid_1's binary_logloss: 0.135306
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:19&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008631 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12943
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123486 valid_1's binary_logloss: 0.137957
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007884 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:20&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 60%|██████    | 30/50 [01:21&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 60%|██████    | 30/50 [01:21&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:21&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124936 valid_1's binary_logloss: 0.13456
 60%|██████    | 30/50 [01:21&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:21&lt;00:57,  2.88s/trial, best loss: -0.8353293081416346] 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013677 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12896
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:21&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123182 valid_1's binary_logloss: 0.13554
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010535 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12947
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:22&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12218  valid_1's binary_logloss: 0.138117
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007774 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12908
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:23&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123535 valid_1's binary_logloss: 0.134833
 62%|██████▏   | 31/50 [01:24&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:24&lt;00:53,  2.80s/trial, best loss: -0.8353293081416346] 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016300 seconds.
You can set `force_col_wise=true` to remove the overhead.
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.119038 valid_1's binary_logloss: 0.134688
 64%|██████▍   | 32/50 [01:24&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008723 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12943
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.118066 valid_1's binary_logloss: 0.137695
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012208 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:25&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 64%|██████▍   | 32/50 [01:26&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 64%|██████▍   | 32/50 [01:26&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:26&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.120385 valid_1's binary_logloss: 0.134541
 64%|██████▍   | 32/50 [01:26&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:26&lt;00:49,  2.77s/trial, best loss: -0.8353293081416346] 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007626 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:26&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[42]    training's binary_logloss: 0.113008 valid_1's binary_logloss: 0.135691
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012098 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12903
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[31]    training's binary_logloss: 0.117041 valid_1's binary_logloss: 0.138101
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:27&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009215 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[42]    training's binary_logloss: 0.113562 valid_1's binary_logloss: 0.134568
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:28&lt;00:43,  2.56s/trial, best loss: -0.8353293081416346] 68%|██████▊   | 34/50 [01:28&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:28&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:28&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008779 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121864 valid_1's binary_logloss: 0.135488
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007069 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12903
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 68%|██████▊   | 34/50 [01:29&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120934 valid_1's binary_logloss: 0.138049
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012046 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:30&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.122219 valid_1's binary_logloss: 0.134581
 68%|██████▊   | 34/50 [01:31&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:31&lt;00:40,  2.53s/trial, best loss: -0.8353293081416346] 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007576 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12812
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:31&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.129099 valid_1's binary_logloss: 0.137718
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007789 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12943
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:32&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.128316 valid_1's binary_logloss: 0.140125
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010383 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12879
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:33&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.129766 valid_1's binary_logloss: 0.136703
 70%|███████   | 35/50 [01:34&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:34&lt;00:38,  2.60s/trial, best loss: -0.8353293081416346] 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009033 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:34&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.120857 valid_1's binary_logloss: 0.135392
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007741 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12903
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[31]    training's binary_logloss: 0.118985 valid_1's binary_logloss: 0.137371
 72%|███████▏  | 36/50 [01:35&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010013 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12817
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.120758 valid_1's binary_logloss: 0.134185
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:37,  2.71s/trial, best loss: -0.8353293081416346] 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008582 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12804
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:36&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.122004 valid_1's binary_logloss: 0.134892
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 74%|███████▍  | 37/50 [01:37&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007974 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12838
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120947 valid_1's binary_logloss: 0.137433
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008872 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Total Bins 12817
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:38&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.122411 valid_1's binary_logloss: 0.134261
 74%|███████▍  | 37/50 [01:39&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:39&lt;00:33,  2.54s/trial, best loss: -0.8353293081416346] 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014886 seconds.
You can set `force_col_wise=true` to remove the overhead.
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12804
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:39&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[15]    training's binary_logloss: 0.120015 valid_1's binary_logloss: 0.136651
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012741 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12838
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[15]    training's binary_logloss: 0.118939 valid_1's binary_logloss: 0.138894
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 76%|███████▌  | 38/50 [01:40&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010061 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12817
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[12]    training's binary_logloss: 0.122953 valid_1's binary_logloss: 0.134958
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:41&lt;00:31,  2.62s/trial, best loss: -0.835331797276512] 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008189 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12804
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:41&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Did not meet early stopping. Best iteration is:
[88]    training's binary_logloss: 0.117929 valid_1's binary_logloss: 0.135205
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008863 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12838
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:42&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Did not meet early stopping. Best iteration is:
[81]    training's binary_logloss: 0.11844  valid_1's binary_logloss: 0.137484
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009540 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12817
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:43&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 Did not meet early stopping. Best iteration is:
[85]    training's binary_logloss: 0.118939 valid_1's binary_logloss: 0.134806
 78%|███████▊  | 39/50 [01:44&lt;00:26,  2.39s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:44&lt;00:26,  2.39s/trial, best loss: -0.835331797276512] 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007878 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12804
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.184987
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[50]    training's binary_logloss: 0.119598 valid_1's binary_logloss: 0.134559
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013151 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12838
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.196685
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[51]    training's binary_logloss: 0.118541 valid_1's binary_logloss: 0.137523
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:45&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011190 seconds.
You can set `force_col_wise=true` to remove the overhead.
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Total Bins 12817
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Info] Start training from score -3.181760
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 Early stopping, best iteration is:
[48]    training's binary_logloss: 0.120332 valid_1's binary_logloss: 0.134165
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:46&lt;00:25,  2.54s/trial, best loss: -0.835331797276512] 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006984 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[33]    training's binary_logloss: 0.119524 valid_1's binary_logloss: 0.135441
 82%|████████▏ | 41/50 [01:47&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008826 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.120257 valid_1's binary_logloss: 0.137469
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007049 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:48&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.121158 valid_1's binary_logloss: 0.134384
 82%|████████▏ | 41/50 [01:49&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:49&lt;00:24,  2.72s/trial, best loss: -0.8357102168343064] 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008870 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:49&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.117395 valid_1's binary_logloss: 0.136137
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012995 seconds.
You can set `force_col_wise=true` to remove the overhead.
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[33]    training's binary_logloss: 0.115053 valid_1's binary_logloss: 0.138202
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:50&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008256 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.112815 valid_1's binary_logloss: 0.134646
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:19,  2.47s/trial, best loss: -0.8357102168343064] 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006767 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.116209 valid_1's binary_logloss: 0.135515
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007724 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 86%|████████▌ | 43/50 [01:52&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[41]    training's binary_logloss: 0.116373 valid_1's binary_logloss: 0.13768
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015228 seconds.
You can set `force_col_wise=true` to remove the overhead.
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:53&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[38]    training's binary_logloss: 0.118563 valid_1's binary_logloss: 0.13498
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.49s/trial, best loss: -0.8357102168343064] 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008542 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.120117 valid_1's binary_logloss: 0.134789
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008040 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.119394 valid_1's binary_logloss: 0.137658
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009460 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:55&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.114421 valid_1's binary_logloss: 0.134479
 88%|████████▊ | 44/50 [01:56&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:56&lt;00:14,  2.47s/trial, best loss: -0.8357102168343064] 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010005 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.115234 valid_1's binary_logloss: 0.135872
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008841 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[27]    training's binary_logloss: 0.115194 valid_1's binary_logloss: 0.138408
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010236 seconds.
You can set `force_col_wise=true` to remove the overhead.
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:57&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 90%|█████████ | 45/50 [01:58&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 90%|█████████ | 45/50 [01:58&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:58&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.116992 valid_1's binary_logloss: 0.135531
 90%|█████████ | 45/50 [01:58&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:58&lt;00:11,  2.31s/trial, best loss: -0.8357102168343064] 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007834 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.115923 valid_1's binary_logloss: 0.13639
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009212 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.117019 valid_1's binary_logloss: 0.138229
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:59&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010055 seconds.
You can set `force_col_wise=true` to remove the overhead.
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.117591 valid_1's binary_logloss: 0.135204
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:00&lt;00:09,  2.32s/trial, best loss: -0.8357102168343064] 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009290 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[27]    training's binary_logloss: 0.117985 valid_1's binary_logloss: 0.135367
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010413 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.117671 valid_1's binary_logloss: 0.137665
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008027 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[23]    training's binary_logloss: 0.120142 valid_1's binary_logloss: 0.135155
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:02&lt;00:06,  2.22s/trial, best loss: -0.8357102168343064] 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014548 seconds.
You can set `force_col_wise=true` to remove the overhead.
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Early stopping, best iteration is:
[69]    training's binary_logloss: 0.117534 valid_1's binary_logloss: 0.134864
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007901 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12838
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Did not meet early stopping. Best iteration is:
[82]    training's binary_logloss: 0.114016 valid_1's binary_logloss: 0.137702
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:04&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012228 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.116413 valid_1's binary_logloss: 0.134882
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:05&lt;00:04,  2.25s/trial, best loss: -0.8357102168343064] 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1611, number of negative: 38933
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013063 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12804
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039735 -&gt; initscore=-3.184987
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.184987
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.115727 valid_1's binary_logloss: 0.135247
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1593, number of negative: 38951
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008711 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12847
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039291 -&gt; initscore=-3.196685
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.196685
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11494  valid_1's binary_logloss: 0.137861
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:07&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of positive: 1616, number of negative: 38928
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007797 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Total Bins 12817
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039858 -&gt; initscore=-3.181760
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Info] Start training from score -3.181760
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.116161 valid_1's binary_logloss: 0.134483
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:08&lt;00:02,  2.49s/trial, best loss: -0.8357102168343064]100%|██████████| 50/50 [02:08&lt;00:00,  2.67s/trial, best loss: -0.8357102168343064]100%|██████████| 50/50 [02:08&lt;00:00,  2.58s/trial, best loss: -0.8357102168343064]
{'learning_rate': 0.07078424888661622, 'max_depth': 143.0, 'min_child_samples': 93.0, 'num_leaves': 33.0, 'subsample': 0.9935662058378432}</code></pre>
</div>
</div>
</section>
<section id="재학습" class="level3">
<h3 class="anchored" data-anchor-id="재학습">재학습</h3>
<div id="8f3a2e85" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb18-2">                          num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb18-3">                          max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb18-4">                          min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb18-5">                          subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-6">                          learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-7">                          early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb18-8">                          eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb18-9"></span>
<span id="cb18-10">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb18-11">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb18-12"></span>
<span id="cb18-13">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1680, number of negative: 40891
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010648 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12882
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039463 -&gt; initscore=-3.192116
[LightGBM] [Info] Start training from score -3.192116
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[43]    training's binary_logloss: 0.120912 valid_1's binary_logloss: 0.136896
[LightGBM] [Warning] Unknown parameter: eval_metric
0.835</code></pre>
</div>
</div>
</section>
</section>
<section id="제출" class="level2">
<h2 class="anchored" data-anchor-id="제출">제출</h2>
<div id="07194e50" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgbm_clf.predict(test_df)</span>
<span id="cb20-2"></span>
<span id="cb20-3">submit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/sample_submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb20-4">submit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TARGET'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target</span>
<span id="cb20-5">submit.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric</code></pre>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/04_archives/adp_실기/notes/machine_learning/03.html</guid>
  <pubDate>Sat, 26 Jul 2025 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
