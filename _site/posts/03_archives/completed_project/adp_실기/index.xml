<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/</link>
<atom:link href="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/index.xml" rel="self" type="application/rss+xml"/>
<description>ADP 실기를 준비해 봅시다.</description>
<image>
<url>https://cryscham123.github.io/profile.jpg</url>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/</link>
</image>
<generator>quarto-1.5.56</generator>
<lastBuildDate>Thu, 18 Dec 2025 15:00:00 GMT</lastBuildDate>
<item>
  <title>결과</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/12.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-12-19-18-58-07.png" class="img-fluid figure-img"></p>
<figcaption>시험 결과(75점 이상 합격)</figcaption>
</figure>
</div>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>결과가 나오고 1달 정도 지나서 후기를 작성한다. 이번 학기가 너무 바빴다.</p>
<p>뭐 어쨌든 결과는...불합격! 예상은 하고 있었으니 뭐 크게 실망스럽진 않지만 한 문제 차이로 떨어질거라곤 생각도 못했다.</p>
</section>
<section id="틀린-문제" class="level2">
<h2 class="anchored" data-anchor-id="틀린-문제">틀린 문제</h2>
<p>기계 학습 분야에서 21점이 감점되었는데 이건 오히려 잘한 것이다. 왜냐하면 20점 짜리 파트 하나를 통으로 날렸기 때문에, 내가 푼 문제에서는 1점만 까였다는 뜻이니까.</p>
<p>통계 분석 파트에서는 아마도 로지스틱 회귀분석 문제에서 점수가 까인것 같은데...사실 이 문제는 틀릴만한 문제는 아니였다. 심지어...시험 전날 adp 커뮤니티에서 누군가 이 문제가 나온다고 예상 글을 올렸을 때도 ‘이런걸 누가 틀려’ 하고 대수롭지 않게 여겼는데 막상 시험에 나와서 풀어보니 내가 예상한대로 동작하지 않았다.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>저 문제를 맞았다면 아마 합격했을테지만 뭐 이제와서 그게 중요한가? 어쨌든 이번 시험 준비 과정에서 많이 성장했다는 것을 체감하고 있고, 떨어진거야 뭐 아쉬운거지.</p>
<p>다음 시험을 준비할지는 아직 잘 모르겠다. 시험을 보긴 하겠지만 이번에 준비한만큼 하진 않을 것 같다.</p>
<p>들리는 소문에 의하면 이번 회차 합격자는 10명이라고 하던데 정말 양심이 없는 시험이 아닐 수 없다. 하지만 이런 시험에서 합격한다면 실력 증명 하나는 확실히 하는게 되는 것 아닐까? 하는 욕심은 남는다. 그래도 멈춰야할 때를 확실히 아는 것도 필요하다고 본다. 여기에만 온전히 집중할 수는 없으니 말이다.</p>
<p>앞으로는 인공지능 쪽 공부에 더 집중해보려 한다.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>adp 실기</category>
  <category>데이터 분석</category>
  <category>후기</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/12.html</guid>
  <pubDate>Thu, 18 Dec 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>try 1 후기</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/11.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>개같이 망했다.</p>
<p>사실 조금 방심했다.</p>
<p>통계파트가 다 아는 문제가 나와서 음..이거 잘 하면 합격하겠는데? 라는 생각이 들었다.</p>
<p>그래서 여유롭게 1시간동안 풀고 화장실도 다녀왔다.</p>
</section>
<section id="머신러닝" class="level2">
<h2 class="anchored" data-anchor-id="머신러닝">머신러닝</h2>
<p>이런 젠장. 머신러닝 파트 왜 이렇게 오래 걸리는거야?</p>
<p>분명 풀 수 있는 문제지만, 시간을 보니 10분밖에 안 남아 있었다.</p>
<p>결국 2번 문제는 전처리조차 하지 못하고 통으로 버릴 수 밖에 없었다.</p>
<p>물론 통으로 버려도 합격은 할 수 있다. 그런 사람이 실제로 있는지는 잘 모르겠지만.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>ADP 실기 시험까지 오는 여정은 의미가 깊었다. 진심으로 그렇게 생각한다.</p>
<p>그래서 결과가 좋지 않더라도 실망은 10% 정도만 할 것 같다.</p>
<p>하지만 계속 시험을 준비하는 것은 다른 이야기다. 내가 6개월을 더 준비할 가치가 아직 남아있을까?</p>
<p>솔직히 잘 모르겠다. 일단은 다른 것들을 병행해보면서 생각을 정리해보려 한다.</p>
<p>당장 눈 앞에 닥친 중간고사나 제대로 준비해보자.</p>
<p>아아! 한 번에 붙을 수 있었는데! (결과는 아직 안나오긴 했다.)</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>adp 실기</category>
  <category>데이터 분석</category>
  <category>후기</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/11.html</guid>
  <pubDate>Fri, 17 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>확률과 통계</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/10.html</link>
  <description><![CDATA[ 




<section id="통계학" class="level2">
<h2 class="anchored" data-anchor-id="통계학">통계학</h2>
<ul>
<li><strong>불확실한 상황</strong> 하에서 데이터에 근거하여 <strong>과학적인 의사결정</strong>을 도출하기 위한 이론과 방법의 체계</li>
<li><strong>모집단</strong>으로 부터 수집된 <strong>데이터</strong>(sample)를 기반으로 모집단의 <strong>특성을 추론</strong>하는 것을 목표로 한다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-03-08-12-28-23.png" class="img-fluid figure-img"></p>
<figcaption>통계적 의사결정 과정</figcaption>
</figure>
</div>
</section>
<section id="확률" class="level2">
<h2 class="anchored" data-anchor-id="확률">확률</h2>
<ul>
<li>고전적 의미: 표본공간에서 특정 사건이 차지하는 비율</li>
<li>통계적 의미: 특정 사건이 발생하는 <strong>상대도수의 극한</strong>
<ul>
<li>각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.</li>
</ul></li>
</ul>
</section>
<section id="확률-분포-정의-단계" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포-정의-단계">확률 분포 정의 단계</h2>
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-03-08-13-00-48.png" class="img-fluid"></p>
<ul>
<li><strong>Experiment(확률실험)</strong>: 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측</li>
<li><strong>Sample space(표본공간)</strong>: 모든 simple event의 집합</li>
<li><strong>Event(사건)</strong>: 실험에서 발생하는 결과 (부분 집합)</li>
<li><strong>Simple event(단순사건)</strong>: 원소가 하나인 사건</li>
<li><strong>확률 변수</strong>: 확률실험의 결과를 수치로 나타낸 변수</li>
</ul>
</section>
<section id="확률-분포" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포">확률 분포</h2>
<section id="이산-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="이산-확률-분포">이산 확률 분포</h3>
<p>이산 표본 공간, 연속 표본공간에서 정의 가능포</p>
<ul>
<li><strong>베르누이 시행</strong>: 각 시행은 서로 <strong>독립적</strong>이고, 실패와 성공 <strong>두 가지 결과만 존재</strong>.
<ul>
<li>단 <strong>모집단의 크기가 충분히 크고</strong>, <strong>표본(시행)의 크기가 충분히 작다면</strong> <strong>비복원 추출</strong>에서도 <strong>유효</strong></li>
<li>평균: p</li>
<li>분산: p(1-p)</li>
</ul></li>
<li><strong>이항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 성공 횟수를 측정
<ul>
<li>X ~ B(n, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bn%7D%7Bx%7D%20p%5Ex%20(1-p)%5E%7Bn-x%7D"></li>
<li>평균: np</li>
<li>분산: np(1-p)</li>
<li>n이 매우 크고, p가 매우 작을 때, <strong>포아송 분포로 근사</strong>할 수 있다. (λ = np)</li>
</ul></li>
<li><strong>음이항 분포</strong>
<ul>
<li>정의: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k번 성공하고, r번 실패한 경우 (n = k + r)
<ol type="1">
<li>r번의 실패가 나오기 전까지, 성공한 횟수 x
<ul>
<li>X ~ NB(r, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx+r-1%7D%7Bx%7D%20p%5Ex%20(1-p)%5Er"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B1-p%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B(1-p)%5E2%7D"></li>
</ul></li>
<li>r번의 실패가 나오기 전까지, 시행한 횟수 x
<ul>
<li>4번에서 성공을 실패로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 실패한 횟수 x
<ul>
<li>1번에서 실패를 성공으로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 시행한 횟수 x
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx-1%7D%7Bk-1%7D%20p%5Ek%20(1-p)%5E%7Bx-k%7D"></li>
<li>k가 1일 때 기하분포와 동일</li>
</ul></li>
<li>n번의 시행 횟수에서, k번 성공 또는 r번 실패한 경우: 이항분포</li>
</ol></li>
</ul></li>
<li><strong>기하 분포</strong>:
<ul>
<li>정의:
<ol type="1">
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 시행 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5E%7Bx-1%7D%20p,%20x%20=%201,%202,%203,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 실패 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5Ex%20p,%20x%20=%200,%201,%202,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
</ol></li>
<li><strong>비기억 특성</strong>: <img src="https://latex.codecogs.com/png.latex?P(X%20%3E%20n+k%20%7C%20X%20%3E%20n)%20=%20P(X%20%3E%20k)"></li>
</ul></li>
<li><strong>초기하 분포</strong>: <strong>베르누이 시행이 아닌 시행</strong>에서 성공하는 횟수
<ul>
<li>X ~ H(n, N, k), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B%5Cbinom%7BK%7D%7Bx%7D%20%5Cbinom%7BN-K%7D%7Bn-x%7D%7D%7B%5Cbinom%7BN%7D%7Bn%7D%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK%7D%7BN%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK(N-K)(N-n)%7D%7BN%5E2(N-1)%7D"></li>
</ul></li>
<li><strong>포아송 분포</strong>: <strong>임의의 기간</strong>동안 <strong>어떤 사건이 간헐적</strong>으로 발생할 때, 동일한 길이의 기간동안 실제 사건이 발생하는 횟수
<ul>
<li>X ~ Poisson(λ), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7Be%5E%7B-%CE%BB%7D%20%CE%BB%5Ex%7D%7Bx!%7D,%20%CE%BB%20%3E%200"></li>
<li>평균: λ</li>
<li>분산: λ</li>
</ul></li>
</ul>
</section>
<section id="연속-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="연속-확률-분포">연속 확률 분포</h3>
<p>연속 표본 공간에서 정의 가능</p>
<ul>
<li><strong>균일 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7Bb-a%7D,%20a%20%E2%89%A4%20x%20%E2%89%A4%20b"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Ba+b%7D%7B2%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(b-a)%5E2%7D%7B12%7D"></li>
</ul></li>
<li><strong>정규 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20+%20Y%20%5Csim%20N(%CE%BC_1%20+%20%CE%BC_2,%20%CF%83_1%5E2%20+%20%CF%83_2%5E2)"></li>
<li>선형 변환: <img src="https://latex.codecogs.com/png.latex?Y%20=%20aX%20+%20b%20%5Csim%20N(a%CE%BC%20+%20b,%20a%5E2%CF%83%5E2)"></li>
</ul></li>
<li><strong>t 분포</strong>
<ul>
<li>자유도가 커질수록 표준 정규분포에 근사함.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BZ%7D%7B%5Csqrt%7BV/n%7D%7D%20%5Csim%20t(n)">, Z: 표준정규분포, V: 자유도가 n인 카이제곱분포</li>
</ul></li>
<li><strong>f 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?F%20=%20%5Cfrac%7BX_1/%CE%BD_1%7D%7BX_2/%CE%BD_2%7D">, <img src="https://latex.codecogs.com/png.latex?X_1%20%5Csim%20%CF%87%5E2(%CE%BD_1)">, <img src="https://latex.codecogs.com/png.latex?X_2%20%5Csim%20%CF%87%5E2(%CE%BD_2)">, X1과 X2는 서로 독립</li>
</ul></li>
<li><strong>감마 분포</strong>
<ul>
<li>α: 분포의 형태 결정, θ: 분포의 크기 결정</li>
<li>평균: αθ</li>
<li>분산: αθ²</li>
<li><strong>카이제곱 분포</strong>: α = v/2, θ = 2 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z_i%20%5Csim%20N(0,1)">일 때, <img src="https://latex.codecogs.com/png.latex?Z_1%5E2%20+%20Z_2%5E2%20+%20...%20%20+%20Z_n%5E2%20%5Csim%20%CF%87%5E2(n)"></li>
<li><img src="https://latex.codecogs.com/png.latex?X_i">가 서로 독립이고, 자유도가 <img src="https://latex.codecogs.com/png.latex?%CE%BD_i">인 카이제곱분포를 따른다면, <img src="https://latex.codecogs.com/png.latex?X_1%20+%20X_2%20+%20...%20+%20X_n%20%5Csim%20x%5E2(%CE%BD_1%20+%20%CE%BD_2%20+%20...%20+%20%CE%BD_n)"></li>
<li>자유도가 커질수록 기댓값을 중심으로 모이고, 대칭에 가까워진다.</li>
</ul></li>
<li><strong>지수 분포</strong>: α = 1, θ = 1/λ 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20~%20Exp(%CE%BB%20=%20%5Cfrac%7B1%7D%7B%CE%B8%7D)">, f(x) = <img src="https://latex.codecogs.com/png.latex?%CE%BBe%5E%7B-%CE%BBx%7D,%20x%20%3E%200"></li>
<li>θ: 평균 사건 발생 간격, λ: 단위 시간당 사건 발생 횟수</li>
<li>포아송 분포에서 사건 발생 간격의 분포</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7Bn%7D%20X_i%20%5Csim%20%CE%93(n,%20%CE%B8)">, <img src="https://latex.codecogs.com/png.latex?%CE%B8%20=%201/%CE%BB"></li>
<li>비기억 특성을 가진다: <img src="https://latex.codecogs.com/png.latex?p(X%20%3E%20s%20+%20t%20%7C%20X%20%3E%20s)%20=%20p(X%20%3E%20t)%20=%20e%5E%7B-%CE%BBt%7D"></li>
<li>독립적으로 동일한 지수분포를 따르는 확률변수 n개의 합은 <img src="https://latex.codecogs.com/png.latex?%CE%B1%20=%20n,%20%CE%B8%20=%20%5Cfrac%7B1%7D%7B%CE%BB%7D">인 감마분포를 따른다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="다변량-분포" class="level3">
<h3 class="anchored" data-anchor-id="다변량-분포">다변량 분포</h3>
<ul>
<li><strong>다항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k개의 범주로 분류
<ul>
<li>X ~ M(n, p1, p2, …, pk), <img src="https://latex.codecogs.com/png.latex?f(x_1,%20x_2,%20...,%20x_k)%20=%20%5Cfrac%7Bn!%7D%7Bx_1!%20x_2!%20...%20x_k!%7D%20p_1%5E%7Bx_1%7D%20p_2%5E%7Bx_2%7D%20...%20p_k%5E%7Bx_k%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1,%20np_2,%20...,%20np_k%5D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1(1-p_1),%20np_2(1-p_2),%20...,%20np_k(1-p_k)%5D"></li>
<li>공분산: <img src="https://latex.codecogs.com/png.latex?-np_ip_j%20(i%20%E2%89%A0%20j)"></li>
<li>독립인 변수의 갯수는 k-1개 (k개의 사건)</li>
</ul></li>
</ul>
</section>
<section id="샘플링" class="level3">
<h3 class="anchored" data-anchor-id="샘플링">샘플링</h3>
</section>
<section id="분포의-동질성-검정" class="level3">
<h3 class="anchored" data-anchor-id="분포의-동질성-검정">분포의 동질성 검정</h3>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="표본의-분포" class="level2">
<h2 class="anchored" data-anchor-id="표본의-분포">표본의 분포</h2>
<ul>
<li><p>샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.</p></li>
<li><p>통계량: 표본의 특성을 나타내는 값</p></li>
<li><p>추정량: 아래의 조건을 만족하는 통계량</p>
<ul>
<li>불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.</li>
<li>효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.</li>
</ul></li>
</ul>
<section id="표본-평균의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-평균의-분포">표본 평균의 분포</h3>
<ul>
<li>모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2">이면, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D">의 평균은 μ이고, 분산은 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2/n">인 정규분포를 따른다.
<ul>
<li>단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리<sup>1</sup>)</li>
</ul></li>
<li>만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.
<ul>
<li>단 이때는 <strong>모집단이 정규분포를 따라야 한다.</strong></li>
</ul></li>
</ul>
</section>
<section id="표본-분산의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-분산의-분포">표본 분산의 분포</h3>
<ul>
<li><strong>정규 모집단으로 부터 나온 표본</strong>의 분산 S에 대하여, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(n-1)S%5E2%7D%7B%CF%83%5E2%7D">은 자유도가 n-1인 카이제곱 분포를 따른다.
<ul>
<li>모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.</li>
</ul></li>
<li>두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.</li>
</ul>
</section>
</section>
<section id="추정" class="level2">
<h2 class="anchored" data-anchor-id="추정">추정</h2>
<ul>
<li>통계적 추론: <strong>모집단에서 추출된 표본</strong>의 <strong>통계량</strong>으로부터 <strong>모수</strong>를 추론하는 것
<ul>
<li>추정
<ul>
<li>점추정</li>
<li>구간추정</li>
</ul></li>
<li>가설 검정</li>
</ul></li>
</ul>
<section id="점-추정" class="level3">
<h3 class="anchored" data-anchor-id="점-추정">점 추정</h3>
<ul>
<li>불편성
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20=%20%CE%B8"></li>
<li>bias = <img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20-%20%5Ctheta">
<ul>
<li>보통 sample size가 커질수록 bias는 0에 수렴</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D,%20X_n">은 μ의 불편추정량이다.</li>
</ul></li>
<li>최소분산
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Var(%5Cbar%7BX%7D)">가 <img src="https://latex.codecogs.com/png.latex?Var(X_n)">보다 분산이 작아서 더 좋은 추정량</li>
<li><img src="https://latex.codecogs.com/png.latex?MSE(%5Chat%7B%5Ctheta%7D)%20=%20E%5B(%5Chat%7B%5Ctheta%7D%20-%20%5Ctheta)%5E2%5D%20=%20Var(%5Chat%7B%5Ctheta%7D)%20+%20bias%5E2">
<ul>
<li>큰 오차에 더 큰 페널티를 주기 위해 제곱</li>
</ul></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-09-06-20-37-05.png" class="img-fluid figure-img"></p>
<figcaption>대표적인 불편추정량</figcaption>
</figure>
</div>
<ul>
<li><strong>전부 중심극한의정리를 적용</strong>할 수 있다. (비율은 0과 1의 평균이므로)</li>
<li>모평균, 모비율의 차이는 서로 독립이라는 가정이 필요하다.</li>
</ul>
</section>
<section id="구간-추정" class="level3">
<h3 class="anchored" data-anchor-id="구간-추정">구간 추정</h3>
<ul>
<li>α: 유의수준</li>
<li>1 - α: 신뢰수준<sup>2</sup></li>
<li>(θ_L, θ_U) = (1 - α) × 100% 신뢰구간</li>
</ul>
<ol type="1">
<li>(<img src="https://latex.codecogs.com/png.latex?%CE%B8_L,%20%CE%B8_U">) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다</li>
<li>구간이 충분히 좁아야 한다
<ul>
<li>표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.</li>
<li>고로 신뢰구간이 대칭임</li>
</ul></li>
</ol>
</section>
<section id="표본의-크기-결정" class="level3">
<h3 class="anchored" data-anchor-id="표본의-크기-결정">표본의 크기 결정</h3>
<p>특정 오차 아래로 하는 표본의 수 구하는 법</p>
<ul>
<li>그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.</li>
<li><strong>모비율</strong>을 모를 때는 일단 <strong>0.5로 보수적으로 놓고 계산</strong></li>
</ul>
</section>
</section>
<section id="모분산-추정" class="level2">
<h2 class="anchored" data-anchor-id="모분산-추정">모분산 추정</h2>
<ul>
<li>카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음
<ul>
<li>그냥 쉽게 구하기 위해 <img src="https://latex.codecogs.com/png.latex?(x%5E2_%7B%CE%B1/2%7D,%20x%5E2_%7B1-%CE%B1/2%7D)">를 사용</li>
</ul></li>
<li>모분산의 신뢰구간: <img src="https://latex.codecogs.com/png.latex?(%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B(1-%5Calpha)/2%7D(n-1)%7D,%20%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B%5Calpha/2%7D(n-1)%7D)"></li>
<li>표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.</li>
</ul>


</section>


<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">각주</h2>

<ol>
<li id="fn1"><p>모집단의 분포와 상관 없이, 표본의 평균은 정규분포에 수렴한다는 정리. 이항분포의 경우, P(X=c) ~ P(c - 0.5 &lt; X &lt; c + 0.5)로 근사 가능하다는 라플라스의 정리를 일반화한 것↩︎</p></li>
<li id="fn2"><p>샘플링을 무한히 반복했을 때, <strong>이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함</strong>한다. 즉, 구간이 확률 변수이다.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/10.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>기타</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/09.html</link>
  <description><![CDATA[ 




<section id="or" class="level2">

<ul>
<li>pulp 이용해서 푼다.</li>
<li>제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pulp example</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="pulp example" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pulp</span>
<span id="cb1-2"></span>
<span id="cb1-3">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpProblem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Problem_name"</span>, pulp.LpMinimize) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문제에 맞게 설정</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)</span></span>
<span id="cb1-6">x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-7">x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-8">x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-9">x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-10">x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-11">x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 목표 함수 정의 (총 운송 비용)</span></span>
<span id="cb1-14">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B3, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"obj_name"</span></span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 제약 조건 정의</span></span>
<span id="cb1-17">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_A_Supply"</span></span>
<span id="cb1-18">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_B_Supply"</span></span>
<span id="cb1-19">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_1_Demand"</span></span>
<span id="cb1-20">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_2_Demand"</span></span>
<span id="cb1-21">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">130</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_3_Demand"</span></span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5. 문제 풀이</span></span>
<span id="cb1-24">prob.solve()</span>
<span id="cb1-25"></span>
<span id="cb1-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 6. 결과 확인</span></span>
<span id="cb1-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Status:"</span>, pulp.LpStatus[prob.status])</span>
<span id="cb1-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Optimal Transportation Plan:"</span>)</span>
<span id="cb1-29"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> prob.variables():</span>
<span id="cb1-30">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(v.name, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"="</span>, v.varValue)</span>
<span id="cb1-31"></span>
<span id="cb1-32"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Total Minimum Cost = "</span>, pulp.value(prob.objective))</span></code></pre></div>
</div>
<ul>
<li>그 외 자주 안나오는 문제 예상 관련 글이 올라올 경우 추가 예정</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/09.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>시계열 분석</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/07.html</link>
  <description><![CDATA[ 




<section id="구성-요소" class="level2">
<h2 class="anchored" data-anchor-id="구성-요소">구성 요소</h2>
<ul>
<li><code>추세</code>(level)</li>
<li><code>계절, 순한</code>: 추세에서 벗어나는 변화의 정도</li>
<li><code>잔차</code>(white noise)</li>
</ul>
</section>
<section id="eda" class="level2">
<h2 class="anchored" data-anchor-id="eda">EDA</h2>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">df_isna <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.asfreq(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'H'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간 기준 결측치 탐색. index가 datetime이어야 함</span></span>
<span id="cb1-2">df_isna[df_isna.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.seasonal <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> STL</span>
<span id="cb2-2"></span>
<span id="cb2-3">decomposition <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> STL(df_final[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], period<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>).fit() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># period는 계절성 주기</span></span>
<span id="cb2-4">fig, (ax1, ax2, ax3, ax4) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sharex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb2-5">ax1.plot(decomposition.observed)</span>
<span id="cb2-6">ax1.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Obseved'</span>)</span>
<span id="cb2-7"></span>
<span id="cb2-8">ax2.plot(decomposition.trend)</span>
<span id="cb2-9">ax2.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Trend'</span>)</span>
<span id="cb2-10"></span>
<span id="cb2-11">ax3.plot(decomposition.seasonal)</span>
<span id="cb2-12">ax3.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Seasonal'</span>)</span>
<span id="cb2-13"></span>
<span id="cb2-14">ax4.plot(decomposition.resid)</span>
<span id="cb2-15">ax4.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Residuals'</span>)</span>
<span id="cb2-16"></span>
<span id="cb2-17">plt.show()</span></code></pre></div>
</section>
<section id="예측-방법" class="level2">
<h2 class="anchored" data-anchor-id="예측-방법">예측 방법</h2>
<p><strong>Rolling Forecast (동적 예측)</strong>: - 각 시점에서 예측을 수행한 후, 실제 값이 관측되면 이를 학습 데이터에 추가하여 다음 시점을 예측 - 실제 운영 환경을 시뮬레이션하는 방법으로, 새로운 정보가 지속적으로 업데이트됨 - 장점: 최신 정보를 반영하여 더 정확한 예측 가능 - 단점: 계산 비용이 높고, 모델 성능 평가 시 과적합 위험</p>
<p><strong>Static Forecast (정적 예측)</strong>: - 초기 학습 데이터로만 모델을 한 번 학습하고, 이후 테스트 기간 전체에 대해 연속적으로 예측 - 고정된 모델로 여러 시점을 예측하므로 모델의 일반화 성능을 더 엄격하게 평가 - 장점: 계산 비용이 낮고, 공정한 모델 평가 가능 - 단점: 시간이 지날수록 예측 정확도가 떨어질 수 있음</p>
</section>
<section id="단순-기법" class="level2">
<h2 class="anchored" data-anchor-id="단순-기법">단순 기법</h2>
<ul>
<li>단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t"></li>
<li>추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t%20+%20p_t%20-%20p_%7Bt-1%7D"></li>
<li>단순 이동평균법: time window를 계속 이동하면서 평균 구하는거
<ul>
<li>time window ↑: 먼 과거까지 보겠다</li>
</ul></li>
<li>가중 이동평균법: 가중치를 다르게 부여한 단순이동평균법</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>단순 기법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="단순 기법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_simple(train_data, test_data, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>):</span>
<span id="cb3-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb3-4">    </span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb3-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>:</span>
<span id="cb3-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 단순예측법: 마지막 값</span></span>
<span id="cb3-8">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-9">            </span>
<span id="cb3-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>:</span>
<span id="cb3-11">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추세분석법</span></span>
<span id="cb3-12">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-13">            </span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>:</span>
<span id="cb3-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4기간 이동평균</span></span>
<span id="cb3-16">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-17">            </span>
<span id="cb3-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>:</span>
<span id="cb3-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12기간 이동평균</span></span>
<span id="cb3-20">            window <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb3-21">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>window:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-22">            </span>
<span id="cb3-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>:</span>
<span id="cb3-24">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 단순예측법 (4분기 전과 동일)</span></span>
<span id="cb3-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:</span>
<span id="cb3-26">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-28">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-29">        </span>
<span id="cb3-30">        predictions.append(pred)</span>
<span id="cb3-31">        </span>
<span id="cb3-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb3-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb3-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-35">    </span>
<span id="cb3-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb3-37"></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 방법별 rolling forecast 수행</span></span>
<span id="cb3-39">methods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>]</span>
<span id="cb3-40">rolling_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb3-41"></span>
<span id="cb3-42"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> methods:</span>
<span id="cb3-43">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_simple(train, test, method)</span>
<span id="cb3-44">    rolling_results[method] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> predictions</span>
<span id="cb3-45">    </span>
<span id="cb3-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결과를 DataFrame으로 정리</span></span>
<span id="cb3-47">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(rolling_results)</span>
<span id="cb3-48">results_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'actual'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>].values</span>
<span id="cb3-49">results_df.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test.index</span>
<span id="cb3-50"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results_df)</span></code></pre></div>
</div>
</section>
<section id="지수-평활법" class="level2">
<h2 class="anchored" data-anchor-id="지수-평활법">지수 평활법</h2>
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-10-14-19-44-17.png" class="img-fluid"></p>
<ul>
<li>α -&gt; 1: 최근 자료에 비중을 둠. α -&gt; 0: 기존 예측을 따름</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>지수 평활법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="지수 평활법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.holtwinters <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SimpleExpSmoothing</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_exponential_smoothing(train_data, test_data, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>):</span>
<span id="cb4-4">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-5">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb4-6">    </span>
<span id="cb4-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb4-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb4-9">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb4-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SimpleExpSmoothing(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-11">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(smoothing_level<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>a, optimized<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-12"></span>
<span id="cb4-13">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb4-14">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.predict(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb4-15">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb4-16"></span>
<span id="cb4-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb4-18">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb4-20">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-21">        </span>
<span id="cb4-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb4-23">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb4-24">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-25">    </span>
<span id="cb4-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb4-27"></span>
<span id="cb4-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 지수 평활법 rolling forecast 실행 예시</span></span>
<span id="cb4-29">exp_smooth_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_exponential_smoothing(train, test, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
</section>
<section id="sarimax-계열" class="level2">
<h2 class="anchored" data-anchor-id="sarimax-계열">SARIMAX 계열</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-10-14-08-07-42.png" class="img-fluid figure-img"></p>
<figcaption>SARIMAX 과정</figcaption>
</figure>
</div>
<section id="정상성-확인" class="level3">
<h3 class="anchored" data-anchor-id="정상성-확인">정상성 확인</h3>
<ul>
<li>정상시계열은 분산과 평균, 자기 상관이 시간에 따라 변하지 않는 시계열</li>
<li>분산이 일정하지 않은 경우: 로그 변환
<ul>
<li>그래프로 확인</li>
</ul></li>
<li>평균이 일정하지 않은 경우: 차분</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>stationarity test</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="stationarity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.stattools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> adfuller</span>
<span id="cb5-2"></span>
<span id="cb5-3">ad_fuller_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> adfuller(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ADF Statistic:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'p-value:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<ul>
<li>p-value &lt; 0.05: 귀무가설 기각, 정상시계열</li>
<li>주로 평균이 일정하지 않은 것을 찾아낼 수 있다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>difference</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff()</span></code></pre></div>
</div>
<ul>
<li>만약 차분 후에도 정상성이 만족되지 않는다면, 계절 차분을 고려</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>seasonal difference</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="seasonal difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12개월 주기</span></span></code></pre></div>
</div>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.graphics.tsaplots <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_acf, plot_pacf</span>
<span id="cb8-2"></span>
<span id="cb8-3">plot_acf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-4">plot_pacf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-5">plt.show()</span></code></pre></div>
<ul>
<li>자기 상관이 존재하는 경우
<ul>
<li>ACF, PACF 그래프로 확인</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-10-14-08-43-44.png" class="img-fluid figure-img"></p>
<figcaption>모델 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>하지만 매우 주관적일 수 있으므로, 직접 여러 모델을 돌려본 후 AIC 기준으로 선택.
<ul>
<li>그리고 다시 검정 진행</li>
</ul></li>
</ul>
</section>
<section id="sarimax-모델-적합" class="level3">
<h3 class="anchored" data-anchor-id="sarimax-모델-적합">SARIMAX 모델 적합</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="SARIMAX" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.statespace.sarimax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SARIMAX</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> product</span>
<span id="cb9-3"></span>
<span id="cb9-4">p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-5">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 차분 횟수</span></span>
<span id="cb9-6">q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-7">P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-8">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 차분 횟수</span></span>
<span id="cb9-9">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-10">s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 주기</span></span>
<span id="cb9-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> product(p, q, P, Q)</span>
<span id="cb9-12">order_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(parameters)</span>
<span id="cb9-13"></span>
<span id="cb9-14">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-15"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> order <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> order_list:</span>
<span id="cb9-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb9-17">        model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb9-18">                        exog,</span>
<span id="cb9-19">                        order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], d, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), </span>
<span id="cb9-20">                        seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], D, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], s),</span>
<span id="cb9-21">                        simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span>:</span>
<span id="cb9-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span></span>
<span id="cb9-24">    aic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.aic</span>
<span id="cb9-25">    results.append([order, aic])</span>
<span id="cb9-26">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(p, q, P, Q)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>])</span>
<span id="cb9-27">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-28">display(results_df)</span></code></pre></div>
</div>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">best_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb10-2">                    exog,</span>
<span id="cb10-3">                    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [2, 3], 0, 1 이런 식으로도 가능(y_t-2, y_t-3 사용하는 방법)</span></span>
<span id="cb10-4">                    simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">best_model.summary()</span></code></pre></div>
<ul>
<li>결과 확인</li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">best_model.plot_diagnostics(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb12-2">plt.show()</span></code></pre></div>
<ul>
<li>등분산성, 정규성 만족하는지 육안으로 확인</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>ljung-box</strong></pre>
</div>
<div class="sourceCode" id="cb13" data-filename="ljung-box" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.diagnostic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> acorr_ljungbox</span>
<span id="cb13-2"></span>
<span id="cb13-3">ljung_box_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> acorr_ljungbox(best_model.resid, lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>], return_df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb13-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(ljung_box_result)</span></code></pre></div>
</div>
<ul>
<li>p-value가 0.05보다 크면 잔차가 백색잡음</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb14" data-filename="SARIMAX rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_sarimax(train_data, test_data, order, seasonal_order, exog_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, exog_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb14-4">    exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exog_train.copy() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_train <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb14-5">    </span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb14-8">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb14-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-10">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-11">                               exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_extended,</span>
<span id="cb14-12">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-13">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-14">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-16">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-17">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-18">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-19">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-20">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-21">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb14-22">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-23">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb14-24">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-25">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-26">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb14-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb14-28">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb14-29">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb14-30">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb14-31">        </span>
<span id="cb14-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb14-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb14-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-35">        </span>
<span id="cb14-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-37">            exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([exog_extended, exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-38">    </span>
<span id="cb14-39">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb14-40"></span>
<span id="cb14-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SARIMAX rolling forecast 실행 예시</span></span>
<span id="cb14-42">sarimax_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_sarimax(</span>
<span id="cb14-43">    train, test, </span>
<span id="cb14-44">    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), </span>
<span id="cb14-45">    seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절성이 없는 경우</span></span>
<span id="cb14-46">)</span></code></pre></div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/07.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>비지도 학습 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/06.html</link>
  <description><![CDATA[ 




<section id="군집-분석" class="level2">
<h2 class="anchored" data-anchor-id="군집-분석">군집 분석</h2>
<section id="distance-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-methods">Distance-based methods</h3>
<ul>
<li>Partitioning methods</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>k-means</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="k-means" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-3"></span>
<span id="cb1-4">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># or RobustScaler</span></span>
<span id="cb1-5">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Elbow method</span></span>
<span id="cb1-8">I <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>):</span>
<span id="cb1-10">    kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>i) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sklearn은 기본적으로 k-means++</span></span>
<span id="cb1-11">    kmeans.fit(df_scaled)</span>
<span id="cb1-12">    I.append(kmeans.inertia_)</span>
<span id="cb1-13">plt.plot(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>), I, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k 선택 후 군집화</span></span>
<span id="cb1-16">kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-17">kmeans.fit(df_scaled)</span>
<span id="cb1-18">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cluster'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans.labels_</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 군집 중심값 정보</span></span>
<span id="cb1-21">centers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.inverse_transform(kmeans.cluster_centers_)</span>
<span id="cb1-22">centers_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(centers, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'cluster_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(centers.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])])</span>
<span id="cb1-23">display(centers_df)</span></code></pre></div>
</div>
<ol type="1">
<li>Kmeans
<ul>
<li>polinominal 시간 안에 해결 가능</li>
<li>noise, outlier에 민감함</li>
<li>수치형만 처리 가능</li>
</ul></li>
<li>k-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함</li>
<li>k-prototype: 범주형, 수치형 섞인거 처리 가능</li>
<li>k-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함
<ul>
<li>PAM: Partitioning Around Medoids
<ul>
<li>scalability 문제 있음</li>
</ul></li>
<li>CLARA: sampling을 통해서 PAM의 scalability 문제를 해결
<ul>
<li>샘플링 과정에서 biased될 수 있음</li>
</ul></li>
<li>CLARANS: medoid 후보를 랜덤하게 선택함</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p>k-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함 ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)</p>
</blockquote>
<ul>
<li>Hierarchical methods</li>
</ul>
<ol type="1">
<li>top-down: divisive, dia</li>
<li>bottom-up: agglomerative</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>agglomerative</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="agglomerative" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.cluster.hierarchy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dendrogram, linkage. cut_tree</span>
<span id="cb2-2"></span>
<span id="cb2-3">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linkage(df_scaled, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ward'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 'single', 'complete', 'average', 'ward' 등등</span></span>
<span id="cb2-4"></span>
<span id="cb2-5">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cut_tree(z, n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).flatten()</span>
<span id="cb2-6"></span>
<span id="cb2-7">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dendogram(z, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index))</span>
<span id="cb2-8">plt.show()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>합쳐지는 거리</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="합쳐지는 거리" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">thr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dcoord'</span>])</span>
<span id="cb3-2">thr <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이</span></span>
<span id="cb3-3">thr[thr[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>].sort_values(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 70 이상의 높이에서 합쳐지는 군집들의 높이</span></span></code></pre></div>
</div>
<ul>
<li>거리기반 군집의 단점:
<ul>
<li>군집의 모양이 구형이 아닐 경우 찾기 어려움</li>
<li>군집의 갯수 결정하기 어려움</li>
<li>군집의 밀도가 높아야함</li>
</ul></li>
</ul>
</section>
<section id="density-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="density-based-methods">Density-based methods</h3>
<ul>
<li>다양한 모양의 군집을 찾을 수 있음</li>
<li>noise, outlier에 강함</li>
</ul>
<ol type="1">
<li>DBSCAN: 잡음 포인트는 군집에서 제외
<ol type="1">
<li>core point를 찾음(eps 이내에 minPts 이상 있는 점)</li>
<li>core point를 중심으로 군집을 확장
<ul>
<li>core point가 아닌 경우 확장 종료</li>
</ul></li>
</ol>
<ul>
<li>고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음</li>
<li>군집간 계층관계를 인식하기 어렵다</li>
<li>대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN eps 결정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="DBSCAN eps 결정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.neighbors <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> NearestNeighbors</span>
<span id="cb4-2"></span>
<span id="cb4-3">MIN_SAMPLES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb4-4">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb4-5"></span>
<span id="cb4-6">neigh <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NearestNeighbors(n_neighbors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES)</span>
<span id="cb4-7">neigh.fit(df_scaled)</span>
<span id="cb4-8">distances, indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> neigh.kneighbors(df_scaled)</span>
<span id="cb4-9"></span>
<span id="cb4-10">k_distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sort(distances[:, MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])[::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot 그리기</span></span>
<span id="cb4-13">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb4-14">plt.plot(k_distances)</span>
<span id="cb4-15">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data Points sorted by distance'</span>)</span>
<span id="cb4-16">plt.ylabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-th Nearest Neighbor Distance'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 자기 자신을 제외한 거리</span></span>
<span id="cb4-17">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'k-distance Graph'</span>)</span>
<span id="cb4-18">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-19">plt.show()</span></code></pre></div>
</div>
<ul>
<li>min samples 기준
<ul>
<li>2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.</li>
<li>이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기</li>
</ul></li>
<li>eps 기준
<ul>
<li>k-dist plot에서 급격히 꺾이는 지점</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="DBSCAN" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DBSCAN</span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 찾은 값으로 DBSCAN 적용</span></span>
<span id="cb5-4">eps_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">18</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 결정한 값</span></span>
<span id="cb5-5"></span>
<span id="cb5-6">db <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DBSCAN(eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eps_value, min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb5-7">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> db.labels_</span></code></pre></div>
</div>
<ol start="2" type="1">
<li>OPTICS: DBSCAN의 단점을 보완
<ul>
<li>군집의 밀도가 다를 때도 잘 처리함</li>
<li>군집의 계층 구조를 인식할 수 있음</li>
<li>minPts 파라미터가 필요함</li>
<li>얘로도 이상치 탐지 가능</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>OPTICS</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="OPTICS" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OPTICS</span>
<span id="cb6-2"></span>
<span id="cb6-3">optics <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OPTICS(min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb6-4">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optics.labels_</span></code></pre></div>
</div>
</section>
<section id="평가" class="level3">
<h3 class="anchored" data-anchor-id="평가">평가</h3>
<ul>
<li>silhuette score: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20s(i)%7D%7Bn%7D">
<ul>
<li>s(i): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bb(i)%20-%20a(i)%7D%7Bmax((a(i),%20b(i)))%7D">
<ul>
<li>a(i): 군집 내 노드간의 평균 거리</li>
<li>b(i): 가장 가까운 군집과의 노드 간 평균 거리</li>
</ul></li>
<li>1에 가까울 수록 좋음</li>
</ul></li>
<li>그 외 sklearn metrics 참고</li>
</ul>
</section>
</section>
<section id="차원-축소" class="level2">
<h2 class="anchored" data-anchor-id="차원-축소">차원 축소</h2>
<ul>
<li>PCA, LSA, t-SNE, UMAP, ICA, MDS, NMF 등등
<ul>
<li>정말 많은 방법들이 있다.</li>
</ul></li>
<li>요인분석에서 확인적 요인분석은 python에서 제공하는 library가 없는걸로 알고 있음</li>
</ul>
</section>
<section id="연관-분석" class="level2">
<h2 class="anchored" data-anchor-id="연관-분석">연관 분석</h2>
<div id="e9ca731d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TransactionEncoder</span>
<span id="cb7-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.frequent_patterns <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> apriori, association_rules</span>
<span id="cb7-3"></span>
<span id="cb7-4">transactions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb7-5">te <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TransactionEncoder()</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 전처리</span></span>
<span id="cb7-8">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>).values</span>
<span id="cb7-9">te_ary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> te.fit_transform(data)</span>
<span id="cb7-10">transactions[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(te_ary, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>te.columns_)</span>
<span id="cb7-11"></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈발패턴 생성</span></span>
<span id="cb7-13">fset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apriori(t, min_support<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, use_colnames<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb7-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"빈발패턴이 존재하지 않습니다."</span>)</span>
<span id="cb7-16"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 연관규칙 생성</span></span>
<span id="cb7-18">    rule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association_rules(fset, metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"confidence"</span>, min_threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb7-19">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_ant'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'antecedents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-20">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'consequents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-21">    display(rule[(rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lift'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.2</span>)].reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</div>
<ul>
<li>지지도: 전체 거래에서 특정 항목 집합이 나타나는 비율</li>
<li>신뢰도: 특정 항목 집합이 주어졌을 때 다른 항목</li>
<li>향상도: 두 항목 집합이 독립적인 경우에 비해 함께 나타날 가능성</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/06.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분산 분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/05.html</link>
  <description><![CDATA[ 




<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-09-07-10-39-35.png" class="img-fluid figure-img"></p>
<figcaption>검정 방법 선택 기준</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-09-07-10-45-15.png" class="img-fluid figure-img"></p>
<figcaption>모수 검정 방법 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, …), 각 케이스에 맞는 모형을 사용해야 함.</li>
</ul>
<section id="정규성-검정" class="level3">
<h3 class="anchored" data-anchor-id="정규성-검정">정규성 검정</h3>
<ul>
<li><p>표본이 정규분포를 따르는지 검정.</p></li>
<li><p>따르지 않더라도 중심극한정리에 의해 <strong>표본의 크기가 충분히 크면</strong> 모수 검정을 사용할 수 있다.</p></li>
<li><p><strong>shapiro wilk 검정</strong>: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>jarque-Bera</strong>: 대표본에 사용.</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>Q-Q plot</strong>: x축이 이론적 분위수, y축이 표본 분위수</p></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>normality test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="normality test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro, jarque_bera, zscore, probplot</span>
<span id="cb1-4"></span>
<span id="cb1-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span>
<span id="cb1-6">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(data)</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jarque_bera(data)</span>
<span id="cb1-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jarque-Bera Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-15"></span>
<span id="cb1-16">zdata <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> zscore(data)</span>
<span id="cb1-17">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-18"></span>
<span id="cb1-19">(osm, odr), (slope, intercept, r) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> probplot(zdata, plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-20">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Q-Q Plot"</span>)</span>
<span id="cb1-21"></span>
<span id="cb1-22">data.hist(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'skyblue'</span>, density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-23">sns.kdeplot(data, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'KDE'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-24">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Histogram"</span>)</span>
<span id="cb1-25"></span>
<span id="cb1-26">plt.show()</span></code></pre></div>
</div>
</section>
<section id="등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="등분산성-검정">등분산성 검정</h3>
<ul>
<li><strong>Barlett 검정</strong>: <strong>정규성을 만족하는 경우에만</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
<li><strong>Levene 검정</strong>: <strong>정규성을 만족하지 않는 경우에도</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>equal variance test</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="equal variance test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> bartlett, levene</span>
<span id="cb2-4"></span>
<span id="cb2-5">group1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb2-6">group2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>]</span>
<span id="cb2-7">group3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span>
<span id="cb2-8"></span>
<span id="cb2-9">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bartlett(group1, group2, group3)</span>
<span id="cb2-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Bartlett's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-11"></span>
<span id="cb2-12">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> levene(group1, group2, group3)</span>
<span id="cb2-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Levene's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="분산-분석" class="level2">
<h2 class="anchored" data-anchor-id="분산-분석">분산 분석</h2>
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-10-06-18-54-33.png" class="img-fluid"></p>
<ul>
<li>정규성을 만족 못할 경우, 서열척도 비모수 검정 사용 가능</li>
<li>코크란 Q 검정: 이항분포를 따르는 반복 측정 자료에 사용
<ul>
<li>만약 대응표본이 아닐 경우 (실패, 성공) 변수를 만들어서 독립성 검정 사용</li>
<li>만약 이항분포가 아닐 경우 프리드만 검정 고려</li>
</ul></li>
<li>독립성 검정의 cell의 기대도수가 5 미만인 경우, Boschloo’s test 혹은 Fisher’s Exact 사용
<ul>
<li>만약 2x2을 넘어갈 경우 몬테카를로 시뮬레이션 사용(python으로 구현하기 복잡하다.)</li>
</ul></li>
<li>크루스칼, 맨휘트니: 동점이 과하게 많을 경우 permutation test, 순열 분산 분석 사용 가능
<ul>
<li>하지만 ADP 환경의 scipy에서는 버전이 낮아서 permutation test를 쓸 수 없다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>사후 검정</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="사후 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.sandbox.stats.multicomp <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MultiComparison</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ttest_ind</span>
<span id="cb3-5"></span>
<span id="cb3-6">mc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultiComparison(data, groups).allpairtest(ttest_ind, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bonf'</span>)</span>
<span id="cb3-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(mc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb3-8"></span>
<span id="cb3-9">mc.plot_simultaneous()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</div>
<section id="적합성-검정" class="level3">
<h3 class="anchored" data-anchor-id="적합성-검정">적합성 검정</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>카이제곱 적합성 검정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="카이제곱 적합성 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb4-2"></span>
<span id="cb4-3">count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].value_counts().sort_index()</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈도 수가 5 미만인 경우 합침</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb4-8">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-9">count.drop([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모수 추정</span></span>
<span id="cb4-12">lam <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].mean()</span>
<span id="cb4-13">poi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> poisson(lam)</span>
<span id="cb4-14"></span>
<span id="cb4-15">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> count.values.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-16">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([(poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), </span>
<span id="cb4-17">       <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>poi.pmf(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)), </span>
<span id="cb4-18">       poi.sf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 마지막 값은 이상 값으로</span></span>
<span id="cb4-19">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> n</span>
<span id="cb4-20"></span>
<span id="cb4-21"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chisquare</span>
<span id="cb4-22"></span>
<span id="cb4-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추정한 모수 갯수 만큼 자유도 차감</span></span>
<span id="cb4-24">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chisquare(count.values, exp, ddof<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-25">p</span></code></pre></div>
</div>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="다변량-분산분석" class="level2">
<h2 class="anchored" data-anchor-id="다변량-분산분석">다변량 분산분석</h2>
<section id="way-anova" class="level3">
<h3 class="anchored" data-anchor-id="way-anova">2-way ANOVA</h3>
<ul>
<li>반복이 없는 경우 교효작용 검정은 불가능</li>
<li>정규성, 등분산성 검정은 잔차에 대해 수행
<ul>
<li>모든 집단이 정규성과 등분산성을 만족하면 오차는 N(0, σ²)를 따른다.</li>
<li>고로 오차의 추정값인 잔차가 정규성과 등분산성을 만족하는지 검정으로 대체 가능</li>
</ul></li>
</ul>
<div id="bf81e60f" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb5-3"></span>
<span id="cb5-4"></span>
<span id="cb5-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)+C(성별):C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data).fit()</span>
<span id="cb5-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb5-7">atab</span></code></pre></div>
</div>
<div id="3788f989" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro</span>
<span id="cb6-2"></span>
<span id="cb6-3">residuals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb6-4"></span>
<span id="cb6-5">shapiro_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(residuals)</span>
<span id="cb6-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test on Residuals: Statistic=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>statistic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pvalue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div id="79e484d1" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>residuals, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-2">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb7-3">plt.show()</span></code></pre></div>
</div>
<ul>
<li>교효작용이 유의하지 않을 경우 오차항에 pooling을 한다.</li>
</ul>
<div id="3a564690" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb8-3"></span>
<span id="cb8-4"></span>
<span id="cb8-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tmp).fit()</span>
<span id="cb8-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb8-7">atab</span></code></pre></div>
</div>
<ul>
<li>main 효과의 사후검정은 anova와 동일
<ul>
<li>하지만 interation 효과가 유의할 경우, 주효과는 무의미하다.</li>
</ul></li>
<li>interaction 효과는 시각적으로 보여주는게 좋다.</li>
</ul>
<div id="9f17fd39" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">grouped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tmp.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>].mean().reset_index()</span>
<span id="cb9-2">pivot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grouped.pivot(index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>, values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>)</span>
<span id="cb9-3">pivot.plot(marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb9-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'평균 성적'</span>)</span>
<span id="cb9-5">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별-교육방법 교호작용 효과'</span>)</span>
<span id="cb9-6">plt.legend(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>)</span>
<span id="cb9-7">plt.tight_layout()</span>
<span id="cb9-8">plt.show()</span></code></pre></div>
</div>
<ul>
<li>2way 이상은 해석이 어려워서 잘 사용하지 않는다.</li>
</ul>
</section>
<section id="반복측정-분산분석" class="level3">
<h3 class="anchored" data-anchor-id="반복측정-분산분석">반복측정 분산분석</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>repeated measures anova</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="repeated measures anova" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pingouin <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pg</span>
<span id="cb10-2"></span>
<span id="cb10-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>],</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">78</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">85</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">82</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">86</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">88</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">83</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>]</span>
<span id="cb10-7">})</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 반복측정 ANOVA 수행 및 구형성 검정, 보정 포함</span></span>
<span id="cb10-10">aov <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg.rm_anova(dv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>, within<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>, subject<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, correction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-11">display(aov)</span></code></pre></div>
</div>
<ul>
<li>sphericity가 true이면 구형성 만족</li>
<li>p-unc: 구형성 보정 전 p-value</li>
<li>p-GG-corr: Greenhouse-Geisser 보정 후 p-value
<ul>
<li>구형성 위반 시 사용</li>
</ul></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/05.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/04.html</link>
  <description><![CDATA[ 




<section id="가정" class="level2">
<h2 class="anchored" data-anchor-id="가정">가정</h2>
<ul>
<li><strong>선형성</strong>: 종속변수와 독립변수 간의 관계는 선형이다.</li>
<li><strong>정규성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 정규분포이다.</li>
<li><strong>등분산성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 동일한 분산을 갖는다.</li>
<li><strong>독립성</strong>: 모든 <strong>잔차값</strong>은 서로 독립이다.</li>
</ul>
</section>
<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<ul>
<li>먼저 모델링을 진행한 후, 잔차를 통해 가정을 검정한다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>linear regression test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="linear regression test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb1-2"></span>
<span id="cb1-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb1-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, Xc).fit()</span>
<span id="cb1-5">resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<section id="이상치-영향치-처리" class="level3">
<h3 class="anchored" data-anchor-id="이상치-영향치-처리">이상치, 영향치 처리</h3>
<ul>
<li>레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차
<ul>
<li>Cook’s distance</li>
<li>Leverage</li>
<li>그 외 DFFITS, DFBETAS 등</li>
</ul></li>
</ul>
<div id="937504e7" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid.mean()</span>
<span id="cb2-2">std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid.std()</span>
<span id="cb2-3">outlier_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> (resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> std)</span>
<span id="cb2-4"></span>
<span id="cb2-5">influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_influence()</span>
<span id="cb2-6">cooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> influence.cooks_distance[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-7">influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(resid) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> model.df_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-8"></span>
<span id="cb2-9">outlier <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> resid[outlier_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> influence].index</span>
<span id="cb2-10"></span>
<span id="cb2-11">y_log <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.drop(outlier)</span>
<span id="cb2-12">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X.drop(outlier))</span>
<span id="cb2-13">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, Xc).fit()</span>
<span id="cb2-14">resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
</section>
<section id="다중공선성-검정" class="level3">
<h3 class="anchored" data-anchor-id="다중공선성-검정">다중공선성 검정</h3>
<ol type="1">
<li>전 변수 집합 대상</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>multicollinearity test</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="multicollinearity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">cor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.corr()</span>
<span id="cb3-2">cond_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.cond(cor)</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Condition Number:"</span>, cond_num)</span></code></pre></div>
</div>
<ul>
<li><strong>30을 초과</strong>하면 다중공선성이 높다고 판단한다.</li>
<li>선형 종속 가능성을 봄.</li>
<li>statsmodels의 ols를 사용해도 볼 수 있음</li>
<li>scaling이 선행되어야 함.</li>
<li>초과 시 해석:
<ul>
<li>독립변수의 전체 차원이 부족한 경우</li>
<li>표본을 더 모으거나 새로운 변수를 도입</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>VIF test</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="VIF test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.outliers_influence <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> variance_inflation_factor</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> check_vif(X, y):</span>
<span id="cb4-5">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb4-6">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, X)</span>
<span id="cb4-7">    model.fit()</span>
<span id="cb4-8">    vif_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>])</span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(model.exog_names)):</span>
<span id="cb4-10">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.exog_names[i]</span>
<span id="cb4-11">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> variance_inflation_factor(model.exog, i)</span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> vif_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-13"></span>
<span id="cb4-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(check_vif(X, y))</span></code></pre></div>
</div>
<ul>
<li>특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우</li>
<li><strong>VIF가 10</strong>을 넘을 경우</li>
<li>덜 중요하다면 제거</li>
<li>중요하다면 변수에 대한 독립적 정보 보강(세분화, …)</li>
</ul>
<ol start="3" type="1">
<li>두 변수의 상관계수가 높은 경우</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>correlation heatmap</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="correlation heatmap" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb5-2"></span>
<span id="cb5-3">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>))</span>
<span id="cb5-4">sns.heatmap(cor, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span></code></pre></div>
</div>
<ul>
<li>둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)</li>
<li>둘 중 하나를 제거</li>
</ul>
</section>
<section id="정규성" class="level3">
<h3 class="anchored" data-anchor-id="정규성">정규성</h3>
<ul>
<li>ols의 summary를 통해 확인 가능</li>
<li>혹은 EDA 과정에서 사용한 정규성 검정 참조</li>
</ul>
</section>
<section id="선형성-등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="선형성-등분산성-검정">선형성, 등분산성 검정</h3>
<div id="b88f7c4a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb6-3"></span>
<span id="cb6-4">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.resid, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb6-5">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb6-6">plt.show()</span></code></pre></div>
</div>
<ul>
<li>선형성:
<ul>
<li>잔차도가 어떠한 패턴도 보이지 않아야 한다.</li>
</ul></li>
<li>등분산성:
<ul>
<li>잔차도가 일정한 폭을 가져야 한다.</li>
</ul></li>
</ul>
<div id="4bdf4d63" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.diagnostic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> het_breuschpagan</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Breusch-Pagan 검정</span></span>
<span id="cb7-4">lm, lm_pvalue, fvalue, f_pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> het_breuschpagan(resid, Xc)</span>
<span id="cb7-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Breusch-Pagan p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lm_pvalue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 잔차의 정규성 만족시 lm_pvalue, 그 외 f_pvalue 사용</span></span>
<span id="cb7-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> lm_pvalue <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>:</span>
<span id="cb7-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"등분산성 가정을 만족합니다."</span>)</span>
<span id="cb7-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"등분산성 가정을 위반합니다."</span>)</span></code></pre></div>
</div>
</section>
<section id="독립성" class="level3">
<h3 class="anchored" data-anchor-id="독립성">독립성</h3>
<ul>
<li>독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다.
<ul>
<li>더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.</li>
<li>2에 가까울수록 독립성 만족</li>
<li>statsmodels의 ols로 확인 가능</li>
</ul></li>
</ul>
</section>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ol type="1">
<li>범주형 변수 처리:
<ul>
<li>더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현
<ul>
<li>각 범주의 회귀계수는 기준 범주와의 차이를 의미</li>
</ul></li>
</ul></li>
<li>이상치 / 영향점: 관측값 제거</li>
<li>선형성 위반: 독립변수 변환, GAM</li>
<li>정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM</li>
<li>다중공산성 위반: 다중공산성 파트 참고
<ul>
<li>혹은 변수 선택법을 사용</li>
</ul></li>
</ol>
<ul>
<li>가정 만족할 때까지 검정, 전처리 계속 반복</li>
</ul>
<section id="변수-변환" class="level3">
<h3 class="anchored" data-anchor-id="변수-변환">변수 변환</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-09-20-18-38-36.png" class="img-fluid figure-img"></p>
<figcaption>회귀 모델 수정 λ</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/img/2025-09-20-18-55-55.png" class="img-fluid figure-img"></p>
<figcaption>경험적인 적절한 λ</figcaption>
</figure>
</div>
<ul>
<li>최적의 λ는 최대 우도 추정법으로 구할 수 있다.</li>
<li>변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.</li>
<li>일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.
<ul>
<li>아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.</li>
<li>통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>box cox transformation</strong></pre>
</div>
<div class="sourceCode" id="cb8" data-filename="box cox transformation" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> boxcox</span>
<span id="cb8-2"></span>
<span id="cb8-3">y_transformed, best_lambda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boxcox(y)</span>
<span id="cb8-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Best lambda: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_lambda<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
<section id="변수-선택법" class="level3">
<h3 class="anchored" data-anchor-id="변수-선택법">변수 선택법</h3>
<ul>
<li>전진 선택법</li>
<li>후진 선택법</li>
<li>단계적 선택법</li>
<li>최적조합 선택법: 모든 조합 다 해봄</li>
<li>기준
<ul>
<li>R2, Adj R2</li>
<li>AIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법</li>
<li>BIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌</li>
<li>Mallows’ Cp</li>
</ul></li>
</ul>
</section>
</section>
<section id="규제-선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="규제-선형-회귀">규제 선형 회귀</h2>
<ul>
<li>지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦</li>
<li>독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)
<ul>
<li>일반적으로는 scale을 하든 안하든 r square에 차이가 없다.</li>
</ul></li>
</ul>
<ol type="1">
<li>릿지회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%CE%B2_j%5E2"></li>
<li>회귀계수 절댓값을 0에 가깝게 함</li>
<li>하지만 0으로 만들지는 않음</li>
<li>작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.</li>
<li>회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음</li>
</ul></li>
<li>라쏘회귀:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%7C%CE%B2_j%7C"></li>
<li>회귀계수를 0으로 만들 수 있음</li>
<li>변수 선택 효과</li>
<li>릿지보다 해석이 쉬움</li>
<li>일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음</li>
</ul></li>
<li>엘라스틱넷 회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB_1%5CSigma%20%7C%CE%B2_j%7C%20+%20%CE%BB_2%5CSigma%20%CE%B2_j%5E2"></li>
<li>릿지와 라쏘의 장점을 모두 가짐</li>
<li>변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦</li>
<li>독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음</li>
</ul></li>
</ol>
</section>
<section id="일반화-선형-회귀glm" class="level2">
<h2 class="anchored" data-anchor-id="일반화-선형-회귀glm">일반화 선형 회귀(GLM)</h2>
<ul>
<li>종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우
<ul>
<li>이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.</li>
<li>포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.</li>
<li>따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.</li>
</ul></li>
</ul>
<section id="logistic-회귀" class="level3">
<h3 class="anchored" data-anchor-id="logistic-회귀">Logistic 회귀</h3>
<ul>
<li>종속변수가 범주형일 경우</li>
<li><img src="https://latex.codecogs.com/png.latex?z%20=%20%CE%B2_0%20+%20%CE%B2_1%20x_1%20+%20%CE%B2_2%20x_2%20+%20...%20+%20%CE%B2_n%20x_n"></li>
<li><img src="https://latex.codecogs.com/png.latex?p%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-z%7D%7D"></li>
<li>오즈: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7B1-p%7D"> = <img src="https://latex.codecogs.com/png.latex?e%5Ez"></li>
<li>오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(e%5E%7B%CE%B2_k%7D)%5Ek"></li>
</ul></li>
<li>LLR의 p-value가 낮다면, 모델이 통계적으로 유의미함을 의미</li>
<li>잔차 검정은 안함</li>
</ul>
</section>
<section id="포아송-회귀" class="level3">
<h3 class="anchored" data-anchor-id="포아송-회귀">포아송 회귀</h3>
<ul>
<li>종속변수가 count 데이터일 경우</li>
</ul>
<div id="22b82123" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb9-2"></span>
<span id="cb9-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb9-4"></span>
<span id="cb9-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, X, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.Poisson())</span>
<span id="cb9-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb9-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x_k">가 한 단위 증가할 때, 빈도 수가 <img src="https://latex.codecogs.com/png.latex?exp(%CE%B2_k)">배 증가</li>
<li>만약 관찰 시간이 다를 경우, offset로 np.log(df[관찰시간])을 넣어줘야 함</li>
<li>Deviance / DF_resid 가 1보다 크면 과산포, 작으면 과소산포
<ul>
<li>과산포: 사건발생 확률이 일정하지 않음</li>
<li>과산포 시 음이항 회귀 사용</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>음이항 회귀</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="음이항 회귀" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb10-2"></span>
<span id="cb10-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb10-4"></span>
<span id="cb10-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, Xc, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.NegativeBinomial())</span>
<span id="cb10-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb10-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li>Quasi-Poisson도 있지만, statsmodels에서는 제공하지 않음</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/04.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>전처리 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/03.html</link>
  <description><![CDATA[ 




<ul>
<li>참고: 이상치, 결측치, 불균형 처리 모두 원칙적으로는 train set에서만 fit 해야함.</li>
</ul>
<section id="결측치-처리" class="level2">
<h2 class="anchored" data-anchor-id="결측치-처리">결측치 처리</h2>
<ul>
<li>결측치가 발생하는 원인과 처리 전략
<ul>
<li>무작위 결측(Missing Completely at Random, MCAR): 결측치가 발생할 확률이 다른 모든 변수와 무관
<ul>
<li>예: 설문지 작성 중 무작위로 일부 페이지가 누락된 경우</li>
<li>이 경우, 결측치를 제거하거나 단순 대체해도 편향이 발생하지 않음</li>
</ul></li>
<li>조건부 무작위 결측(Missing at Random, MAR): 결측치가 발생할 확률이 관측된 데이터에만 의존
<ul>
<li>예: 고소득자일수록 소득 공개를 꺼려하는 경우 (교육수준이 높을수록 소득 결측 확률이 높음)</li>
<li>실무에서 가장 일반적인 가정: 대부분의 실제 데이터에서 결측 패턴은 관측된 변수들로 어느 정도 설명 가능</li>
<li>관측된 데이터를 활용한 예측 기반 대치법이 효과적</li>
</ul></li>
<li>비무작위 결측(Missing Not at Random, MNAR): 결측치가 발생할 확률이 결측된 값 자체와 관련
<ul>
<li>예: 극도로 낮은 소득자가 소득 공개를 꺼리는 경우</li>
<li>통계적 방법만으로는 해결이 어려우며, 도메인 지식이나 외부 정보가 필요</li>
<li><strong>실무에서는 MAR 가정 하에 처리 후, 민감도 분석을 통해 결측 메커니즘과 처리 방법의 적절성을 확인</strong>
<ul>
<li>민감도 분석: 다양한 결측치 처리 방법을 적용하여 결과를 비교
<ul>
<li>단순 방법(평균/중앙값 대치) vs 고급 방법(KNN, MICE)</li>
<li>단순 방법과 고급 방법의 결과가 비슷하면 → MCAR 가능성 높음, 단순 방법으로도 충분</li>
<li>고급 방법이 더 나은 성능을 보이면 → MAR 가능성 높음, 고급 방법 선택</li>
<li>어떤 방법을 써도 결과가 불안정하면 → MNAR 가능성, 도메인 지식과 추가 정보 필요</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>결측치 처리 방법
<ul>
<li>제거: 결측치가 적거나 무작위 결측일 때 사용</li>
<li>대치(대체):
<ul>
<li>일반적인 방법
<ul>
<li>시계열 데이터 o: 이전 값, 이후 값, 선형 보간법</li>
<li>시계열 데이터 x: 평균, 중앙값, 최빈값</li>
</ul></li>
<li>고급 대치법(과적합 발생 가능성 유의)
<ul>
<li>KNN 대치: 유사한 관측치의 값을 사용하여 결측치를 대체. 결측치가 없는 데이터로 예측</li>
<li>다변량 대치: 결측치를 다른 변수들의 값으로 예측하여 대체.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<section id="다변량-대치법" class="level3">
<h3 class="anchored" data-anchor-id="다변량-대치법">다변량 대치법</h3>
<div id="fc6901a5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMRegressor, LGBMClassifier</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multi_impute(df, categorical, max_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>):</span>
<span id="cb1-4">    df_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb1-5">    num_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical]</span>
<span id="cb1-6"></span>
<span id="cb1-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결측치가 적은 column 우선 처리</span></span>
<span id="cb1-8">    null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb1-9">    null_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> null_counts[null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].sort_values().index.tolist()</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 초기값 대체</span></span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical:</span>
<span id="cb1-13">        df_imp[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb1-14">        mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].mode(dropna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-15">        df_imp[col].fillna(mode[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-16"></span>
<span id="cb1-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb1-18">        mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].mean()</span>
<span id="cb1-19">        df_imp[col].fillna(mean, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-20"></span>
<span id="cb1-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 반복 임퓨팅</span></span>
<span id="cb1-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_iter):</span>
<span id="cb1-23">        prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.copy()</span>
<span id="cb1-24"></span>
<span id="cb1-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> null_cols:</span>
<span id="cb1-26">            idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].isnull()</span>
<span id="cb1-27">            idx_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>idx_missing</span>
<span id="cb1-28">            predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb1-29"></span>
<span id="cb1-30">            X_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_obs, predictors]</span>
<span id="cb1-31">            y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_obs, col]</span>
<span id="cb1-32">            X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_missing, predictors]</span>
<span id="cb1-33"></span>
<span id="cb1-34">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># column type에 따라 다른 모델 선택</span></span>
<span id="cb1-35">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LightGBM으로 randomforest를 사용한 이유: sklearn의 RandomForest는 categorical 변수를 직접 처리하지 못함</span></span>
<span id="cb1-36">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical:</span>
<span id="cb1-37">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(</span>
<span id="cb1-38">                    boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb1-39">                    bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, feature_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span></span>
<span id="cb1-40">                )</span>
<span id="cb1-41">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-42">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(</span>
<span id="cb1-43">                    boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb1-44">                    bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, feature_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span></span>
<span id="cb1-45">                )</span>
<span id="cb1-46">            model.fit(X_obs, y_obs)</span>
<span id="cb1-47">            y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb1-48">            df_imp.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb1-49">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df_imp.equals(prev):</span>
<span id="cb1-50">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb1-51">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df_imp</span></code></pre></div>
</div>
<ul>
<li>현재 결측치 처리 방법 중 가장 성능이 좋은걸로 알려져 있는 missing forest를 모방한 방법.
<ul>
<li>missforest 라이브러리는 adp 환경에서 설치 불가</li>
</ul></li>
<li>다중 대치를 안 하고 있지만, sklearn 공식 문서에 따르면 대부분 single 대치로도 충분하다고 한다.</li>
<li>참고로 이 방법은 missforest의 정확한 구현은 아니기 때문에, 시험에서는 다변량 대치법을 사용했다고만 쓰자.</li>
<li>만약 train set에서만 fit을 시키고 싶다면, 이 방법은 그냥 안 쓰는걸 추천.
<ul>
<li>그렇게 하려면 class 형태로 바꿔야 하는데, too much인가 싶은 느낌이 슬슬 나기 시작.</li>
</ul></li>
</ul>
<div id="7a1e75b4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MultiImputer:</span>
<span id="cb2-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], max_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb2-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> categorical</span>
<span id="cb2-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_iter</span>
<span id="cb2-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n_estimators</span>
<span id="cb2-6">        </span>
<span id="cb2-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb2-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb2-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-11">        </span>
<span id="cb2-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-13">        df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb2-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical]</span>
<span id="cb2-15">        </span>
<span id="cb2-16">        null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb2-17">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> null_counts[null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].sort_values().index.tolist()</span>
<span id="cb2-18">        </span>
<span id="cb2-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 초기값 계산 및 저장 (Train의 통계량만 사용!)</span></span>
<span id="cb2-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-21">            df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb2-22">            mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mode(dropna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-23">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mode[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-24">            df[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-25">        </span>
<span id="cb2-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_:</span>
<span id="cb2-27">            mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mean()</span>
<span id="cb2-28">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean</span>
<span id="cb2-29">            df[col].fillna(mean, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-30">        </span>
<span id="cb2-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter):</span>
<span id="cb2-32">            prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb2-33">            </span>
<span id="cb2-34">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_:</span>
<span id="cb2-35">                idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[col].isnull()</span>
<span id="cb2-36">                idx_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>idx_missing</span>
<span id="cb2-37">                    </span>
<span id="cb2-38">                predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb2-39">                X_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_obs, predictors]</span>
<span id="cb2-40">                y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_obs, col]</span>
<span id="cb2-41">                X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_missing, predictors]</span>
<span id="cb2-42">                </span>
<span id="cb2-43">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 선택 및 학습</span></span>
<span id="cb2-44">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-45">                    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(</span>
<span id="cb2-46">                        boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, </span>
<span id="cb2-47">                        n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators,</span>
<span id="cb2-48">                        bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, </span>
<span id="cb2-49">                        bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-50">                        feature_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,</span>
<span id="cb2-51">                        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-52">                    )</span>
<span id="cb2-53">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb2-54">                    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(</span>
<span id="cb2-55">                        boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, </span>
<span id="cb2-56">                        n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators,</span>
<span id="cb2-57">                        bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, </span>
<span id="cb2-58">                        bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-59">                        feature_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,</span>
<span id="cb2-60">                        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-61">                    )</span>
<span id="cb2-62">                </span>
<span id="cb2-63">                model.fit(X_obs, y_obs)</span>
<span id="cb2-64">                </span>
<span id="cb2-65">                y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb2-66">                df.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb2-67">                </span>
<span id="cb2-68">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb2-69">            </span>
<span id="cb2-70">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df.equals(prev):</span>
<span id="cb2-71">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-72">        </span>
<span id="cb2-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb2-74">    </span>
<span id="cb2-75">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb2-76">        df_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb2-77">        </span>
<span id="cb2-78">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-79">            df_imp[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb2-80">            df_imp[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-81">        </span>
<span id="cb2-82">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_:</span>
<span id="cb2-83">            df_imp[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-84">        </span>
<span id="cb2-85">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter):</span>
<span id="cb2-86">            prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.copy()</span>
<span id="cb2-87">            </span>
<span id="cb2-88">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_:                   </span>
<span id="cb2-89">                idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[col].isnull()</span>
<span id="cb2-90">                </span>
<span id="cb2-91">                predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df_imp.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb2-92">                X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_missing, predictors]</span>
<span id="cb2-93">                </span>
<span id="cb2-94">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_[col]</span>
<span id="cb2-95">                y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb2-96">                df_imp.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb2-97">            </span>
<span id="cb2-98">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df_imp.equals(prev):</span>
<span id="cb2-99">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-100">        </span>
<span id="cb2-101">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df_imp</span>
<span id="cb2-102">    </span>
<span id="cb2-103">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit_transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-104">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fit(X, y)</span>
<span id="cb2-105">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.transform(X)</span></code></pre></div>
</div>
<ul>
<li>혹시몰라서 class 형태로 만든 다변량 대치법</li>
<li>이걸 언제 다 적고 있을까</li>
</ul>
</section>
</section>
<section id="이상치-처리" class="level2">
<h2 class="anchored" data-anchor-id="이상치-처리">이상치 처리</h2>
<ul>
<li>이상치 탐지는 EDA 참고</li>
<li>이상치는 그 자체로 중요한 정보일 수 있다.</li>
<li>따라서 수집중 오류, 측정 과정에서의 오류 등과 같은 상황이 의심되는 비정상적인 데이터가 아니라면 제거하지 않는 것이 좋다.</li>
</ul>
</section>
<section id="불균형-처리" class="level2">
<h2 class="anchored" data-anchor-id="불균형-처리">불균형 처리</h2>
<ul>
<li>잘 알려져 있는 방법 대충 잘 선택해서 사용.</li>
<li>딱히 SOTA(가장 좋은 방법)가 있지 않음.</li>
</ul>
<section id="성능-비교" class="level3">
<h3 class="anchored" data-anchor-id="성능-비교">성능 비교</h3>
<ul>
<li>각각의 처리법에 대해서 어떤게 제일 좋은지 비교</li>
</ul>
<div id="bcd02a19" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMRegressor</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb3-3"></span>
<span id="cb3-4">df[cat_cols] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cat_cols].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-5"></span>
<span id="cb3-6">df1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.dropna()</span>
<span id="cb3-7">df2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[num_cols].fillna(df[num_cols].mean())</span>
<span id="cb3-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cat_cols:</span>
<span id="cb3-9">    df2[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target[col].fillna(target[col].mode()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb3-10">df3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_impute(df, categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_cols)</span>
<span id="cb3-11"></span>
<span id="cb3-12">candis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-13">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Nothing'</span>, df), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 종속변수는 결측치가 없어야 된다.</span></span>
<span id="cb3-14">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Just Delete'</span>, df1),</span>
<span id="cb3-15">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Simple Impute'</span>, df2),</span>
<span id="cb3-16">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MultiImputer'</span>, df3)</span>
<span id="cb3-17">]</span>
<span id="cb3-18"></span>
<span id="cb3-19">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb3-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> name, df <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> candis:</span>
<span id="cb3-21">    rf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, </span>
<span id="cb3-22">                       n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb3-23">                       bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, </span>
<span id="cb3-24">                       bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb3-25">                       feature_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>)</span>
<span id="cb3-26">    result[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(rf, </span>
<span id="cb3-27">                                   df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), </span>
<span id="cb3-28">                                   df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>], </span>
<span id="cb3-29">                                   scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'neg_mean_squared_error'</span>)</span>
<span id="cb3-30">                                    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># classifier인 경우 'accuracy' 등등</span></span>
<span id="cb3-31"></span>
<span id="cb3-32">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb3-33"></span>
<span id="cb3-34">means <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>result.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># regressor인 경우, classifier인 경우 양수</span></span>
<span id="cb3-35">errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result.std()</span>
<span id="cb3-36"></span>
<span id="cb3-37">means.plot.barh(xerr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>errors, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb3-38">ax.set_yticks(np.arange(means.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]))</span>
<span id="cb3-39">ax.set_yticklabels(means.index)</span>
<span id="cb3-40">plt.show()</span></code></pre></div>
</div>
<ul>
<li>Imbalance 처리 방법에 대해서도 적용 가능</li>
</ul>
</section>
</section>
<section id="encoding" class="level2">
<h2 class="anchored" data-anchor-id="encoding">Encoding</h2>
<ul>
<li>LGBM, XGBoost, CatBoost 등은 범주형 변수를 직접 처리 가능</li>
<li>범주형 인코딩의 경우, 문제에서 범주 값이 주어지거나, 실무에서 범주 값이 미리 정해져 있는 경우 굳이 train, test set 나눈 후에 fit할 필요는 없다고 생각한다. (개인적인 의견이다.)</li>
</ul>
</section>
<section id="scaling" class="level2">
<h2 class="anchored" data-anchor-id="scaling">Scaling</h2>
<ul>
<li>이상치가 많다면 Robust Scaling, 정규분포를 따른다면 Standard Scaling
<ul>
<li>Standard Scaling 이후 회귀계수의 단위는 표준편차 단위</li>
<li>Robust Scaling 이후 회귀계수의 단위는 IQR 단위</li>
</ul></li>
<li>일반적으로 같은 scaling 방법을 사용하는게 권장된다.</li>
</ul>
</section>
<section id="feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="feature-selection">Feature Selection</h2>
<section id="filter-method" class="level3">
<h3 class="anchored" data-anchor-id="filter-method">Filter Method</h3>
<ol type="1">
<li>basic methods</li>
</ol>
<ul>
<li>하나의 값만 가지는 변수 혹은 분산이 너무 낮은 변수는 제거</li>
<li>하지만 분산이 낮아도 종속변수와 관계가 있을 수 있다. 일단은 mutual info 까지 확인을 해보는게 좋을 듯.</li>
</ul>
<div id="61ca6497" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> VarianceThreshold</span>
<span id="cb4-2"></span>
<span id="cb4-3">sel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VarianceThreshold(threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb4-4"></span>
<span id="cb4-5">selected_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.columns[sel.get_support()]</span>
<span id="cb4-6">df_selected <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[selected_cols]</span></code></pre></div>
</div>
<ol start="2" type="1">
<li>Univariate selection methods</li>
</ol>
<div id="744b05b3" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SelectKBest, SelectPercentile, chi2</span>
<span id="cb5-2"></span>
<span id="cb5-3">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectKBest(chi2, k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최상위 2개</span></span>
<span id="cb5-4"></span>
<span id="cb5-5">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectPercentile(chi2, percentile<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상위 10%</span></span></code></pre></div>
</div>
<ul>
<li>카이제곱 검정량이 가장 높은 변수들만 선택.</li>
<li>연속형 변수에 대해서는 KBinsDiscretizer 작업 필요.</li>
</ul>
<div id="d578ea10" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SelectKBest, SelectPercentile, mutual_info_classif, mutual_info_regression</span>
<span id="cb6-2"></span>
<span id="cb6-3">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectKBest(mutual_info_classif, k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최상위 2개</span></span>
<span id="cb6-4">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectPercentile(mutual_info_regression, percentile<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상위 10%</span></span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>mutual info</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="mutual info" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mutual_info_classif</span>
<span id="cb7-2"></span>
<span id="cb7-3">mi_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mutual_info_classif(X, y,</span>
<span id="cb7-4">                              discrete_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cat_cols <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns])</span>
<span id="cb7-5">mi_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(mi_score, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.coulmns).sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-6">mi_result</span></code></pre></div>
</div>
<ul>
<li>EDA 파트 mutual info 참고</li>
<li>추가: 상관계수, ANOVA F-value 등등 사용 가능</li>
</ul>
</section>
<section id="wrapper-method" class="level3">
<h3 class="anchored" data-anchor-id="wrapper-method">Wrapper Method</h3>
<ul>
<li>forward, backward, 등등</li>
</ul>
</section>
<section id="embedded-method" class="level3">
<h3 class="anchored" data-anchor-id="embedded-method">Embedded Method</h3>
<ul>
<li>L1, L2, Elasticnet 등등</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/03.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/02.html</link>
  <description><![CDATA[ 




<section id="load-library" class="level2">
<h2 class="anchored" data-anchor-id="load-library">Load Library</h2>
<div id="3210dbb0" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> platform</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span></code></pre></div>
</div>
</section>
<section id="settings" class="level2">
<h2 class="anchored" data-anchor-id="settings">Settings</h2>
<div id="5637542e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> platform.system() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Darwin'</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#맥</span></span>
<span id="cb2-4">        plt.rc(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font'</span>, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AppleGothic'</span>) </span>
<span id="cb2-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> platform.system() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Windows'</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#윈도우</span></span>
<span id="cb2-6">        plt.rc(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font'</span>, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Malgun Gothic'</span>) </span>
<span id="cb2-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> platform.system() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Linux'</span>: <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#리눅스 (구글 콜랩)</span></span>
<span id="cb2-8">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#!wget "https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf"</span></span>
<span id="cb2-9">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#!mv malgun.ttf /usr/share/fonts/truetype/</span></span>
<span id="cb2-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#import matplotlib.font_manager as fm </span></span>
<span id="cb2-11">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#fm._rebuild() </span></span>
<span id="cb2-12">        plt.rc(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font'</span>, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Malgun Gothic'</span>) </span>
<span id="cb2-13">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'axes.unicode_minus'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#한글 폰트 사용시 마이너스 폰트 깨짐 해결</span></span>
<span id="cb2-14">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span></code></pre></div>
</div>
</section>
<section id="load-data" class="level2">
<h2 class="anchored" data-anchor-id="load-data">Load Data</h2>
<div id="82d66155" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'df.csv'</span>)</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"df shape: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
<section id="missing-values" class="level2">
<h2 class="anchored" data-anchor-id="missing-values">Missing Values</h2>
<div id="d357ff90" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(df.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>())</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(df[df.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)])</span></code></pre></div>
</div>
<ul>
<li>결측치 처리는 전처리 참조</li>
</ul>
</section>
<section id="단일-column-분석" class="level2">
<h2 class="anchored" data-anchor-id="단일-column-분석">단일 column 분석</h2>
<div id="178b3752" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.head()</span></code></pre></div>
</div>
<div id="c1ddea8e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df.info()</span></code></pre></div>
</div>
<ul>
<li>순서형 변수는 연속형으로 처리하던가 범주형으로 처리하던가 알아서 정하면 됨.</li>
<li>순서형 그 자체로 보고 분석할 수 있는 방법들도 있긴 있음.</li>
</ul>
<section id="범주형" class="level3">
<h3 class="anchored" data-anchor-id="범주형">범주형</h3>
<div id="c48fedb9" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.describe(include<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>)</span></code></pre></div>
</div>
<div id="c5fb3eaa" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb8-2"></span>
<span id="cb8-3">cat_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb8-4">num_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cat_cols:</span>
<span id="cb8-7">    target_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].value_counts().sort_index()</span>
<span id="cb8-8">    target_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].value_counts(normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).sort_index()</span>
<span id="cb8-9"></span>
<span id="cb8-10">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb8-11"></span>
<span id="cb8-12">    target_counts.plot.bar(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb8-13">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Count"</span>)</span>
<span id="cb8-14">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Level"</span>)</span>
<span id="cb8-15">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb8-16"></span>
<span id="cb8-17">    wedges, texts, autotexts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].pie(target_ratio, autopct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%1.1f%%</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb8-18">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Distribution"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, pad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>)</span>
<span id="cb8-19">    plt.show()</span>
<span id="cb8-20"></span>
<span id="cb8-21">    summary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([target_counts, target_ratio], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-22">    summary.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'counts'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'probs'</span>]</span>
<span id="cb8-23">    summary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> summary.sort_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'counts'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb8-24">    display(summary)</span></code></pre></div>
</div>
</section>
<section id="연속형" class="level3">
<h3 class="anchored" data-anchor-id="연속형">연속형</h3>
<div class="sourceCode" id="cb9" data-filname="basic statistic" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">detailed_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb9-3">    stats_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mean'</span>: df[col].mean(),</span>
<span id="cb9-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Median'</span>: df[col].median(),</span>
<span id="cb9-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Min'</span>: df[col].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(),</span>
<span id="cb9-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Max'</span>: df[col].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(),</span>
<span id="cb9-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Std Dev'</span>: df[col].std(),</span>
<span id="cb9-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'IQR'</span>: df[col].quantile(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> df[col].quantile(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>),</span>
<span id="cb9-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Skewness'</span>: df[col].skew(),</span>
<span id="cb9-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Kurtosis'</span>: df[col].kurtosis(),</span>
<span id="cb9-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'CV (%)'</span>: df[col].std() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> df[col].mean() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb9-13">    }</span>
<span id="cb9-14">    detailed_stats.append(pd.Series(stats_dict, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>col))</span>
<span id="cb9-15"></span>
<span id="cb9-16">display(pd.concat(detailed_stats, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).T)</span></code></pre></div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>distribution</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="distribution" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> probplot</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb10-4">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb10-5"></span>
<span id="cb10-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 히스토그램과 KDE</span></span>
<span id="cb10-7">    df[col].hist(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'skyblue'</span>, density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-8">    sns.kdeplot(df[col], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'KDE'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb10-9">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Distribution'</span>)</span>
<span id="cb10-10">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(col)</span>
<span id="cb10-11">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb10-12"></span>
<span id="cb10-13">    mean_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mean()</span>
<span id="cb10-14">    median_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].median()</span>
<span id="cb10-15">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].axvline(mean_val, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Mean: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mean_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb10-16">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].axvline(median_val, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Median: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>median_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb10-17">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].legend()</span>
<span id="cb10-18"></span>
<span id="cb10-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 박스플롯</span></span>
<span id="cb10-20">    sns.boxplot(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df[col], ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lightgreen'</span>)</span>
<span id="cb10-21"></span>
<span id="cb10-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Q-Q 플롯</span></span>
<span id="cb10-23">    probplot(df[col], plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb10-24"></span>
<span id="cb10-25">    plt.show()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>normality</strong></pre>
</div>
<div class="sourceCode" id="cb11" data-filename="normality" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro, anderson, jarque_bera, normaltest</span>
<span id="cb11-2"></span>
<span id="cb11-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb11-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 소표본 적합</span></span>
<span id="cb11-5">    stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(df[col].dropna())</span>
<span id="cb11-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-7"></span>
<span id="cb11-8">    result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anderson(df[col].dropna())</span>
<span id="cb11-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Anderson-Darling Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>statistic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, critical values=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>critical_values<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, significance levels=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>significance_level<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-10"></span>
<span id="cb11-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 왜도, 첨도 기준 판별. 대표본 적합</span></span>
<span id="cb11-12">    stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jarque_bera(df[col].dropna())</span>
<span id="cb11-13">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jarque-Bera Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-14"></span>
<span id="cb11-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 왜도, 첨도 기준 판별. 범용적</span></span>
<span id="cb11-16">    stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normaltest(df[col].dropna())</span>
<span id="cb11-17">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"D'Agostino's K-squared Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb11-18"></span>
<span id="cb11-19">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>outliers - IQR</strong></pre>
</div>
<div class="sourceCode" id="cb12" data-filename="outliers - IQR" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">outliers_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb12-2">outliers_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.index, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns)</span>
<span id="cb12-3"></span>
<span id="cb12-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb12-5">    Q1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].quantile(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span>
<span id="cb12-6">    Q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].quantile(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)</span>
<span id="cb12-7">    IQR <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> Q1</span>
<span id="cb12-8"></span>
<span id="cb12-9">    lower_bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> IQR</span>
<span id="cb12-10">    upper_bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> IQR</span>
<span id="cb12-11">    out_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> lower_bound) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> (df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> upper_bound)</span>
<span id="cb12-12">    outliers_mask[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|=</span> out_idx</span>
<span id="cb12-13">    outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[out_idx]</span>
<span id="cb12-14">    </span>
<span id="cb12-15">    outliers_info.append(pd.Series({</span>
<span id="cb12-16">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Q1'</span>: Q1,</span>
<span id="cb12-17">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Q3'</span>: Q3,</span>
<span id="cb12-18">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'IQR'</span>: IQR,</span>
<span id="cb12-19">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lower Bound'</span>: lower_bound,</span>
<span id="cb12-20">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Upper Bound'</span>: upper_bound,</span>
<span id="cb12-21">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outlier Count'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(outliers),</span>
<span id="cb12-22">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outlier %'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(outliers) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(df) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb12-23">    }, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>col))</span>
<span id="cb12-24">outliers_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat(outliers_info, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-25">outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[outliers_mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb12-26">outliers[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reason'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outliers_mask[outliers_mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>.join(x.index[x]), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-27">normal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>outliers_mask).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb12-28"></span>
<span id="cb12-29">display(outliers_info.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).T)</span>
<span id="cb12-30">display(outliers)</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>outliers - Z-score</strong></pre>
</div>
<div class="sourceCode" id="cb13" data-filename="outliers - Z-score" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">outliers_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb13-2">outliers_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.index, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns)</span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb13-5">    mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mean()</span>
<span id="cb13-6">    std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].std()</span>
<span id="cb13-7">    z_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mean) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> std</span>
<span id="cb13-8">    out_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (z_scores.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb13-9">    outliers_mask[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|=</span> out_idx</span>
<span id="cb13-10">    outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[out_idx]</span>
<span id="cb13-11"></span>
<span id="cb13-12">    outliers_info.append(pd.Series({</span>
<span id="cb13-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mean'</span>: mean,</span>
<span id="cb13-14">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Std Dev'</span>: std,</span>
<span id="cb13-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outlier Count'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(outliers),</span>
<span id="cb13-16">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outlier %'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(outliers) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(df) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb13-17">    }, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>col))</span>
<span id="cb13-18">outliers_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat(outliers_info, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb13-19">outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[outliers_mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb13-20">outliers[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reason'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outliers_mask[outliers_mask.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>.join(x.index[x]), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb13-21">normal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>outliers_mask).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">all</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb13-22"></span>
<span id="cb13-23">display(outliers_info.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).T)</span>
<span id="cb13-24">display(outliers)</span></code></pre></div>
</div>
<ul>
<li>이상치 처리는 전처리 참조</li>
<li>HUOE(Heterogeneous Univariate Outlier Ensembles) 방법도 있음 (하지만 이를 위한 library는 없는듯)</li>
</ul>
</section>
</section>
<section id="변수간-관계" class="level2">
<h2 class="anchored" data-anchor-id="변수간-관계">변수간 관계</h2>
<section id="연속-연속" class="level3">
<h3 class="anchored" data-anchor-id="연속-연속">연속, 연속</h3>
<ul>
<li>산점도로 시각화</li>
<li>spearman 상관계수 확인</li>
<li>spearman - pearson 차이로 비선형 관계 확인 가능
<ul>
<li>0.1 이상 차이가 나는지 확인. (공식 기준 아님)</li>
</ul></li>
</ul>
</section>
<section id="범주-연속" class="level3">
<h3 class="anchored" data-anchor-id="범주-연속">범주, 연속</h3>
<div id="631f4541" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb14-2">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb14-3"></span>
<span id="cb14-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> target_val <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>].unique():</span>
<span id="cb14-5">        subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> target_val][col]</span>
<span id="cb14-6">        subset.hist(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>target_val, density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-7">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Distribution'</span>)</span>
<span id="cb14-8">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(col)</span>
<span id="cb14-9">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb14-10"></span>
<span id="cb14-11">    plt.show()</span></code></pre></div>
</div>
<ul>
<li><p>이런 식으로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.</p></li>
<li><p>분포 확인 후, ANOVA나 Kruskal-Wallis 등으로 그룹 간 차이 검정 진행</p></li>
<li><p>t 검정을 사용했다면, Cohen’s d 등으로 효과 크기 확인 가능</p></li>
<li><p>자세한건 분산 분석 파트 참조</p></li>
</ul>
</section>
<section id="범주-범주" class="level3">
<h3 class="anchored" data-anchor-id="범주-범주">범주, 범주</h3>
<ul>
<li>시각화는 마찬가지로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.</li>
<li>chi-square 검정 등으로 독립성 검정 진행</li>
<li>Cramer’s V 등으로 연관성 정도 확인
<ul>
<li>두 변수가 level이 2개면 phi 계수로 확인 가능 (Cramer’s V와 동일한 값 나옴)</li>
</ul></li>
</ul>
<div id="d3618080" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chi2_contingency</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.contingency <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> association</span>
<span id="cb15-3"></span>
<span id="cb15-4">contingency_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.crosstab(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb15-5">chi2, p, dof, expected <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chi2_contingency(contingency_table, correction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb15-6">cramers_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(contingency_table, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cramer'</span>)</span>
<span id="cb15-7"></span>
<span id="cb15-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Chi-square Test: chi2=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>chi2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb15-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Cramer's V: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>cramers_v<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="독립변수-vs-종속변수" class="level2">
<h2 class="anchored" data-anchor-id="독립변수-vs-종속변수">독립변수 vs 종속변수</h2>
<ul>
<li>간단한 트리 모델링 이후 feature importance 확인 가능</li>
<li>0.01이 넘는 변수들만 따로 분석 진행</li>
</ul>
<div id="5a23840c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mutual_info_regression</span>
<span id="cb16-2"></span>
<span id="cb16-3">mi_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mutual_info_regression(df[num_cols], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># df['target']이 범주형이면 mutual_info_classif 사용</span></span>
<span id="cb16-4">mi_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(mi_score, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_cols).sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb16-5">mi_result</span></code></pre></div>
</div>
<ul>
<li>mutual information로 선형 + 비선형 관계 확인 가능</li>
<li>구체적으로 어떤 관계인지는 scatter plot으로 확인 (연속 + 연속이면)</li>
</ul>
<div id="086bbb64" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pingouin <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pg</span>
<span id="cb17-2"></span>
<span id="cb17-3">pcorr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg.partial_corr(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>, covar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>)</span>
<span id="cb17-4">pcorr</span></code></pre></div>
</div>
<ul>
<li>하나의 변수를 통제했을 때, 두 변수 간의 상관관계를 분석</li>
<li>내부적으로는 회귀분석을 사용</li>
<li>도메인 지식으로 통제변수 선정해서 진행
<ul>
<li>혹은 EDA 과정에서 발견한 관계로 선정 (한 마디로 본인이 알아서 잘 선정하기)</li>
</ul></li>
<li>인과분석을 하면 이 과정은 필요 없으나, ADP 환경에서 인과분석 라이브러리는 제공 안하는듯</li>
</ul>
<section id="interation-effect" class="level3">
<h3 class="anchored" data-anchor-id="interation-effect">Interation Effect</h3>
<ul>
<li>두 개 이상의 독립변수가 결합하여 종속변수에 미치는 영향</li>
<li>유효한 상호작용 효과가 있다면 새로운 변수를 만들어서 분석에 포함 (예: new_var = var1 * var2)</li>
<li>자세한건 분산분석 파트 참조</li>
</ul>
</section>
<section id="polinorminal-relationships" class="level3">
<h3 class="anchored" data-anchor-id="polinorminal-relationships">Polinorminal Relationships</h3>
<ul>
<li>회귀분석 파트 참조</li>
</ul>
</section>
</section>
<section id="다변량-분석" class="level2">
<h2 class="anchored" data-anchor-id="다변량-분석">다변량 분석</h2>
<section id="차원-축소" class="level3">
<h3 class="anchored" data-anchor-id="차원-축소">차원 축소</h3>
<div id="4fcc4ce4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">scatter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.scatter(X_pca[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], X_pca[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'viridis'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>)</span>
<span id="cb18-2">plt.xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'PC1 (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>explained_var_ratio[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> variance)'</span>)</span>
<span id="cb18-3">plt.ylabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'PC2 (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>explained_var_ratio[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1%}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> variance)'</span>)</span>
<span id="cb18-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PCA Projection by Support Level'</span>)</span>
<span id="cb18-5">plt.colorbar(scatter, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Support Level'</span>)</span></code></pre></div>
</div>
<div id="1e236188" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.manifold <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TSNE</span>
<span id="cb19-2"></span>
<span id="cb19-3">tsne <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TSNE(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb19-4">tsne_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tsne.fit_transform(df[num_cols])</span>
<span id="cb19-5"></span>
<span id="cb19-6">plt.scatter(tsne_result[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], tsne_result[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>])</span></code></pre></div>
</div>
<ul>
<li>UMAP이 전역적으로는 구조를 더 잘 보존하고, 속도도 빠르다.</li>
<li>ADP 환경에서 UMAP 라이브러리는 제공 안하는듯</li>
<li>애초에 2차원으로 정사영해서 plot한 이 자료들로 의미있는 해석을 더 할 수 있는지 의문</li>
</ul>
</section>
<section id="dbscan-isolation-forest-등으로-이상치-탐지-가능" class="level3">
<h3 class="anchored" data-anchor-id="dbscan-isolation-forest-등으로-이상치-탐지-가능">DBSCAN, Isolation Forest 등으로 이상치 탐지 가능</h3>
<ul>
<li>DBSCAN은 비지도 학습 파트 군집분석 참조</li>
</ul>
<div id="11f52a3a" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OrdinalEncoder</span>
<span id="cb20-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> IsolationForest</span>
<span id="cb20-3"></span>
<span id="cb20-4">iso_forest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> IsolationForest()</span>
<span id="cb20-5">oe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OrdinalEncoder()</span>
<span id="cb20-6"></span>
<span id="cb20-7">df_encoded <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb20-8">df_encoded[cat_cols] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> oe.fit_transform(df[cat_cols])</span>
<span id="cb20-9">outlier_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iso_forest.fit_predict(df_encoded)</span>
<span id="cb20-10">outliers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[outlier_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb20-11">normal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[outlier_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb20-12">display(outliers)</span></code></pre></div>
</div>
<ul>
<li>해당 결과들을 voting 등으로 종합해서 이상치로 판단할 수도 있음.</li>
<li>자세한건 modeling 파트의 voting 부분 참조</li>
<li>개인적인 생각: 지워야 하는 이상치는 측정 과정에서의 오류 등인데, 이런 종합적인 방법은 그런 이상치를 찾는데 유용하지는 않은듯 하다.
<ul>
<li>그래서 이상치 분석 목적이 아니라면 이 방법은 굳이 안써도 되지 않을까</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>outlier 연속형 분포</strong></pre>
</div>
<div class="sourceCode" id="cb21" data-filename="outlier 연속형 분포" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb21-2">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb21-3"></span>
<span id="cb21-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 박스플롯</span></span>
<span id="cb21-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (이상치 변수 생성 필요)</span></span>
<span id="cb21-6">    sns.boxplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'이상치'</span>], y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>normal[col], ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df)</span>
<span id="cb21-7">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Boxplot by Outlier Status'</span>)</span>
<span id="cb21-8"></span>
<span id="cb21-9">    sns.kdeplot(normal[col], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Normal'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb21-10">    sns.kdeplot(outliers[col], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outliers'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb21-11">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> Distribution by Outlier Status'</span>)</span>
<span id="cb21-12"></span>
<span id="cb21-13">    plt.show()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>outlier 범주형 분포</strong></pre>
</div>
<div class="sourceCode" id="cb22" data-filename="outlier 범주형 분포" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cat_cols:</span>
<span id="cb22-2">    normal_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normal[col].value_counts().sort_index()</span>
<span id="cb22-3">    outlier_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outliers[col].value_counts().sort_index()</span>
<span id="cb22-4"></span>
<span id="cb22-5">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb22-6"></span>
<span id="cb22-7">    normal_counts.plot.bar(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb22-8">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Count"</span>)</span>
<span id="cb22-9">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Level"</span>)</span>
<span id="cb22-10">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb22-11"></span>
<span id="cb22-12">    outlier_counts.plot.bar(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'orange'</span>)</span>
<span id="cb22-13">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Count (Outliers)"</span>)</span>
<span id="cb22-14">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Level"</span>)</span>
<span id="cb22-15">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb22-16"></span>
<span id="cb22-17">    combined_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Normal'</span>: normal_counts, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outliers'</span>: outlier_counts}).fillna(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb22-18">    combined_df.plot.bar(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], stacked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb22-19">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].set_title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Count Comparison"</span>)</span>
<span id="cb22-20">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s Level"</span>)</span>
<span id="cb22-21">    ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb22-22">    plt.show()</span></code></pre></div>
</div>
<ul>
<li>그 외 profiling 분석이나 맨 휘트니, 카이제곱 독립성 검정 등으로 이상치와 정상치의 차이 분석 가능</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/02.html</guid>
  <pubDate>Sun, 28 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>모델링, 평가 템플릿</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/01.html</link>
  <description><![CDATA[ 




<section id="모델-정의" class="level2">

<div id="319b284b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler, OneHotEncoder, OrdinalEncoder</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.compose <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ColumnTransformer</span>
<span id="cb1-4"></span>
<span id="cb1-5">preprocessor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ColumnTransformer(transformers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-6">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num'</span>, StandardScaler(), num_features), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># outlier가 많다면 RobustScaler를 고려하자</span></span>
<span id="cb1-7">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cat'</span>, OneHotEncoder(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'first'</span>), cat_features),</span>
<span id="cb1-8">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ord'</span>, OrdinalEncoder(), ord_features) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 순서형 변수. 하지만 이렇게 처리하는 것보다 그냥 DataFrame.map()으로 직접 인코딩해주는게 더 나을듯</span></span>
<span id="cb1-9">])</span>
<span id="cb1-10">pipeline <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb1-11">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'preprocessor'</span>, preprocessor),</span>
<span id="cb1-12">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model_name'</span>, YourModel(random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>))</span>
<span id="cb1-13">])</span></code></pre></div>
</div>
<ul>
<li>category 형을 있는 그대로 처리하고 싶다면, pipeline보다는 LGBM, CatBoost, XGBoost 등의 모델을 바로 사용하는게 더 나음.
<ul>
<li>sklearn은 데이터들을 numpy array로 변환하기 때문에 category 형을 유지하지 못함</li>
</ul></li>
</ul>
</section>
<section id="grid-search-bayesian-optimization" class="level2">
<h2 class="anchored" data-anchor-id="grid-search-bayesian-optimization">Grid Search &amp; Bayesian Optimization</h2>
<div id="a57f017a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GridSearchCV</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score, precision_recall_fscore_support, roc_auc_score</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_absolute_error, r2_score, mean_squared_error</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 파라미터 이름 앞에 pipeline에서 사용한 '모델이름__'을 붙여야 함</span></span>
<span id="cb2-6">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb2-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>],</span>
<span id="cb2-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>],</span>
<span id="cb2-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],</span>
<span id="cb2-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]</span>
<span id="cb2-11">}</span>
<span id="cb2-12"></span>
<span id="cb2-13">grid_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GridSearchCV(estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pipeline, </span>
<span id="cb2-14">                           param_grid<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, </span>
<span id="cb2-15">                           cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, </span>
<span id="cb2-16">                           scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'accuracy'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error</span></span>
<span id="cb2-17">grid_search.fit(X_train, y_train)</span>
<span id="cb2-18">best_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grid_search.best_estimator_</span>
<span id="cb2-19"></span>
<span id="cb2-20">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> best_model.predict(X_test)</span>
<span id="cb2-21"></span>
<span id="cb2-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 분류 scoring ============</span></span>
<span id="cb2-23"></span>
<span id="cb2-24">y_pred_proba <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> best_model.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중클래스인 경우 [:, 1] 제거</span></span>
<span id="cb2-25"></span>
<span id="cb2-26">test_acc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb2-27">test_precision, test_recall, test_f1, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> precision_recall_fscore_support(</span>
<span id="cb2-28">    y_test, </span>
<span id="cb2-29">    y_pred, </span>
<span id="cb2-30">    average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary'</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택</span></span>
<span id="cb2-31">)</span>
<span id="cb2-32">test_auc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, y_pred_proba) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 multi_class='ovr', 'ovo' 중 선택</span></span>
<span id="cb2-33"></span>
<span id="cb2-34"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_acc<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-35"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_precision: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_precision<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-36"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_recall: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_recall<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-37"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_f1_score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_f1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-38"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_auc: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>test_auc<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-39"></span>
<span id="cb2-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 회귀 scoring ============</span></span>
<span id="cb2-41"></span>
<span id="cb2-42">rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_squared_error(y_test, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb2-43">mae <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb2-44">r2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_test, y_pred)</span>
<span id="cb2-45"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_rmse: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-46"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_mae: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mae<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-47"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_r2: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<ul>
<li>👇 시험에서 사용 불가. 하지만 kaggle이나 dacon에서 주로 사용됨</li>
</ul>
<div id="a4151298" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> precision_recall_fscore_support, accuracy_score, roc_auc_score</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_absolute_error, r2_score, mean_squared_error</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective(trial):</span>
<span id="cb3-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델에 적합한 파라미터에 맞게 수정</span></span>
<span id="cb3-8">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb3-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_estimators"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>),</span>
<span id="cb3-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_features'</span>: trial.suggest_categorical(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_features"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sqrt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log2'</span>]),</span>
<span id="cb3-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">110</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb3-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_samples_split"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb3-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_samples_leaf"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-14">    }</span>
<span id="cb3-15">    pipeline.set_params(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb3-16"></span>
<span id="cb3-17">    cv_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(pipeline,</span>
<span id="cb3-18">                               X_train,</span>
<span id="cb3-19">                               y_train,</span>
<span id="cb3-20">                               cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb3-21">                               scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'accuracy'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error</span></span>
<span id="cb3-22">    mean_cv_accuracy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv_score.mean()</span>
<span id="cb3-23">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mean_cv_accuracy</span>
<span id="cb3-24"></span>
<span id="cb3-25">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'maximize'</span>)</span>
<span id="cb3-26">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb3-27"></span>
<span id="cb3-28">pipeline.set_params(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>study.best_params)</span>
<span id="cb3-29">pipeline.fit(X_train, y_train)</span>
<span id="cb3-30"></span>
<span id="cb3-31">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipeline.predict(X_test)</span>
<span id="cb3-32"></span>
<span id="cb3-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 분류 scoring ============</span></span>
<span id="cb3-34"></span>
<span id="cb3-35">y_pred_proba <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipeline.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb3-36"></span>
<span id="cb3-37"></span>
<span id="cb3-38">test_acc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb3-39">test_precision, test_recall, test_f1, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> precision_recall_fscore_support(</span>
<span id="cb3-40">    y_test, </span>
<span id="cb3-41">    y_pred, </span>
<span id="cb3-42">    average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary'</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택해서 사용</span></span>
<span id="cb3-43">)</span>
<span id="cb3-44">test_auc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, y_pred_proba) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 multiclass='ovr', 'ovo' 중 선택해서 사용</span></span>
<span id="cb3-45"></span>
<span id="cb3-46"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_accuracy:"</span>, test_acc)</span>
<span id="cb3-47"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_precision:"</span>, test_precision)</span>
<span id="cb3-48"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_recall:"</span>, test_recall)</span>
<span id="cb3-49"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_f1_score:"</span>, test_f1)</span>
<span id="cb3-50"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_auc:"</span>, test_auc)</span>
<span id="cb3-51"></span>
<span id="cb3-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 회귀 scoring ============</span></span>
<span id="cb3-53"></span>
<span id="cb3-54">rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_squared_error(y_test, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb3-55">mae <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb3-56">r2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_test, y_pred)</span>
<span id="cb3-57"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_rmse: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-58"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_mae: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mae<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-59"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_r2: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<ul>
<li>👇 시험에서 사용 가능</li>
</ul>
<div id="34891639" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, STATUS_OK, fmin, tpe, Trials</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb4-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> precision_recall_fscore_support, accuracy_score, roc_auc_score</span>
<span id="cb4-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_absolute_error, r2_score, mean_squared_error</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 파라미터 공간 정의</span></span>
<span id="cb4-7">search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb4-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'n_estimators'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>),</span>
<span id="cb4-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_features'</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_features'</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sqrt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log2'</span>]),</span>
<span id="cb4-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">110</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb4-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_samples_split'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb4-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_samples_leaf'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-13">}</span>
<span id="cb4-14"></span>
<span id="cb4-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(params):</span>
<span id="cb4-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 파라미터 타입 변환 (hyperopt는 float로 반환하므로 int로 변환 필요)</span></span>
<span id="cb4-17">    params_int <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb4-18">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>]),</span>
<span id="cb4-19">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_features'</span>: params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_features'</span>],</span>
<span id="cb4-20">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>]),</span>
<span id="cb4-21">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>]),</span>
<span id="cb4-22">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>])</span>
<span id="cb4-23">    }</span>
<span id="cb4-24">    pipeline.set_params(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params_int)</span>
<span id="cb4-25">    </span>
<span id="cb4-26">    cv_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(pipeline,</span>
<span id="cb4-27">                               X_train,</span>
<span id="cb4-28">                               y_train,</span>
<span id="cb4-29">                               cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb4-30">                               scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'accuracy'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error</span></span>
<span id="cb4-31">    mean_cv_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cv_score.mean()</span>
<span id="cb4-32">    </span>
<span id="cb4-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># hyperopt는 최소화하므로 음수 반환 (최대화하려는 경우)</span></span>
<span id="cb4-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'loss'</span>: <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>mean_cv_score, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'status'</span>: STATUS_OK}</span>
<span id="cb4-35"></span>
<span id="cb4-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최적화 실행</span></span>
<span id="cb4-37">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb4-38">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb4-39">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space,</span>
<span id="cb4-40">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb4-41">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb4-42">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb4-43"></span>
<span id="cb4-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최적 파라미터 적용 (타입 변환 포함)</span></span>
<span id="cb4-45">best_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb4-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__n_estimators'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'n_estimators'</span>]),</span>
<span id="cb4-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_features'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sqrt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log2'</span>][<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_features'</span>])],</span>
<span id="cb4-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__max_depth'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb4-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_split'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_samples_split'</span>]),</span>
<span id="cb4-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier__min_samples_leaf'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_samples_leaf'</span>])</span>
<span id="cb4-51">}</span>
<span id="cb4-52"></span>
<span id="cb4-53">pipeline.set_params(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>best_params)</span>
<span id="cb4-54">pipeline.fit(X_train, y_train)</span>
<span id="cb4-55"></span>
<span id="cb4-56">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipeline.predict(X_test)</span>
<span id="cb4-57"></span>
<span id="cb4-58"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 분류 scoring ============</span></span>
<span id="cb4-59"></span>
<span id="cb4-60">y_pred_proba <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipeline.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-61"></span>
<span id="cb4-62">test_acc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb4-63">test_precision, test_recall, test_f1, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> precision_recall_fscore_support(</span>
<span id="cb4-64">    y_test, </span>
<span id="cb4-65">    y_pred, </span>
<span id="cb4-66">    average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary'</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택해서 사용</span></span>
<span id="cb4-67">)</span>
<span id="cb4-68">test_auc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, y_pred_proba) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 다중 클래스인 경우 multiclass='ovr', 'ovo' 중 선택해서 사용</span></span>
<span id="cb4-69"></span>
<span id="cb4-70"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_accuracy:"</span>, test_acc)</span>
<span id="cb4-71"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_precision:"</span>, test_precision)</span>
<span id="cb4-72"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_recall:"</span>, test_recall)</span>
<span id="cb4-73"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_f1_score:"</span>, test_f1)</span>
<span id="cb4-74"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_auc:"</span>, test_auc)</span>
<span id="cb4-75"></span>
<span id="cb4-76"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ============ 회귀 scoring ============</span></span>
<span id="cb4-77"></span>
<span id="cb4-78">rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_squared_error(y_test, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-79">mae <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb4-80">r2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> r2_score(y_test, y_pred)</span>
<span id="cb4-81"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_rmse: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-82"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_mae: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>mae<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-83"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"test_r2: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>r2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
<section id="모델-평가-visualization" class="level2">
<h2 class="anchored" data-anchor-id="모델-평가-visualization">모델 평가 visualization</h2>
<section id="roc-auc" class="level3">
<h3 class="anchored" data-anchor-id="roc-auc">ROC AUC</h3>
<div id="ab16c708" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_curve, auc</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ROC 곡선 계산</span></span>
<span id="cb5-5">y_pred_proba <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> best_model.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 양성 클래스 확률</span></span>
<span id="cb5-6">fpr, tpr, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_curve(y_test, y_pred_proba)</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ROC 곡선 시각화</span></span>
<span id="cb5-9">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb5-10">plt.plot(fpr, tpr, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, lw<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'ROC curve (AUC = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>roc_auc<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>)</span>
<span id="cb5-11">plt.plot([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, lw<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Random'</span>)</span>
<span id="cb5-12">plt.xlim([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>])</span>
<span id="cb5-13">plt.ylim([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>])</span>
<span id="cb5-14">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'False Positive Rate'</span>)</span>
<span id="cb5-15">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'True Positive Rate'</span>)</span>
<span id="cb5-16">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ROC Curve'</span>)</span>
<span id="cb5-17">plt.legend(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lower right'</span>)</span>
<span id="cb5-18">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-19">plt.show()</span></code></pre></div>
</div>
</section>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">Feature Importance</h3>
<div id="826baa45" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-3"></span>
<span id="cb6-4">importances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pipeline.named_steps[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'classifier'</span>].feature_importances_</span>
<span id="cb6-5">ftr_importance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(importances, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.columns)</span>
<span id="cb6-6">ftr_top <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ftr_importance.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 원하는 수 만큼 설정</span></span>
<span id="cb6-7">sns.barplot(ftr_top20, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ftr_top.index)</span>
<span id="cb6-8">plt.show()</span></code></pre></div>
</div>
</section>
<section id="drop-column-importance" class="level3">
<h3 class="anchored" data-anchor-id="drop-column-importance">Drop Column importance</h3>
<div id="acc16862" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.base <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> clone</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> X_train.columns:</span>
<span id="cb7-4">    X_train_dropped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[col])</span>
<span id="cb7-5">    X_test_dropped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_test.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[col])</span>
<span id="cb7-6"></span>
<span id="cb7-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간의 단축을 위해 하이퍼파라미터는 그냥 그대로 사용</span></span>
<span id="cb7-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 엄밀하게 하고 싶다면 각 iteration마다 다시 튜닝</span></span>
<span id="cb7-9">    model_dropped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clone(best_model)</span>
<span id="cb7-10">    model_dropped.fit(X_train_dropped, y_train)</span>
<span id="cb7-11"></span>
<span id="cb7-12">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_dropped.score(X_test_dropped, y_test)</span>
<span id="cb7-13">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>col<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'s difference:"</span>, best_model.score(X_test, y_test) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> score) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 혹은 다른 평가 지표 사용</span></span></code></pre></div>
</div>
</section>
<section id="permutation-importance" class="level3">
<h3 class="anchored" data-anchor-id="permutation-importance">Permutation importance</h3>
<div id="477028d0" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> eli5</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> eli5.sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PermutationImportance</span>
<span id="cb8-3"></span>
<span id="cb8-4">perm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PermutationImportance(best_model).fit(X_test, y_test)</span>
<span id="cb8-5">eli5.show_weights(perm, feature_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test.columns.tolist())</span></code></pre></div>
</div>
<ul>
<li>음수 값은 제거해도 된다는 뜻</li>
<li>단점: 상관관계가 높은 feature에 대해서 비현실적인 데이터 조합이 생성될 가능성이 높다.
<ul>
<li>(ex. shuffle을 통해 키 180cm, 몸무게 30kg의 데이터가 조합되는 경우)</li>
</ul></li>
</ul>
</section>
</section>
<section id="ensemble" class="level2">
<h2 class="anchored" data-anchor-id="ensemble">ensemble</h2>
<ul>
<li>stacking:
<ul>
<li>여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.</li>
<li>메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.</li>
</ul></li>
<li>voting, averaging:
<ul>
<li>여러 모델을 학습시킨 후, 각 모델의 예측 결과를 투표(voting)하거나 평균(averaging)하여 최종 예측을 수행한다.</li>
<li>분류 문제에서는 다수결 투표(hard voting) 또는 확률 평균(soft voting)을 사용하고, 회귀 문제에서는 단순 평균 또는 가중 평균을 사용한다.</li>
</ul></li>
<li>bagging
<ul>
<li>vs cross validation:
<ul>
<li>cross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님</li>
<li>bagging은 분산을 줄이기 위해 사용함</li>
</ul></li>
</ul></li>
<li>boosting: sequentially 학습
<ul>
<li>이전 모델의 오차를 보완하는 방식으로 학습한다.</li>
</ul></li>
</ul>
<div id="baea40c9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StackingClassifier, StackingRegressor</span>
<span id="cb9-2"></span>
<span id="cb9-3">stacking_lf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StackingClassifier(estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb9-4">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lr'</span>, LogisticRegression()),</span>
<span id="cb9-5">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>, DecisionTreeClassifier()),</span>
<span id="cb9-6">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rf'</span>, RandomForestClassifier())</span>
<span id="cb9-7">], final_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LogisticRegression())</span>
<span id="cb9-8"></span>
<span id="cb9-9">stacking_rf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StackingRegressor(estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb9-10">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lr'</span>, LinearRegression()),</span>
<span id="cb9-11">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>, DecisionTreeRegressor()),</span>
<span id="cb9-12">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rf'</span>, RandomForestRegressor())</span>
<span id="cb9-13">], final_estimator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LinearRegression())</span>
<span id="cb9-14"></span>
<span id="cb9-15">stacking_lf.fit(X_train, y_train)</span>
<span id="cb9-16">stacking_rf.fit(X_train, y_train)</span>
<span id="cb9-17">stacking_lf.predict(X_test)</span>
<span id="cb9-18">stacking_rf.predict(X_test)</span></code></pre></div>
</div>
<div id="ee09d049" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> VotingClassifier, VotingRegressor</span>
<span id="cb10-2"></span>
<span id="cb10-3">voting_lf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VotingClassifier(estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb10-4">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lr'</span>, LogisticRegression()),</span>
<span id="cb10-5">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>, DecisionTreeClassifier()),</span>
<span id="cb10-6">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rf'</span>, RandomForestClassifier())</span>
<span id="cb10-7">], voting<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'soft'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># or hard</span></span>
<span id="cb10-8"></span>
<span id="cb10-9"></span>
<span id="cb10-10">voting_rf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VotingRegressor(estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb10-11">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lr'</span>, LinearRegression()),</span>
<span id="cb10-12">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>, DecisionTreeRegressor()),</span>
<span id="cb10-13">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rf'</span>, RandomForestRegressor())</span>
<span id="cb10-14">], weights<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb10-15"></span>
<span id="cb10-16"></span>
<span id="cb10-17">voting_lf.fit(X_train, y_train)</span>
<span id="cb10-18">voting_rf.fit(X_train, y_train)</span>
<span id="cb10-19">voting_lf.predict(X_test)</span>
<span id="cb10-20">voting_rf.predict(X_test)</span></code></pre></div>
</div>
</section>
<section id="모델-파라미터" class="level2">
<h2 class="anchored" data-anchor-id="모델-파라미터">모델 파라미터</h2>
<ul>
<li>아래부터는 ai의 도움을 받았습니다.</li>
<li>적당히 몇 개만 골라서 사용하면 됩니다.</li>
</ul>
<section id="logistic-회귀분석" class="level3">
<h3 class="anchored" data-anchor-id="logistic-회귀분석">Logistic 회귀분석</h3>
<div id="b585c6e2" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb11-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb11-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__penalty"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'elasticnet'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 정규화 방법</span></span>
<span id="cb11-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__C"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 정규화 강도 (작을수록 강한 정규화)</span></span>
<span id="cb11-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__l1_ratio"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># elasticnet일 때만 사용</span></span>
<span id="cb11-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__solver"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lbfgs'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'newton-cg'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sag'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saga'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최적화 알고리즘</span></span>
<span id="cb11-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_iter"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최대 반복 횟수</span></span>
<span id="cb11-8">}</span>
<span id="cb11-9"></span>
<span id="cb11-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb11-11">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb11-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__penalty"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"penalty"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'elasticnet'</span>]),</span>
<span id="cb11-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__C"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)),</span>
<span id="cb11-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__l1_ratio"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"l1_ratio"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>),</span>
<span id="cb11-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__solver"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"solver"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saga'</span>]),</span>
<span id="cb11-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_iter"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_iter"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb11-17">}</span></code></pre></div>
</div>
</section>
<section id="knn" class="level3">
<h3 class="anchored" data-anchor-id="knn">KNN</h3>
<div id="67681ee1" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb12-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb12-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_neighbors"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">21</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k값 (이웃의 수)</span></span>
<span id="cb12-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__weights"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'uniform'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'distance'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 가중치 방법</span></span>
<span id="cb12-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__algorithm"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ball_tree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kd_tree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'brute'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 알고리즘</span></span>
<span id="cb12-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__leaf_size"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 크기 (ball_tree, kd_tree용)</span></span>
<span id="cb12-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__p"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 거리 측도 (1: 맨하탄, 2: 유클리드)</span></span>
<span id="cb12-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__metric"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minkowski'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'euclidean'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'manhattan'</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 거리 함수</span></span>
<span id="cb12-9">}</span>
<span id="cb12-10"></span>
<span id="cb12-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb12-12">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb12-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_neighbors"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_neighbors"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">21</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb12-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__weights"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"weights"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'uniform'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'distance'</span>]),</span>
<span id="cb12-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__algorithm"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"algorithm"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ball_tree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kd_tree'</span>]),</span>
<span id="cb12-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__leaf_size"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"leaf_size"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb12-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__p"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"p"</span>, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]),</span>
<span id="cb12-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__metric"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"metric"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minkowski'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'euclidean'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'manhattan'</span>])</span>
<span id="cb12-19">}</span></code></pre></div>
</div>
</section>
<section id="support-vector-machine" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machine">Support Vector Machine</h3>
<div id="2d4eea03" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb13-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb13-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__C"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 정규화 파라미터 (작을수록 강한 정규화)</span></span>
<span id="cb13-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__kernel"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'poly'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rbf'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sigmoid'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 커널 함수</span></span>
<span id="cb13-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__degree"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># poly 커널 차수 (poly일 때만 사용)</span></span>
<span id="cb13-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__gamma"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scale'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 커널 계수 (rbf, poly, sigmoid용)</span></span>
<span id="cb13-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__coef0"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 커널의 독립항 (poly, sigmoid용)</span></span>
<span id="cb13-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__shrinking"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수축 휴리스틱 사용 여부</span></span>
<span id="cb13-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__tol"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 정지 기준 허용 오차</span></span>
<span id="cb13-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_iter"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최대 반복 횟수 (-1: 제한 없음)</span></span>
<span id="cb13-11">}</span>
<span id="cb13-12"></span>
<span id="cb13-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb13-14">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb13-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__C"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)),</span>
<span id="cb13-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__kernel"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"kernel"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'poly'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rbf'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sigmoid'</span>]),</span>
<span id="cb13-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__degree"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"degree"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># poly일 때만</span></span>
<span id="cb13-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__gamma"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gamma"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scale'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gamma_float"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))]),</span>
<span id="cb13-19">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__coef0"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"coef0"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb13-20">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__shrinking"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"shrinking"</span>, [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>]),</span>
<span id="cb13-21">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__tol"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tol"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span>), np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>)),</span>
<span id="cb13-22">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_iter"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_iter"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb13-23">}</span></code></pre></div>
</div>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">Random Forest</h3>
<div id="4f9c8e86" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb14-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb14-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 개수</span></span>
<span id="cb14-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__criterion"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gini'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'entropy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log_loss'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 불순도 측정 기준</span></span>
<span id="cb14-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리의 최대 깊이 (None: 제한 없음)</span></span>
<span id="cb14-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_samples_split"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 내부 노드 분할에 필요한 최소 샘플 수</span></span>
<span id="cb14-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_samples_leaf"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드에 필요한 최소 샘플 수</span></span>
<span id="cb14-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_weight_fraction_leaf"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드에 필요한 최소 가중치 비율</span></span>
<span id="cb14-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_features"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sqrt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log2'</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 분할에서 고려할 피처 수</span></span>
<span id="cb14-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_leaf_nodes"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최대 리프 노드 수</span></span>
<span id="cb14-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_impurity_decrease"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분할에 필요한 최소 불순도 감소량</span></span>
<span id="cb14-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bootstrap"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부트스트랩 사용 여부</span></span>
<span id="cb14-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__oob_score"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># OOB 스코어 계산 여부</span></span>
<span id="cb14-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_samples"</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 트리에 사용할 샘플 비율</span></span>
<span id="cb14-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__ccp_alpha"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 복잡도 가지치기 파라미터</span></span>
<span id="cb14-16">}</span>
<span id="cb14-17"></span>
<span id="cb14-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb14-19">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb14-20">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_estimators"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>),</span>
<span id="cb14-21">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__criterion"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"criterion"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gini'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'entropy'</span>]),</span>
<span id="cb14-22">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth"</span>, [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth_val"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)]),</span>
<span id="cb14-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_samples_split"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_samples_split"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb14-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_samples_leaf"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_samples_leaf"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb14-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_features"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_features"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sqrt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log2'</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>]),</span>
<span id="cb14-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_impurity_decrease"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_impurity_decrease"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>),</span>
<span id="cb14-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bootstrap"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bootstrap"</span>, [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>]),</span>
<span id="cb14-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_samples"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_samples"</span>, [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_samples_val"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>)]),</span>
<span id="cb14-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__ccp_alpha"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ccp_alpha"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb14-30">}</span></code></pre></div>
</div>
</section>
<section id="xgboost" class="level3">
<h3 class="anchored" data-anchor-id="xgboost">XGBOOST</h3>
<div id="cc9076dc" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb15-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb15-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부스팅 라운드 수</span></span>
<span id="cb15-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 학습률 (eta)</span></span>
<span id="cb15-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 최대 깊이</span></span>
<span id="cb15-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_weight"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드의 최소 가중치 합</span></span>
<span id="cb15-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__gamma"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분할에 필요한 최소 손실 감소</span></span>
<span id="cb15-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 행 샘플링 비율</span></span>
<span id="cb15-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bytree"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 열 샘플링 비율 (트리별)</span></span>
<span id="cb15-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bylevel"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 열 샘플링 비율 (레벨별)</span></span>
<span id="cb15-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bynode"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 열 샘플링 비율 (노드별)</span></span>
<span id="cb15-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_alpha"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># L1 정규화 파라미터</span></span>
<span id="cb15-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_lambda"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># L2 정규화 파라미터</span></span>
<span id="cb15-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_delta_step"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 트리 가중치 변화의 최대값</span></span>
<span id="cb15-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__scale_pos_weight"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 양성 클래스 가중치 (불균형 데이터용)</span></span>
<span id="cb15-16">}</span>
<span id="cb15-17"></span>
<span id="cb15-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb15-19">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb15-20">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_estimators"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>),</span>
<span id="cb15-21">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)),</span>
<span id="cb15-22">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb15-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_weight"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_child_weight"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb15-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__gamma"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gamma"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>),</span>
<span id="cb15-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subsample"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb15-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bytree"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colsample_bytree"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb15-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bylevel"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colsample_bylevel"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb15-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bynode"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colsample_bynode"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb15-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_alpha"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"reg_alpha"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)),</span>
<span id="cb15-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_lambda"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"reg_lambda"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)),</span>
<span id="cb15-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_delta_step"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_delta_step"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb15-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__scale_pos_weight"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scale_pos_weight"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-33">}</span></code></pre></div>
</div>
</section>
<section id="lightgbm" class="level3">
<h3 class="anchored" data-anchor-id="lightgbm">LightGBM</h3>
<div id="a1d15d24" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb16-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부스팅 라운드 수</span></span>
<span id="cb16-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 학습률</span></span>
<span id="cb16-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 최대 깊이 (-1: 제한 없음)</span></span>
<span id="cb16-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__num_leaves"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">31</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">63</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">127</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드 수 (2^max_depth - 1보다 작게)</span></span>
<span id="cb16-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_samples"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드의 최소 샘플 수</span></span>
<span id="cb16-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_weight"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드의 최소 가중치 합</span></span>
<span id="cb16-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 행 샘플링 비율</span></span>
<span id="cb16-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bytree"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 열 샘플링 비율</span></span>
<span id="cb16-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_alpha"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># L1 정규화</span></span>
<span id="cb16-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_lambda"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># L2 정규화</span></span>
<span id="cb16-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_split_gain"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 분할에 필요한 최소 gain</span></span>
<span id="cb16-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_data_in_leaf"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프의 최소 데이터 수</span></span>
<span id="cb16-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__boosting_type"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gbdt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dart'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'goss'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부스팅 타입</span></span>
<span id="cb16-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__feature_fraction"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 피처 샘플링 비율</span></span>
<span id="cb16-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_fraction"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 데이터 샘플링 비율</span></span>
<span id="cb16-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_freq"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 배깅 빈도</span></span>
<span id="cb16-19">}</span>
<span id="cb16-20"></span>
<span id="cb16-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb16-22">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_estimators"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>),</span>
<span id="cb16-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)),</span>
<span id="cb16-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_depth"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth"</span>, [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_depth_val"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]),</span>
<span id="cb16-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__num_leaves"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"num_leaves"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>),</span>
<span id="cb16-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_samples"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_child_samples"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb16-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_child_weight"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_child_weight"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)),</span>
<span id="cb16-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subsample"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb16-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__colsample_bytree"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"colsample_bytree"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb16-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_alpha"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"reg_alpha"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)),</span>
<span id="cb16-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__reg_lambda"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"reg_lambda"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)),</span>
<span id="cb16-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_split_gain"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_split_gain"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>),</span>
<span id="cb16-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_data_in_leaf"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_data_in_leaf"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb16-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__boosting_type"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"boosting_type"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gbdt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dart'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'goss'</span>]),</span>
<span id="cb16-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__feature_fraction"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"feature_fraction"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb16-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_fraction"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bagging_fraction"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb16-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_freq"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bagging_freq"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb16-39">}</span></code></pre></div>
</div>
</section>
<section id="catboost" class="level3">
<h3 class="anchored" data-anchor-id="catboost">Catboost</h3>
<div id="5c58f308" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Grid Search용</span></span>
<span id="cb17-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb17-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부스팅 라운드 수</span></span>
<span id="cb17-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 학습률</span></span>
<span id="cb17-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__depth"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 깊이 (max_depth 대신 depth 사용)</span></span>
<span id="cb17-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__l2_leaf_reg"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># L2 정규화 파라미터</span></span>
<span id="cb17-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__border_count"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수치형 피처의 분할점 개수</span></span>
<span id="cb17-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_temperature"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 배깅 온도 (0: 비활성화)</span></span>
<span id="cb17-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__random_strength"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 구조의 무작위성</span></span>
<span id="cb17-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__od_type"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'IncToDec'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Iter'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 조기 종료 타입</span></span>
<span id="cb17-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__od_wait"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 조기 종료 대기 라운드</span></span>
<span id="cb17-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bootstrap_type"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bernoulli'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MVS'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Poisson'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 부트스트랩 타입</span></span>
<span id="cb17-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 샘플링 비율 (Bernoulli일 때만)</span></span>
<span id="cb17-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__rsm"</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 무작위 서브스페이스 방법 (피처 샘플링)</span></span>
<span id="cb17-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__leaf_estimation_iterations"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 값 추정 반복 횟수</span></span>
<span id="cb17-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__grow_policy"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SymmetricTree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Depthwise'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lossguide'</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 트리 성장 정책</span></span>
<span id="cb17-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_data_in_leaf"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>],  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 리프 노드의 최소 샘플 수</span></span>
<span id="cb17-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_leaves"</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">31</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">127</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최대 리프 수 (Lossguide일 때만)</span></span>
<span id="cb17-19">}</span>
<span id="cb17-20"></span>
<span id="cb17-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bayesian Optimization용 (hyperopt)</span></span>
<span id="cb17-22">params_bayes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb17-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__n_estimators"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n_estimators"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>),</span>
<span id="cb17-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__learning_rate"</span>: hp.loguniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>, np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>), np.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)),</span>
<span id="cb17-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__depth"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"depth"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb17-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__l2_leaf_reg"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"l2_leaf_reg"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb17-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__border_count"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"border_count"</span>, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">255</span>]),</span>
<span id="cb17-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bagging_temperature"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bagging_temperature"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb17-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__random_strength"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"random_strength"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb17-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__od_type"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"od_type"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'IncToDec'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Iter'</span>]),</span>
<span id="cb17-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__od_wait"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"od_wait"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb17-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__bootstrap_type"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bootstrap_type"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bernoulli'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MVS'</span>]),</span>
<span id="cb17-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__subsample"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subsample"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb17-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__rsm"</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rsm"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb17-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__leaf_estimation_iterations"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"leaf_estimation_iterations"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb17-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__grow_policy"</span>: hp.choice(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"grow_policy"</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SymmetricTree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Depthwise'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lossguide'</span>]),</span>
<span id="cb17-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__min_data_in_leaf"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"min_data_in_leaf"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb17-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"classifier__max_leaves"</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"max_leaves"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">127</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb17-39">}</span>
<span id="cb17-40"></span>
<span id="cb17-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 주요 특징:</span></span>
<span id="cb17-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - GPU 지원 (gpu_device_id=-1로 설정하면 GPU 사용)</span></span>
<span id="cb17-43"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - 범주형 변수 자동 처리 (cat_features 파라미터로 지정)</span></span>
<span id="cb17-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - 내장된 교차검증 및 조기 종료</span></span>
<span id="cb17-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - 텍스트 및 임베딩 피처 지원</span></span></code></pre></div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/01.html</guid>
  <pubDate>Sat, 20 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>전처리</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/core/01.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="결측치-처리" class="level2">
<h2 class="anchored" data-anchor-id="결측치-처리">결측치 처리</h2>
<ul>
<li>대푯값으로 대체</li>
<li>단순확률대치법</li>
<li>다른 모델로 예측</li>
<li>보간법: 시계열에서 주로 사용.</li>
</ul>
</section>
<section id="이상치-처리" class="level2">
<h2 class="anchored" data-anchor-id="이상치-처리">이상치 처리</h2>
<ul>
<li>ESD</li>
<li>IQR</li>
<li>DBSCAN</li>
</ul>
</section>
<section id="클래스-불균형" class="level2">
<h2 class="anchored" data-anchor-id="클래스-불균형">클래스 불균형</h2>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/core/01.html</guid>
  <pubDate>Fri, 15 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/core/00.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="질적-변수" class="level2">
<h2 class="anchored" data-anchor-id="질적-변수">질적 변수</h2>
<section id="상관분석" class="level3">
<h3 class="anchored" data-anchor-id="상관분석">상관분석</h3>
<div id="9701a42b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> spearmanr, kendalltau</span>
<span id="cb1-2"></span>
<span id="cb1-3">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spearmanr(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-4"></span>
<span id="cb1-5">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kendalltau(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-6"></span>
<span id="cb1-7">df.corr(method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kendall'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># kendal, spearman</span></span></code></pre></div>
</div>
<div id="d8377161" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.contingeny <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> association</span>
<span id="cb2-2"></span>
<span id="cb2-3">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tschuprow"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># phi 계수</span></span>
<span id="cb2-4">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cramer'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 크래머 v</span></span>
<span id="cb2-5">v2</span></code></pre></div>
</div>
<ul>
<li>상관계수: 공분산을 각 변수의 표준편차로 나눈 것</li>
<li>스피어만 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.</li>
<li>켄달의 타우: 서열척도 vs 서열척도.
<ul>
<li>둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.</li>
<li>샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.</li>
<li>두 변수의 크기는 같아야함.</li>
</ul></li>
<li>phi 계수: 명목척도 vs 명목척도
<ul>
<li>두 변인 모두 level이 2개일 때 사용</li>
<li>두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산</li>
</ul></li>
<li>크래머 v: 명목척도 vs 명목척도.
<ul>
<li>적어도 하나의 변수가 3개 이상의 level을 가지면 사용</li>
<li>범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.</li>
</ul></li>
<li>Point-biserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 2개일 때</li>
</ul></li>
<li>Polyserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 3개 이상일 때</li>
</ul></li>
<li>명목과 순서의 경우
<ul>
<li>level이 2개: Mann-Whitney U검정</li>
<li>3개 이상: Kruskal-Wallis H test</li>
</ul></li>
</ul>
</section>
<section id="시각화" class="level3">
<h3 class="anchored" data-anchor-id="시각화">시각화</h3>
<div id="bfd6610f" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-2"></span>
<span id="cb3-3">cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'your'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>, ...]</span>
<span id="cb3-4">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df[cols].value_counts()) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 도수분포표</span></span>
<span id="cb3-5">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'proportion'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cols].value_counts(normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상대도수분포표</span></span></code></pre></div>
</div>
<div id="f8e22565" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].plot.bar(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), subplots<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb4-2">plt.tight_layout()</span>
<span id="cb4-3">plt.show()</span></code></pre></div>
</div>
<div id="1643c37d" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.pie(freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].values, </span>
<span id="cb5-2">        labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>freq.index, </span>
<span id="cb5-3">        autopct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%1.1f%%</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb5-4">        colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sns.color_palette(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pastel'</span>, n_colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(freq)))</span>
<span id="cb5-5">plt.show()</span></code></pre></div>
</div>
</section>
</section>
<section id="양적-변수" class="level2">
<h2 class="anchored" data-anchor-id="양적-변수">양적 변수</h2>
<section id="기술통계" class="level3">
<h3 class="anchored" data-anchor-id="기술통계">기술통계</h3>
<div id="104b2309" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.mstats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gmean, hmean, tmean</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb6-3"></span>
<span id="cb6-4">np.mean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 산술평균</span></span>
<span id="cb6-5">gmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 기하평균</span></span>
<span id="cb6-6">hmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 조화평균</span></span>
<span id="cb6-7">tmean(example, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 절사평균</span></span>
<span id="cb6-8">np.sqrt(np.mean(np.array(example) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 평방평균</span></span></code></pre></div>
</div>
<ul>
<li>기하평균: 비율의 평균에 주로 사용됨. 한 값이라도 0이면 전체가 0이 됨</li>
<li>조화평균: 속도, 밀도 등의 평균에 주로 사용됨.</li>
<li>절사평균: 극단값의 영향을 줄이기 위해 상위, 하위 몇 %를 제외한 평균</li>
<li>평방평균: 신호, 파동 등에서 자주 사용</li>
</ul>
<div id="21630964" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.median()</span>
<span id="cb7-2">df.mode()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb7-3">df.quantile(q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span></code></pre></div>
</div>
<ul>
<li>상관계수: 피어슨</li>
</ul>
</section>
<section id="시각화-1" class="level3">
<h3 class="anchored" data-anchor-id="시각화-1">시각화</h3>
<ul>
<li>도수분포표</li>
<li>상대도수분포표</li>
<li>줄기잎그림</li>
<li>히스토그램</li>
<li>상자그림</li>
<li>산점도</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/core/00.html</guid>
  <pubDate>Mon, 04 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 신용 카드 사기 검출</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/04.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="under-over-sampling" class="level2">
<h2 class="anchored" data-anchor-id="under-over-sampling">under, over sampling</h2>
<ul>
<li>under sampling: 많은 비중을 차지하는 레이블을 작은 비중의 레이블에 맞추는것</li>
<li>over sampling: 반대
<ul>
<li>smote: k 최근접 이웃 진행 후, 이웃 간 간격을 맞추는 record를 새로 생성하는 방식</li>
</ul></li>
</ul>
<div id="32d3f284" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/04.html</guid>
  <pubDate>Fri, 01 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 감성 분석</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/09.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/09.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 20 뉴스그룹 분류</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<div id="44a78fea" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fetch_20newsgroups</span>
<span id="cb1-2"></span>
<span id="cb1-3">train_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-4">X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.data</span>
<span id="cb1-5">y_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.target</span>
<span id="cb1-6"></span>
<span id="cb1-7">test_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-8">X_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.data</span>
<span id="cb1-9">y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.target</span></code></pre></div>
</div>
</section>
<section id="학습" class="level2">
<h2 class="anchored" data-anchor-id="학습">학습</h2>
<section id="count-vector" class="level3">
<h3 class="anchored" data-anchor-id="count-vector">Count Vector</h3>
<div id="23c1f25b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CountVectorizer</span>
<span id="cb2-2"></span>
<span id="cb2-3">cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CountVectorizer()</span>
<span id="cb2-4">X_train_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.fit_transform(X_train)</span>
<span id="cb2-5">X_test_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.transform(X_test)</span></code></pre></div>
</div>
<div id="025ca4ae" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb3-4"></span>
<span id="cb3-5">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb3-8">lr_clf.fit(X_train_cnt_vect, y_train)</span>
<span id="cb3-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_cnt_vect)</span>
<span id="cb3-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.731 </code></pre>
</div>
</div>
</section>
<section id="tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf">TF-IDF</h3>
<div id="6f669acf" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TfidfVectorizer</span>
<span id="cb5-2"></span>
<span id="cb5-3">tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TfidfVectorizer()</span>
<span id="cb5-4">X_train_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.fit_transform(X_train)</span>
<span id="cb5-5">X_test_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.transform(X_test)</span>
<span id="cb5-6"></span>
<span id="cb5-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb5-8">lr_clf.fit(X_train_tfidf_vect, y_train)</span>
<span id="cb5-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_tfidf_vect)</span>
<span id="cb5-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.778 </code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">overview</h2>
<section id="nlp-vs-텍스트-분석" class="level3">
<h3 class="anchored" data-anchor-id="nlp-vs-텍스트-분석">NLP vs 텍스트 분석</h3>
<ul>
<li>NLP(자연어 처리)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술을 의미</li>
<li>텍스트 분석은 주로 비정형 텍스트 데이터를 머신러닝, 통계 등의 방법으로 예측 분석이나 유용한 정보를 추출하는 데 중점을 둔다.</li>
</ul>
</section>
<section id="종류" class="level3">
<h3 class="anchored" data-anchor-id="종류">종류</h3>
<ul>
<li>텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측 (연예 / 정치 / 스포츠 같은 카테고리 분류 혹은 스팸 메일 검출). 지도 학습</li>
<li>감성 분석: 텍스트에서 주관적 요소를 분석하는 기법. 지도 혹은 비지도.</li>
<li>텍스트 요약: 텍스트 내에서 주제나 중심 사상을 추출</li>
<li>텍스트 군집화: 비슷한 유형의 문서를 군집화 하는 것. 비지도 학습</li>
</ul>
</section>
</section>
<section id="프로세스" class="level2">
<h2 class="anchored" data-anchor-id="프로세스">프로세스</h2>
<ol type="1">
<li>텍스트 전처리: 대 / 소문자 변경, 특수 문자 제거, 토큰화, 불용어 제거, 어근 추출 등의 정규화 작업</li>
<li>피처 벡터화 / 추출: 텍스트에서 피처를 추출하고 벡터 값을 할당. BOW와 Word2Vec이 대표적</li>
<li>ML 모델 수립 및 학습 / 예측 / 평가</li>
</ol>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ul>
<li>클렌징: 문자, 기호 등을 사전에 제거</li>
<li>토큰화
<ul>
<li>문장 토큰화: 마침표, 개행문자 등을 기준으로 문장을 분리. 각 문장이 가지는 의미가 중요한 경우 사용.</li>
<li>단어 토큰화: 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리.
<ul>
<li>n-gram: 단어의 연속된 n개를 묶어서 하나의 단위로 처리하는 방법. 문장이 가지는 의미를 조금이라도 보존할 수 있다.</li>
</ul></li>
</ul></li>
</ul>
<div id="127f9e8f" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sent_tokenize</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb1-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문장을 분리하는 마침표, 개행문자 등의 데이터 셋 다운로드</span></span>
<span id="cb1-4">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt_tab'</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">text_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes."</span></span>
<span id="cb1-7">sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text_sample)</span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sentences)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The Matrix is everywhere its all around us, here even in this room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package punkt to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!</code></pre>
</div>
</div>
<div id="871339ae" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> word_tokenize</span>
<span id="cb4-2"></span>
<span id="cb4-3">sentence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room."</span></span>
<span id="cb4-4">words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word_tokenize(sentence)</span>
<span id="cb4-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(words)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']</code></pre>
</div>
</div>
<div id="7cdedba4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tokenize_text(text):</span>
<span id="cb6-2">    sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text)</span>
<span id="cb6-3">    words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [word_tokenize(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb6-4"></span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> words</span>
<span id="cb6-6"></span>
<span id="cb6-7">word_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize_text(text_sample)</span>
<span id="cb6-8">word_tokens</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[['The',
  'Matrix',
  'is',
  'everywhere',
  'its',
  'all',
  'around',
  'us',
  ',',
  'here',
  'even',
  'in',
  'this',
  'room',
  '.'],
 ['You',
  'can',
  'see',
  'it',
  'when',
  'you',
  'look',
  'out',
  'your',
  'window',
  'or',
  'when',
  'you',
  'turn',
  'on',
  'your',
  'television',
  '.'],
 ['You',
  'can',
  'feel',
  'it',
  'when',
  'you',
  'go',
  'to',
  'work',
  ',',
  'when',
  'you',
  'go',
  'to',
  'church',
  ',',
  'when',
  'you',
  'pay',
  'your',
  'taxes',
  '.']]</code></pre>
</div>
</div>
<ul>
<li>stopword 제거: 분석에 필요하지 않은 단어를 제거하는 작업. 예) 관사, 전치사, 접속사 등</li>
</ul>
<div id="f573c3bd" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.corpus <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stopwords</span>
<span id="cb8-2">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'stopwords'</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># stopwords 데이터 셋 다운로드</span></span>
<span id="cb8-3"></span>
<span id="cb8-4">stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>['a',
 'about',
 'above',
 'after',
 'again',
 'against',
 'ain',
 'all',
 'am',
 'an',
 'and',
 'any',
 'are',
 'aren',
 "aren't",
 'as',
 'at',
 'be',
 'because',
 'been']</code></pre>
</div>
</div>
<div id="8a07bdaa" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)</span>
<span id="cb11-2">all_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> word_tokens:</span>
<span id="cb11-4">    filtered_words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentence:</span>
<span id="cb11-6">        word <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word.lower()</span>
<span id="cb11-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sw:</span>
<span id="cb11-8">            filtered_words.append(word)</span>
<span id="cb11-9">    all_tokens.append(filtered_words)</span>
<span id="cb11-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(all_tokens)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'look', 'window', 'turn', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', ',', 'pay', 'taxes', '.']]</code></pre>
</div>
</div>
<ul>
<li>stemming, lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것
<ul>
<li>stemming이 더 단순하고 빠르지만 lemmatization 이 더 저오학함</li>
</ul></li>
</ul>
<div id="58a6e2c6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LancasterStemmer</span>
<span id="cb13-2"></span>
<span id="cb13-3">stemmer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LancasterStemmer()</span>
<span id="cb13-4"></span>
<span id="cb13-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'working'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'works'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worked'</span>))</span>
<span id="cb13-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>))</span>
<span id="cb13-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>))</span>
<span id="cb13-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>work work work
amus amus amus
happy happiest
fant fanciest</code></pre>
</div>
</div>
<div id="cf2d13b3" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> WordNetLemmatizer</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb15-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wordnet'</span>)</span>
<span id="cb15-4"></span>
<span id="cb15-5">lemma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> WordNetLemmatizer()</span>
<span id="cb15-6"></span>
<span id="cb15-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>))</span>
<span id="cb15-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span>
<span id="cb15-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package wordnet to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>amuse amuse amuse
happy happy
fancy fancy</code></pre>
</div>
</div>
</section>
<section id="bow" class="level2">
<h2 class="anchored" data-anchor-id="bow">BOW</h2>
<ul>
<li>문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 빈도 값을 부여해 피처 값을 추출하는 모델</li>
<li>count 기반 벡터화: 빈도가 높을수록 중요한 단어로 인식</li>
<li>TF-IDF(term frequency - inverse document frequency) 기반 벡터화: 빈도가 높을수록 좋으나, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 패털티를 줌
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i%20*%20log%5Cfrac%7BN%7D%7BDF_i%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i">: 개별 문서에서의 단어 i 빈도</li>
<li><img src="https://latex.codecogs.com/png.latex?DF_i">: 단어 i를 가지고 있는 문서 개수</li>
<li>N: 전체 문서 개수</li>
</ul></li>
</ul></li>
<li>희소행렬 문제: 불필요한 0 값이 많아지는 문제
<ul>
<li>COO</li>
<li>CSR</li>
<li>혹은 희소행렬을 잘 처리하는 알고리즘: 로지스틱 회귀, 선형 svm, 나이브 베이즈 등</li>
</ul></li>
</ul>
<section id="coo" class="level3">
<h3 class="anchored" data-anchor-id="coo">COO</h3>
<ul>
<li>0이 아닌 데이터만 별도의 array에 저장.</li>
</ul>
<div id="25a7d323" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb18-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb18-3"></span>
<span id="cb18-4">dense <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb18-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb18-6">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-7">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-8">sparse_coo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.coo_matrix((data, (row_pos, col_pos)))</span>
<span id="cb18-9">sparse_coo</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;COOrdinate sparse matrix of dtype 'int64'
    with 3 stored elements and shape (2, 3)&gt;</code></pre>
</div>
</div>
<div id="9967cbb9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">sparse_coo.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([[3, 0, 1],
       [0, 2, 0]])</code></pre>
</div>
</div>
</section>
<section id="csr" class="level3">
<h3 class="anchored" data-anchor-id="csr">CSR</h3>
<ul>
<li>COO + 시작위치만 기록하는 방법</li>
</ul>
<div id="743d64fb" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb22-2"></span>
<span id="cb22-3">dense2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-4">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-5">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-6">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-7">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb22-8">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb22-9">data2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb22-10">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>])</span>
<span id="cb22-11">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb22-12">row_pos_ind <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>])</span>
<span id="cb22-13"></span>
<span id="cb22-14">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix((data2, col_pos, row_pos_ind))</span>
<span id="cb22-15">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>
<div id="bd983a7a" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix(dense2)</span>
<span id="cb24-2">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>차원 축소</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="pca" class="level2">
<h2 class="anchored" data-anchor-id="pca">PCA</h2>
<div id="68042073" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"></span>
<span id="cb1-5">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb1-6">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_width'</span>]</span>
<span id="cb1-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>columns)</span>
<span id="cb1-8">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb1-9">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb1-12">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>]</span>
<span id="cb1-13">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]</span>
<span id="cb1-14">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb1-15">plt.legend()</span>
<span id="cb1-16">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length'</span>)</span>
<span id="cb1-17">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width'</span>)</span>
<span id="cb1-18">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06_files/figure-html/cell-2-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>PCA는 scaling의 영향을 받음.</li>
</ul>
<div id="17419a61" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.decomposition <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PCA</span>
<span id="cb2-3"></span>
<span id="cb2-4">scaled_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb2-5"></span>
<span id="cb2-6">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pca.fit_transform(scaled_df)</span></code></pre></div>
</div>
<div id="4de1b6be" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">pca_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb3-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pca_columns)</span>
<span id="cb3-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span></code></pre></div>
</div>
<div id="a91a157b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb4-4">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>]</span>
<span id="cb4-5">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb4-6">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb4-7">plt.legend()</span>
<span id="cb4-8">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>)</span>
<span id="cb4-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>)</span>
<span id="cb4-10">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06_files/figure-html/cell-5-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="신용카드-고객-데이터" class="level2">
<h2 class="anchored" data-anchor-id="신용카드-고객-데이터">신용카드 고객 데이터</h2>
<div id="c8b211c5" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_excel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/creadit_card.xls'</span>, header<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sheet_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data'</span>).iloc[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb5-2">df.rename(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_0'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default payment next month'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>}, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>]</span>
<span id="cb5-4">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="17cfec59" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"></span>
<span id="cb6-3">corr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> features.corr()</span>
<span id="cb6-4">sns.heatmap(corr, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, fmt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.1g'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06_files/figure-html/cell-7-output-1.png" width="610" height="482" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>BILL_AMT1~6, PAY_1~6의 상관도가 높다.</li>
</ul>
<div id="ecf443d9" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">cols_bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BILL_AMT'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)]</span>
<span id="cb7-2">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb7-3">df_cols_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(features[cols_bill])</span>
<span id="cb7-4">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-5">pca.fit(df_cols_scaled)</span>
<span id="cb7-6">pca.explained_variance_ratio_</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([0.90555253, 0.0509867 ])</code></pre>
</div>
</div>
<ul>
<li>PCA 할 때 column 전부 다 안 넣어도 되나?</li>
<li>다 넣어야 하는 듯</li>
</ul>
</section>
<section id="lda" class="level2">
<h2 class="anchored" data-anchor-id="lda">LDA</h2>
<ul>
<li>클래스 분리를 최대화하는 축을 찾음</li>
<li>PCA와 다르게 지도 학습임.</li>
</ul>
<div id="e48e1eeb" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.discriminant_analysis <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearDiscriminantAnalysis</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb9-3"></span>
<span id="cb9-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb9-5">iris_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(iris.data)</span></code></pre></div>
</div>
<div id="f7d8e00d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearDiscriminantAnalysis(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-2">iris_lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lda.fit_transform(iris_scaled, iris.target)</span></code></pre></div>
</div>
<div id="cab3bcf1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">lda_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris_lda, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lda_columns)</span>
<span id="cb11-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb11-4"></span>
<span id="cb11-5">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb11-8">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>]</span>
<span id="cb11-9">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-10">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb11-11">plt.legend()</span>
<span id="cb11-12">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>)</span>
<span id="cb11-13">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>)</span>
<span id="cb11-14">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06_files/figure-html/cell-11-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html</guid>
  <pubDate>Mon, 28 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="경사하강법" class="level2">
<h2 class="anchored" data-anchor-id="경사하강법">경사하강법</h2>
<div id="97d2e534" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"></span>
<span id="cb1-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-5">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-6">plt.scatter(X, y)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/05_files/figure-html/cell-2-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f25ebc09" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_cost(y, y_pred):</span>
<span id="cb2-2">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-3">    cost <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.square(y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> N</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> cost</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>):</span>
<span id="cb2-7">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-8">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w1)</span>
<span id="cb2-9">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w0)</span>
<span id="cb2-10">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(X, w1.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb2-11">    diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred</span>
<span id="cb2-12"></span>
<span id="cb2-13">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.dot(X.T, diff)</span>
<span id="cb2-14">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(diff)</span>
<span id="cb2-15"></span>
<span id="cb2-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1_update, w0_update</span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>):</span>
<span id="cb2-19">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-20">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb2-23">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb2-24">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb2-25">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb2-26"></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="50781e31" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-4">plt.scatter(X, y)</span>
<span id="cb3-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 6.394 w1: 3.611, total cost: 1.009</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/05_files/figure-html/cell-4-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>일반 경사하강법은 시간이 오래걸려서 잘 안씀</li>
</ul>
</section>
<section id="미니-배치-확률적-경사-하강법" class="level2">
<h2 class="anchored" data-anchor-id="미니-배치-확률적-경사-하강법">미니 배치 확률적 경사 하강법</h2>
<div id="89b39973" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> stochastic_gradient_descent_steps(X, y, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb5-2">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-3">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ind <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb5-6">        stochastic_random_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-7">        sample_X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-8">        sample_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-9"></span>
<span id="cb5-10">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, sample_X, sample_y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb5-11">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb5-12">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb5-13"></span>
<span id="cb5-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="3652a861" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stochastic_gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb6-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb6-4">plt.scatter(X, y)</span>
<span id="cb6-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 6.372 w1: 3.619, total cost: 1.010</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/05_files/figure-html/cell-6-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀">선형 회귀</h2>
<div id="09b6ca21" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb8-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb8-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_boston</span>
<span id="cb8-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb8-6"></span>
<span id="cb8-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb8-8"></span>
<span id="cb8-9">boston <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_boston()</span>
<span id="cb8-10"></span>
<span id="cb8-11">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(boston.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>boston.feature_names)</span>
<span id="cb8-12">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boston.target</span>
<span id="cb8-13">df.head()</span></code></pre></div>
</div>
<div id="6d74b36e" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">lm_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RM'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ZN'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'INDUS'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NOX'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AGE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PTRAIO'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LSTAT'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RAD'</span>]</span>
<span id="cb9-2"></span>
<span id="cb9-3">fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>), ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(lm_features) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, feature <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(lm_features):</span>
<span id="cb9-6">    row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-7">    col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-8"></span>
<span id="cb9-9">    sns.regplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feature, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[row][col])</span></code></pre></div>
</div>
<p>boston 데이터가 윤리적 문제로 사용 불가능하다고 한다.</p>
<div id="9266576f" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb10-3"></span>
<span id="cb10-4">y_target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>]</span>
<span id="cb10-5">X_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb10-6">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression()</span>
<span id="cb10-7"></span>
<span id="cb10-8">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(lr, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb10-9">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb10-10">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
<p>cross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함</p>
</section>
<section id="다항-회귀" class="level2">
<h2 class="anchored" data-anchor-id="다항-회귀">다항 회귀</h2>
<div id="3cee10ca" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PolynomialFeatures</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> polynominal_func(X):</span>
<span id="cb11-6">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y</span>
<span id="cb11-8"></span>
<span id="cb11-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'poly'</span>, PolynomialFeatures(degree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)),</span>
<span id="cb11-10">                  (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>, LinearRegression())])</span>
<span id="cb11-11">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb11-12">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> polynominal_func(X)</span>
<span id="cb11-13"></span>
<span id="cb11-14">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(X, y)</span>
<span id="cb11-15"></span>
<span id="cb11-16">np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(model.named_steps[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>].coef_, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])</code></pre>
</div>
</div>
</section>
<section id="규제" class="level2">
<h2 class="anchored" data-anchor-id="규제">규제</h2>
<ul>
<li><p>L2 규제(Ridge): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C%5E2)"></p></li>
<li><p>L1 규제(Lasso): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C_1)"></p></li>
<li><p>λ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐</p></li>
<li><p>L1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음</p></li>
</ul>
<section id="릿지" class="level3">
<h3 class="anchored" data-anchor-id="릿지">릿지</h3>
<div id="70c12e31" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge</span>
<span id="cb13-2"></span>
<span id="cb13-3">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb13-4">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(ridge, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb13-5">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb13-6">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
</section>
<section id="라쏘-엘라스틱넷" class="level3">
<h3 class="anchored" data-anchor-id="라쏘-엘라스틱넷">라쏘 엘라스틱넷</h3>
<div id="37549c8d" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb14-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge, Lasso, ElasticNet</span>
<span id="cb14-3"></span>
<span id="cb14-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_linear_reg_eval(model_name, params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, X_data_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, y_target_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-5">    coeff_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> params:</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ridge'</span>:</span>
<span id="cb14-8">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lasso'</span>:</span>
<span id="cb14-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Lasso(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ElasticNet'</span>:</span>
<span id="cb14-12">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ElasticNet(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param, l1_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-13">        neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(model, X_data_n, y_target_n, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb14-14">        rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb14-15">        avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span>
<span id="cb14-16">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>avg_rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb14-17"></span>
<span id="cb14-18">        model.fit(X_data_n, y_target_n)</span>
<span id="cb14-19">        coeff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.coef_, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_data_n.columns)</span>
<span id="cb14-20">        colname <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alpha:'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(param)</span>
<span id="cb14-21">        coeff_df[colname] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> coeff</span>
<span id="cb14-22"></span>
<span id="cb14-23">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> coeff_df</span></code></pre></div>
</div>
</section>
</section>
<section id="선형-회귀-모델을-위한-데이터-변환" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀-모델을-위한-데이터-변환">선형 회귀 모델을 위한 데이터 변환</h2>
<ul>
<li>로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.</li>
</ul>
<div id="8bcd77fe" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">np.log1p(data)</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html</guid>
  <pubDate>Sun, 27 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 산탄데르 고객 만족 예측</title>
  <link>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<div id="30c7f328" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb1-5"></span>
<span id="cb1-6">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.family'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Noto Sans KR'</span></span>
<span id="cb1-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/train.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb1-10">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 76020 entries, 0 to 76019
Columns: 371 entries, ID to TARGET
dtypes: float64(111), int64(260)
memory usage: 215.2 MB</code></pre>
</div>
</div>
<div id="2847c582" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ID</th>
<th data-quarto-table-cell-role="th">var3</th>
<th data-quarto-table-cell-role="th">var15</th>
<th data-quarto-table-cell-role="th">imp_ent_var16_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult3</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult3</th>
<th data-quarto-table-cell-role="th">var38</th>
<th data-quarto-table-cell-role="th">TARGET</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>...</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>7.602000e+04</td>
<td>76020.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>75964.050723</td>
<td>-1523.199277</td>
<td>33.212865</td>
<td>86.208265</td>
<td>72.363067</td>
<td>119.529632</td>
<td>3.559130</td>
<td>6.472698</td>
<td>0.412946</td>
<td>0.567352</td>
<td>...</td>
<td>7.935824</td>
<td>1.365146</td>
<td>12.215580</td>
<td>8.784074</td>
<td>31.505324</td>
<td>1.858575</td>
<td>76.026165</td>
<td>56.614351</td>
<td>1.172358e+05</td>
<td>0.039569</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>43781.947379</td>
<td>39033.462364</td>
<td>12.956486</td>
<td>1614.757313</td>
<td>339.315831</td>
<td>546.266294</td>
<td>93.155749</td>
<td>153.737066</td>
<td>30.604864</td>
<td>36.513513</td>
<td>...</td>
<td>455.887218</td>
<td>113.959637</td>
<td>783.207399</td>
<td>538.439211</td>
<td>2013.125393</td>
<td>147.786584</td>
<td>4040.337842</td>
<td>2852.579397</td>
<td>1.826646e+05</td>
<td>0.194945</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000000</td>
<td>-999999.000000</td>
<td>5.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.163750e+03</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>38104.750000</td>
<td>2.000000</td>
<td>23.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.787061e+04</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>76043.000000</td>
<td>2.000000</td>
<td>28.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.064092e+05</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>113748.750000</td>
<td>2.000000</td>
<td>40.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.187563e+05</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>151838.000000</td>
<td>238.000000</td>
<td>105.000000</td>
<td>210000.000000</td>
<td>12888.030000</td>
<td>21024.810000</td>
<td>8237.820000</td>
<td>11073.570000</td>
<td>6600.000000</td>
<td>6600.000000</td>
<td>...</td>
<td>50003.880000</td>
<td>20385.720000</td>
<td>138831.630000</td>
<td>91778.730000</td>
<td>438329.220000</td>
<td>24650.010000</td>
<td>681462.900000</td>
<td>397884.300000</td>
<td>2.203474e+07</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

<p>8 rows × 371 columns</p>
</div>
</div>
</div>
<div id="86d1cfd8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-2">df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-3"></span>
<span id="cb4-4">X_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-5">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<div id="1b6dbbee" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/test.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb5-2">test_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">test_df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<div id="d4714bf3" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_features, labels, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span></code></pre></div>
</div>
<ul>
<li>train, test의 label의 비율이 동일한게 좋은걸까</li>
</ul>
</section>
<section id="xgboost" class="level2">
<h2 class="anchored" data-anchor-id="xgboost">XGBoost</h2>
<div id="b5e6ed7d" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">X_tr, X_val, y_tr, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_train, y_train, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
<div id="78fd7b83" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb8-3"></span>
<span id="cb8-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb8-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>, </span>
<span id="cb8-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, </span>
<span id="cb8-7">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb8-8">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb8-9">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb8-10">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb8-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
<section id="베이지안-최적화" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화">베이지안 최적화</h3>
<div id="27c21265" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb9-5">    xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb9-6">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb9-7">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb9-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb9-9">                            min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb9-10">                            colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>],</span>
<span id="cb9-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb9-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb9-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb9-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb9-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb9-17"></span>
<span id="cb9-18">        xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb9-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb9-20">        roc_auc_list.append(score)</span>
<span id="cb9-21"></span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="73547293" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb10-2"></span>
<span id="cb10-3">xgb_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span>),</span>
<span id="cb10-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb10-8">}</span>
<span id="cb10-9"></span>
<span id="cb10-10">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb10-11">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb10-12">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>xgb_search_space,</span>
<span id="cb10-13">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb10-14">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb10-15">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb10-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
</div>
</section>
<section id="재-학습" class="level3">
<h3 class="anchored" data-anchor-id="재-학습">재 학습</h3>
<div id="77a10666" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb11-3"></span>
<span id="cb11-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb11-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb11-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-7">                    max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb11-8">                    min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb11-9">                    colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-10">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb11-11">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb11-12">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb11-13">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb11-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
</section>
<section id="plot-importance" class="level3">
<h3 class="anchored" data-anchor-id="plot-importance">plot importance</h3>
<div id="ac38b14f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_importance</span>
<span id="cb12-2"></span>
<span id="cb12-3">plot_importance(xgb_clf, max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="lightgbm" class="level2">
<h2 class="anchored" data-anchor-id="lightgbm">LightGBM</h2>
<div id="0ccfd20a" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMClassifier</span>
<span id="cb13-3"></span>
<span id="cb13-4">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb13-5"></span>
<span id="cb13-6">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb13-7">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb13-8"></span>
<span id="cb13-9">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb13-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1694, number of negative: 40877
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010137 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 13592
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 248
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039792 -&gt; initscore=-3.183475
[LightGBM] [Info] Start training from score -3.183475
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[33]    training's binary_logloss: 0.117693 valid_1's binary_logloss: 0.137269
[LightGBM] [Warning] Unknown parameter: eval_metric
0.836</code></pre>
</div>
</div>
<section id="베이지안-최적화-1" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화-1">베이지안 최적화</h3>
<div id="a629edb1" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb15-4">    lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb15-5">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb15-6">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb15-7">                            num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb15-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb15-9">                            min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb15-10">                            subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>],</span>
<span id="cb15-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb15-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb15-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb15-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb15-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb15-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb15-17"></span>
<span id="cb15-18">        lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb15-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb15-20">        roc_auc_list.append(score)</span>
<span id="cb15-21"></span>
<span id="cb15-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="eed26a4e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb16-2"></span>
<span id="cb16-3">lgbm_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">160</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-8">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb16-9">}</span>
<span id="cb16-10"></span>
<span id="cb16-11">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb16-12">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb16-13">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lgbm_search_space,</span>
<span id="cb16-14">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb16-15">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb16-16">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb16-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007286 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12947
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.161962
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119239 valid_1's binary_logloss: 0.131547
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009173 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 13055
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.210495
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11513  valid_1's binary_logloss: 0.139265
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008461 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12996
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.179828
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.116828 valid_1's binary_logloss: 0.136952
  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12835
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.134361 valid_1's binary_logloss: 0.134539
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008662 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.129831 valid_1's binary_logloss: 0.142347
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008359 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.131634 valid_1's binary_logloss: 0.139054
  2%|▏         | 1/50 [00:06&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:06&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008437 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12902
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[88]    training's binary_logloss: 0.113936 valid_1's binary_logloss: 0.131766
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008792 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.11326  valid_1's binary_logloss: 0.139317
  4%|▍         | 2/50 [00:08&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:08&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009763 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:
[77]    training's binary_logloss: 0.113657 valid_1's binary_logloss: 0.136864
  4%|▍         | 2/50 [00:10&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:10&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009179 seconds.
You can set `force_col_wise=true` to remove the overhead.
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12835
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:
[39]    training's binary_logloss: 0.12109  valid_1's binary_logloss: 0.131246
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008369 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:
[39]    training's binary_logloss: 0.116743 valid_1's binary_logloss: 0.139211
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008111 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:
[35]    training's binary_logloss: 0.120149 valid_1's binary_logloss: 0.136702
  6%|▌         | 3/50 [00:12&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:12&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008782 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[30]    training's binary_logloss: 0.111064 valid_1's binary_logloss: 0.131895
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010004 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 13055
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[27]    training's binary_logloss: 0.108994 valid_1's binary_logloss: 0.139854
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009049 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12996
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.116146 valid_1's binary_logloss: 0.13756
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522] 10%|█         | 5/50 [00:14&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008251 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[25]    training's binary_logloss: 0.120067 valid_1's binary_logloss: 0.131511
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007845 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 13055
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[31]    training's binary_logloss: 0.112434 valid_1's binary_logloss: 0.139423
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009165 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12996
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[28]    training's binary_logloss: 0.115651 valid_1's binary_logloss: 0.136891
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522] 12%|█▏        | 6/50 [00:16&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010475 seconds.
You can set `force_col_wise=true` to remove the overhead.
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12902
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.128605 valid_1's binary_logloss: 0.133093
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008203 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12988
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124147 valid_1's binary_logloss: 0.141061
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007711 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12898
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.125878 valid_1's binary_logloss: 0.13813
 12%|█▏        | 6/50 [00:20&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:20&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522] 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008418 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:
[73]    training's binary_logloss: 0.119266 valid_1's binary_logloss: 0.131216
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007783 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12998
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[63]    training's binary_logloss: 0.116719 valid_1's binary_logloss: 0.139009
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008308 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12968
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:
[56]    training's binary_logloss: 0.120087 valid_1's binary_logloss: 0.136444
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522] 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009077 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12835
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.161962
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[57]    training's binary_logloss: 0.120993 valid_1's binary_logloss: 0.131385
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007612 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12988
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.210495
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[62]    training's binary_logloss: 0.115325 valid_1's binary_logloss: 0.13881
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12898
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.179828
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[50]    training's binary_logloss: 0.120231 valid_1's binary_logloss: 0.136346
 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:25&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264] 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008124 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12835
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.161962
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.116031 valid_1's binary_logloss: 0.132494
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007737 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12988
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.210495
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[22]    training's binary_logloss: 0.112419 valid_1's binary_logloss: 0.140329
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007861 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12898
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.179828
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.115687 valid_1's binary_logloss: 0.137694
 18%|█▊        | 9/50 [00:27&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:27&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264] 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007751 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[45]    training's binary_logloss: 0.117033 valid_1's binary_logloss: 0.131893
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007925 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.11876  valid_1's binary_logloss: 0.139543
 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008278 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[38]    training's binary_logloss: 0.117423 valid_1's binary_logloss: 0.136738
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264] 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12902
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[78]    training's binary_logloss: 0.115732 valid_1's binary_logloss: 0.13138
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007739 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[74]    training's binary_logloss: 0.112624 valid_1's binary_logloss: 0.139339
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008653 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[74]    training's binary_logloss: 0.114351 valid_1's binary_logloss: 0.136737
 22%|██▏       | 11/50 [00:32&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:32&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264] 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010321 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12947
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[46]    training's binary_logloss: 0.11276  valid_1's binary_logloss: 0.13165
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009865 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13055
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.111011 valid_1's binary_logloss: 0.139831
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008053 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12996
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.111276 valid_1's binary_logloss: 0.137335
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264] 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008260 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12844
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12681  valid_1's binary_logloss: 0.13222
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.122208 valid_1's binary_logloss: 0.139981
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009314 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124131 valid_1's binary_logloss: 0.137316
 26%|██▌       | 13/50 [00:39&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:39&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264] 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010560 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12943
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.119948 valid_1's binary_logloss: 0.132615
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009233 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.116812 valid_1's binary_logloss: 0.140251
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009001 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12958
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.117331 valid_1's binary_logloss: 0.137237
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264] 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010186 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12943
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.120445 valid_1's binary_logloss: 0.132691
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010950 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.117054 valid_1's binary_logloss: 0.139941
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008302 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12906
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.115401 valid_1's binary_logloss: 0.137413
 30%|███       | 15/50 [00:44&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:44&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264] 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009370 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.11605  valid_1's binary_logloss: 0.133209
 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008886 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[15]    training's binary_logloss: 0.114923 valid_1's binary_logloss: 0.140959
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008924 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.117846 valid_1's binary_logloss: 0.13746
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264] 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009462 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.11538  valid_1's binary_logloss: 0.131723
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010342 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[31]    training's binary_logloss: 0.117853 valid_1's binary_logloss: 0.139219
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007465 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.120676 valid_1's binary_logloss: 0.136931
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264] 36%|███▌      | 18/50 [00:48&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007036 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.119523 valid_1's binary_logloss: 0.131926
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008304 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[27]    training's binary_logloss: 0.115902 valid_1's binary_logloss: 0.139583
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010535 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[24]    training's binary_logloss: 0.11906  valid_1's binary_logloss: 0.137256
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264] 38%|███▊      | 19/50 [00:50&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014995 seconds.
You can set `force_col_wise=true` to remove the overhead.
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117616 valid_1's binary_logloss: 0.132237
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007897 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[23]    training's binary_logloss: 0.115822 valid_1's binary_logloss: 0.140243
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020219 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.116668 valid_1's binary_logloss: 0.137218
 38%|███▊      | 19/50 [00:53&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:53&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264] 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009889 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13057
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.118472 valid_1's binary_logloss: 0.130909
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008639 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13161
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.116535 valid_1's binary_logloss: 0.138826
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009875 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13044
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.118597 valid_1's binary_logloss: 0.136638
 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:56&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264] 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008395 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13057
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:
[96]    training's binary_logloss: 0.116806 valid_1's binary_logloss: 0.131693
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008654 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13161
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.116396 valid_1's binary_logloss: 0.138474
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009684 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13044
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:
[65]    training's binary_logloss: 0.119662 valid_1's binary_logloss: 0.136275
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356] 44%|████▍     | 22/50 [00:58&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13057
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:
[63]    training's binary_logloss: 0.117775 valid_1's binary_logloss: 0.131201
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009518 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13161
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:
[55]    training's binary_logloss: 0.11576  valid_1's binary_logloss: 0.138797
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009738 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13044
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:
[48]    training's binary_logloss: 0.119114 valid_1's binary_logloss: 0.136592
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356] 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009989 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 12993
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121674 valid_1's binary_logloss: 0.131107
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009480 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13086
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11765  valid_1's binary_logloss: 0.138596
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009169 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 12996
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.119781 valid_1's binary_logloss: 0.136145
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356] 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008895 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 12993
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123945 valid_1's binary_logloss: 0.131312
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009337 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 13086
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119785 valid_1's binary_logloss: 0.138758
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009450 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 12996
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121345 valid_1's binary_logloss: 0.136253
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913] 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010296 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.115076 valid_1's binary_logloss: 0.131544
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011525 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.110704 valid_1's binary_logloss: 0.139171
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009677 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[74]    training's binary_logloss: 0.117386 valid_1's binary_logloss: 0.137077
 50%|█████     | 25/50 [01:10&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:10&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197] 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009136 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.118745 valid_1's binary_logloss: 0.13174
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009702 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.115548 valid_1's binary_logloss: 0.138995
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010037 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[32]    training's binary_logloss: 0.119523 valid_1's binary_logloss: 0.136814
 52%|█████▏    | 26/50 [01:13&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:13&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197] 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009837 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.119337 valid_1's binary_logloss: 0.131417
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010306 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.114456 valid_1's binary_logloss: 0.139157
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008807 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.11659  valid_1's binary_logloss: 0.136713
 54%|█████▍    | 27/50 [01:16&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:16&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197] 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008596 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.121698 valid_1's binary_logloss: 0.132138
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008173 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.11611  valid_1's binary_logloss: 0.139307
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008090 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.122317 valid_1's binary_logloss: 0.136889
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197] 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009576 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.117385 valid_1's binary_logloss: 0.131388
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008772 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[98]    training's binary_logloss: 0.113488 valid_1's binary_logloss: 0.139105
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008569 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.114829 valid_1's binary_logloss: 0.136714
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197] 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.136137 valid_1's binary_logloss: 0.135864
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007794 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.131758 valid_1's binary_logloss: 0.143909
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008173 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.133319 valid_1's binary_logloss: 0.140365
 60%|██████    | 30/50 [01:24&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:24&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197] 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007810 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[52]    training's binary_logloss: 0.120557 valid_1's binary_logloss: 0.131463
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008104 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[42]    training's binary_logloss: 0.119216 valid_1's binary_logloss: 0.138844
 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008458 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.120672 valid_1's binary_logloss: 0.136077
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197] 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008051 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.116782 valid_1's binary_logloss: 0.132291
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008398 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.112525 valid_1's binary_logloss: 0.139834
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008790 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.116376 valid_1's binary_logloss: 0.13759
 64%|██████▍   | 32/50 [01:29&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:29&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197] 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007899 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121235 valid_1's binary_logloss: 0.131795
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010711 seconds.
You can set `force_col_wise=true` to remove the overhead.
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13161
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.117131 valid_1's binary_logloss: 0.139355
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009935 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13044
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11886  valid_1's binary_logloss: 0.136817
 66%|██████▌   | 33/50 [01:32&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:32&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197] 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009479 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[87]    training's binary_logloss: 0.119217 valid_1's binary_logloss: 0.131162
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009390 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13130
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.11377  valid_1's binary_logloss: 0.138774
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008385 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13000
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.119355 valid_1's binary_logloss: 0.136516
 68%|██████▊   | 34/50 [01:34&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:34&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197] 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008260 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.117227 valid_1's binary_logloss: 0.132479
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007966 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.110745 valid_1's binary_logloss: 0.140016
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007917 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.116325 valid_1's binary_logloss: 0.136868
 70%|███████   | 35/50 [01:36&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:36&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197] 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007671 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.133676 valid_1's binary_logloss: 0.135443
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007838 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.129111 valid_1's binary_logloss: 0.143846
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008862 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.131021 valid_1's binary_logloss: 0.140157
 72%|███████▏  | 36/50 [01:39&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:39&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197] 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007790 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[65]    training's binary_logloss: 0.118118 valid_1's binary_logloss: 0.131584
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007983 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[65]    training's binary_logloss: 0.113718 valid_1's binary_logloss: 0.139017
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008166 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12906
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[59]    training's binary_logloss: 0.116916 valid_1's binary_logloss: 0.136382
 74%|███████▍  | 37/50 [01:42&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:42&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197] 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008362 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.120211 valid_1's binary_logloss: 0.131444
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011524 seconds.
You can set `force_col_wise=true` to remove the overhead.
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[47]    training's binary_logloss: 0.114602 valid_1's binary_logloss: 0.139106
 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008487 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12968
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.118651 valid_1's binary_logloss: 0.136544
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197] 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007906 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[40]    training's binary_logloss: 0.119116 valid_1's binary_logloss: 0.131438
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010484 seconds.
You can set `force_col_wise=true` to remove the overhead.
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.116085 valid_1's binary_logloss: 0.138771
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008602 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.11889  valid_1's binary_logloss: 0.136703
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197] 80%|████████  | 40/50 [01:46&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009081 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.119904 valid_1's binary_logloss: 0.131539
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011067 seconds.
You can set `force_col_wise=true` to remove the overhead.
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13055
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.115561 valid_1's binary_logloss: 0.139612
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007772 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.118624 valid_1's binary_logloss: 0.136879
 80%|████████  | 40/50 [01:49&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:49&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197] 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012833 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.121601 valid_1's binary_logloss: 0.131732
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012323 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.118977 valid_1's binary_logloss: 0.13905
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008469 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.120814 valid_1's binary_logloss: 0.136438
 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197] 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119106 valid_1's binary_logloss: 0.131122
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011181 seconds.
You can set `force_col_wise=true` to remove the overhead.
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[90]    training's binary_logloss: 0.116567 valid_1's binary_logloss: 0.138873
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007625 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12968
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[85]    training's binary_logloss: 0.118801 valid_1's binary_logloss: 0.136111
 84%|████████▍ | 42/50 [01:54&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:54&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197] 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008375 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12624  valid_1's binary_logloss: 0.132688
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008360 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13130
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121959 valid_1's binary_logloss: 0.140097
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012750 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13000
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123583 valid_1's binary_logloss: 0.137169
 86%|████████▌ | 43/50 [01:57&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:57&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197] 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009549 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[23]    training's binary_logloss: 0.114805 valid_1's binary_logloss: 0.132779
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007723 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.11472  valid_1's binary_logloss: 0.140404
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012523 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.115511 valid_1's binary_logloss: 0.137588
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197] 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008368 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12835
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[70]    training's binary_logloss: 0.115612 valid_1's binary_logloss: 0.131625
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007622 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[57]    training's binary_logloss: 0.114417 valid_1's binary_logloss: 0.139373
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007511 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[62]    training's binary_logloss: 0.114805 valid_1's binary_logloss: 0.136936
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197] 92%|█████████▏| 46/50 [02:02&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008690 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.123339 valid_1's binary_logloss: 0.132372
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009379 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[13]    training's binary_logloss: 0.119633 valid_1's binary_logloss: 0.141193
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008788 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12906
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[13]    training's binary_logloss: 0.12138  valid_1's binary_logloss: 0.137428
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197] 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008990 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12835
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[52]    training's binary_logloss: 0.119292 valid_1's binary_logloss: 0.131268
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007926 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.11486  valid_1's binary_logloss: 0.139012
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007646 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[45]    training's binary_logloss: 0.118593 valid_1's binary_logloss: 0.13685
 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197] 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009040 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123923 valid_1's binary_logloss: 0.132366
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009230 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119581 valid_1's binary_logloss: 0.140569
 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010048 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.121237 valid_1's binary_logloss: 0.137601
 96%|█████████▌| 48/50 [02:10&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:10&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197] 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010404 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.118459 valid_1's binary_logloss: 0.131829
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010339 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.11189  valid_1's binary_logloss: 0.139652
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010263 seconds.
You can set `force_col_wise=true` to remove the overhead.
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.115607 valid_1's binary_logloss: 0.137612
 98%|█████████▊| 49/50 [02:12&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:12&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]100%|██████████| 50/50 [02:12&lt;00:00,  2.55s/trial, best loss: -0.8365225708987197]100%|██████████| 50/50 [02:12&lt;00:00,  2.65s/trial, best loss: -0.8365225708987197]
{'learning_rate': 0.028291797782733982, 'max_depth': 154.0, 'min_child_samples': 64.0, 'num_leaves': 32.0, 'subsample': 0.9145203867432408}</code></pre>
</div>
</div>
</section>
<section id="재학습" class="level3">
<h3 class="anchored" data-anchor-id="재학습">재학습</h3>
<div id="305e505b" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb18-2">                          num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb18-3">                          max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb18-4">                          min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb18-5">                          subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-6">                          learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-7">                          early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb18-8">                          eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb18-9"></span>
<span id="cb18-10">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb18-11">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb18-12"></span>
<span id="cb18-13">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1694, number of negative: 40877
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012601 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 13334
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 209
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039792 -&gt; initscore=-3.183475
[LightGBM] [Info] Start training from score -3.183475
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[131]   training's binary_logloss: 0.118645 valid_1's binary_logloss: 0.137022
[LightGBM] [Warning] Unknown parameter: eval_metric
0.839</code></pre>
</div>
</div>
</section>
</section>
<section id="제출" class="level2">
<h2 class="anchored" data-anchor-id="제출">제출</h2>
<div id="4640afe3" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgbm_clf.predict(test_df)</span>
<span id="cb20-2"></span>
<span id="cb20-3">submit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/sample_submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb20-4">submit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TARGET'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target</span>
<span id="cb20-5">submit.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric</code></pre>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html</guid>
  <pubDate>Sat, 26 Jul 2025 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
