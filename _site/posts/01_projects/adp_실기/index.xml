<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/01_projects/adp_실기/</link>
<atom:link href="https://cryscham123.github.io/posts/01_projects/adp_실기/index.xml" rel="self" type="application/rss+xml"/>
<description>ADP 실기를 준비해 봅시다.</description>
<image>
<url>https://cryscham123.github.io/profile.jpg</url>
<title>김형훈의 학습 블로그</title>
<link>https://cryscham123.github.io/posts/01_projects/adp_실기/</link>
</image>
<generator>quarto-1.5.56</generator>
<lastBuildDate>Fri, 17 Oct 2025 15:00:00 GMT</lastBuildDate>
<item>
  <title>try 1 후기</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/11.html</link>
  <description><![CDATA[ 




<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>개같이 망했다.</p>
<p>사실 조금 방심했다.</p>
<p>통계파트가 다 아는 문제가 나와서 음..이거 잘 하면 합격하겠는데? 라는 생각이 들었다.</p>
<p>그래서 여유롭게 1시간동안 풀고 화장실도 다녀왔다.</p>
</section>
<section id="머신러닝" class="level2">
<h2 class="anchored" data-anchor-id="머신러닝">머신러닝</h2>
<p>이런 젠장. 머신러닝 파트 왜 이렇게 오래 걸리는거야?</p>
<p>분명 풀 수 있는 문제지만, 시간을 보니 10분밖에 안 남아 있었다.</p>
<p>결국 2번 문제는 전처리조차 하지 못하고 통으로 버릴 수 밖에 없었다.</p>
<p>물론 통으로 버려도 합격은 할 수 있다. 그런 사람이 실제로 있는지는 잘 모르겠지만.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>ADP 실기 시험까지 오는 여정은 의미가 깊었다. 진심으로 그렇게 생각한다.</p>
<p>그래서 결과가 좋지 않더라도 실망은 10% 정도만 할 것 같다.</p>
<p>하지만 계속 시험을 준비하는 것은 다른 이야기다. 내가 6개월을 더 준비할 가치가 아직 남아있을까?</p>
<p>솔직히 잘 모르겠다. 일단은 다른 것들을 병행해보면서 생각을 정리해보려 한다.</p>
<p>당장 눈 앞에 닥친 중간고사나 제대로 준비해보자.</p>
<p>아아! 한 번에 붙을 수 있었는데! (결과는 아직 안나오긴 했다.)</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/11.html</guid>
  <pubDate>Fri, 17 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/00.pdf</link>
  <description><![CDATA[ undefined ]]></description>
  <category>자격증</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/00.pdf</guid>
  <pubDate>Tue, 07 Oct 2025 12:59:26 GMT</pubDate>
</item>
<item>
  <title>확률과 통계</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/10.html</link>
  <description><![CDATA[ 




<section id="통계학" class="level2">
<h2 class="anchored" data-anchor-id="통계학">통계학</h2>
<ul>
<li><strong>불확실한 상황</strong> 하에서 데이터에 근거하여 <strong>과학적인 의사결정</strong>을 도출하기 위한 이론과 방법의 체계</li>
<li><strong>모집단</strong>으로 부터 수집된 <strong>데이터</strong>(sample)를 기반으로 모집단의 <strong>특성을 추론</strong>하는 것을 목표로 한다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-03-08-12-28-23.png" class="img-fluid figure-img"></p>
<figcaption>통계적 의사결정 과정</figcaption>
</figure>
</div>
</section>
<section id="확률" class="level2">
<h2 class="anchored" data-anchor-id="확률">확률</h2>
<ul>
<li>고전적 의미: 표본공간에서 특정 사건이 차지하는 비율</li>
<li>통계적 의미: 특정 사건이 발생하는 <strong>상대도수의 극한</strong>
<ul>
<li>각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.</li>
</ul></li>
</ul>
</section>
<section id="확률-분포-정의-단계" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포-정의-단계">확률 분포 정의 단계</h2>
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-03-08-13-00-48.png" class="img-fluid"></p>
<ul>
<li><strong>Experiment(확률실험)</strong>: 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측</li>
<li><strong>Sample space(표본공간)</strong>: 모든 simple event의 집합</li>
<li><strong>Event(사건)</strong>: 실험에서 발생하는 결과 (부분 집합)</li>
<li><strong>Simple event(단순사건)</strong>: 원소가 하나인 사건</li>
<li><strong>확률 변수</strong>: 확률실험의 결과를 수치로 나타낸 변수</li>
</ul>
</section>
<section id="확률-분포" class="level2">
<h2 class="anchored" data-anchor-id="확률-분포">확률 분포</h2>
<section id="이산-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="이산-확률-분포">이산 확률 분포</h3>
<p>이산 표본 공간, 연속 표본공간에서 정의 가능포</p>
<ul>
<li><strong>베르누이 시행</strong>: 각 시행은 서로 <strong>독립적</strong>이고, 실패와 성공 <strong>두 가지 결과만 존재</strong>.
<ul>
<li>단 <strong>모집단의 크기가 충분히 크고</strong>, <strong>표본(시행)의 크기가 충분히 작다면</strong> <strong>비복원 추출</strong>에서도 <strong>유효</strong></li>
<li>평균: p</li>
<li>분산: p(1-p)</li>
</ul></li>
<li><strong>이항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 성공 횟수를 측정
<ul>
<li>X ~ B(n, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bn%7D%7Bx%7D%20p%5Ex%20(1-p)%5E%7Bn-x%7D"></li>
<li>평균: np</li>
<li>분산: np(1-p)</li>
<li>n이 매우 크고, p가 매우 작을 때, <strong>포아송 분포로 근사</strong>할 수 있다. (λ = np)</li>
</ul></li>
<li><strong>음이항 분포</strong>
<ul>
<li>정의: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k번 성공하고, r번 실패한 경우 (n = k + r)
<ol type="1">
<li>r번의 실패가 나오기 전까지, 성공한 횟수 x
<ul>
<li>X ~ NB(r, p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx+r-1%7D%7Bx%7D%20p%5Ex%20(1-p)%5Er"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B1-p%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Brp%7D%7B(1-p)%5E2%7D"></li>
</ul></li>
<li>r번의 실패가 나오기 전까지, 시행한 횟수 x
<ul>
<li>4번에서 성공을 실패로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 실패한 횟수 x
<ul>
<li>1번에서 실패를 성공으로 바꿈</li>
</ul></li>
<li>k번의 성공이 나오기 전까지, 시행한 횟수 x
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cbinom%7Bx-1%7D%7Bk-1%7D%20p%5Ek%20(1-p)%5E%7Bx-k%7D"></li>
<li>k가 1일 때 기하분포와 동일</li>
</ul></li>
<li>n번의 시행 횟수에서, k번 성공 또는 r번 실패한 경우: 이항분포</li>
</ol></li>
</ul></li>
<li><strong>기하 분포</strong>:
<ul>
<li>정의:
<ol type="1">
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 시행 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5E%7Bx-1%7D%20p,%20x%20=%201,%202,%203,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
<li>성공 확률이 p인 <strong>베르누이 시행</strong>에서 첫 성공까지의 실패 횟수
<ul>
<li>X ~ G(p), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20(1-p)%5Ex%20p,%20x%20=%200,%201,%202,%20..."></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1-p%7D%7Bp%5E2%7D"></li>
</ul></li>
</ol></li>
<li><strong>비기억 특성</strong>: <img src="https://latex.codecogs.com/png.latex?P(X%20%3E%20n+k%20%7C%20X%20%3E%20n)%20=%20P(X%20%3E%20k)"></li>
</ul></li>
<li><strong>초기하 분포</strong>: <strong>베르누이 시행이 아닌 시행</strong>에서 성공하는 횟수
<ul>
<li>X ~ H(n, N, k), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B%5Cbinom%7BK%7D%7Bx%7D%20%5Cbinom%7BN-K%7D%7Bn-x%7D%7D%7B%5Cbinom%7BN%7D%7Bn%7D%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK%7D%7BN%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BnK(N-K)(N-n)%7D%7BN%5E2(N-1)%7D"></li>
</ul></li>
<li><strong>포아송 분포</strong>: <strong>임의의 기간</strong>동안 <strong>어떤 사건이 간헐적</strong>으로 발생할 때, 동일한 길이의 기간동안 실제 사건이 발생하는 횟수
<ul>
<li>X ~ Poisson(λ), <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7Be%5E%7B-%CE%BB%7D%20%CE%BB%5Ex%7D%7Bx!%7D,%20%CE%BB%20%3E%200"></li>
<li>평균: λ</li>
<li>분산: λ</li>
</ul></li>
</ul>
</section>
<section id="연속-확률-분포" class="level3">
<h3 class="anchored" data-anchor-id="연속-확률-분포">연속 확률 분포</h3>
<p>연속 표본 공간에서 정의 가능</p>
<ul>
<li><strong>균일 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7Bb-a%7D,%20a%20%E2%89%A4%20x%20%E2%89%A4%20b"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Ba+b%7D%7B2%7D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(b-a)%5E2%7D%7B12%7D"></li>
</ul></li>
<li><strong>정규 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20+%20Y%20%5Csim%20N(%CE%BC_1%20+%20%CE%BC_2,%20%CF%83_1%5E2%20+%20%CF%83_2%5E2)"></li>
<li>선형 변환: <img src="https://latex.codecogs.com/png.latex?Y%20=%20aX%20+%20b%20%5Csim%20N(a%CE%BC%20+%20b,%20a%5E2%CF%83%5E2)"></li>
</ul></li>
<li><strong>t 분포</strong>
<ul>
<li>자유도가 커질수록 표준 정규분포에 근사함.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BZ%7D%7B%5Csqrt%7BV/n%7D%7D%20%5Csim%20t(n)">, Z: 표준정규분포, V: 자유도가 n인 카이제곱분포</li>
</ul></li>
<li><strong>f 분포</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?F%20=%20%5Cfrac%7BX_1/%CE%BD_1%7D%7BX_2/%CE%BD_2%7D">, <img src="https://latex.codecogs.com/png.latex?X_1%20%5Csim%20%CF%87%5E2(%CE%BD_1)">, <img src="https://latex.codecogs.com/png.latex?X_2%20%5Csim%20%CF%87%5E2(%CE%BD_2)">, X1과 X2는 서로 독립</li>
</ul></li>
<li><strong>감마 분포</strong>
<ul>
<li>α: 분포의 형태 결정, θ: 분포의 크기 결정</li>
<li>평균: αθ</li>
<li>분산: αθ²</li>
<li><strong>카이제곱 분포</strong>: α = v/2, θ = 2 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z_i%20%5Csim%20N(0,1)">일 때, <img src="https://latex.codecogs.com/png.latex?Z_1%5E2%20+%20Z_2%5E2%20+%20...%20%20+%20Z_n%5E2%20%5Csim%20%CF%87%5E2(n)"></li>
<li><img src="https://latex.codecogs.com/png.latex?X_i">가 서로 독립이고, 자유도가 <img src="https://latex.codecogs.com/png.latex?%CE%BD_i">인 카이제곱분포를 따른다면, <img src="https://latex.codecogs.com/png.latex?X_1%20+%20X_2%20+%20...%20+%20X_n%20%5Csim%20x%5E2(%CE%BD_1%20+%20%CE%BD_2%20+%20...%20+%20%CE%BD_n)"></li>
<li>자유도가 커질수록 기댓값을 중심으로 모이고, 대칭에 가까워진다.</li>
</ul></li>
<li><strong>지수 분포</strong>: α = 1, θ = 1/λ 인 감마분포
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20~%20Exp(%CE%BB%20=%20%5Cfrac%7B1%7D%7B%CE%B8%7D)">, f(x) = <img src="https://latex.codecogs.com/png.latex?%CE%BBe%5E%7B-%CE%BBx%7D,%20x%20%3E%200"></li>
<li>θ: 평균 사건 발생 간격, λ: 단위 시간당 사건 발생 횟수</li>
<li>포아송 분포에서 사건 발생 간격의 분포</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7Bn%7D%20X_i%20%5Csim%20%CE%93(n,%20%CE%B8)">, <img src="https://latex.codecogs.com/png.latex?%CE%B8%20=%201/%CE%BB"></li>
<li>비기억 특성을 가진다: <img src="https://latex.codecogs.com/png.latex?p(X%20%3E%20s%20+%20t%20%7C%20X%20%3E%20s)%20=%20p(X%20%3E%20t)%20=%20e%5E%7B-%CE%BBt%7D"></li>
<li>독립적으로 동일한 지수분포를 따르는 확률변수 n개의 합은 <img src="https://latex.codecogs.com/png.latex?%CE%B1%20=%20n,%20%CE%B8%20=%20%5Cfrac%7B1%7D%7B%CE%BB%7D">인 감마분포를 따른다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="다변량-분포" class="level3">
<h3 class="anchored" data-anchor-id="다변량-분포">다변량 분포</h3>
<ul>
<li><strong>다항 분포</strong>: n번의 독립적인 <strong>베르누이 시행</strong>을 수행하여 k개의 범주로 분류
<ul>
<li>X ~ M(n, p1, p2, …, pk), <img src="https://latex.codecogs.com/png.latex?f(x_1,%20x_2,%20...,%20x_k)%20=%20%5Cfrac%7Bn!%7D%7Bx_1!%20x_2!%20...%20x_k!%7D%20p_1%5E%7Bx_1%7D%20p_2%5E%7Bx_2%7D%20...%20p_k%5E%7Bx_k%7D"></li>
<li>평균: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1,%20np_2,%20...,%20np_k%5D"></li>
<li>분산: <img src="https://latex.codecogs.com/png.latex?%5Bnp_1(1-p_1),%20np_2(1-p_2),%20...,%20np_k(1-p_k)%5D"></li>
<li>공분산: <img src="https://latex.codecogs.com/png.latex?-np_ip_j%20(i%20%E2%89%A0%20j)"></li>
<li>독립인 변수의 갯수는 k-1개 (k개의 사건)</li>
</ul></li>
</ul>
</section>
<section id="샘플링" class="level3">
<h3 class="anchored" data-anchor-id="샘플링">샘플링</h3>
</section>
<section id="분포의-동질성-검정" class="level3">
<h3 class="anchored" data-anchor-id="분포의-동질성-검정">분포의 동질성 검정</h3>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="표본의-분포" class="level2">
<h2 class="anchored" data-anchor-id="표본의-분포">표본의 분포</h2>
<ul>
<li><p>샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.</p></li>
<li><p>통계량: 표본의 특성을 나타내는 값</p></li>
<li><p>추정량: 아래의 조건을 만족하는 통계량</p>
<ul>
<li>불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.</li>
<li>효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.</li>
</ul></li>
</ul>
<section id="표본-평균의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-평균의-분포">표본 평균의 분포</h3>
<ul>
<li>모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2">이면, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D">의 평균은 μ이고, 분산은 <img src="https://latex.codecogs.com/png.latex?%CF%83%5E2/n">인 정규분포를 따른다.
<ul>
<li>단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리<sup>1</sup>)</li>
</ul></li>
<li>만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.
<ul>
<li>단 이때는 <strong>모집단이 정규분포를 따라야 한다.</strong></li>
</ul></li>
</ul>
</section>
<section id="표본-분산의-분포" class="level3">
<h3 class="anchored" data-anchor-id="표본-분산의-분포">표본 분산의 분포</h3>
<ul>
<li><strong>정규 모집단으로 부터 나온 표본</strong>의 분산 S에 대하여, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B(n-1)S%5E2%7D%7B%CF%83%5E2%7D">은 자유도가 n-1인 카이제곱 분포를 따른다.
<ul>
<li>모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.</li>
</ul></li>
<li>두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.</li>
</ul>
</section>
</section>
<section id="추정" class="level2">
<h2 class="anchored" data-anchor-id="추정">추정</h2>
<ul>
<li>통계적 추론: <strong>모집단에서 추출된 표본</strong>의 <strong>통계량</strong>으로부터 <strong>모수</strong>를 추론하는 것
<ul>
<li>추정
<ul>
<li>점추정</li>
<li>구간추정</li>
</ul></li>
<li>가설 검정</li>
</ul></li>
</ul>
<section id="점-추정" class="level3">
<h3 class="anchored" data-anchor-id="점-추정">점 추정</h3>
<ul>
<li>불편성
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20=%20%CE%B8"></li>
<li>bias = <img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Ctheta%7D)%20-%20%5Ctheta">
<ul>
<li>보통 sample size가 커질수록 bias는 0에 수렴</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D,%20X_n">은 μ의 불편추정량이다.</li>
</ul></li>
<li>최소분산
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Var(%5Cbar%7BX%7D)">가 <img src="https://latex.codecogs.com/png.latex?Var(X_n)">보다 분산이 작아서 더 좋은 추정량</li>
<li><img src="https://latex.codecogs.com/png.latex?MSE(%5Chat%7B%5Ctheta%7D)%20=%20E%5B(%5Chat%7B%5Ctheta%7D%20-%20%5Ctheta)%5E2%5D%20=%20Var(%5Chat%7B%5Ctheta%7D)%20+%20bias%5E2">
<ul>
<li>큰 오차에 더 큰 페널티를 주기 위해 제곱</li>
</ul></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-09-06-20-37-05.png" class="img-fluid figure-img"></p>
<figcaption>대표적인 불편추정량</figcaption>
</figure>
</div>
<ul>
<li><strong>전부 중심극한의정리를 적용</strong>할 수 있다. (비율은 0과 1의 평균이므로)</li>
<li>모평균, 모비율의 차이는 서로 독립이라는 가정이 필요하다.</li>
</ul>
</section>
<section id="구간-추정" class="level3">
<h3 class="anchored" data-anchor-id="구간-추정">구간 추정</h3>
<ul>
<li>α: 유의수준</li>
<li>1 - α: 신뢰수준<sup>2</sup></li>
<li>(θ_L, θ_U) = (1 - α) × 100% 신뢰구간</li>
</ul>
<ol type="1">
<li>(<img src="https://latex.codecogs.com/png.latex?%CE%B8_L,%20%CE%B8_U">) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다</li>
<li>구간이 충분히 좁아야 한다
<ul>
<li>표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.</li>
<li>고로 신뢰구간이 대칭임</li>
</ul></li>
</ol>
</section>
<section id="표본의-크기-결정" class="level3">
<h3 class="anchored" data-anchor-id="표본의-크기-결정">표본의 크기 결정</h3>
<p>특정 오차 아래로 하는 표본의 수 구하는 법</p>
<ul>
<li>그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.</li>
<li><strong>모비율</strong>을 모를 때는 일단 <strong>0.5로 보수적으로 놓고 계산</strong></li>
</ul>
</section>
</section>
<section id="모분산-추정" class="level2">
<h2 class="anchored" data-anchor-id="모분산-추정">모분산 추정</h2>
<ul>
<li>카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음
<ul>
<li>그냥 쉽게 구하기 위해 <img src="https://latex.codecogs.com/png.latex?(x%5E2_%7B%CE%B1/2%7D,%20x%5E2_%7B1-%CE%B1/2%7D)">를 사용</li>
</ul></li>
<li>모분산의 신뢰구간: <img src="https://latex.codecogs.com/png.latex?(%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B(1-%5Calpha)/2%7D(n-1)%7D,%20%5Cfrac%7B(n-1)s%5E2%7D%7Bx%5E2_%7B%5Calpha/2%7D(n-1)%7D)"></li>
<li>표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.</li>
</ul>


</section>


<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">각주</h2>

<ol>
<li id="fn1"><p>모집단의 분포와 상관 없이, 표본의 평균은 정규분포에 수렴한다는 정리. 이항분포의 경우, P(X=c) ~ P(c - 0.5 &lt; X &lt; c + 0.5)로 근사 가능하다는 라플라스의 정리를 일반화한 것↩︎</p></li>
<li id="fn2"><p>샘플링을 무한히 반복했을 때, <strong>이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함</strong>한다. 즉, 구간이 확률 변수이다.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/10.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>기타</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/09.html</link>
  <description><![CDATA[ 




<section id="or" class="level2">

<ul>
<li>pulp 이용해서 푼다.</li>
<li>제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pulp example</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="pulp example" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pulp</span>
<span id="cb1-2"></span>
<span id="cb1-3">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpProblem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Problem_name"</span>, pulp.LpMinimize) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문제에 맞게 설정</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)</span></span>
<span id="cb1-6">x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-7">x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-8">x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-9">x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_1"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-10">x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_2"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-11">x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pulp.LpVariable(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B_to_3"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, pulp.LpInteger)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 목표 함수 정의 (총 운송 비용)</span></span>
<span id="cb1-14">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x_B3, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"obj_name"</span></span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 제약 조건 정의</span></span>
<span id="cb1-17">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_A_Supply"</span></span>
<span id="cb1-18">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Factory_B_Supply"</span></span>
<span id="cb1-19">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_1_Demand"</span></span>
<span id="cb1-20">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_2_Demand"</span></span>
<span id="cb1-21">prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x_A3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x_B3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">130</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Warehouse_3_Demand"</span></span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5. 문제 풀이</span></span>
<span id="cb1-24">prob.solve()</span>
<span id="cb1-25"></span>
<span id="cb1-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 6. 결과 확인</span></span>
<span id="cb1-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Status:"</span>, pulp.LpStatus[prob.status])</span>
<span id="cb1-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Optimal Transportation Plan:"</span>)</span>
<span id="cb1-29"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> prob.variables():</span>
<span id="cb1-30">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(v.name, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"="</span>, v.varValue)</span>
<span id="cb1-31"></span>
<span id="cb1-32"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Total Minimum Cost = "</span>, pulp.value(prob.objective))</span></code></pre></div>
</div>
<ul>
<li>그 외 자주 안나오는 문제 예상 관련 글이 올라올 경우 추가 예정</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/09.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>시계열 분석</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/07.html</link>
  <description><![CDATA[ 




<section id="구성-요소" class="level2">
<h2 class="anchored" data-anchor-id="구성-요소">구성 요소</h2>
<ul>
<li><code>추세</code>(level)</li>
<li><code>계절, 순한</code>: 추세에서 벗어나는 변화의 정도</li>
<li><code>잔차</code>(white noise)</li>
</ul>
</section>
<section id="eda" class="level2">
<h2 class="anchored" data-anchor-id="eda">EDA</h2>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">df_isna <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.asfreq(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'H'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간 기준 결측치 탐색. index가 datetime이어야 함</span></span>
<span id="cb1-2">df_isna[df_isna.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.seasonal <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> STL</span>
<span id="cb2-2"></span>
<span id="cb2-3">decomposition <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> STL(df_final[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], period<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>).fit() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># period는 계절성 주기</span></span>
<span id="cb2-4">fig, (ax1, ax2, ax3, ax4) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sharex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb2-5">ax1.plot(decomposition.observed)</span>
<span id="cb2-6">ax1.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Obseved'</span>)</span>
<span id="cb2-7"></span>
<span id="cb2-8">ax2.plot(decomposition.trend)</span>
<span id="cb2-9">ax2.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Trend'</span>)</span>
<span id="cb2-10"></span>
<span id="cb2-11">ax3.plot(decomposition.seasonal)</span>
<span id="cb2-12">ax3.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Seasonal'</span>)</span>
<span id="cb2-13"></span>
<span id="cb2-14">ax4.plot(decomposition.resid)</span>
<span id="cb2-15">ax4.set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Residuals'</span>)</span>
<span id="cb2-16"></span>
<span id="cb2-17">plt.show()</span></code></pre></div>
</section>
<section id="예측-방법" class="level2">
<h2 class="anchored" data-anchor-id="예측-방법">예측 방법</h2>
<p><strong>Rolling Forecast (동적 예측)</strong>: - 각 시점에서 예측을 수행한 후, 실제 값이 관측되면 이를 학습 데이터에 추가하여 다음 시점을 예측 - 실제 운영 환경을 시뮬레이션하는 방법으로, 새로운 정보가 지속적으로 업데이트됨 - 장점: 최신 정보를 반영하여 더 정확한 예측 가능 - 단점: 계산 비용이 높고, 모델 성능 평가 시 과적합 위험</p>
<p><strong>Static Forecast (정적 예측)</strong>: - 초기 학습 데이터로만 모델을 한 번 학습하고, 이후 테스트 기간 전체에 대해 연속적으로 예측 - 고정된 모델로 여러 시점을 예측하므로 모델의 일반화 성능을 더 엄격하게 평가 - 장점: 계산 비용이 낮고, 공정한 모델 평가 가능 - 단점: 시간이 지날수록 예측 정확도가 떨어질 수 있음</p>
</section>
<section id="단순-기법" class="level2">
<h2 class="anchored" data-anchor-id="단순-기법">단순 기법</h2>
<ul>
<li>단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t"></li>
<li>추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp_%7Bt+1%7D%7D%20=%20p_t%20+%20p_t%20-%20p_%7Bt-1%7D"></li>
<li>단순 이동평균법: time window를 계속 이동하면서 평균 구하는거
<ul>
<li>time window ↑: 먼 과거까지 보겠다</li>
</ul></li>
<li>가중 이동평균법: 가중치를 다르게 부여한 단순이동평균법</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>단순 기법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="단순 기법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_simple(train_data, test_data, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>):</span>
<span id="cb3-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb3-4">    </span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb3-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>:</span>
<span id="cb3-7">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 단순예측법: 마지막 값</span></span>
<span id="cb3-8">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-9">            </span>
<span id="cb3-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>:</span>
<span id="cb3-11">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추세분석법</span></span>
<span id="cb3-12">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-13">            </span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>:</span>
<span id="cb3-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4기간 이동평균</span></span>
<span id="cb3-16">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-17">            </span>
<span id="cb3-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>:</span>
<span id="cb3-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12기간 이동평균</span></span>
<span id="cb3-20">            window <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb3-21">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>window:][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>])</span>
<span id="cb3-22">            </span>
<span id="cb3-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> method <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>:</span>
<span id="cb3-24">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 단순예측법 (4분기 전과 동일)</span></span>
<span id="cb3-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>:</span>
<span id="cb3-26">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-27">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb3-28">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>]</span>
<span id="cb3-29">        </span>
<span id="cb3-30">        predictions.append(pred)</span>
<span id="cb3-31">        </span>
<span id="cb3-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb3-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb3-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-35">    </span>
<span id="cb3-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb3-37"></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 방법별 rolling forecast 수행</span></span>
<span id="cb3-39">methods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'naive'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'trend'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_4'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ma_12'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'seasonal_naive'</span>]</span>
<span id="cb3-40">rolling_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb3-41"></span>
<span id="cb3-42"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> methods:</span>
<span id="cb3-43">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_simple(train, test, method)</span>
<span id="cb3-44">    rolling_results[method] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> predictions</span>
<span id="cb3-45">    </span>
<span id="cb3-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결과를 DataFrame으로 정리</span></span>
<span id="cb3-47">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(rolling_results)</span>
<span id="cb3-48">results_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'actual'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data'</span>].values</span>
<span id="cb3-49">results_df.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test.index</span>
<span id="cb3-50"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results_df)</span></code></pre></div>
</div>
</section>
<section id="지수-평활법" class="level2">
<h2 class="anchored" data-anchor-id="지수-평활법">지수 평활법</h2>
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-10-14-19-44-17.png" class="img-fluid"></p>
<ul>
<li>α -&gt; 1: 최근 자료에 비중을 둠. α -&gt; 0: 기존 예측을 따름</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>지수 평활법 rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="지수 평활법 rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.holtwinters <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SimpleExpSmoothing</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_exponential_smoothing(train_data, test_data, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>):</span>
<span id="cb4-4">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb4-5">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb4-6">    </span>
<span id="cb4-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb4-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb4-9">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb4-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SimpleExpSmoothing(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-11">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(smoothing_level<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>a, optimized<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-12"></span>
<span id="cb4-13">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb4-14">            pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.predict(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_extended))</span>
<span id="cb4-15">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb4-16"></span>
<span id="cb4-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb4-18">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb4-19">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb4-20">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb4-21">        </span>
<span id="cb4-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb4-23">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb4-24">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-25">    </span>
<span id="cb4-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb4-27"></span>
<span id="cb4-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 지수 평활법 rolling forecast 실행 예시</span></span>
<span id="cb4-29">exp_smooth_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_exponential_smoothing(train, test, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
</section>
<section id="sarimax-계열" class="level2">
<h2 class="anchored" data-anchor-id="sarimax-계열">SARIMAX 계열</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-10-14-08-07-42.png" class="img-fluid figure-img"></p>
<figcaption>SARIMAX 과정</figcaption>
</figure>
</div>
<section id="정상성-확인" class="level3">
<h3 class="anchored" data-anchor-id="정상성-확인">정상성 확인</h3>
<ul>
<li>정상시계열은 분산과 평균, 자기 상관이 시간에 따라 변하지 않는 시계열</li>
<li>분산이 일정하지 않은 경우: 로그 변환
<ul>
<li>그래프로 확인</li>
</ul></li>
<li>평균이 일정하지 않은 경우: 차분</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>stationarity test</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="stationarity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.stattools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> adfuller</span>
<span id="cb5-2"></span>
<span id="cb5-3">ad_fuller_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> adfuller(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ADF Statistic:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'p-value:'</span>, ad_fuller_result[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<ul>
<li>p-value &lt; 0.05: 귀무가설 기각, 정상시계열</li>
<li>주로 평균이 일정하지 않은 것을 찾아낼 수 있다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>difference</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff()</span></code></pre></div>
</div>
<ul>
<li>만약 차분 후에도 정상성이 만족되지 않는다면, 계절 차분을 고려</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>seasonal difference</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="seasonal difference" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.diff(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 12개월 주기</span></span></code></pre></div>
</div>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.graphics.tsaplots <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_acf, plot_pacf</span>
<span id="cb8-2"></span>
<span id="cb8-3">plot_acf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-4">plot_pacf(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>].dropna(), lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>)</span>
<span id="cb8-5">plt.show()</span></code></pre></div>
<ul>
<li>자기 상관이 존재하는 경우
<ul>
<li>ACF, PACF 그래프로 확인</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-10-14-08-43-44.png" class="img-fluid figure-img"></p>
<figcaption>모델 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>하지만 매우 주관적일 수 있으므로, 직접 여러 모델을 돌려본 후 AIC 기준으로 선택.
<ul>
<li>그리고 다시 검정 진행</li>
</ul></li>
</ul>
</section>
<section id="sarimax-모델-적합" class="level3">
<h3 class="anchored" data-anchor-id="sarimax-모델-적합">SARIMAX 모델 적합</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="SARIMAX" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.tsa.statespace.sarimax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SARIMAX</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> product</span>
<span id="cb9-3"></span>
<span id="cb9-4">p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-5">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 차분 횟수</span></span>
<span id="cb9-6">q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-7">P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-8">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 차분 횟수</span></span>
<span id="cb9-9">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb9-10">s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절 주기</span></span>
<span id="cb9-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> product(p, q, P, Q)</span>
<span id="cb9-12">order_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(parameters)</span>
<span id="cb9-13"></span>
<span id="cb9-14">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-15"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> order <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> order_list:</span>
<span id="cb9-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb9-17">        model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb9-18">                        exog,</span>
<span id="cb9-19">                        order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], d, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), </span>
<span id="cb9-20">                        seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], D, order[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], s),</span>
<span id="cb9-21">                        simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span>:</span>
<span id="cb9-23">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span></span>
<span id="cb9-24">    aic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.aic</span>
<span id="cb9-25">    results.append([order, aic])</span>
<span id="cb9-26">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(p, q, P, Q)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>])</span>
<span id="cb9-27">results_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AIC'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb9-28">display(results_df)</span></code></pre></div>
</div>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">best_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb10-2">                    exog,</span>
<span id="cb10-3">                    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [2, 3], 0, 1 이런 식으로도 가능(y_t-2, y_t-3 사용하는 방법)</span></span>
<span id="cb10-4">                    simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">best_model.summary()</span></code></pre></div>
<ul>
<li>결과 확인</li>
</ul>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">best_model.plot_diagnostics(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb12-2">plt.show()</span></code></pre></div>
<ul>
<li>등분산성, 정규성 만족하는지 육안으로 확인</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>ljung-box</strong></pre>
</div>
<div class="sourceCode" id="cb13" data-filename="ljung-box" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.diagnostic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> acorr_ljungbox</span>
<span id="cb13-2"></span>
<span id="cb13-3">ljung_box_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> acorr_ljungbox(best_model.resid, lags<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>], return_df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb13-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(ljung_box_result)</span></code></pre></div>
</div>
<ul>
<li>p-value가 0.05보다 크면 잔차가 백색잡음</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>SARIMAX rolling forecast</strong></pre>
</div>
<div class="sourceCode" id="cb14" data-filename="SARIMAX rolling forecast" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> rolling_forecast_sarimax(train_data, test_data, order, seasonal_order, exog_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, exog_test<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-2">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-3">    train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data.copy()</span>
<span id="cb14-4">    exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exog_train.copy() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_train <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb14-5">    </span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(test_data)):</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb14-8">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 적합</span></span>
<span id="cb14-9">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-10">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-11">                               exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_extended,</span>
<span id="cb14-12">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-13">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-14">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-16">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SARIMAX(train_extended[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>], </span>
<span id="cb14-17">                               order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>order, </span>
<span id="cb14-18">                               seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seasonal_order,</span>
<span id="cb14-19">                               simple_differencing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-20">            model_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb14-21">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측</span></span>
<span id="cb14-22">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-23">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, exog<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb14-24">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb14-25">                pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_fit.forecast(steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-26">            predictions.append(pred.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb14-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb14-28">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Error at step </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb14-29">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 오류 발생시 naive 예측 사용</span></span>
<span id="cb14-30">            predictions.append(train_extended.iloc[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span>
<span id="cb14-31">        </span>
<span id="cb14-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 실제 값을 train에 추가 (rolling)</span></span>
<span id="cb14-33">        actual_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_data.iloc[i]</span>
<span id="cb14-34">        train_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([train_extended, actual_value.to_frame().T], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-35">        </span>
<span id="cb14-36">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exog_extended <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> exog_test <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-37">            exog_extended <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([exog_extended, exog_test.iloc[i:i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]], ignore_index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb14-38">    </span>
<span id="cb14-39">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> predictions</span>
<span id="cb14-40"></span>
<span id="cb14-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SARIMAX rolling forecast 실행 예시</span></span>
<span id="cb14-42">sarimax_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rolling_forecast_sarimax(</span>
<span id="cb14-43">    train, test, </span>
<span id="cb14-44">    order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), </span>
<span id="cb14-45">    seasonal_order<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 계절성이 없는 경우</span></span>
<span id="cb14-46">)</span></code></pre></div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/07.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>비지도 학습 템플릿</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/06.html</link>
  <description><![CDATA[ 




<section id="군집-분석" class="level2">
<h2 class="anchored" data-anchor-id="군집-분석">군집 분석</h2>
<section id="distance-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="distance-based-methods">Distance-based methods</h3>
<ul>
<li>Partitioning methods</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>k-means</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="k-means" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KMeans</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-3"></span>
<span id="cb1-4">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># or RobustScaler</span></span>
<span id="cb1-5">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Elbow method</span></span>
<span id="cb1-8">I <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>):</span>
<span id="cb1-10">    kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>i) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sklearn은 기본적으로 k-means++</span></span>
<span id="cb1-11">    kmeans.fit(df_scaled)</span>
<span id="cb1-12">    I.append(kmeans.inertia_)</span>
<span id="cb1-13">plt.plot(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>), I, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k 선택 후 군집화</span></span>
<span id="cb1-16">kmeans <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KMeans(n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-17">kmeans.fit(df_scaled)</span>
<span id="cb1-18">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cluster'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kmeans.labels_</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 군집 중심값 정보</span></span>
<span id="cb1-21">centers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.inverse_transform(kmeans.cluster_centers_)</span>
<span id="cb1-22">centers_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(centers, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df.columns[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'cluster_</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(centers.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])])</span>
<span id="cb1-23">display(centers_df)</span></code></pre></div>
</div>
<ol type="1">
<li>Kmeans
<ul>
<li>polinominal 시간 안에 해결 가능</li>
<li>noise, outlier에 민감함</li>
<li>수치형만 처리 가능</li>
</ul></li>
<li>k-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함</li>
<li>k-prototype: 범주형, 수치형 섞인거 처리 가능</li>
<li>k-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함
<ul>
<li>PAM: Partitioning Around Medoids
<ul>
<li>scalability 문제 있음</li>
</ul></li>
<li>CLARA: sampling을 통해서 PAM의 scalability 문제를 해결
<ul>
<li>샘플링 과정에서 biased될 수 있음</li>
</ul></li>
<li>CLARANS: medoid 후보를 랜덤하게 선택함</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p>k-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함 ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)</p>
</blockquote>
<ul>
<li>Hierarchical methods</li>
</ul>
<ol type="1">
<li>top-down: divisive, dia</li>
<li>bottom-up: agglomerative</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>agglomerative</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="agglomerative" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.cluster.hierarchy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dendrogram, linkage. cut_tree</span>
<span id="cb2-2"></span>
<span id="cb2-3">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linkage(df_scaled, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ward'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 'single', 'complete', 'average', 'ward' 등등</span></span>
<span id="cb2-4"></span>
<span id="cb2-5">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cut_tree(z, n_clusters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).flatten()</span>
<span id="cb2-6"></span>
<span id="cb2-7">d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dendogram(z, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index))</span>
<span id="cb2-8">plt.show()</span></code></pre></div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>합쳐지는 거리</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="합쳐지는 거리" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">thr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dcoord'</span>])</span>
<span id="cb3-2">thr <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이</span></span>
<span id="cb3-3">thr[thr[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>].sort_values(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 70 이상의 높이에서 합쳐지는 군집들의 높이</span></span></code></pre></div>
</div>
<ul>
<li>거리기반 군집의 단점:
<ul>
<li>군집의 모양이 구형이 아닐 경우 찾기 어려움</li>
<li>군집의 갯수 결정하기 어려움</li>
<li>군집의 밀도가 높아야함</li>
</ul></li>
</ul>
</section>
<section id="density-based-methods" class="level3">
<h3 class="anchored" data-anchor-id="density-based-methods">Density-based methods</h3>
<ul>
<li>다양한 모양의 군집을 찾을 수 있음</li>
<li>noise, outlier에 강함</li>
</ul>
<ol type="1">
<li>DBSCAN: 잡음 포인트는 군집에서 제외
<ol type="1">
<li>core point를 찾음(eps 이내에 minPts 이상 있는 점)</li>
<li>core point를 중심으로 군집을 확장
<ul>
<li>core point가 아닌 경우 확장 종료</li>
</ul></li>
</ol>
<ul>
<li>고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음</li>
<li>군집간 계층관계를 인식하기 어렵다</li>
<li>대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN eps 결정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="DBSCAN eps 결정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.neighbors <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> NearestNeighbors</span>
<span id="cb4-2"></span>
<span id="cb4-3">MIN_SAMPLES <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb4-4">df_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(df)</span>
<span id="cb4-5"></span>
<span id="cb4-6">neigh <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NearestNeighbors(n_neighbors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES)</span>
<span id="cb4-7">neigh.fit(df_scaled)</span>
<span id="cb4-8">distances, indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> neigh.kneighbors(df_scaled)</span>
<span id="cb4-9"></span>
<span id="cb4-10">k_distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sort(distances[:, MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])[::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot 그리기</span></span>
<span id="cb4-13">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb4-14">plt.plot(k_distances)</span>
<span id="cb4-15">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data Points sorted by distance'</span>)</span>
<span id="cb4-16">plt.ylabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>MIN_SAMPLES<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">-th Nearest Neighbor Distance'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 자기 자신을 제외한 거리</span></span>
<span id="cb4-17">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'k-distance Graph'</span>)</span>
<span id="cb4-18">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-19">plt.show()</span></code></pre></div>
</div>
<ul>
<li>min samples 기준
<ul>
<li>2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.</li>
<li>이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기</li>
</ul></li>
<li>eps 기준
<ul>
<li>k-dist plot에서 급격히 꺾이는 지점</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>DBSCAN</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="DBSCAN" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DBSCAN</span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 찾은 값으로 DBSCAN 적용</span></span>
<span id="cb5-4">eps_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">18</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># k-dist plot에서 결정한 값</span></span>
<span id="cb5-5"></span>
<span id="cb5-6">db <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DBSCAN(eps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eps_value, min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb5-7">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> db.labels_</span></code></pre></div>
</div>
<ol start="2" type="1">
<li>OPTICS: DBSCAN의 단점을 보완
<ul>
<li>군집의 밀도가 다를 때도 잘 처리함</li>
<li>군집의 계층 구조를 인식할 수 있음</li>
<li>minPts 파라미터가 필요함</li>
<li>얘로도 이상치 탐지 가능</li>
</ul></li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>OPTICS</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="OPTICS" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.cluster <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OPTICS</span>
<span id="cb6-2"></span>
<span id="cb6-3">optics <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OPTICS(min_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>MIN_SAMPLES).fit(df_scaled)</span>
<span id="cb6-4">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optics.labels_</span></code></pre></div>
</div>
</section>
<section id="평가" class="level3">
<h3 class="anchored" data-anchor-id="평가">평가</h3>
<ul>
<li>silhuette score: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20s(i)%7D%7Bn%7D">
<ul>
<li>s(i): <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bb(i)%20-%20a(i)%7D%7Bmax((a(i),%20b(i)))%7D">
<ul>
<li>a(i): 군집 내 노드간의 평균 거리</li>
<li>b(i): 가장 가까운 군집과의 노드 간 평균 거리</li>
</ul></li>
<li>1에 가까울 수록 좋음</li>
</ul></li>
<li>그 외 sklearn metrics 참고</li>
</ul>
</section>
</section>
<section id="차원-축소" class="level2">
<h2 class="anchored" data-anchor-id="차원-축소">차원 축소</h2>
<ul>
<li>PCA, LSA, t-SNE, UMAP, ICA, MDS, NMF 등등
<ul>
<li>정말 많은 방법들이 있다.</li>
</ul></li>
<li>요인분석에서 확인적 요인분석은 python에서 제공하는 library가 없는걸로 알고 있음</li>
</ul>
</section>
<section id="연관-분석" class="level2">
<h2 class="anchored" data-anchor-id="연관-분석">연관 분석</h2>
<div id="d0b281f6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TransactionEncoder</span>
<span id="cb7-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlxtend.frequent_patterns <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> apriori, association_rules</span>
<span id="cb7-3"></span>
<span id="cb7-4">transactions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb7-5">te <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TransactionEncoder()</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 전처리</span></span>
<span id="cb7-8">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>).values</span>
<span id="cb7-9">te_ary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> te.fit_transform(data)</span>
<span id="cb7-10">transactions[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(te_ary, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>te.columns_)</span>
<span id="cb7-11"></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈발패턴 생성</span></span>
<span id="cb7-13">fset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apriori(t, min_support<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, use_colnames<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb7-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb7-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"빈발패턴이 존재하지 않습니다."</span>)</span>
<span id="cb7-16"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 연관규칙 생성</span></span>
<span id="cb7-18">    rule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association_rules(fset, metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"confidence"</span>, min_threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb7-19">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_ant'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'antecedents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-20">    rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'consequents'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(x))</span>
<span id="cb7-21">    display(rule[(rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'len_con'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (rule[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lift'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.2</span>)].reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</div>
<ul>
<li>지지도: 전체 거래에서 특정 항목 집합이 나타나는 비율</li>
<li>신뢰도: 특정 항목 집합이 주어졌을 때 다른 항목</li>
<li>향상도: 두 항목 집합이 독립적인 경우에 비해 함께 나타날 가능성</li>
</ul>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/06.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분산 분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/05.html</link>
  <description><![CDATA[ 




<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-09-07-10-39-35.png" class="img-fluid figure-img"></p>
<figcaption>검정 방법 선택 기준</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-09-07-10-45-15.png" class="img-fluid figure-img"></p>
<figcaption>모수 검정 방법 선택 기준</figcaption>
</figure>
</div>
<ul>
<li>관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, …), 각 케이스에 맞는 모형을 사용해야 함.</li>
</ul>
<section id="정규성-검정" class="level3">
<h3 class="anchored" data-anchor-id="정규성-검정">정규성 검정</h3>
<ul>
<li><p>표본이 정규분포를 따르는지 검정.</p></li>
<li><p>따르지 않더라도 중심극한정리에 의해 <strong>표본의 크기가 충분히 크면</strong> 모수 검정을 사용할 수 있다.</p></li>
<li><p><strong>shapiro wilk 검정</strong>: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>jarque-Bera</strong>: 대표본에 사용.</p>
<ul>
<li>H0: 데이터가 정규분포를 따른다.</li>
<li>H1: 데이터가 정규분포를 따르지 않는다.</li>
</ul></li>
<li><p><strong>Q-Q plot</strong>: x축이 이론적 분위수, y축이 표본 분위수</p></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>normality test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="normality test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro, jarque_bera, zscore, probplot</span>
<span id="cb1-4"></span>
<span id="cb1-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span>
<span id="cb1-6">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(data)</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jarque_bera(data)</span>
<span id="cb1-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Jarque-Bera Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-15"></span>
<span id="cb1-16">zdata <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> zscore(data)</span>
<span id="cb1-17">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-18"></span>
<span id="cb1-19">(osm, odr), (slope, intercept, r)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> probplot(zdata, plot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-20">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Q-Q Plot"</span>)</span>
<span id="cb1-21"></span>
<span id="cb1-22">sns.histplot(data, kde<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-23">ax[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Histogram"</span>)</span>
<span id="cb1-24"></span>
<span id="cb1-25">plt.show()</span></code></pre></div>
</div>
</section>
<section id="등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="등분산성-검정">등분산성 검정</h3>
<ul>
<li><strong>Barlett 검정</strong>: <strong>정규성을 만족하는 경우에만</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
<li><strong>Levene 검정</strong>: <strong>정규성을 만족하지 않는 경우에도</strong> 사용 가능
<ul>
<li>H0: <img src="https://latex.codecogs.com/png.latex?%CF%83_1%5E2%20=%20%CF%83_2%5E2%20=%20...%20=%20%CF%83_k%5E2"></li>
<li>H1: <img src="https://latex.codecogs.com/png.latex?%CF%83_i%20%E2%89%A0%20%CF%83_j"> for some i, j</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>equal variance test</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="equal variance test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> bartlett, levene</span>
<span id="cb2-4"></span>
<span id="cb2-5">group1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb2-6">group2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>]</span>
<span id="cb2-7">group3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]</span>
<span id="cb2-8"></span>
<span id="cb2-9">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bartlett(group1, group2, group3)</span>
<span id="cb2-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Bartlett's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-11"></span>
<span id="cb2-12">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> levene(group1, group2, group3)</span>
<span id="cb2-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Levene's Test: stat=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="분산-분석" class="level2">
<h2 class="anchored" data-anchor-id="분산-분석">분산 분석</h2>
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-10-06-18-54-33.png" class="img-fluid"></p>
<ul>
<li>정규성을 만족 못할 경우, 서열척도 비모수 검정 사용 가능</li>
<li>코크란 Q 검정: 이항분포를 따르는 반복 측정 자료에 사용
<ul>
<li>만약 대응표본이 아닐 경우 (실패, 성공) 변수를 만들어서 독립성 검정 사용</li>
<li>만약 이항분포가 아닐 경우 프리드만 검정 고려</li>
</ul></li>
<li>독립성 검정의 cell의 기대도수가 5 미만인 경우, 피셔의 정확검정 사용
<ul>
<li>만약 2x3을 넘어갈 경우 몬테카를로 시뮬레이션 사용(python으로 구현하기 복잡하다.)</li>
</ul></li>
<li>크루스칼, 맨휘트니: 동점이 과하게 많을 경우 permutation test, 순열 분산 분석 사용 가능
<ul>
<li>하지만 ADP 환경의 scipy에서는 버전이 낮아서 permutation test를 쓸 수 없다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>사후 검정</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="사후 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.sandbox.stats.multicomp <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MultiComparison</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ttest_ind</span>
<span id="cb3-5"></span>
<span id="cb3-6">mc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultiComparison(data, groups).allpairtest(ttest_ind, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bonf'</span>)</span>
<span id="cb3-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(mc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb3-8"></span>
<span id="cb3-9">mc.plot_simultaneous()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</div>
<section id="적합성-검정" class="level3">
<h3 class="anchored" data-anchor-id="적합성-검정">적합성 검정</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>카이제곱 적합성 검정</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="카이제곱 적합성 검정" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#| eval: false</span></span>
<span id="cb4-2"></span>
<span id="cb4-3">count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].value_counts().sort_index()</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 빈도 수가 5 미만인 경우 합침</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb4-8">count.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> count.loc[[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>]].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-9">count.drop([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-10"></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모수 추정</span></span>
<span id="cb4-12">lam <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_count[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].mean()</span>
<span id="cb4-13">poi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> poisson(lam)</span>
<span id="cb4-14"></span>
<span id="cb4-15">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> count.values.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb4-16">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([(poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> poi.pmf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)), </span>
<span id="cb4-17">       <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>poi.pmf(np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)), </span>
<span id="cb4-18">       poi.sf(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 마지막 값은 이상 값으로</span></span>
<span id="cb4-19">exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> n</span>
<span id="cb4-20"></span>
<span id="cb4-21"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chisquare</span>
<span id="cb4-22"></span>
<span id="cb4-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 추정한 모수 갯수 만큼 자유도 차감</span></span>
<span id="cb4-24">stat, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chisquare(count.values, exp, ddof<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-25">p</span></code></pre></div>
</div>
<ul>
<li>연속형
<ul>
<li>이표본 검정: 콜모고로프-스미르노프 검정 사용</li>
<li>일표본 검정:
<ul>
<li>정규분포, 지수분포: 앤더슨-달링 검정 사용</li>
<li>그 외: 몬테카를로 방법 사용</li>
</ul></li>
</ul></li>
<li>이산형
<ul>
<li>이표본: 카이제곱 독립성 검정</li>
<li>일표본: 카이제곱 동질성 검정</li>
</ul></li>
</ul>
</section>
</section>
<section id="다변량-분산분석" class="level2">
<h2 class="anchored" data-anchor-id="다변량-분산분석">다변량 분산분석</h2>
<section id="way-anova" class="level3">
<h3 class="anchored" data-anchor-id="way-anova">2-way ANOVA</h3>
<ul>
<li>반복이 없는 경우 교효작용 검정은 불가능</li>
<li>정규성, 등분산성 검정은 잔차에 대해 수행</li>
</ul>
<div id="8492f05c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb5-3"></span>
<span id="cb5-4"></span>
<span id="cb5-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)+C(성별):C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data).fit()</span>
<span id="cb5-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb5-7">atab</span></code></pre></div>
</div>
<div id="0b5d2035" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapiro</span>
<span id="cb6-2"></span>
<span id="cb6-3">residuals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb6-4"></span>
<span id="cb6-5">shapiro_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shapiro(residuals)</span>
<span id="cb6-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shapiro-Wilk Test on Residuals: Statistic=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>statistic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, p-value=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>shapiro_test<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pvalue<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
<div id="ce4dc196" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>residuals, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-2">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb7-3">plt.show()</span></code></pre></div>
</div>
<ul>
<li>교효작용이 유의하지 않을 경우 오차항에 pooling을 한다.</li>
</ul>
<div id="1855b8ec" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.anova <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> anova_lm</span>
<span id="cb8-3"></span>
<span id="cb8-4"></span>
<span id="cb8-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"성적~C(성별)+C(교육방법)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tmp).fit()</span>
<span id="cb8-6">atab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> anova_lm(model)</span>
<span id="cb8-7">atab</span></code></pre></div>
</div>
<ul>
<li>main 효과의 사후검정은 anova와 동일
<ul>
<li>하지만 interation 효과가 유의할 경우, 주효과는 무의미하다.</li>
</ul></li>
<li>interaction 효과는 시각적으로 보여주는게 좋다.</li>
</ul>
<div id="55e1b1ec" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">grouped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tmp.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>].mean().reset_index()</span>
<span id="cb9-2">pivot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grouped.pivot(index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별'</span>, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>, values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성적'</span>)</span>
<span id="cb9-3">pivot.plot(marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>)</span>
<span id="cb9-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'평균 성적'</span>)</span>
<span id="cb9-5">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'성별-교육방법 교호작용 효과'</span>)</span>
<span id="cb9-6">plt.legend(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'교육방법'</span>)</span>
<span id="cb9-7">plt.tight_layout()</span>
<span id="cb9-8">plt.show()</span></code></pre></div>
</div>
<ul>
<li>2way 이상은 해석이 어려워서 잘 사용하지 않는다.</li>
</ul>
</section>
<section id="반복측정-분산분석" class="level3">
<h3 class="anchored" data-anchor-id="반복측정-분산분석">반복측정 분산분석</h3>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>repeated measures anova</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="repeated measures anova" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pingouin <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pg</span>
<span id="cb10-2"></span>
<span id="cb10-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'T3'</span>],</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">78</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">85</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">82</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">86</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">88</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">83</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>]</span>
<span id="cb10-7">})</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 반복측정 ANOVA 수행 및 구형성 검정, 보정 포함</span></span>
<span id="cb10-10">aov <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pg.rm_anova(dv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>, within<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'condition'</span>, subject<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, correction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-11">display(aov)</span></code></pre></div>
</div>
<ul>
<li>sphericity가 true이면 구형성 만족</li>
<li>p-unc: 구형성 보정 전 p-value</li>
<li>p-GG-corr: Greenhouse-Geisser 보정 후 p-value
<ul>
<li>구형성 위반 시 사용</li>
</ul></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/05.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀분석 템플릿</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/04.html</link>
  <description><![CDATA[ 




<section id="가정" class="level2">
<h2 class="anchored" data-anchor-id="가정">가정</h2>
<ul>
<li><strong>선형성</strong>: 종속변수와 독립변수 간의 관계는 선형이다.</li>
<li><strong>정규성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 정규분포이다.</li>
<li><strong>등분산성</strong>: 종속변수 <strong>잔차들의 분포</strong>는 동일한 분산을 갖는다.</li>
<li><strong>독립성</strong>: 모든 <strong>잔차값</strong>은 서로 독립이다.</li>
</ul>
</section>
<section id="가정-검정" class="level2">
<h2 class="anchored" data-anchor-id="가정-검정">가정 검정</h2>
<ul>
<li>먼저 모델링을 진행한 후, 잔차를 통해 가정을 검정한다.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>linear regression test</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="linear regression test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb1-2"></span>
<span id="cb1-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb1-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, Xc).fit()</span>
<span id="cb1-5">resid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.resid</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<section id="영향치-처리" class="level3">
<h3 class="anchored" data-anchor-id="영향치-처리">영향치 처리</h3>
<ul>
<li>레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차
<ul>
<li>Cook’s distance</li>
<li>Leverage</li>
<li>그 외 DFFITS, DFBETAS 등</li>
</ul></li>
</ul>
<div id="fd6054ba" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_influence(df, model):</span>
<span id="cb2-2">    influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_influence()</span>
<span id="cb2-3"></span>
<span id="cb2-4">    cooks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> influence.cooks_distance[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-5">    leverage <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> influence.hat_matrix_diag</span>
<span id="cb2-6">    df_influence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb2-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cooks'</span>: cooks,</span>
<span id="cb2-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'leverage'</span>: leverage,</span>
<span id="cb2-9">    })</span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df_influence</span>
<span id="cb2-11"></span>
<span id="cb2-12">influence_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_influence(X, model)</span>
<span id="cb2-13">display(influence_df)</span></code></pre></div>
</div>
</section>
<section id="다중공선성-검정" class="level3">
<h3 class="anchored" data-anchor-id="다중공선성-검정">다중공선성 검정</h3>
<ol type="1">
<li>전 변수 집합 대상</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>multicollinearity test</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="multicollinearity test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"></span>
<span id="cb3-2">cor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.corr()</span>
<span id="cb3-3">cond_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.cond(cor)</span>
<span id="cb3-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Condition Number:"</span>, cond_num)</span></code></pre></div>
</div>
<ul>
<li><strong>30을 초과</strong>하면 다중공선성이 높다고 판단한다.</li>
<li>선형 종속 가능성을 봄.</li>
<li>statsmodels의 ols를 사용해도 볼 수 있음</li>
<li>scaling이 선행되어야 함.</li>
<li>초과 시 해석:
<ul>
<li>독립변수의 전체 차원이 부족한 경우</li>
<li>표본을 더 모으거나 새로운 변수를 도입</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>VIF test</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="VIF test" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.stats.outliers_influence <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> variance_inflation_factor</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> check_vif(X, y):</span>
<span id="cb4-5">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb4-6">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, X)</span>
<span id="cb4-7">    model.fit()</span>
<span id="cb4-8">    vif_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>])</span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(model.exog_names)):</span>
<span id="cb4-10">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.exog_names[i]</span>
<span id="cb4-11">        vif_df.loc[i, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> variance_inflation_factor(model.exog, i)</span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> vif_df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'VIF'</span>, ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-13"></span>
<span id="cb4-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(check_vif(X, y))</span></code></pre></div>
</div>
<ul>
<li>특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우</li>
<li><strong>VIF가 10</strong>을 넘을 경우</li>
<li>덜 중요하다면 제거</li>
<li>중요하다면 변수에 대한 독립적 정보 보강(세분화, …)</li>
</ul>
<ol start="3" type="1">
<li>두 변수의 상관계수가 높은 경우</li>
</ol>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>correlation heatmap</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="correlation heatmap" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb5-2"></span>
<span id="cb5-3">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>))</span>
<span id="cb5-4">sns.heatmap(cor, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span></code></pre></div>
</div>
<ul>
<li>둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)</li>
<li>둘 중 하나를 제거</li>
</ul>
</section>
<section id="정규성" class="level3">
<h3 class="anchored" data-anchor-id="정규성">정규성</h3>
<ul>
<li>ols의 summary를 통해 확인 가능</li>
<li>혹은 EDA 과정에서 사용한 정규성 검정 참조</li>
</ul>
</section>
<section id="선형성-등분산성-검정" class="level3">
<h3 class="anchored" data-anchor-id="선형성-등분산성-검정">선형성, 등분산성 검정</h3>
<div id="eda631a4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb6-3"></span>
<span id="cb6-4">sns.residplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>fitted.fittedvalues, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>fitted.resid, lowess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb6-5">              line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lw'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>})</span>
<span id="cb6-6">plt.show()</span></code></pre></div>
</div>
<ul>
<li>선형성:
<ul>
<li>잔차도가 어떠한 패턴도 보이지 않아야 한다.</li>
</ul></li>
<li>등분산성:
<ul>
<li>잔차도가 일정한 폭을 가져야 한다.</li>
</ul></li>
</ul>
</section>
<section id="독립성" class="level3">
<h3 class="anchored" data-anchor-id="독립성">독립성</h3>
<ul>
<li>독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다.
<ul>
<li>더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.</li>
<li>2에 가까울수록 독립성 만족</li>
<li>statsmodels의 ols로 확인 가능</li>
</ul></li>
</ul>
</section>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ol type="1">
<li>범주형 변수 처리:
<ul>
<li>더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현
<ul>
<li>각 범주의 회귀계수는 기준 범주와의 차이를 의미</li>
</ul></li>
</ul></li>
<li>이상치 / 영향점: 관측값 제거</li>
<li>선형성 위반: 독립변수 변환, GAM</li>
<li>정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM</li>
<li>다중공산성 위반: 다중공산성 파트 참고
<ul>
<li>혹은 변수 선택법을 사용</li>
</ul></li>
</ol>
<ul>
<li>가정 만족할 때까지 검정, 전처리 계속 반복</li>
</ul>
<section id="변수-변환" class="level3">
<h3 class="anchored" data-anchor-id="변수-변환">변수 변환</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-09-20-18-38-36.png" class="img-fluid figure-img"></p>
<figcaption>회귀 모델 수정 λ</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/img/2025-09-20-18-55-55.png" class="img-fluid figure-img"></p>
<figcaption>경험적인 적절한 λ</figcaption>
</figure>
</div>
<ul>
<li>최적의 λ는 최대 우도 추정법으로 구할 수 있다.</li>
<li>변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.</li>
<li>일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.
<ul>
<li>아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.</li>
<li>통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>box cox transformation</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="box cox transformation" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> boxcox</span>
<span id="cb7-2"></span>
<span id="cb7-3">y_transformed, best_lambda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boxcox(y)</span>
<span id="cb7-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Best lambda: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_lambda<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
</div>
</section>
<section id="변수-선택법" class="level3">
<h3 class="anchored" data-anchor-id="변수-선택법">변수 선택법</h3>
<ul>
<li>전진 선택법</li>
<li>후진 선택법</li>
<li>단계적 선택법</li>
<li>최적조합 선택법: 모든 조합 다 해봄</li>
<li>기준
<ul>
<li>R2, Adj R2</li>
<li>AIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법</li>
<li>BIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌</li>
<li>Mallows’ Cp</li>
</ul></li>
</ul>
</section>
</section>
<section id="규제-선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="규제-선형-회귀">규제 선형 회귀</h2>
<ul>
<li>지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦</li>
<li>독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)
<ul>
<li>일반적으로는 scale을 하든 안하든 r square에 차이가 없다.</li>
</ul></li>
</ul>
<ol type="1">
<li>릿지회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%CE%B2_j%5E2"></li>
<li>회귀계수 절댓값을 0에 가깝게 함</li>
<li>하지만 0으로 만들지는 않음</li>
<li>작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.</li>
<li>회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음</li>
</ul></li>
<li>라쏘회귀:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB%5CSigma%20%7C%CE%B2_j%7C"></li>
<li>회귀계수를 0으로 만들 수 있음</li>
<li>변수 선택 효과</li>
<li>릿지보다 해석이 쉬움</li>
<li>일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음</li>
</ul></li>
<li>엘라스틱넷 회귀
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5CSigma%20(y_i%20-%20%5Chat%7By_i%7D)%5E2%20+%20%CE%BB_1%5CSigma%20%7C%CE%B2_j%7C%20+%20%CE%BB_2%5CSigma%20%CE%B2_j%5E2"></li>
<li>릿지와 라쏘의 장점을 모두 가짐</li>
<li>변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦</li>
<li>독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음</li>
</ul></li>
</ol>
</section>
<section id="일반화-선형-회귀glm" class="level2">
<h2 class="anchored" data-anchor-id="일반화-선형-회귀glm">일반화 선형 회귀(GLM)</h2>
<ul>
<li>종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우
<ul>
<li>이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.</li>
<li>포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.</li>
<li>따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.</li>
</ul></li>
</ul>
<section id="logistic-회귀" class="level3">
<h3 class="anchored" data-anchor-id="logistic-회귀">Logistic 회귀</h3>
<ul>
<li>종속변수가 범주형일 경우</li>
<li><img src="https://latex.codecogs.com/png.latex?z%20=%20%CE%B2_0%20+%20%CE%B2_1%20x_1%20+%20%CE%B2_2%20x_2%20+%20...%20+%20%CE%B2_n%20x_n"></li>
<li><img src="https://latex.codecogs.com/png.latex?p%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-z%7D%7D"></li>
<li>오즈: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7B1-p%7D"> = <img src="https://latex.codecogs.com/png.latex?e%5Ez"></li>
<li>오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율
<ul>
<li><img src="https://latex.codecogs.com/png.latex?(e%5E%7B%CE%B2_k%7D)%5Ek"></li>
</ul></li>
<li>LLR의 p-value가 낮다면, 모델이 통계적으로 유의미함을 의미</li>
<li>잔차 검정은 안함</li>
</ul>
</section>
<section id="포아송-회귀" class="level3">
<h3 class="anchored" data-anchor-id="포아송-회귀">포아송 회귀</h3>
<ul>
<li>종속변수가 count 데이터일 경우</li>
</ul>
<div id="4afddc77" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb8-2"></span>
<span id="cb8-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb8-4"></span>
<span id="cb8-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, X, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.Poisson())</span>
<span id="cb8-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb8-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x_k">가 한 단위 증가할 때, 빈도 수가 <img src="https://latex.codecogs.com/png.latex?exp(%CE%B2_k)">배 증가</li>
<li>만약 관찰 시간이 다를 경우, offset로 np.log(df[관찰시간])을 넣어줘야 함</li>
<li>Deviance / DF_resid 가 1보다 크면 과산포, 작으면 과소산포
<ul>
<li>과산포: 사건발생 확률이 일정하지 않음</li>
<li>과산포 시 음이항 회귀 사용</li>
</ul></li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>음이항 회귀</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="음이항 회귀" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb9-2"></span>
<span id="cb9-3">Xc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(X)</span>
<span id="cb9-4"></span>
<span id="cb9-5">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.GLM(y, Xc, family<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sm.families.NegativeBinomial())</span>
<span id="cb9-6">fitted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit()</span>
<span id="cb9-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(model.summary())</span></code></pre></div>
</div>
<ul>
<li>Quasi-Poisson도 있지만, statsmodels에서는 제공하지 않음</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/04.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>전처리 템플릿</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/03.html</link>
  <description><![CDATA[ 




<ul>
<li>참고: 이상치, 결측치, 불균형 처리 모두 원칙적으로는 train set에서만 fit 해야함.</li>
</ul>
<section id="결측치-처리" class="level2">
<h2 class="anchored" data-anchor-id="결측치-처리">결측치 처리</h2>
<ul>
<li>결측치가 발생하는 원인과 처리 전략
<ul>
<li>무작위 결측(Missing Completely at Random, MCAR): 결측치가 발생할 확률이 다른 모든 변수와 무관
<ul>
<li>예: 설문지 작성 중 무작위로 일부 페이지가 누락된 경우</li>
<li>이 경우, 결측치를 제거하거나 단순 대체해도 편향이 발생하지 않음</li>
</ul></li>
<li>조건부 무작위 결측(Missing at Random, MAR): 결측치가 발생할 확률이 관측된 데이터에만 의존
<ul>
<li>예: 고소득자일수록 소득 공개를 꺼려하는 경우 (교육수준이 높을수록 소득 결측 확률이 높음)</li>
<li>실무에서 가장 일반적인 가정: 대부분의 실제 데이터에서 결측 패턴은 관측된 변수들로 어느 정도 설명 가능</li>
<li>관측된 데이터를 활용한 예측 기반 대치법이 효과적</li>
</ul></li>
<li>비무작위 결측(Missing Not at Random, MNAR): 결측치가 발생할 확률이 결측된 값 자체와 관련
<ul>
<li>예: 극도로 낮은 소득자가 소득 공개를 꺼리는 경우</li>
<li>통계적 방법만으로는 해결이 어려우며, 도메인 지식이나 외부 정보가 필요</li>
<li><strong>실무에서는 MAR 가정 하에 처리 후, 민감도 분석을 통해 결측 메커니즘과 처리 방법의 적절성을 확인</strong>
<ul>
<li>민감도 분석: 다양한 결측치 처리 방법을 적용하여 결과를 비교
<ul>
<li>단순 방법(평균/중앙값 대치) vs 고급 방법(KNN, MICE)</li>
<li>단순 방법과 고급 방법의 결과가 비슷하면 → MCAR 가능성 높음, 단순 방법으로도 충분</li>
<li>고급 방법이 더 나은 성능을 보이면 → MAR 가능성 높음, 고급 방법 선택</li>
<li>어떤 방법을 써도 결과가 불안정하면 → MNAR 가능성, 도메인 지식과 추가 정보 필요</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>결측치 처리 방법
<ul>
<li>제거: 결측치가 적거나 무작위 결측일 때 사용</li>
<li>대치(대체):
<ul>
<li>일반적인 방법
<ul>
<li>시계열 데이터 o: 이전 값, 이후 값, 선형 보간법</li>
<li>시계열 데이터 x: 평균, 중앙값, 최빈값</li>
</ul></li>
<li>고급 대치법(과적합 발생 가능성 유의)
<ul>
<li>KNN 대치: 유사한 관측치의 값을 사용하여 결측치를 대체. 결측치가 없는 데이터로 예측</li>
<li>다변량 대치: 결측치를 다른 변수들의 값으로 예측하여 대체.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<section id="다변량-대치법" class="level3">
<h3 class="anchored" data-anchor-id="다변량-대치법">다변량 대치법</h3>
<div id="a2710298" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMRegressor, LGBMClassifier</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multi_impute(df, categorical, max_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>):</span>
<span id="cb1-4">    df_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb1-5">    num_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical]</span>
<span id="cb1-6"></span>
<span id="cb1-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결측치가 많은 column 우선 처리</span></span>
<span id="cb1-8">    null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb1-9">    null_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> null_counts[null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].sort_values().index.tolist()</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 초기값 대체</span></span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical:</span>
<span id="cb1-13">        df_imp[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb1-14">        mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].mode(dropna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-15">        df_imp[col].fillna(mode[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-16"></span>
<span id="cb1-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> num_cols:</span>
<span id="cb1-18">        mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].mean()</span>
<span id="cb1-19">        df_imp[col].fillna(mean, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-20"></span>
<span id="cb1-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 반복 임퓨팅</span></span>
<span id="cb1-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_iter):</span>
<span id="cb1-23">        prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.copy()</span>
<span id="cb1-24"></span>
<span id="cb1-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> null_cols:</span>
<span id="cb1-26">            idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].isnull()</span>
<span id="cb1-27">            idx_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>idx_missing</span>
<span id="cb1-28">            predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb1-29"></span>
<span id="cb1-30">            X_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_obs, predictors]</span>
<span id="cb1-31">            y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_obs, col]</span>
<span id="cb1-32">            X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_missing, predictors]</span>
<span id="cb1-33"></span>
<span id="cb1-34">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># column type에 따라 다른 모델 선택</span></span>
<span id="cb1-35">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LightGBM으로 randomforest를 사용한 이유: sklearn의 RandomForest는 categorical 변수를 직접 처리하지 못함</span></span>
<span id="cb1-36">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> categorical:</span>
<span id="cb1-37">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(</span>
<span id="cb1-38">                    boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb1-39">                    bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-40">                )</span>
<span id="cb1-41">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-42">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(</span>
<span id="cb1-43">                    boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb1-44">                    bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-45">                )</span>
<span id="cb1-46">            model.fit(X_obs, y_obs)</span>
<span id="cb1-47">            y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb1-48">            df_imp.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb1-49">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df_imp.equals(prev):</span>
<span id="cb1-50">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb1-51">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df_imp</span></code></pre></div>
</div>
<ul>
<li>현재 결측치 처리 방법 중 가장 성능이 좋은걸로 알려져 있는 missing forest를 모방한 방법.
<ul>
<li>missforest 라이브러리는 adp 환경에서 설치 불가</li>
</ul></li>
<li>다중 대치를 안 하고 있지만, sklearn 공식 문서에 따르면 대부분 single 대치로도 충분하다고 한다.</li>
<li>참고로 이 방법은 missforest의 정확한 구현은 아니기 때문에, 시험에서는 다변량 대치법을 사용했다고만 쓰자.</li>
<li>만약 train set에서만 fit을 시키고 싶다면, 이 방법은 그냥 안 쓰는걸 추천.
<ul>
<li>그렇게 하려면 class 형태로 바꿔야 하는데, too much인가 싶은 느낌이 슬슬 나기 시작.</li>
</ul></li>
</ul>
<div id="d831e548" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MultiImputer:</span>
<span id="cb2-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], max_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb2-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> categorical</span>
<span id="cb2-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_iter</span>
<span id="cb2-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n_estimators</span>
<span id="cb2-6">        </span>
<span id="cb2-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb2-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb2-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-11">        </span>
<span id="cb2-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-13">        df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb2-14">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [col <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical]</span>
<span id="cb2-15">        </span>
<span id="cb2-16">        null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.isnull().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb2-17">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> null_counts[null_counts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].sort_values().index.tolist()</span>
<span id="cb2-18">        </span>
<span id="cb2-19">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 초기값 계산 및 저장 (Train의 통계량만 사용!)</span></span>
<span id="cb2-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-21">            df[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb2-22">            mode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mode(dropna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-23">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mode[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-24">            df[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-25">        </span>
<span id="cb2-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_:</span>
<span id="cb2-27">            mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[col].mean()</span>
<span id="cb2-28">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean</span>
<span id="cb2-29">            df[col].fillna(mean, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-30">        </span>
<span id="cb2-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter):</span>
<span id="cb2-32">            prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb2-33">            </span>
<span id="cb2-34">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_:</span>
<span id="cb2-35">                idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[col].isnull()</span>
<span id="cb2-36">                idx_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>idx_missing</span>
<span id="cb2-37">                    </span>
<span id="cb2-38">                predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb2-39">                X_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_obs, predictors]</span>
<span id="cb2-40">                y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_obs, col]</span>
<span id="cb2-41">                X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.loc[idx_missing, predictors]</span>
<span id="cb2-42">                </span>
<span id="cb2-43">                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모델 선택 및 학습</span></span>
<span id="cb2-44">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-45">                    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(</span>
<span id="cb2-46">                        boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, </span>
<span id="cb2-47">                        n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators,</span>
<span id="cb2-48">                        bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, </span>
<span id="cb2-49">                        bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-50">                        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-51">                    )</span>
<span id="cb2-52">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb2-53">                    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(</span>
<span id="cb2-54">                        boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, </span>
<span id="cb2-55">                        n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators,</span>
<span id="cb2-56">                        bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, </span>
<span id="cb2-57">                        bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-58">                        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-59">                    )</span>
<span id="cb2-60">                </span>
<span id="cb2-61">                model.fit(X_obs, y_obs)</span>
<span id="cb2-62">                </span>
<span id="cb2-63">                y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb2-64">                df.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb2-65">                </span>
<span id="cb2-66">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb2-67">            </span>
<span id="cb2-68">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df.equals(prev):</span>
<span id="cb2-69">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-70">        </span>
<span id="cb2-71">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span></span>
<span id="cb2-72">    </span>
<span id="cb2-73">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb2-74">        df_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.copy()</span>
<span id="cb2-75">        </span>
<span id="cb2-76">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.categorical:</span>
<span id="cb2-77">            df_imp[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp[col].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb2-78">            df_imp[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-79">        </span>
<span id="cb2-80">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_cols_:</span>
<span id="cb2-81">            df_imp[col].fillna(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.initial_fill_[col], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-82">        </span>
<span id="cb2-83">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_iter):</span>
<span id="cb2-84">            prev <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.copy()</span>
<span id="cb2-85">            </span>
<span id="cb2-86">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.null_cols_:                   </span>
<span id="cb2-87">                idx_missing <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[col].isnull()</span>
<span id="cb2-88">                </span>
<span id="cb2-89">                predictors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df_imp.columns <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> col]</span>
<span id="cb2-90">                X_mis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_imp.loc[idx_missing, predictors]</span>
<span id="cb2-91">                </span>
<span id="cb2-92">                model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.models_[col]</span>
<span id="cb2-93">                y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_mis)</span>
<span id="cb2-94">                df_imp.loc[idx_missing, col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_pred</span>
<span id="cb2-95">            </span>
<span id="cb2-96">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> df_imp.equals(prev):</span>
<span id="cb2-97">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-98">        </span>
<span id="cb2-99">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> df_imp</span>
<span id="cb2-100">    </span>
<span id="cb2-101">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fit_transform(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-102">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fit(X, y)</span>
<span id="cb2-103">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.transform(X)</span></code></pre></div>
</div>
<ul>
<li>혹시몰라서 class 형태로 만든 다변량 대치법</li>
<li>이걸 언제 다 적고 있을까</li>
</ul>
</section>
</section>
<section id="이상치-처리" class="level2">
<h2 class="anchored" data-anchor-id="이상치-처리">이상치 처리</h2>
<ul>
<li>이상치 탐지는 EDA 참고</li>
<li>처리는 알아서 잘 하자.</li>
</ul>
</section>
<section id="불균형-처리" class="level2">
<h2 class="anchored" data-anchor-id="불균형-처리">불균형 처리</h2>
<ul>
<li>잘 알려져 있는 방법 대충 잘 선택해서 사용.</li>
<li>딱히 SOTA(가장 좋은 방법)가 있지 않음.</li>
</ul>
<section id="성능-비교" class="level3">
<h3 class="anchored" data-anchor-id="성능-비교">성능 비교</h3>
<ul>
<li>각각의 처리법에 대해서 어떤게 제일 좋은지 비교</li>
</ul>
<div id="e6cb2883" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMRegressor</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.impute <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KNNImputer</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb3-4"></span>
<span id="cb3-5">df[cat_cols] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cat_cols].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7">df1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.dropna()</span>
<span id="cb3-8">df2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[num_cols].fillna(df[num_cols].mean())</span>
<span id="cb3-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> col <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cat_cols:</span>
<span id="cb3-10">    df2[col] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target[col].fillna(target[col].mode()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb3-11">df3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_impute(df, categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_cols)</span>
<span id="cb3-12"></span>
<span id="cb3-13">candis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-14">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Nothing'</span>, df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>]),</span>
<span id="cb3-15">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Just Delete'</span>, df1.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), df1[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>]),</span>
<span id="cb3-16">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Simple Impute'</span>, df2.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), df2[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>]),</span>
<span id="cb3-17">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MultiImputer'</span>, df3.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), df3[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dead'</span>])</span>
<span id="cb3-18">]</span>
<span id="cb3-19"></span>
<span id="cb3-20">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb3-21"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> name, X, y <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> candis:</span>
<span id="cb3-22">    rf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMRegressor(boosting_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rf"</span>, n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, bagging_fraction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bagging_freq<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-23">    result[name] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(rf, X, y, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'neg_mean_squared_error'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># classifier인 경우 'accuracy' 등등</span></span>
<span id="cb3-24"></span>
<span id="cb3-25">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb3-26"></span>
<span id="cb3-27">means <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>result.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># regressor인 경우, classifier인 경우 양수</span></span>
<span id="cb3-28">errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result.std()</span>
<span id="cb3-29"></span>
<span id="cb3-30">means.plot.barh(xerr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>errors, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb3-31">ax.set_yticks(np.arange(means.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]))</span>
<span id="cb3-32">ax.set_yticklabels(means.index)</span>
<span id="cb3-33">plt.show()</span></code></pre></div>
</div>
</section>
</section>
<section id="feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="feature-selection">Feature Selection</h2>
<section id="filter-method" class="level3">
<h3 class="anchored" data-anchor-id="filter-method">Filter Method</h3>
<ol type="1">
<li>basic methods</li>
</ol>
<ul>
<li>하나의 값만 가지는 변수 혹은 분산이 너무 낮은 변수는 제거</li>
</ul>
<div id="b79c3147" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> VarianceThreshold</span>
<span id="cb4-2"></span>
<span id="cb4-3">sel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VarianceThreshold(threshold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb4-4"></span>
<span id="cb4-5">selected_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.columns[sel.get_support()]</span>
<span id="cb4-6">df_selected <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[selected_cols]</span></code></pre></div>
</div>
<ol start="2" type="1">
<li>Univariate selection methods</li>
</ol>
<div id="3bc0ade7" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SelectKBest, SelectPercentile, chi2</span>
<span id="cb5-2"></span>
<span id="cb5-3">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectKBest(chi2, k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최상위 2개</span></span>
<span id="cb5-4"></span>
<span id="cb5-5">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectPercentile(chi2, percentile<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상위 10%</span></span></code></pre></div>
</div>
<ul>
<li>카이제곱 검정량이 가장 높은 변수들만 선택.</li>
<li>연속형 변수에 대해서는 KBinsDiscretizer 작업 필요.</li>
</ul>
<div id="42a40fdd" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SelectKBest, SelectPercentile, mutual_info_classif, mutual_info_regression</span>
<span id="cb6-2"></span>
<span id="cb6-3">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectKBest(mutual_info_classif, k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 최상위 2개</span></span>
<span id="cb6-4">X_new <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SelectPercentile(mutual_info_regression, percentile<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).fit_transform(X, y) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상위 10%</span></span></code></pre></div>
</div>
<ul>
<li>EDA 파트 mutual info 참고</li>
<li>추가: 상관계수, ANOVA F-value 등등 사용 가능</li>
</ul>
</section>
<section id="wrapper-method" class="level3">
<h3 class="anchored" data-anchor-id="wrapper-method">Wrapper Method</h3>
<ul>
<li>forward, backward, 등등</li>
</ul>
</section>
<section id="embedded-method" class="level3">
<h3 class="anchored" data-anchor-id="embedded-method">Embedded Method</h3>
<ul>
<li>L1, L2, Elasticnet 등등</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/03.html</guid>
  <pubDate>Sat, 04 Oct 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA와 시각화</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="edaexploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="edaexploratory-data-analysis">EDA(Exploratory Data Analysis)</h2>
<p>: 데이터의 특징과 데이터에 내재된 관계를 알아내기 위해 그래프와 통계적 분석 방법을 활용하여 탐구하는 것</p>
<section id="주제" class="level3">
<h3 class="anchored" data-anchor-id="주제">주제</h3>
<ol type="1">
<li>저항성 강조: 부분적 변동(이상치 등)에 대한 민감성 확인</li>
<li>잔차 계산</li>
<li>자료변수의 재표현: 변수를 적당한 척도로 바꾸는 것</li>
<li>그래프를 통한 현시성</li>
</ol>
</section>
</section>
<section id="막대-그래프" class="level2">
<h2 class="anchored" data-anchor-id="막대-그래프">막대 그래프</h2>
<p><code>범주형 데이터</code>를 요약하고 시각적으로 비교하는 데 활용</p>
<div id="cb7e8a6f" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_wine</span>
<span id="cb1-4"></span>
<span id="cb1-5">wine_load <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_wine()</span>
<span id="cb1-6">wine <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(wine_load.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>wine_load.feature_names)</span>
<span id="cb1-7">wine_load</span>
<span id="cb1-8">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine_load.target</span>
<span id="cb1-9">wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_0'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_1'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class_2'</span>})</span>
<span id="cb1-10"></span>
<span id="cb1-11">wine_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> wine[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Class'</span>].value_counts()</span>
<span id="cb1-12">wine_type</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Class
class_1    71
class_0    59
class_2    48
Name: count, dtype: int64</code></pre>
</div>
</div>
<div id="d2162de6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수직 막대</span></span>
<span id="cb3-2">plt.bar(wine_type.index, wine_type.values, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, align <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'center'</span>)</span>
<span id="cb3-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-3-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0065c7fe" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 수평 막대</span></span>
<span id="cb4-2">plt.barh(wine_type.index, wine_type.values, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, left<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, align <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'center'</span>)</span>
<span id="cb4-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-4-output-1.png" width="598" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>각 범주의 값의 갯수 <code>차이가 극단적</code>인지 확인한다. 극단적일 경우, 전처리 과정에서 <code>업/다운 샘플링 등을 통해 갯수가 유사해지도록 조정</code>해야한다.</p>
</section>
<section id="히스토그램" class="level2">
<h2 class="anchored" data-anchor-id="히스토그램">히스토그램</h2>
<p><code>연속형 데이터</code>의 분포를 확인하는 데 활용</p>
<div id="3aa395ce" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Wine alcohol histogram'</span>)</span>
<span id="cb5-2">plt.hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alcohol'</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'purple'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>wine)</span>
<span id="cb5-3">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-5-output-1.png" width="566" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="box-plot" class="level2">
<h2 class="anchored" data-anchor-id="box-plot">box plot</h2>
<p><code>수치형 변수</code>의 분포를 확인하는 그래프</p>
<div id="71671800" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb6-2"></span>
<span id="cb6-3">iris_load <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb6-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris_load.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_load.feature_names)</span>
<span id="cb6-5">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris_load.target</span>
<span id="cb6-6">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>({<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'setosa'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'versicolor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'virginica'</span>})</span>
<span id="cb6-7"></span>
<span id="cb6-8">plt.boxplot(iris.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>))</span>
<span id="cb6-9">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-6-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="6e1b5a42" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb7-2"></span>
<span id="cb7-3">sns.boxplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"class"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sepal width (cm)"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris)</span>
<span id="cb7-4">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-7-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="산점도" class="level2">
<h2 class="anchored" data-anchor-id="산점도">산점도</h2>
<p>두 개의 <code>수치형 변수</code>의 <code>분포와 관계</code>를 확인하는 그래프</p>
<div id="0699d108" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'iris scatter'</span>)</span>
<span id="cb8-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>)</span>
<span id="cb8-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>)</span>
<span id="cb8-4"></span>
<span id="cb8-5">plt.scatter(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb8-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-8-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="65717dc9" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">sns.scatterplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>, hue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris, style<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb9-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-9-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="선그래프" class="level2">
<h2 class="anchored" data-anchor-id="선그래프">선그래프</h2>
<section id="수평-수직-선" class="level3">
<h3 class="anchored" data-anchor-id="수평-수직-선">수평 / 수직 선</h3>
<div id="9c6a5dda" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">plt.hlines(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, xmin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, xmax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linestyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'solid'</span>)</span>
<span id="cb10-2">plt.vlines(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, ymin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, ymax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, linestyles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dashed'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-10-output-1.png" width="590" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="함수식" class="level3">
<h3 class="anchored" data-anchor-id="함수식">함수식</h3>
<div id="b1cdb42f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> linear_func(x):</span>
<span id="cb11-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb11-3"></span>
<span id="cb11-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>]</span>
<span id="cb11-5">plt.plot(X, linear_func(X), c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb11-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-11-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="회귀선" class="level3">
<h3 class="anchored" data-anchor-id="회귀선">회귀선</h3>
<div id="d95c5bec" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb12-2"></span>
<span id="cb12-3">X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>], iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width (cm)'</span>]</span>
<span id="cb12-4">plt.scatter(X, Y, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb12-5">a, b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.polyfit(X, Y, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb12-6">plt.plot(X, a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb12-7">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-12-output-1.png" width="571" height="413" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>2차 이상의 그래프는 X값에 대하여 정렬해야 한다.</p>
<div id="2061621e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">iris2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>)</span>
<span id="cb13-2">X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris2[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>], iris2[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length (cm)'</span>]</span>
<span id="cb13-3">b2, b1, b0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.polyfit(X, Y, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb13-4">plt.scatter(X, Y, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span>
<span id="cb13-5">plt.plot(X, b0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>)</span>
<span id="cb13-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-13-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="꺾은선" class="level3">
<h3 class="anchored" data-anchor-id="꺾은선">꺾은선</h3>
<div id="5b05c474" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">plt.plot(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length (cm)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length (cm)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris2)</span>
<span id="cb14-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-14-output-1.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="상관관계-시각화" class="level2">
<h2 class="anchored" data-anchor-id="상관관계-시각화">상관관계 시각화</h2>
<section id="산점도-행렬" class="level3">
<h3 class="anchored" data-anchor-id="산점도-행렬">산점도 행렬</h3>
<div id="3a077aad" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pandas.plotting <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> scatter_matrix</span>
<span id="cb15-2"></span>
<span id="cb15-3">scatter_matrix(iris, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>), diagonal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>)</span>
<span id="cb15-4">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-15-output-1.png" width="649" height="655" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="06b84c9b" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">sns.pairplot(iris, diag_kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auto'</span>, hue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb16-2">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-16-output-1.png" width="1069" height="947" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="상관계수-행렬-그래프" class="level3">
<h3 class="anchored" data-anchor-id="상관계수-행렬-그래프">상관계수 행렬 그래프</h3>
<div id="d4cac182" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">iris_corr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>).corr(method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pearson'</span>)</span>
<span id="cb17-2">sns.heatmap(iris_corr, xticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_corr.columns, yticklabels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris_corr.columns, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"RdBu_r"</span>, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02_files/figure-html/cell-17-output-1.png" width="648" height="520" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="pandas-profiling" class="level2">
<h2 class="anchored" data-anchor-id="pandas-profiling">Pandas Profiling</h2>
<div id="e2f08bc6" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># from pandas_profiling import ProfileReport</span></span>
<span id="cb18-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#</span></span>
<span id="cb18-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ProfileReport(iris)</span></span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/02.html</guid>
  <pubDate>Sun, 28 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>pandas data 구조</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/01.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<p>pandas: numpy를 라벨링한거</p>
<section id="series" class="level1">
<h1>Series</h1>
<p>1차원 배열 구조, 이름과 형식을 가지고 모든 값에 고유한 인덱스를 가짐</p>
<div id="6272c162" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"></span>
<span id="cb1-3">data1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'haha'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>}, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb1-4">data1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>ulala    1
haha     2
Name: class, dtype: int64</code></pre>
</div>
</div>
<div id="ad44486d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">data1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ulala'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>}, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb3-2">data1</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>ulala    2
Name: class, dtype: int64</code></pre>
</div>
</div>
<div id="0b0b7f8f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">data2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>)</span>
<span id="cb5-2">data2</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>0    1
1    2
Name: class, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="dataframe" class="level1">
<h1>DataFrame</h1>
<p>2차원 배열 구조, 각 행은 인덱스를 가지고, 각 열은 이름과 형식을 가짐</p>
<section id="before" class="level2">
<h2 class="anchored" data-anchor-id="before">Before</h2>
<p>데이터를 호출하고, 데이터 내용과 요약 / 통계 정보를 확인해야함</p>
<p>칼럼명이 칼럼 타입을 변경해야할 때도 있음</p>
<section id="pandas-사용-준비" class="level3">
<h3 class="anchored" data-anchor-id="pandas-사용-준비">Pandas 사용 준비</h3>
<ol type="1">
<li>라이브러리 설치</li>
<li>라이브러리 호출</li>
</ol>
<div id="b20fb1ea" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb7-2"></span>
<span id="cb7-3">pd.set_option(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'display.max_rows'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
</div>
</section>
<section id="dataframe-선언" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-선언">DataFrame 선언</h3>
<div id="29f91ddf" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb8-2">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>], [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]])</span>
<span id="cb8-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 1</span></span>
<span id="cb8-4">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(dataset, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>])</span>
<span id="cb8-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 2</span></span>
<span id="cb8-6">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>], [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]], columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>])</span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># declare df 3</span></span>
<span id="cb8-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kor'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'math'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">80</span>]})</span>
<span id="cb8-9">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="dataframe-읽고-저장" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-읽고-저장">DataFrame 읽고 저장</h3>
<div id="dd1a332a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># filepath = '../book/data/data.csv'</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data = pd.read_csv(filepath, na_values='NA', encoding='utf8')</span></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data.to_csv('result.csv', header=True, index=True, encoding='utf8')</span></span></code></pre></div>
</div>
</section>
<section id="dataframe-출력" class="level3">
<h3 class="anchored" data-anchor-id="dataframe-출력">DataFrame 출력</h3>
<div id="3d24f7a5" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb10-2"></span>
<span id="cb10-3">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb10-4">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>{'data': array([[5.1, 3.5, 1.4, 0.2],
        [4.9, 3. , 1.4, 0.2],
        [4.7, 3.2, 1.3, 0.2],
        [4.6, 3.1, 1.5, 0.2],
        [5. , 3.6, 1.4, 0.2],
        [5.4, 3.9, 1.7, 0.4],
        [4.6, 3.4, 1.4, 0.3],
        [5. , 3.4, 1.5, 0.2],
        [4.4, 2.9, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.1],
        [5.4, 3.7, 1.5, 0.2],
        [4.8, 3.4, 1.6, 0.2],
        [4.8, 3. , 1.4, 0.1],
        [4.3, 3. , 1.1, 0.1],
        [5.8, 4. , 1.2, 0.2],
        [5.7, 4.4, 1.5, 0.4],
        [5.4, 3.9, 1.3, 0.4],
        [5.1, 3.5, 1.4, 0.3],
        [5.7, 3.8, 1.7, 0.3],
        [5.1, 3.8, 1.5, 0.3],
        [5.4, 3.4, 1.7, 0.2],
        [5.1, 3.7, 1.5, 0.4],
        [4.6, 3.6, 1. , 0.2],
        [5.1, 3.3, 1.7, 0.5],
        [4.8, 3.4, 1.9, 0.2],
        [5. , 3. , 1.6, 0.2],
        [5. , 3.4, 1.6, 0.4],
        [5.2, 3.5, 1.5, 0.2],
        [5.2, 3.4, 1.4, 0.2],
        [4.7, 3.2, 1.6, 0.2],
        [4.8, 3.1, 1.6, 0.2],
        [5.4, 3.4, 1.5, 0.4],
        [5.2, 4.1, 1.5, 0.1],
        [5.5, 4.2, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.2],
        [5. , 3.2, 1.2, 0.2],
        [5.5, 3.5, 1.3, 0.2],
        [4.9, 3.6, 1.4, 0.1],
        [4.4, 3. , 1.3, 0.2],
        [5.1, 3.4, 1.5, 0.2],
        [5. , 3.5, 1.3, 0.3],
        [4.5, 2.3, 1.3, 0.3],
        [4.4, 3.2, 1.3, 0.2],
        [5. , 3.5, 1.6, 0.6],
        [5.1, 3.8, 1.9, 0.4],
        [4.8, 3. , 1.4, 0.3],
        [5.1, 3.8, 1.6, 0.2],
        [4.6, 3.2, 1.4, 0.2],
        [5.3, 3.7, 1.5, 0.2],
        [5. , 3.3, 1.4, 0.2],
        [7. , 3.2, 4.7, 1.4],
        [6.4, 3.2, 4.5, 1.5],
        [6.9, 3.1, 4.9, 1.5],
        [5.5, 2.3, 4. , 1.3],
        [6.5, 2.8, 4.6, 1.5],
        [5.7, 2.8, 4.5, 1.3],
        [6.3, 3.3, 4.7, 1.6],
        [4.9, 2.4, 3.3, 1. ],
        [6.6, 2.9, 4.6, 1.3],
        [5.2, 2.7, 3.9, 1.4],
        [5. , 2. , 3.5, 1. ],
        [5.9, 3. , 4.2, 1.5],
        [6. , 2.2, 4. , 1. ],
        [6.1, 2.9, 4.7, 1.4],
        [5.6, 2.9, 3.6, 1.3],
        [6.7, 3.1, 4.4, 1.4],
        [5.6, 3. , 4.5, 1.5],
        [5.8, 2.7, 4.1, 1. ],
        [6.2, 2.2, 4.5, 1.5],
        [5.6, 2.5, 3.9, 1.1],
        [5.9, 3.2, 4.8, 1.8],
        [6.1, 2.8, 4. , 1.3],
        [6.3, 2.5, 4.9, 1.5],
        [6.1, 2.8, 4.7, 1.2],
        [6.4, 2.9, 4.3, 1.3],
        [6.6, 3. , 4.4, 1.4],
        [6.8, 2.8, 4.8, 1.4],
        [6.7, 3. , 5. , 1.7],
        [6. , 2.9, 4.5, 1.5],
        [5.7, 2.6, 3.5, 1. ],
        [5.5, 2.4, 3.8, 1.1],
        [5.5, 2.4, 3.7, 1. ],
        [5.8, 2.7, 3.9, 1.2],
        [6. , 2.7, 5.1, 1.6],
        [5.4, 3. , 4.5, 1.5],
        [6. , 3.4, 4.5, 1.6],
        [6.7, 3.1, 4.7, 1.5],
        [6.3, 2.3, 4.4, 1.3],
        [5.6, 3. , 4.1, 1.3],
        [5.5, 2.5, 4. , 1.3],
        [5.5, 2.6, 4.4, 1.2],
        [6.1, 3. , 4.6, 1.4],
        [5.8, 2.6, 4. , 1.2],
        [5. , 2.3, 3.3, 1. ],
        [5.6, 2.7, 4.2, 1.3],
        [5.7, 3. , 4.2, 1.2],
        [5.7, 2.9, 4.2, 1.3],
        [6.2, 2.9, 4.3, 1.3],
        [5.1, 2.5, 3. , 1.1],
        [5.7, 2.8, 4.1, 1.3],
        [6.3, 3.3, 6. , 2.5],
        [5.8, 2.7, 5.1, 1.9],
        [7.1, 3. , 5.9, 2.1],
        [6.3, 2.9, 5.6, 1.8],
        [6.5, 3. , 5.8, 2.2],
        [7.6, 3. , 6.6, 2.1],
        [4.9, 2.5, 4.5, 1.7],
        [7.3, 2.9, 6.3, 1.8],
        [6.7, 2.5, 5.8, 1.8],
        [7.2, 3.6, 6.1, 2.5],
        [6.5, 3.2, 5.1, 2. ],
        [6.4, 2.7, 5.3, 1.9],
        [6.8, 3. , 5.5, 2.1],
        [5.7, 2.5, 5. , 2. ],
        [5.8, 2.8, 5.1, 2.4],
        [6.4, 3.2, 5.3, 2.3],
        [6.5, 3. , 5.5, 1.8],
        [7.7, 3.8, 6.7, 2.2],
        [7.7, 2.6, 6.9, 2.3],
        [6. , 2.2, 5. , 1.5],
        [6.9, 3.2, 5.7, 2.3],
        [5.6, 2.8, 4.9, 2. ],
        [7.7, 2.8, 6.7, 2. ],
        [6.3, 2.7, 4.9, 1.8],
        [6.7, 3.3, 5.7, 2.1],
        [7.2, 3.2, 6. , 1.8],
        [6.2, 2.8, 4.8, 1.8],
        [6.1, 3. , 4.9, 1.8],
        [6.4, 2.8, 5.6, 2.1],
        [7.2, 3. , 5.8, 1.6],
        [7.4, 2.8, 6.1, 1.9],
        [7.9, 3.8, 6.4, 2. ],
        [6.4, 2.8, 5.6, 2.2],
        [6.3, 2.8, 5.1, 1.5],
        [6.1, 2.6, 5.6, 1.4],
        [7.7, 3. , 6.1, 2.3],
        [6.3, 3.4, 5.6, 2.4],
        [6.4, 3.1, 5.5, 1.8],
        [6. , 3. , 4.8, 1.8],
        [6.9, 3.1, 5.4, 2.1],
        [6.7, 3.1, 5.6, 2.4],
        [6.9, 3.1, 5.1, 2.3],
        [5.8, 2.7, 5.1, 1.9],
        [6.8, 3.2, 5.9, 2.3],
        [6.7, 3.3, 5.7, 2.5],
        [6.7, 3. , 5.2, 2.3],
        [6.3, 2.5, 5. , 1.9],
        [6.5, 3. , 5.2, 2. ],
        [6.2, 3.4, 5.4, 2.3],
        [5.9, 3. , 5.1, 1.8]]),
 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),
 'frame': None,
 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),
 'DESCR': '.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 150 (50 in each of three classes)\n:Number of Attributes: 4 numeric, predictive attributes and the class\n:Attribute Information:\n    - sepal length in cm\n    - sepal width in cm\n    - petal length in cm\n    - petal width in cm\n    - class:\n            - Iris-Setosa\n            - Iris-Versicolour\n            - Iris-Virginica\n\n:Summary Statistics:\n\n============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD   Class Correlation\n============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n============== ==== ==== ======= ===== ====================\n\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n:Creator: R.A. Fisher\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n:Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. dropdown:: References\n\n  - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n    Mathematical Statistics" (John Wiley, NY, 1950).\n  - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n    (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n  - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n    Structure and Classification Rule for Recognition in Partially Exposed\n    Environments".  IEEE Transactions on Pattern Analysis and Machine\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n  - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n    on Information Theory, May 1972, 431-433.\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n    conceptual clustering system finds 3 classes in the data.\n  - Many, many more ...\n',
 'feature_names': ['sepal length (cm)',
  'sepal width (cm)',
  'petal length (cm)',
  'petal width (cm)'],
 'filename': 'iris.csv',
 'data_module': 'sklearn.datasets.data'}</code></pre>
</div>
</div>
<div id="a3ff6655" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.feature_names)</span>
<span id="cb12-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
<div id="3f909627" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">iris.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 4 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   sepal length (cm)  150 non-null    float64
 1   sepal width (cm)   150 non-null    float64
 2   petal length (cm)  150 non-null    float64
 3   petal width (cm)   150 non-null    float64
dtypes: float64(4)
memory usage: 4.8 KB</code></pre>
</div>
</div>
<div id="5786b73e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">iris.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>5.843333</td>
<td>3.057333</td>
<td>3.758000</td>
<td>1.199333</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>0.828066</td>
<td>0.435866</td>
<td>1.765298</td>
<td>0.762238</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>4.300000</td>
<td>2.000000</td>
<td>1.000000</td>
<td>0.100000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>5.100000</td>
<td>2.800000</td>
<td>1.600000</td>
<td>0.300000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>5.800000</td>
<td>3.000000</td>
<td>4.350000</td>
<td>1.300000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>6.400000</td>
<td>3.300000</td>
<td>5.100000</td>
<td>1.800000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>7.900000</td>
<td>4.400000</td>
<td>6.900000</td>
<td>2.500000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>sepal length와 petal width의 값의 차이가 크다.</p>
<p>전처리 과정에서 변수 정규화 수행의 근거가 된다.</p>
</section>
<section id="index-column-명-변경" class="level3">
<h3 class="anchored" data-anchor-id="index-column-명-변경">index / column 명 변경</h3>
<div id="ac723533" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">df.index</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>RangeIndex(start=0, stop=2, step=1)</code></pre>
</div>
</div>
<div id="71f2bd87" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(df.index)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[0, 1]</code></pre>
</div>
</div>
<div id="adfb5ad1" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">df.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>]</span>
<span id="cb20-2">df.index</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>Index(['A', 'B'], dtype='object')</code></pre>
</div>
</div>
<div id="e82e85d6" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">A</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">B</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="6017c0c1" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">df.set_index(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>, drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, append<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb23-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="35ed08bd" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df.reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb24-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="0292c0cf" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">iris.columns</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',
       'petal width (cm)'],
      dtype='object')</code></pre>
</div>
</div>
<div id="d04428a6" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">iris.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal width'</span>]</span>
<span id="cb27-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length</th>
<th data-quarto-table-cell-role="th">sepal width</th>
<th data-quarto-table-cell-role="th">petal length</th>
<th data-quarto-table-cell-role="th">petal width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
<div id="4b4600e2" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">iris.columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.columns.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_'</span>)</span>
<span id="cb28-2">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6.7</td>
<td>3.0</td>
<td>5.2</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6.3</td>
<td>2.5</td>
<td>5.0</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6.5</td>
<td>3.0</td>
<td>5.2</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6.2</td>
<td>3.4</td>
<td>5.4</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5.9</td>
<td>3.0</td>
<td>5.1</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
</section>
<section id="데이터-타입-변경" class="level3">
<h3 class="anchored" data-anchor-id="데이터-타입-변경">데이터 타입 변경</h3>
<p>사용 가능한 타입</p>
<ul>
<li>int</li>
<li>float</li>
<li>bool</li>
<li>datetime</li>
<li>category</li>
<li>object</li>
</ul>
<div id="12b5f4f5" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">iris.dtypes</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>sepal_length    float64
sepal_width     float64
petal_length    float64
petal_width     float64
dtype: object</code></pre>
</div>
</div>
<div id="55ef10f6" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'int'</span>)</span>
<span id="cb31-2">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb31-3">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>]].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'int'</span>)</span>
<span id="cb31-4">iris</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>2.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5</td>
<td>3</td>
<td>5</td>
<td>1.8</td>
</tr>
</tbody>
</table>

<p>150 rows × 4 columns</p>
</div>
</div>
</div>
</section>
</section>
<section id="row-coumn-선택-추가-삭제" class="level2">
<h2 class="anchored" data-anchor-id="row-coumn-선택-추가-삭제">row / coumn 선택 추가 삭제</h2>
<section id="row-선택" class="level3">
<h3 class="anchored" data-anchor-id="row-선택">row 선택</h3>
<div id="f302c291" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">iris[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>0.2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-선택" class="level3">
<h3 class="anchored" data-anchor-id="column-선택">column 선택</h3>
<p>Series 형식으로 출력</p>
<div id="120e90b4" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0      5
1      4
2      4
3      4
4      5
      ..
145    6
146    6
147    6
148    6
149    5
Name: sepal_length, Length: 150, dtype: int64</code></pre>
</div>
</div>
<p>DataFrame 형식으로 출력</p>
<div id="f48d69b4" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">iris[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">145</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">147</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">148</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">149</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>

<p>150 rows × 2 columns</p>
</div>
</div>
</div>
</section>
<section id="column-row-선택" class="level3">
<h3 class="anchored" data-anchor-id="column-row-선택">column, row 선택</h3>
<div id="99f447c1" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">iris.loc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="dced888b" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">iris.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="row-추가" class="level3">
<h3 class="anchored" data-anchor-id="row-추가">row 추가</h3>
<div id="e1ff00a2" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 방법 1: concat 사용</span></span>
<span id="cb38-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># df = pd.concat([df, pd.DataFrame([{'class': 'eng', 'score': 90}])], ignore_index=True)</span></span>
<span id="cb38-3"></span>
<span id="cb38-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 방법 2: loc 사용 </span></span>
<span id="cb38-5">df.loc[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(df)] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'class'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eng'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>}</span>
<span id="cb38-6">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>eng</td>
<td>90</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-추가" class="level3">
<h3 class="anchored" data-anchor-id="column-추가">column 추가</h3>
<div id="4ccbe3be" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'yo'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb39-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">yo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>80</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>90</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>eng</td>
<td>90</td>
<td>100</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="row-삭제" class="level3">
<h3 class="anchored" data-anchor-id="row-삭제">row 삭제</h3>
<div id="225929fd" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">df.drop(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb40-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">yo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>80</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>90</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="column-삭제" class="level3">
<h3 class="anchored" data-anchor-id="column-삭제">column 삭제</h3>
<div id="7a7a9991" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">df.drop(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'yo'</span>], inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb41-2">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="조건-선택" class="level2">
<h2 class="anchored" data-anchor-id="조건-선택">조건 선택</h2>
<div id="f1891b9d" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">iris[(iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (iris[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">54</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">58</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">62</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">63</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">68</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">130</td>
<td>7</td>
<td>2</td>
<td>6</td>
<td>1.9</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">132</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>2.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">133</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">134</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.4</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">146</td>
<td>6</td>
<td>2</td>
<td>5</td>
<td>1.9</td>
</tr>
</tbody>
</table>

<p>29 rows × 4 columns</p>
</div>
</div>
</div>
<div id="be8c4f64" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">df.loc[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pass'</span></span>
<span id="cb43-2">df.loc[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pass'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'합격'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fail'</span></span>
<span id="cb43-3">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="9f0874d8" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb44-2"></span>
<span id="cb44-3">condition_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>), </span>
<span id="cb44-4">                  (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>),</span>
<span id="cb44-5">                  (df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>)]</span>
<span id="cb44-6">grade_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]</span>
<span id="cb44-7">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'grade'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.select(condition_list, grade_list, default<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'F'</span>)</span>
<span id="cb44-8">df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
<th data-quarto-table-cell-role="th">grade</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
<td>A</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
<td>A</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<section id="결측치-탐색" class="level3">
<h3 class="anchored" data-anchor-id="결측치-탐색">결측치 탐색</h3>
<div id="e68561ed" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">df.isna().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>class    0
score    0
합격       0
grade    0
dtype: int64</code></pre>
</div>
</div>
<div id="6c3f43d2" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">df.notna().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 행 기준</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0    4
1    4
dtype: int64</code></pre>
</div>
</div>
</section>
<section id="결측치-제거" class="level3">
<h3 class="anchored" data-anchor-id="결측치-제거">결측치 제거</h3>
<div id="ec9e100e" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># dropna(axis=0, how='any' or 'all', thresh=None, subset=None, inplace=False)</span></span>
<span id="cb49-2">df.dropna()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">class</th>
<th data-quarto-table-cell-role="th">score</th>
<th data-quarto-table-cell-role="th">합격</th>
<th data-quarto-table-cell-role="th">grade</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>kor</td>
<td>70</td>
<td>Fail</td>
<td>A</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>math</td>
<td>80</td>
<td>Pass</td>
<td>A</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="결측치-대체" class="level3">
<h3 class="anchored" data-anchor-id="결측치-대체">결측치 대체</h3>
<div id="3f9eab98" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># fillna(value=None, method=None ('pad', 'ffill', 'backfill', 'bfill'), axis=None, inplace=False, limit=None)</span></span></code></pre></div>
</div>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>데이터 분석</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/01.html</guid>
  <pubDate>Sat, 20 Sep 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>전처리</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/core/01.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="결측치-처리" class="level2">
<h2 class="anchored" data-anchor-id="결측치-처리">결측치 처리</h2>
<ul>
<li>대푯값으로 대체</li>
<li>단순확률대치법</li>
<li>다른 모델로 예측</li>
<li>보간법: 시계열에서 주로 사용.</li>
</ul>
</section>
<section id="이상치-처리" class="level2">
<h2 class="anchored" data-anchor-id="이상치-처리">이상치 처리</h2>
<ul>
<li>ESD</li>
<li>IQR</li>
<li>DBSCAN</li>
</ul>
</section>
<section id="클래스-불균형" class="level2">
<h2 class="anchored" data-anchor-id="클래스-불균형">클래스 불균형</h2>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/core/01.html</guid>
  <pubDate>Fri, 15 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>EDA</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/core/00.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="질적-변수" class="level2">
<h2 class="anchored" data-anchor-id="질적-변수">질적 변수</h2>
<section id="상관분석" class="level3">
<h3 class="anchored" data-anchor-id="상관분석">상관분석</h3>
<div id="0c85a918" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> spearmanr, kendalltau</span>
<span id="cb1-2"></span>
<span id="cb1-3">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spearmanr(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-4"></span>
<span id="cb1-5">corr, p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kendalltau(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var1'</span>], df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var2'</span>])</span>
<span id="cb1-6"></span>
<span id="cb1-7">df.corr(method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'kendall'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># kendal, spearman</span></span></code></pre></div>
</div>
<div id="91a526a6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.contingeny <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> association</span>
<span id="cb2-2"></span>
<span id="cb2-3">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tschuprow"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># phi 계수</span></span>
<span id="cb2-4">v2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> association(table.values, method<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cramer'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 크래머 v</span></span>
<span id="cb2-5">v2</span></code></pre></div>
</div>
<ul>
<li>상관계수: 공분산을 각 변수의 표준편차로 나눈 것</li>
<li>스피어만 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.</li>
<li>켄달의 타우: 서열척도 vs 서열척도.
<ul>
<li>둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.</li>
<li>샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.</li>
<li>두 변수의 크기는 같아야함.</li>
</ul></li>
<li>phi 계수: 명목척도 vs 명목척도
<ul>
<li>두 변인 모두 level이 2개일 때 사용</li>
<li>두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산</li>
</ul></li>
<li>크래머 v: 명목척도 vs 명목척도.
<ul>
<li>적어도 하나의 변수가 3개 이상의 level을 가지면 사용</li>
<li>범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.</li>
</ul></li>
<li>Point-biserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 2개일 때</li>
</ul></li>
<li>Polyserial correlation: 명목척도 vs 연속형
<ul>
<li>명목척도의 level이 3개 이상일 때</li>
</ul></li>
<li>명목과 순서의 경우
<ul>
<li>level이 2개: Mann-Whitney U검정</li>
<li>3개 이상: Kruskal-Wallis H test</li>
</ul></li>
</ul>
</section>
<section id="시각화" class="level3">
<h3 class="anchored" data-anchor-id="시각화">시각화</h3>
<div id="331bf90e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-2"></span>
<span id="cb3-3">cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'your'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cols'</span>, ...]</span>
<span id="cb3-4">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df[cols].value_counts()) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 도수분포표</span></span>
<span id="cb3-5">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'proportion'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[cols].value_counts(normalize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 상대도수분포표</span></span></code></pre></div>
</div>
<div id="14b5320c" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].plot.bar(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), subplots<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, layout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb4-2">plt.tight_layout()</span>
<span id="cb4-3">plt.show()</span></code></pre></div>
</div>
<div id="35130288" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">plt.pie(freq[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>].values, </span>
<span id="cb5-2">        labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>freq.index, </span>
<span id="cb5-3">        autopct<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%1.1f%%</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb5-4">        colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sns.color_palette(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pastel'</span>, n_colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(freq)))</span>
<span id="cb5-5">plt.show()</span></code></pre></div>
</div>
</section>
</section>
<section id="양적-변수" class="level2">
<h2 class="anchored" data-anchor-id="양적-변수">양적 변수</h2>
<section id="기술통계" class="level3">
<h3 class="anchored" data-anchor-id="기술통계">기술통계</h3>
<div id="eabaed26" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats.mstats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> gmean, hmean, tmean</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb6-3"></span>
<span id="cb6-4">np.mean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 산술평균</span></span>
<span id="cb6-5">gmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 기하평균</span></span>
<span id="cb6-6">hmean(example) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 조화평균</span></span>
<span id="cb6-7">tmean(example, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 절사평균</span></span>
<span id="cb6-8">np.sqrt(np.mean(np.array(example) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 평방평균</span></span></code></pre></div>
</div>
<ul>
<li>기하평균: 비율의 평균에 주로 사용됨. 한 값이라도 0이면 전체가 0이 됨</li>
<li>조화평균: 속도, 밀도 등의 평균에 주로 사용됨.</li>
<li>절사평균: 극단값의 영향을 줄이기 위해 상위, 하위 몇 %를 제외한 평균</li>
<li>평방평균: 신호, 파동 등에서 자주 사용</li>
</ul>
<div id="a848a5fa" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.median()</span>
<span id="cb7-2">df.mode()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb7-3">df.quantile(q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span></code></pre></div>
</div>
<ul>
<li>상관계수: 피어슨</li>
</ul>
</section>
<section id="시각화-1" class="level3">
<h3 class="anchored" data-anchor-id="시각화-1">시각화</h3>
<ul>
<li>도수분포표</li>
<li>상대도수분포표</li>
<li>줄기잎그림</li>
<li>히스토그램</li>
<li>상자그림</li>
<li>산점도</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>확률 통계</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/core/00.html</guid>
  <pubDate>Mon, 04 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 신용 카드 사기 검출</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/04.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="under-over-sampling" class="level2">
<h2 class="anchored" data-anchor-id="under-over-sampling">under, over sampling</h2>
<ul>
<li>under sampling: 많은 비중을 차지하는 레이블을 작은 비중의 레이블에 맞추는것</li>
<li>over sampling: 반대
<ul>
<li>smote: k 최근접 이웃 진행 후, 이웃 간 간격을 맞추는 record를 새로 생성하는 방식</li>
</ul></li>
</ul>
<div id="05f6e255" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/04.html</guid>
  <pubDate>Fri, 01 Aug 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 감성 분석</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/09.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/09.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석 - 20 뉴스그룹 분류</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/08.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<div id="e4db7dfd" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fetch_20newsgroups</span>
<span id="cb1-2"></span>
<span id="cb1-3">train_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-4">X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.data</span>
<span id="cb1-5">y_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_news.target</span>
<span id="cb1-6"></span>
<span id="cb1-7">test_news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_20newsgroups(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test'</span>, remove<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'headers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'footers'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'quates'</span>))</span>
<span id="cb1-8">X_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.data</span>
<span id="cb1-9">y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> test_news.target</span></code></pre></div>
</div>
</section>
<section id="학습" class="level2">
<h2 class="anchored" data-anchor-id="학습">학습</h2>
<section id="count-vector" class="level3">
<h3 class="anchored" data-anchor-id="count-vector">Count Vector</h3>
<div id="232c2224" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CountVectorizer</span>
<span id="cb2-2"></span>
<span id="cb2-3">cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CountVectorizer()</span>
<span id="cb2-4">X_train_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.fit_transform(X_train)</span>
<span id="cb2-5">X_test_cnt_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cnt_vect.transform(X_test)</span></code></pre></div>
</div>
<div id="0069beaf" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb3-4"></span>
<span id="cb3-5">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb3-8">lr_clf.fit(X_train_cnt_vect, y_train)</span>
<span id="cb3-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_cnt_vect)</span>
<span id="cb3-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.731 </code></pre>
</div>
</div>
</section>
<section id="tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="tf-idf">TF-IDF</h3>
<div id="dcfcfa23" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.feature_extraction.text <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TfidfVectorizer</span>
<span id="cb5-2"></span>
<span id="cb5-3">tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TfidfVectorizer()</span>
<span id="cb5-4">X_train_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.fit_transform(X_train)</span>
<span id="cb5-5">X_test_tfidf_vect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tfidf_vect.transform(X_test)</span>
<span id="cb5-6"></span>
<span id="cb5-7">lr_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liblinear'</span>)</span>
<span id="cb5-8">lr_clf.fit(X_train_tfidf_vect, y_train)</span>
<span id="cb5-9">pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr_clf.predict(X_test_tfidf_vect)</span>
<span id="cb5-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>accuracy_score(y_test, pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.778 </code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/08.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>텍스트 분석</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/07.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">overview</h2>
<section id="nlp-vs-텍스트-분석" class="level3">
<h3 class="anchored" data-anchor-id="nlp-vs-텍스트-분석">NLP vs 텍스트 분석</h3>
<ul>
<li>NLP(자연어 처리)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술을 의미</li>
<li>텍스트 분석은 주로 비정형 텍스트 데이터를 머신러닝, 통계 등의 방법으로 예측 분석이나 유용한 정보를 추출하는 데 중점을 둔다.</li>
</ul>
</section>
<section id="종류" class="level3">
<h3 class="anchored" data-anchor-id="종류">종류</h3>
<ul>
<li>텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측 (연예 / 정치 / 스포츠 같은 카테고리 분류 혹은 스팸 메일 검출). 지도 학습</li>
<li>감성 분석: 텍스트에서 주관적 요소를 분석하는 기법. 지도 혹은 비지도.</li>
<li>텍스트 요약: 텍스트 내에서 주제나 중심 사상을 추출</li>
<li>텍스트 군집화: 비슷한 유형의 문서를 군집화 하는 것. 비지도 학습</li>
</ul>
</section>
</section>
<section id="프로세스" class="level2">
<h2 class="anchored" data-anchor-id="프로세스">프로세스</h2>
<ol type="1">
<li>텍스트 전처리: 대 / 소문자 변경, 특수 문자 제거, 토큰화, 불용어 제거, 어근 추출 등의 정규화 작업</li>
<li>피처 벡터화 / 추출: 텍스트에서 피처를 추출하고 벡터 값을 할당. BOW와 Word2Vec이 대표적</li>
<li>ML 모델 수립 및 학습 / 예측 / 평가</li>
</ol>
</section>
<section id="전처리" class="level2">
<h2 class="anchored" data-anchor-id="전처리">전처리</h2>
<ul>
<li>클렌징: 문자, 기호 등을 사전에 제거</li>
<li>토큰화
<ul>
<li>문장 토큰화: 마침표, 개행문자 등을 기준으로 문장을 분리. 각 문장이 가지는 의미가 중요한 경우 사용.</li>
<li>단어 토큰화: 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리.
<ul>
<li>n-gram: 단어의 연속된 n개를 묶어서 하나의 단위로 처리하는 방법. 문장이 가지는 의미를 조금이라도 보존할 수 있다.</li>
</ul></li>
</ul></li>
</ul>
<div id="63c1ee71" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sent_tokenize</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb1-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt'</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 문장을 분리하는 마침표, 개행문자 등의 데이터 셋 다운로드</span></span>
<span id="cb1-4">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'punkt_tab'</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">text_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes."</span></span>
<span id="cb1-7">sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text_sample)</span>
<span id="cb1-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sentences)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The Matrix is everywhere its all around us, here even in this room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package punkt to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!</code></pre>
</div>
</div>
<div id="6b8b73f5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> word_tokenize</span>
<span id="cb4-2"></span>
<span id="cb4-3">sentence <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The Matrix is everywhere its all around us, here even in this room."</span></span>
<span id="cb4-4">words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word_tokenize(sentence)</span>
<span id="cb4-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(words)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']</code></pre>
</div>
</div>
<div id="7f605e1a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tokenize_text(text):</span>
<span id="cb6-2">    sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sent_tokenize(text)</span>
<span id="cb6-3">    words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [word_tokenize(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb6-4"></span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> words</span>
<span id="cb6-6"></span>
<span id="cb6-7">word_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize_text(text_sample)</span>
<span id="cb6-8">word_tokens</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>[['The',
  'Matrix',
  'is',
  'everywhere',
  'its',
  'all',
  'around',
  'us',
  ',',
  'here',
  'even',
  'in',
  'this',
  'room',
  '.'],
 ['You',
  'can',
  'see',
  'it',
  'when',
  'you',
  'look',
  'out',
  'your',
  'window',
  'or',
  'when',
  'you',
  'turn',
  'on',
  'your',
  'television',
  '.'],
 ['You',
  'can',
  'feel',
  'it',
  'when',
  'you',
  'go',
  'to',
  'work',
  ',',
  'when',
  'you',
  'go',
  'to',
  'church',
  ',',
  'when',
  'you',
  'pay',
  'your',
  'taxes',
  '.']]</code></pre>
</div>
</div>
<ul>
<li>stopword 제거: 분석에 필요하지 않은 단어를 제거하는 작업. 예) 관사, 전치사, 접속사 등</li>
</ul>
<div id="198f8e8c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.corpus <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stopwords</span>
<span id="cb8-2">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'stopwords'</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># stopwords 데이터 셋 다운로드</span></span>
<span id="cb8-3"></span>
<span id="cb8-4">stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>]</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>['a',
 'about',
 'above',
 'after',
 'again',
 'against',
 'ain',
 'all',
 'am',
 'an',
 'and',
 'any',
 'are',
 'aren',
 "aren't",
 'as',
 'at',
 'be',
 'because',
 'been']</code></pre>
</div>
</div>
<div id="88ec22e1" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stopwords.words(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'english'</span>)</span>
<span id="cb11-2">all_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> word_tokens:</span>
<span id="cb11-4">    filtered_words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentence:</span>
<span id="cb11-6">        word <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word.lower()</span>
<span id="cb11-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sw:</span>
<span id="cb11-8">            filtered_words.append(word)</span>
<span id="cb11-9">    all_tokens.append(filtered_words)</span>
<span id="cb11-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(all_tokens)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'look', 'window', 'turn', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', ',', 'pay', 'taxes', '.']]</code></pre>
</div>
</div>
<ul>
<li>stemming, lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것
<ul>
<li>stemming이 더 단순하고 빠르지만 lemmatization 이 더 저오학함</li>
</ul></li>
</ul>
<div id="dab74859" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LancasterStemmer</span>
<span id="cb13-2"></span>
<span id="cb13-3">stemmer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LancasterStemmer()</span>
<span id="cb13-4"></span>
<span id="cb13-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'working'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'works'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worked'</span>))</span>
<span id="cb13-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>))</span>
<span id="cb13-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>))</span>
<span id="cb13-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>), stemmer.stem(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>work work work
amus amus amus
happy happiest
fant fanciest</code></pre>
</div>
</div>
<div id="96465f65" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> nltk.stem <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> WordNetLemmatizer</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> nltk</span>
<span id="cb15-3">nltk.download(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wordnet'</span>)</span>
<span id="cb15-4"></span>
<span id="cb15-5">lemma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> WordNetLemmatizer()</span>
<span id="cb15-6"></span>
<span id="cb15-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amusing'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amuses'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'amused'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'v'</span>))</span>
<span id="cb15-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'happiest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span>
<span id="cb15-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fancier'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>), lemma.lemmatize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fanciest'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>amuse amuse amuse
happy happy
fancy fancy</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package wordnet to
[nltk_data]     /home/cryscham123/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!</code></pre>
</div>
</div>
</section>
<section id="bow" class="level2">
<h2 class="anchored" data-anchor-id="bow">BOW</h2>
<ul>
<li>문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 빈도 값을 부여해 피처 값을 추출하는 모델</li>
<li>count 기반 벡터화: 빈도가 높을수록 중요한 단어로 인식</li>
<li>TF-IDF(term frequency - inverse document frequency) 기반 벡터화: 빈도가 높을수록 좋으나, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 패털티를 줌
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i%20*%20log%5Cfrac%7BN%7D%7BDF_i%7D">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?TF_i">: 개별 문서에서의 단어 i 빈도</li>
<li><img src="https://latex.codecogs.com/png.latex?DF_i">: 단어 i를 가지고 있는 문서 개수</li>
<li>N: 전체 문서 개수</li>
</ul></li>
</ul></li>
<li>희소행렬 문제: 불필요한 0 값이 많아지는 문제
<ul>
<li>COO</li>
<li>CSR</li>
<li>혹은 희소행렬을 잘 처리하는 알고리즘: 로지스틱 회귀, 선형 svm, 나이브 베이즈 등</li>
</ul></li>
</ul>
<section id="coo" class="level3">
<h3 class="anchored" data-anchor-id="coo">COO</h3>
<ul>
<li>0이 아닌 데이터만 별도의 array에 저장.</li>
</ul>
<div id="aab7eb13" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb18-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb18-3"></span>
<span id="cb18-4">dense <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb18-5">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb18-6">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-7">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-8">sparse_coo <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.coo_matrix((data, (row_pos, col_pos)))</span>
<span id="cb18-9">sparse_coo</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>&lt;COOrdinate sparse matrix of dtype 'int64'
    with 3 stored elements and shape (2, 3)&gt;</code></pre>
</div>
</div>
<div id="920b655f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">sparse_coo.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>array([[3, 0, 1],
       [0, 2, 0]])</code></pre>
</div>
</div>
</section>
<section id="csr" class="level3">
<h3 class="anchored" data-anchor-id="csr">CSR</h3>
<ul>
<li>COO + 시작위치만 기록하는 방법</li>
</ul>
<div id="29fe3804" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sparse</span>
<span id="cb22-2"></span>
<span id="cb22-3">dense2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-4">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>],</span>
<span id="cb22-5">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-6">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb22-7">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>],</span>
<span id="cb22-8">                   [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb22-9">data2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb22-10">row_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>])</span>
<span id="cb22-11">col_pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb22-12">row_pos_ind <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>])</span>
<span id="cb22-13"></span>
<span id="cb22-14">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix((data2, col_pos, row_pos_ind))</span>
<span id="cb22-15">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>
<div id="e6d22228" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">sparse_csr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sparse.csr_matrix(dense2)</span>
<span id="cb24-2">sparse_csr.toarray()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>array([[0, 0, 1, 0, 0, 5],
       [1, 4, 0, 3, 2, 5],
       [0, 6, 0, 3, 0, 0],
       [2, 0, 0, 0, 0, 0],
       [0, 0, 0, 7, 0, 8],
       [1, 0, 0, 0, 0, 0]])</code></pre>
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/07.html</guid>
  <pubDate>Tue, 29 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>차원 축소</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="pca" class="level2">
<h2 class="anchored" data-anchor-id="pca">PCA</h2>
<div id="cb1d936b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_iris</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"></span>
<span id="cb1-5">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb1-6">columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_length'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'petal_width'</span>]</span>
<span id="cb1-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>columns)</span>
<span id="cb1-8">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb1-9">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb1-12">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_length'</span>]</span>
<span id="cb1-13">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal_width'</span>]</span>
<span id="cb1-14">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb1-15">plt.legend()</span>
<span id="cb1-16">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal length'</span>)</span>
<span id="cb1-17">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sepal width'</span>)</span>
<span id="cb1-18">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06_files/figure-html/cell-2-output-1.png" width="589" height="432" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>PCA는 scaling의 영향을 받음.</li>
</ul>
<div id="5cda3239" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.decomposition <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PCA</span>
<span id="cb2-3"></span>
<span id="cb2-4">scaled_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb2-5"></span>
<span id="cb2-6">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-7">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pca.fit_transform(scaled_df)</span></code></pre></div>
</div>
<div id="2991346c" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">pca_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb3-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(df, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pca_columns)</span>
<span id="cb3-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span></code></pre></div>
</div>
<div id="26863b93" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb4-4">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>]</span>
<span id="cb4-5">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>]</span>
<span id="cb4-6">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb4-7">plt.legend()</span>
<span id="cb4-8">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_1'</span>)</span>
<span id="cb4-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pca_component_2'</span>)</span>
<span id="cb4-10">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06_files/figure-html/cell-5-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="신용카드-고객-데이터" class="level2">
<h2 class="anchored" data-anchor-id="신용카드-고객-데이터">신용카드 고객 데이터</h2>
<div id="7ddc060a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_excel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/creadit_card.xls'</span>, header<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, sheet_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Data'</span>).iloc[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:]</span>
<span id="cb5-2">df.rename(columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_0'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PAY_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default payment next month'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>}, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>]</span>
<span id="cb5-4">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'default'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="689f1e58" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb6-2"></span>
<span id="cb6-3">corr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> features.corr()</span>
<span id="cb6-4">sns.heatmap(corr, annot<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, fmt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.1g'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06_files/figure-html/cell-7-output-1.png" width="610" height="482" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>BILL_AMT1~6, PAY_1~6의 상관도가 높다.</li>
</ul>
<div id="d35a5afa" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">cols_bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BILL_AMT'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(i) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)]</span>
<span id="cb7-2">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb7-3">df_cols_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(features[cols_bill])</span>
<span id="cb7-4">pca <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PCA(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb7-5">pca.fit(df_cols_scaled)</span>
<span id="cb7-6">pca.explained_variance_ratio_</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([0.90555253, 0.0509867 ])</code></pre>
</div>
</div>
<ul>
<li>PCA 할 때 column 전부 다 안 넣어도 되나?</li>
<li>다 넣어야 하는 듯</li>
</ul>
</section>
<section id="lda" class="level2">
<h2 class="anchored" data-anchor-id="lda">LDA</h2>
<ul>
<li>클래스 분리를 최대화하는 축을 찾음</li>
<li>PCA와 다르게 지도 학습임.</li>
</ul>
<div id="7ca579cb" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.discriminant_analysis <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearDiscriminantAnalysis</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb9-3"></span>
<span id="cb9-4">iris <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_iris()</span>
<span id="cb9-5">iris_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler().fit_transform(iris.data)</span></code></pre></div>
</div>
<div id="4f389cb7" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearDiscriminantAnalysis(n_components<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-2">iris_lda <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lda.fit_transform(iris_scaled, iris.target)</span></code></pre></div>
</div>
<div id="1fdd19bc" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">lda_columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(iris_lda, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lda_columns)</span>
<span id="cb11-3">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> iris.target</span>
<span id="cb11-4"></span>
<span id="cb11-5">markers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'^'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'s'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>]</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, marker <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(markers):</span>
<span id="cb11-8">    x_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>]</span>
<span id="cb11-9">    y_axis_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>]</span>
<span id="cb11-10">    plt.scatter(x_axis_data, y_axis_data, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>marker, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>iris.target_names[i])</span>
<span id="cb11-11">plt.legend()</span>
<span id="cb11-12">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_1'</span>)</span>
<span id="cb11-13">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'lda_components_2'</span>)</span>
<span id="cb11-14">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06_files/figure-html/cell-11-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/06.html</guid>
  <pubDate>Mon, 28 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>회귀</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/05.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="경사하강법" class="level2">
<h2 class="anchored" data-anchor-id="경사하강법">경사하강법</h2>
<div id="72757eac" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"></span>
<span id="cb1-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.random.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-5">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-6">plt.scatter(X, y)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/05_files/figure-html/cell-2-output-1.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="679facc8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_cost(y, y_pred):</span>
<span id="cb2-2">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-3">    cost <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.square(y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> N</span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> cost</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>):</span>
<span id="cb2-7">    N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(y)</span>
<span id="cb2-8">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w1)</span>
<span id="cb2-9">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros_like(w0)</span>
<span id="cb2-10">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(X, w1.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb2-11">    diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred</span>
<span id="cb2-12"></span>
<span id="cb2-13">    w1_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.dot(X.T, diff)</span>
<span id="cb2-14">    w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(diff)</span>
<span id="cb2-15"></span>
<span id="cb2-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1_update, w0_update</span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>):</span>
<span id="cb2-19">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-20">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb2-21"></span>
<span id="cb2-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb2-23">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, X, y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb2-24">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb2-25">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb2-26"></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="b9d1775e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb3-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-4">plt.scatter(X, y)</span>
<span id="cb3-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 5.955 w1: 3.940, total cost: 1.018</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/05_files/figure-html/cell-4-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>일반 경사하강법은 시간이 오래걸려서 잘 안씀</li>
</ul>
</section>
<section id="미니-배치-확률적-경사-하강법" class="level2">
<h2 class="anchored" data-anchor-id="미니-배치-확률적-경사-하강법">미니 배치 확률적 경사 하강법</h2>
<div id="81b437bc" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> stochastic_gradient_descent_steps(X, y, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>):</span>
<span id="cb5-2">    w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-3">    w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ind <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(iters):</span>
<span id="cb5-6">        stochastic_random_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.permutation(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb5-7">        sample_X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-8">        sample_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[stochastic_random_index[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:batch_size]]</span>
<span id="cb5-9"></span>
<span id="cb5-10">        w1_update, w0_update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_weight_updates(w1, w0, sample_X, sample_y, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb5-11">        w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w1_update</span>
<span id="cb5-12">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> w0_update</span>
<span id="cb5-13"></span>
<span id="cb5-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w1, w0</span></code></pre></div>
</div>
<div id="7ce5cf77" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">w1, w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stochastic_gradient_descent_steps(X, y, iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb6-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w0</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'w0: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w0[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> w1: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>w1[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, total cost: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_cost(y, y_pred)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb6-4">plt.scatter(X, y)</span>
<span id="cb6-5">plt.plot(X, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>w0: 5.931 w1: 3.978, total cost: 1.021</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/05_files/figure-html/cell-6-output-2.png" width="566" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="선형-회귀" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀">선형 회귀</h2>
<div id="6c4fd569" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb8-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb8-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_boston</span>
<span id="cb8-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb8-6"></span>
<span id="cb8-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb8-8"></span>
<span id="cb8-9">boston <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_boston()</span>
<span id="cb8-10"></span>
<span id="cb8-11">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(boston.data, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>boston.feature_names)</span>
<span id="cb8-12">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boston.target</span>
<span id="cb8-13">df.head()</span></code></pre></div>
</div>
<div id="a9d30dd9" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">lm_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RM'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ZN'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'INDUS'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NOX'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'AGE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PTRAIO'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LSTAT'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RAD'</span>]</span>
<span id="cb9-2"></span>
<span id="cb9-3">fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>), ncols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(lm_features) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, nrows<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, feature <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(lm_features):</span>
<span id="cb9-6">    row <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-7">    col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb9-8"></span>
<span id="cb9-9">    sns.regplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>feature, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axs[row][col])</span></code></pre></div>
</div>
<p>boston 데이터가 윤리적 문제로 사용 불가능하다고 한다.</p>
<div id="a3b571cf" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cross_val_score</span>
<span id="cb10-3"></span>
<span id="cb10-4">y_target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>]</span>
<span id="cb10-5">X_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.drop([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb10-6">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression()</span>
<span id="cb10-7"></span>
<span id="cb10-8">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(lr, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb10-9">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb10-10">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
<p>cross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함</p>
</section>
<section id="다항-회귀" class="level2">
<h2 class="anchored" data-anchor-id="다항-회귀">다항 회귀</h2>
<div id="6e15120c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PolynomialFeatures</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb11-4"></span>
<span id="cb11-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> polynominal_func(X):</span>
<span id="cb11-6">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> X[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb11-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y</span>
<span id="cb11-8"></span>
<span id="cb11-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'poly'</span>, PolynomialFeatures(degree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)),</span>
<span id="cb11-10">                  (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>, LinearRegression())])</span>
<span id="cb11-11">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb11-12">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> polynominal_func(X)</span>
<span id="cb11-13"></span>
<span id="cb11-14">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.fit(X, y)</span>
<span id="cb11-15"></span>
<span id="cb11-16">np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(model.named_steps[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'linear'</span>].coef_, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])</code></pre>
</div>
</div>
</section>
<section id="규제" class="level2">
<h2 class="anchored" data-anchor-id="규제">규제</h2>
<ul>
<li><p>L2 규제(Ridge): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C%5E2)"></p></li>
<li><p>L1 규제(Lasso): <img src="https://latex.codecogs.com/png.latex?min(RSS(W)%20+%20%5Clambda%20%7C%7CW%7C%7C_1)"></p></li>
<li><p>λ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐</p></li>
<li><p>L1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음</p></li>
</ul>
<section id="릿지" class="level3">
<h3 class="anchored" data-anchor-id="릿지">릿지</h3>
<div id="b3d47d18" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge</span>
<span id="cb13-2"></span>
<span id="cb13-3">ridge <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb13-4">neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(ridge, X_data, y_target, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb13-5">rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb13-6">avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span></code></pre></div>
</div>
</section>
<section id="라쏘-엘라스틱넷" class="level3">
<h3 class="anchored" data-anchor-id="라쏘-엘라스틱넷">라쏘 엘라스틱넷</h3>
<div id="5e7600e7" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb14-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Ridge, Lasso, ElasticNet</span>
<span id="cb14-3"></span>
<span id="cb14-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_linear_reg_eval(model_name, params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, X_data_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, y_target_n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb14-5">    coeff_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> params:</span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ridge'</span>:</span>
<span id="cb14-8">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Ridge(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lasso'</span>:</span>
<span id="cb14-10">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Lasso(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param)</span>
<span id="cb14-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ElasticNet'</span>:</span>
<span id="cb14-12">            model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ElasticNet(alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param, l1_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-13">        neg_mse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_val_score(model, X_data_n, y_target_n, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"neg_mean_squared_error"</span>, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb14-14">        rmse_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> neg_mse_scores)</span>
<span id="cb14-15">        avg_rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(rmse_scores)</span>
<span id="cb14-16">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>avg_rmse<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb14-17"></span>
<span id="cb14-18">        model.fit(X_data_n, y_target_n)</span>
<span id="cb14-19">        coeff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.coef_, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_data_n.columns)</span>
<span id="cb14-20">        colname <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alpha:'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(param)</span>
<span id="cb14-21">        coeff_df[colname] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> coeff</span>
<span id="cb14-22"></span>
<span id="cb14-23">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> coeff_df</span></code></pre></div>
</div>
</section>
</section>
<section id="선형-회귀-모델을-위한-데이터-변환" class="level2">
<h2 class="anchored" data-anchor-id="선형-회귀-모델을-위한-데이터-변환">선형 회귀 모델을 위한 데이터 변환</h2>
<ul>
<li>로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.</li>
</ul>
<div id="b210089d" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">np.log1p(data)</span></code></pre></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/05.html</guid>
  <pubDate>Sun, 27 Jul 2025 15:00:00 GMT</pubDate>
</item>
<item>
  <title>분류 - 산탄데르 고객 만족 예측</title>
  <link>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/03.html</link>
  <description><![CDATA[ 




<p><img src="https://cryscham123.github.io/img/stat-thumb.jpg" class="post-thumbnail img-fluid"></p>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<div id="d5947441" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb1-5"></span>
<span id="cb1-6">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.family'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Noto Sans KR'</span></span>
<span id="cb1-7">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ignore'</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/train.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb1-10">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 76020 entries, 0 to 76019
Columns: 371 entries, ID to TARGET
dtypes: float64(111), int64(260)
memory usage: 215.2 MB</code></pre>
</div>
</div>
<div id="64745c05" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df.describe()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ID</th>
<th data-quarto-table-cell-role="th">var3</th>
<th data-quarto-table-cell-role="th">var15</th>
<th data-quarto-table-cell-role="th">imp_ent_var16_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var39_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_comer_ult3</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult1</th>
<th data-quarto-table-cell-role="th">imp_op_var40_efect_ult3</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var33_ult3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace2</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_hace3</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult1</th>
<th data-quarto-table-cell-role="th">saldo_medio_var44_ult3</th>
<th data-quarto-table-cell-role="th">var38</th>
<th data-quarto-table-cell-role="th">TARGET</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>...</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>76020.000000</td>
<td>7.602000e+04</td>
<td>76020.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>75964.050723</td>
<td>-1523.199277</td>
<td>33.212865</td>
<td>86.208265</td>
<td>72.363067</td>
<td>119.529632</td>
<td>3.559130</td>
<td>6.472698</td>
<td>0.412946</td>
<td>0.567352</td>
<td>...</td>
<td>7.935824</td>
<td>1.365146</td>
<td>12.215580</td>
<td>8.784074</td>
<td>31.505324</td>
<td>1.858575</td>
<td>76.026165</td>
<td>56.614351</td>
<td>1.172358e+05</td>
<td>0.039569</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>43781.947379</td>
<td>39033.462364</td>
<td>12.956486</td>
<td>1614.757313</td>
<td>339.315831</td>
<td>546.266294</td>
<td>93.155749</td>
<td>153.737066</td>
<td>30.604864</td>
<td>36.513513</td>
<td>...</td>
<td>455.887218</td>
<td>113.959637</td>
<td>783.207399</td>
<td>538.439211</td>
<td>2013.125393</td>
<td>147.786584</td>
<td>4040.337842</td>
<td>2852.579397</td>
<td>1.826646e+05</td>
<td>0.194945</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>1.000000</td>
<td>-999999.000000</td>
<td>5.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>5.163750e+03</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>38104.750000</td>
<td>2.000000</td>
<td>23.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>6.787061e+04</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>76043.000000</td>
<td>2.000000</td>
<td>28.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.064092e+05</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>113748.750000</td>
<td>2.000000</td>
<td>40.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>...</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>1.187563e+05</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>151838.000000</td>
<td>238.000000</td>
<td>105.000000</td>
<td>210000.000000</td>
<td>12888.030000</td>
<td>21024.810000</td>
<td>8237.820000</td>
<td>11073.570000</td>
<td>6600.000000</td>
<td>6600.000000</td>
<td>...</td>
<td>50003.880000</td>
<td>20385.720000</td>
<td>138831.630000</td>
<td>91778.730000</td>
<td>438329.220000</td>
<td>24650.010000</td>
<td>681462.900000</td>
<td>397884.300000</td>
<td>2.203474e+07</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

<p>8 rows × 371 columns</p>
</div>
</div>
</div>
<div id="e314ea19" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-2">df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-3"></span>
<span id="cb4-4">X_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, :<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb4-5">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.iloc[:, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<div id="f9cec085" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/test.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb5-2">test_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'var3'</span>].replace(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb5-3">test_df.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ID'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, inplace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<div id="05d26bff" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_features, labels, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span></code></pre></div>
</div>
<ul>
<li>train, test의 label의 비율이 동일한게 좋은걸까</li>
</ul>
</section>
<section id="xgboost" class="level2">
<h2 class="anchored" data-anchor-id="xgboost">XGBoost</h2>
<div id="8340b7d0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">X_tr, X_val, y_tr, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_train, y_train, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span></code></pre></div>
</div>
<div id="43b1a238" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb8-3"></span>
<span id="cb8-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb8-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>, </span>
<span id="cb8-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, </span>
<span id="cb8-7">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb8-8">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb8-9">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb8-10">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb8-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
<section id="베이지안-최적화" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화">베이지안 최적화</h3>
<div id="ffd62d7c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb9-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb9-5">    xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb9-6">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb9-7">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb9-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb9-9">                            min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb9-10">                            colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>],</span>
<span id="cb9-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb9-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb9-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb9-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb9-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb9-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb9-17"></span>
<span id="cb9-18">        xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb9-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb9-20">        roc_auc_list.append(score)</span>
<span id="cb9-21"></span>
<span id="cb9-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="c5c43e8f" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb10-2"></span>
<span id="cb10-3">xgb_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb10-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span>),</span>
<span id="cb10-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb10-8">}</span>
<span id="cb10-9"></span>
<span id="cb10-10">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb10-11">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb10-12">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>xgb_search_space,</span>
<span id="cb10-13">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb10-14">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb10-15">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb10-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
</div>
</section>
<section id="재-학습" class="level3">
<h3 class="anchored" data-anchor-id="재-학습">재 학습</h3>
<div id="84dbf898" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> XGBClassifier</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb11-3"></span>
<span id="cb11-4">evals <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb11-5">xgb_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb11-6">                    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-7">                    max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb11-8">                    min_child_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>]),</span>
<span id="cb11-9">                    colsample_bytree<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bytree'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb11-10">                    early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb11-11">                    eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>])</span>
<span id="cb11-12">xgb_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb11-13">xgb_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb11-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>xgb_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
</section>
<section id="plot-importance" class="level3">
<h3 class="anchored" data-anchor-id="plot-importance">plot importance</h3>
<div id="7efd1ad9" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot_importance</span>
<span id="cb12-2"></span>
<span id="cb12-3">plot_importance(xgb_clf, max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>)</span></code></pre></div>
</div>
</section>
</section>
<section id="lightgbm" class="level2">
<h2 class="anchored" data-anchor-id="lightgbm">LightGBM</h2>
<div id="58532da1" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> roc_auc_score</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMClassifier</span>
<span id="cb13-3"></span>
<span id="cb13-4">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb13-5"></span>
<span id="cb13-6">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb13-7">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb13-8"></span>
<span id="cb13-9">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb13-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1653, number of negative: 40918
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007850 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 13447
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 251
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978
[LightGBM] [Info] Start training from score -3.208978
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117279 valid_1's binary_logloss: 0.137813
[LightGBM] [Warning] Unknown parameter: eval_metric
0.834</code></pre>
</div>
</div>
<section id="베이지안-최적화-1" class="level3">
<h3 class="anchored" data-anchor-id="베이지안-최적화-1">베이지안 최적화</h3>
<div id="d3b3256c" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> KFold</span>
<span id="cb15-2"></span>
<span id="cb15-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective_func(search_space):</span>
<span id="cb15-4">    lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb15-5">                            early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>,</span>
<span id="cb15-6">                            eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>,</span>
<span id="cb15-7">                            num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb15-8">                            max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb15-9">                            min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb15-10">                            subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>],</span>
<span id="cb15-11">                            learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>search_space[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb15-12">    roc_auc_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb15-13">    kf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> KFold(n_splits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb15-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> tr_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> kf.split(X_train):</span>
<span id="cb15-15">        X_tr, y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.iloc[tr_index], y_train.iloc[tr_index]</span>
<span id="cb15-16">        X_val, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  X_train.iloc[val_index], y_train.iloc[val_index]</span>
<span id="cb15-17"></span>
<span id="cb15-18">        lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_tr, y_tr), (X_val, y_val)])</span>
<span id="cb15-19">        score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb15-20">        roc_auc_list.append(score)</span>
<span id="cb15-21"></span>
<span id="cb15-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.mean(roc_auc_list)</span></code></pre></div>
</div>
<div id="b6d8769a" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> hyperopt <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> hp, fmin, tpe, Trials</span>
<span id="cb16-2"></span>
<span id="cb16-3">lgbm_search_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">160</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>: hp.quniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-7">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb16-8">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: hp.uniform(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb16-9">}</span>
<span id="cb16-10"></span>
<span id="cb16-11">trials <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trials()</span>
<span id="cb16-12">best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fmin(fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>objective_func,</span>
<span id="cb16-13">            space<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lgbm_search_space,</span>
<span id="cb16-14">            algo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tpe.suggest,</span>
<span id="cb16-15">            max_evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb16-16">            trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>trials)</span>
<span id="cb16-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(best)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009804 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12809
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.168309
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[36]    training's binary_logloss: 0.121676 valid_1's binary_logloss: 0.127049
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011714 seconds.
You can set `force_col_wise=true` to remove the overhead.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.194075
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[44]    training's binary_logloss: 0.115084 valid_1's binary_logloss: 0.135595
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.233233
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds
  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:
[50]    training's binary_logloss: 0.110571 valid_1's binary_logloss: 0.140209
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric
  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006776 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12809
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Early stopping, best iteration is:
[68]    training's binary_logloss: 0.119949 valid_1's binary_logloss: 0.127337
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008185 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12874
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:
[74]    training's binary_logloss: 0.114945 valid_1's binary_logloss: 0.135003
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005666 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12865
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds
  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.140191
  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006637 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.121998 valid_1's binary_logloss: 0.127349
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006521 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12970
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[26]    training's binary_logloss: 0.115056 valid_1's binary_logloss: 0.136143
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008614 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 13049
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[30]    training's binary_logloss: 0.110308 valid_1's binary_logloss: 0.140967
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]  6%|▌         | 3/50 [00:05&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006389 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.119702 valid_1's binary_logloss: 0.127682
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005965 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.11491  valid_1's binary_logloss: 0.13632
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006447 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[19]    training's binary_logloss: 0.113764 valid_1's binary_logloss: 0.141398
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006690 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.114179 valid_1's binary_logloss: 0.127237
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006128 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[57]    training's binary_logloss: 0.113751 valid_1's binary_logloss: 0.136174
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007084 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[58]    training's binary_logloss: 0.11113  valid_1's binary_logloss: 0.140897
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884] 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006562 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.120721 valid_1's binary_logloss: 0.127623
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005976 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.117914 valid_1's binary_logloss: 0.135692
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[18]    training's binary_logloss: 0.117142 valid_1's binary_logloss: 0.141073
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884] 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005795 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[23]    training's binary_logloss: 0.122492 valid_1's binary_logloss: 0.127389
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005857 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12882
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[20]    training's binary_logloss: 0.119931 valid_1's binary_logloss: 0.13599
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007378 seconds.
You can set `force_col_wise=true` to remove the overhead.
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12883
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[21]    training's binary_logloss: 0.116742 valid_1's binary_logloss: 0.14122
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884] 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006486 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[37]    training's binary_logloss: 0.118076 valid_1's binary_logloss: 0.12711
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008708 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[27]    training's binary_logloss: 0.118244 valid_1's binary_logloss: 0.135768
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007417 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[34]    training's binary_logloss: 0.11261  valid_1's binary_logloss: 0.140798
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884] 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007634 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[93]    training's binary_logloss: 0.113871 valid_1's binary_logloss: 0.127108
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007168 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12934
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[75]    training's binary_logloss: 0.113106 valid_1's binary_logloss: 0.135792
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008788 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12989
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:
[82]    training's binary_logloss: 0.109277 valid_1's binary_logloss: 0.140921
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884] 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007124 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[57]    training's binary_logloss: 0.120677 valid_1's binary_logloss: 0.127111
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009349 seconds.
You can set `force_col_wise=true` to remove the overhead.
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[50]    training's binary_logloss: 0.118347 valid_1's binary_logloss: 0.135488
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013450 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds
 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:
[54]    training's binary_logloss: 0.114424 valid_1's binary_logloss: 0.140196
 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric
 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884] 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006974 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12907
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[78]    training's binary_logloss: 0.111459 valid_1's binary_logloss: 0.12715
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006741 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12943
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.112371 valid_1's binary_logloss: 0.13579
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007176 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13017
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.105423 valid_1's binary_logloss: 0.141018
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884] 22%|██▏       | 11/50 [00:25&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006851 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.123258 valid_1's binary_logloss: 0.127671
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008232 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[19]    training's binary_logloss: 0.118847 valid_1's binary_logloss: 0.135735
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006765 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.115967 valid_1's binary_logloss: 0.140884
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884] 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007459 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.118279 valid_1's binary_logloss: 0.128419
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010830 seconds.
You can set `force_col_wise=true` to remove the overhead.
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.114963 valid_1's binary_logloss: 0.136964
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008271 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.111041 valid_1's binary_logloss: 0.141811
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884] 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006716 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.117911 valid_1's binary_logloss: 0.127609
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007818 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.112846 valid_1's binary_logloss: 0.135945
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007760 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[21]    training's binary_logloss: 0.11335  valid_1's binary_logloss: 0.141758
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884] 28%|██▊       | 14/50 [00:31&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011169 seconds.
You can set `force_col_wise=true` to remove the overhead.
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[15]    training's binary_logloss: 0.123793 valid_1's binary_logloss: 0.127794
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006417 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[17]    training's binary_logloss: 0.117509 valid_1's binary_logloss: 0.136341
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012417 seconds.
You can set `force_col_wise=true` to remove the overhead.
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.118131 valid_1's binary_logloss: 0.141827
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884] 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.116669 valid_1's binary_logloss: 0.127316
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008685 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.112195 valid_1's binary_logloss: 0.13634
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008114 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.110627 valid_1's binary_logloss: 0.141491
 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 30%|███       | 15/50 [00:36&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884] 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006910 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[44]    training's binary_logloss: 0.11468  valid_1's binary_logloss: 0.127009
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008679 seconds.
You can set `force_col_wise=true` to remove the overhead.
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.116699 valid_1's binary_logloss: 0.136132
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009082 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.110851 valid_1's binary_logloss: 0.140914
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884] 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009004 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.133099 valid_1's binary_logloss: 0.130118
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010889 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.128591 valid_1's binary_logloss: 0.138373
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006967 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.125837 valid_1's binary_logloss: 0.144181
 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884] 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007196 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.117838 valid_1's binary_logloss: 0.127509
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.114266 valid_1's binary_logloss: 0.136132
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[44]    training's binary_logloss: 0.107097 valid_1's binary_logloss: 0.141644
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884] 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006872 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[32]    training's binary_logloss: 0.120651 valid_1's binary_logloss: 0.12748
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007690 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.117806 valid_1's binary_logloss: 0.135748
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007569 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:
[39]    training's binary_logloss: 0.111183 valid_1's binary_logloss: 0.140593
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884] 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010567 seconds.
You can set `force_col_wise=true` to remove the overhead.
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127621 valid_1's binary_logloss: 0.127975
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028233 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.123117 valid_1's binary_logloss: 0.136484
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007220 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120407 valid_1's binary_logloss: 0.141888
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884] 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008396 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[94]    training's binary_logloss: 0.120321 valid_1's binary_logloss: 0.12706
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007023 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[95]    training's binary_logloss: 0.116042 valid_1's binary_logloss: 0.135298
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007237 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds
 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.112382 valid_1's binary_logloss: 0.140103
 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884] 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007562 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120691 valid_1's binary_logloss: 0.127296
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009653 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[99]    training's binary_logloss: 0.116561 valid_1's binary_logloss: 0.135521
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007650 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.113883 valid_1's binary_logloss: 0.140482
 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328] 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009103 seconds.
You can set `force_col_wise=true` to remove the overhead.
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12506  valid_1's binary_logloss: 0.127517
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007298 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120784 valid_1's binary_logloss: 0.135909
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006853 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.11795  valid_1's binary_logloss: 0.141026
 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328] 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006418 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.120728 valid_1's binary_logloss: 0.12726
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007255 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.116085 valid_1's binary_logloss: 0.13534
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006430 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[60]    training's binary_logloss: 0.11095  valid_1's binary_logloss: 0.140369
 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328] 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.118162 valid_1's binary_logloss: 0.127362
 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006679 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[93]    training's binary_logloss: 0.114804 valid_1's binary_logloss: 0.135408
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007134 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[98]    training's binary_logloss: 0.111333 valid_1's binary_logloss: 0.140214
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328] 52%|█████▏    | 26/50 [01:05&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006927 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.11927  valid_1's binary_logloss: 0.127628
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007967 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[33]    training's binary_logloss: 0.11608  valid_1's binary_logloss: 0.136215
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007403 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[37]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.141059
 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328] 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011520 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[65]    training's binary_logloss: 0.11962  valid_1's binary_logloss: 0.127063
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007475 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[72]    training's binary_logloss: 0.114002 valid_1's binary_logloss: 0.135602
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008925 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[71]    training's binary_logloss: 0.111386 valid_1's binary_logloss: 0.140329
 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328] 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006519 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[26]    training's binary_logloss: 0.123961 valid_1's binary_logloss: 0.127545
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010127 seconds.
You can set `force_col_wise=true` to remove the overhead.
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[32]    training's binary_logloss: 0.117126 valid_1's binary_logloss: 0.13534
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007890 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[41]    training's binary_logloss: 0.111145 valid_1's binary_logloss: 0.140511
 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328] 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010351 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.135256 valid_1's binary_logloss: 0.131344
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007594 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.130846 valid_1's binary_logloss: 0.139185
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006775 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127867 valid_1's binary_logloss: 0.14514
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328] 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006594 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.127239 valid_1's binary_logloss: 0.127384
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006879 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12288  valid_1's binary_logloss: 0.135571
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007380 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120284 valid_1's binary_logloss: 0.140863
 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 60%|██████    | 30/50 [01:18&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328] 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008255 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.119732 valid_1's binary_logloss: 0.127276
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007609 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[70]    training's binary_logloss: 0.116807 valid_1's binary_logloss: 0.135585
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008248 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[80]    training's binary_logloss: 0.112368 valid_1's binary_logloss: 0.14032
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328] 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.120657 valid_1's binary_logloss: 0.126949
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006888 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[42]    training's binary_logloss: 0.118801 valid_1's binary_logloss: 0.135645
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006469 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[51]    training's binary_logloss: 0.113559 valid_1's binary_logloss: 0.140513
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328] 66%|██████▌   | 33/50 [01:22&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012104 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[36]    training's binary_logloss: 0.121629 valid_1's binary_logloss: 0.127166
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008693 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.117903 valid_1's binary_logloss: 0.13587
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011244 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[34]    training's binary_logloss: 0.115553 valid_1's binary_logloss: 0.140845
 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 66%|██████▌   | 33/50 [01:25&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328] 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007149 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.118882 valid_1's binary_logloss: 0.128522
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[11]    training's binary_logloss: 0.12009  valid_1's binary_logloss: 0.136809
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012520 seconds.
You can set `force_col_wise=true` to remove the overhead.
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[14]    training's binary_logloss: 0.114296 valid_1's binary_logloss: 0.141912
 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328] 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009374 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12870
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[67]    training's binary_logloss: 0.117834 valid_1's binary_logloss: 0.127248
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12934
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[55]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.135743
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008382 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12939
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[62]    training's binary_logloss: 0.112207 valid_1's binary_logloss: 0.140686
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328] 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008790 seconds.
You can set `force_col_wise=true` to remove the overhead.
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[72]    training's binary_logloss: 0.116081 valid_1's binary_logloss: 0.12713
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008642 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[49]    training's binary_logloss: 0.117446 valid_1's binary_logloss: 0.135845
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006554 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:
[54]    training's binary_logloss: 0.113495 valid_1's binary_logloss: 0.140635
 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328] 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008911 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[97]    training's binary_logloss: 0.11803  valid_1's binary_logloss: 0.126746
 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008127 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[85]    training's binary_logloss: 0.115626 valid_1's binary_logloss: 0.135332
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013095 seconds.
You can set `force_col_wise=true` to remove the overhead.
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:
[89]    training's binary_logloss: 0.112365 valid_1's binary_logloss: 0.140135
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328] 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007640 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[98]    training's binary_logloss: 0.123399 valid_1's binary_logloss: 0.126912
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007213 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.119208 valid_1's binary_logloss: 0.135189
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011345 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.116397 valid_1's binary_logloss: 0.140343
 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017] 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006904 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.13496  valid_1's binary_logloss: 0.13089
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006719 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.130487 valid_1's binary_logloss: 0.138913
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009924 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.12765  valid_1's binary_logloss: 0.144942
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017] 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008446 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[91]    training's binary_logloss: 0.119162 valid_1's binary_logloss: 0.126781
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010092 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12943
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[76]    training's binary_logloss: 0.117526 valid_1's binary_logloss: 0.135504
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018761 seconds.
You can set `force_col_wise=true` to remove the overhead.
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[84]    training's binary_logloss: 0.113084 valid_1's binary_logloss: 0.140427
 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017] 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007948 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.124667 valid_1's binary_logloss: 0.127059
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006578 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.120428 valid_1's binary_logloss: 0.135621
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006144 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:
[100]   training's binary_logloss: 0.117733 valid_1's binary_logloss: 0.140661
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017] 84%|████████▍ | 42/50 [01:47&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008411 seconds.
You can set `force_col_wise=true` to remove the overhead.
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.120438 valid_1's binary_logloss: 0.127484
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006109 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.118015 valid_1's binary_logloss: 0.13605
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006744 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.115064 valid_1's binary_logloss: 0.14127
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017] 86%|████████▌ | 43/50 [01:49&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008998 seconds.
You can set `force_col_wise=true` to remove the overhead.
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[16]    training's binary_logloss: 0.117532 valid_1's binary_logloss: 0.128445
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[13]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.136088
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007065 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13049
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[18]    training's binary_logloss: 0.10854  valid_1's binary_logloss: 0.14215
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017] 88%|████████▊ | 44/50 [01:51&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009960 seconds.
You can set `force_col_wise=true` to remove the overhead.
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[31]    training's binary_logloss: 0.120736 valid_1's binary_logloss: 0.127726
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007563 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117682 valid_1's binary_logloss: 0.135689
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010431 seconds.
You can set `force_col_wise=true` to remove the overhead.
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[35]    training's binary_logloss: 0.112434 valid_1's binary_logloss: 0.141034
 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017] 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007370 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[25]    training's binary_logloss: 0.114951 valid_1's binary_logloss: 0.127139
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[20]    training's binary_logloss: 0.114337 valid_1's binary_logloss: 0.136683
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007329 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[22]    training's binary_logloss: 0.110257 valid_1's binary_logloss: 0.141881
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017] 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011444 seconds.
You can set `force_col_wise=true` to remove the overhead.
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[45]    training's binary_logloss: 0.120473 valid_1's binary_logloss: 0.127224
 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007092 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[47]    training's binary_logloss: 0.115765 valid_1's binary_logloss: 0.135738
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[43]    training's binary_logloss: 0.114087 valid_1's binary_logloss: 0.140748
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017] 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.117042 valid_1's binary_logloss: 0.127461
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006729 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[58]    training's binary_logloss: 0.110513 valid_1's binary_logloss: 0.135839
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011866 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[61]    training's binary_logloss: 0.107023 valid_1's binary_logloss: 0.140678
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017] 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008940 seconds.
You can set `force_col_wise=true` to remove the overhead.
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12818
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[50]    training's binary_logloss: 0.116856 valid_1's binary_logloss: 0.127122
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009554 seconds.
You can set `force_col_wise=true` to remove the overhead.
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[36]    training's binary_logloss: 0.117447 valid_1's binary_logloss: 0.13599
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006844 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[48]    training's binary_logloss: 0.110674 valid_1's binary_logloss: 0.140834
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017] 98%|█████████▊| 49/50 [02:03&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007160 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[28]    training's binary_logloss: 0.117901 valid_1's binary_logloss: 0.128002
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008421 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[29]    training's binary_logloss: 0.11285  valid_1's binary_logloss: 0.135927
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007163 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds
 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:
[30]    training's binary_logloss: 0.110102 valid_1's binary_logloss: 0.141424
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric
 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.48s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.53s/trial, best loss: -0.8368093643173017]
{'learning_rate': 0.043324531254078945, 'max_depth': 133.0, 'min_child_samples': 85.0, 'num_leaves': 36.0, 'subsample': 0.7305792288105732}</code></pre>
</div>
</div>
</section>
<section id="재학습" class="level3">
<h3 class="anchored" data-anchor-id="재학습">재학습</h3>
<div id="05fb8493" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">lgbm_clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, </span>
<span id="cb18-2">                          num_leaves<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_leaves'</span>]),</span>
<span id="cb18-3">                          max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>]),</span>
<span id="cb18-4">                          min_child_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_samples'</span>]),</span>
<span id="cb18-5">                          subsample<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-6">                          learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(best[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb18-7">                          early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, </span>
<span id="cb18-8">                          eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auc'</span>)</span>
<span id="cb18-9"></span>
<span id="cb18-10">eval_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(X_tr, y_tr), (X_val, y_val)]</span>
<span id="cb18-11">lgbm_clf.fit(X_tr, y_tr, eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eval_set)</span>
<span id="cb18-12"></span>
<span id="cb18-13">lgbm_roc_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb18-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lgbm_roc_score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Info] Number of positive: 1653, number of negative: 40918
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009941 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 12969
[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192
[LightGBM] [Warning] Unknown parameter: eval_metric
[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978
[LightGBM] [Info] Start training from score -3.208978
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[62]    training's binary_logloss: 0.119449 valid_1's binary_logloss: 0.137449
[LightGBM] [Warning] Unknown parameter: eval_metric
0.838</code></pre>
</div>
</div>
</section>
</section>
<section id="제출" class="level2">
<h2 class="anchored" data-anchor-id="제출">제출</h2>
<div id="7af1ef06" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgbm_clf.predict(test_df)</span>
<span id="cb20-2"></span>
<span id="cb20-3">submit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/sample_submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>)</span>
<span id="cb20-4">submit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TARGET'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target</span>
<span id="cb20-5">submit.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'_data/santander/submission.csv'</span>, encoding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latin-1'</span>, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[LightGBM] [Warning] Unknown parameter: eval_metric</code></pre>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> 맨 위로</a> ]]></description>
  <category>머신 러닝</category>
  <guid>https://cryscham123.github.io/posts/01_projects/adp_실기/notes/machine_learning/03.html</guid>
  <pubDate>Sat, 26 Jul 2025 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
