{
  "hash": "a05eac5379deaa8951dd1f4af3a8b08b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"자연어와 단어의 분산 표현\"\ndate: 2025-08-10\ncategories: [\"deep learning\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## 자연어 처리(NLP)\n\n- 우리 말을 컴퓨터에게 이해시키기 위한 기술\n    - 기계 번역, 검색, 등\n\n## 시소러스\n\n- 사람이 직접 레이블링 한 유의어 사전\n- 대표적으로 WordNet이 있다.\n\n### 한계\n\n- 시대 변화에 따라 단어의 의미가 바뀌는 경우가 있다.\n- 사람 쓰는 비용이 크다.\n- 단어의 미묘한 차이를 표현할 수 없다.\n\n## 통계 기반 기법\n\n- 말뭉치: 언어의 실제 사용 예시를 모은 텍스트 데이터\n- 위키백과, 뉴스 기사, 블로그 글 등\n\n::: {#13993a04 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\ndef preprocess(text):\n    text = text.lower()\n    text = text.replace(\".\", \" .\")\n    words = text.split(' ')\n\n    word_to_id = {}\n    id_to_word = {}\n\n    for word in words:\n        if word not in word_to_id:\n            new_id = len(word_to_id)\n            word_to_id[word] = new_id\n            id_to_word[new_id] = word\n    corpus = np.array([word_to_id[w] for w in words])\n    return corpus, word_to_id, id_to_word\n```\n:::\n\n\n::: {#556b4d68 .cell execution_count=2}\n``` {.python .cell-code}\ntext = \"The quick brown fox jumps over the lazy dog.\"\ncorpus, word_to_id, id_to_word = preprocess(text)\n```\n:::\n\n\n### 단어의 분산 표현\n\n- 단어의 의미를 벡터로 표현하는 방법\n- 분포 가설: 단어의 의미는 그 단어의 주변 단어들(맥락)에 의해 결정된다.\n\n::: {#6c9272e7 .cell execution_count=3}\n``` {.python .cell-code}\ndef create_co_matrix(corpus, vocab_size, window_size=1):\n    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n    corpus_size = len(corpus)\n\n    for idx, word_id in enumerate(corpus):\n        left = max(0, idx - window_size)\n        right = min(corpus_size, idx + window_size + 1)\n        for i in range(left, right):\n            if i == idx:\n                continue\n            co_matrix[word_id, corpus[i]] += 1\n    return co_matrix\n```\n:::\n\n\n::: {#bc300a12 .cell execution_count=4}\n``` {.python .cell-code}\ndef cos_similarity(x, y, eps=1e-8):\n    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n    return np.dot(nx, ny)\n```\n:::\n\n\n::: {#c3f5b88e .cell execution_count=5}\n``` {.python .cell-code}\nvocab_size = len(word_to_id)\nC = create_co_matrix(corpus, vocab_size, window_size=1)\n\nC0 = C[word_to_id['the']]\nC1 = C[word_to_id['dog']]\nsimilarity = cos_similarity(C0, C1)\nsimilarity\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.4082482852200891\n```\n:::\n:::\n\n\n### 유사 단어 랭킹\n\n::: {#0d05ecfe .cell execution_count=6}\n``` {.python .cell-code}\ndef most_similar(query, word_to_id, id_to_word, C, top=5):\n    if query not in word_to_id:\n        print(f\"{query}는 사전에 없습니다.\")\n        return None\n\n    query_id = word_to_id[query]\n    query_vec = C[query_id]\n\n    vocab_size = C.shape[0]\n    similarity = np.zeros(vocab_size)\n\n    for i in range(vocab_size):\n        similarity[i] = cos_similarity(query_vec, C[i])\n\n    # 유사도 순으로 정렬\n    count = 0\n    for i in (-1 * similarity).argsort():\n        if id_to_word[i] == query:\n            continue\n        print(f\"{id_to_word[i]}: {similarity[i]:.4f}\")\n        count += 1\n        if count >= top:\n            return\n```\n:::\n\n\n## 통계 기반 기법 개선\n\n### 이전 방법의 한계\n\n- 단어의 의미를 벡터로 표현하는 방법이 단순히 주변 단어의 빈도수에 의존한다.\n    - 점별 상호정보량(PMI)\n\n::: {#3ab18496 .cell execution_count=7}\n``` {.python .cell-code}\ndef ppmi(C, eps=1e-8):\n    M = np.zeros_like(C, dtype=np.float32)\n    N = np.sum(C)\n    S = np.sum(C, axis=0)\n\n    for i in range(C.shape[0]):\n        for j in range(C.shape[1]):\n            pmi = np.log2((C[i, j] * N) / (S[i] * S[j]) + eps)\n            M[i, j] = max(0, pmi)\n    return M\n```\n:::\n\n\n",
    "supporting": [
      "06_files"
    ],
    "filters": [],
    "includes": {}
  }
}