{
  "hash": "17e6b8f75cf5e2977542f0d6e4cfd681",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"학습 관련 기술들\"\ndate: 2025-08-06\ncategories: [\"deep learning\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## 매개변수 갱신\n\n- 확률적 경사하강법은 매개변수를 찾는 과정이 비효율적이다.\n    - 비등방성 함수에서 탐색 경로가 비효율적임\n\n### 모멘텀\n\n- $v = αv - η \\frac{dL}{dW}$\n- $W = W + v$\n\n::: {#6f568cb1 .cell execution_count=1}\n``` {.python .cell-code}\nclass Momentum:\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n        for key in params.keys():\n            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n            params[key] += self.v[key]\n```\n:::\n\n\n### AdaGrad\n\n- 각각의 매개변수에 대해 학습률을 점점 낮추는 방법\n- $h = h + \\frac{dL}{dW} ⊙ \\frac{dL}{dW}$\n- $W = W - η\\frac{1}{\\sqrt{h}}\\frac{dL}{dW}$\n\n::: {#993065f3 .cell execution_count=2}\n``` {.python .cell-code}\nclass AdaGrad:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n        for key, in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n```\n:::\n\n\n- 하지만 시간이 지나면 결국 기울기가 0으로 되버림\n    - 이것을 개선한 기법: RMSProp\n\n### Adam\n\n## 가중치의 초깃값\n\n- 초깃값은 무작위로 설정되어야 한다.\n\n### Xavier 초깃값\n\n- 표준편차가 $\\frac{1}{\\sqrt{n}}$인 초깃값\n- sigmoid, tanh에서 사용됨.\n\n### He 초깃값\n\n- 표준편차가 $\\frac{2}{\\sqrt{n}}$인 초깃값\n- ReLU에 특화됨\n\n## 배치 정규화\n\n- 각 층의 활성화를 적당히 퍼뜨리도록 강제 하는 것\n- 학습속도 개선, 초깃값 의존도 감소, 과대적합 억제의 장점이 있음\n- 활성화 함수 앞이나 뒤에서 standardization scaling을 진행\n- 이후 $y_i = \\gamma \\hat{x}_i + β$의 수식으로, 두 파라미터를 적합한 값으로 학습해 나감\n\n## 과대적합 방지\n\n### 가중치 감소\n\n- 손실함수에 l2($\\frac{1}{2}λ W^2$) l1 norm을 더함\n\n### 드롭아웃\n\n- 신경망 모델이 복잡해지면 가중치 감소만으로 대응하기 어려움\n- 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다.\n- 시험 때 각 뉴련의 출력에 훈련 때 삭제 안 한 비율을 곱한다.\n\n::: {#7f7fc2a5 .cell execution_count=3}\n``` {.python .cell-code}\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n            return x * self.mask\n        return x * (1.0 - self.dropout_ratio)\n\n    def backward(self, dout):\n        return dout * self.mask\n```\n:::\n\n\n",
    "supporting": [
      "04_files"
    ],
    "filters": [],
    "includes": {}
  }
}