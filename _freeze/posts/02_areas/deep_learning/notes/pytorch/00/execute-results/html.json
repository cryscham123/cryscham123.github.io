{
  "hash": "c983a06a2d6a73c7fffd9adc7d403819",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"pytorch 기초\"\ndate: 2025-12-17\ncategories: [\"deep learning\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## 특징\n\n1. 동적 계산 그래프(Dynamic Computation Graph)\n   - 파이토치는 `동적 계산 그래프`를 사용하여 모델을 정의하고 수정할 수 있다. 모델의 구조를 실행 시점에 변경할 수 있다.\n2. GPU 가속 지원\n3. 직관적 인터페이스\n    - `실행모드`를 지원하고, 계산 그래프를 빌드하지 않고 코드를 실행할 수 있다.\n4. 제한된 프로덕션 지원\n    - 주로 연구 목적이다.\n\n## 텐서\n\n::: {#c3652dbf .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\ntensor = torch.rand(1, 2)\n\nprint(tensor)\nprint(tensor.shape) # 크기\nprint(tensor.dtype) # 자료형\nprint(tensor.device) # GPU 가속 여부\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.4927, 0.4537]])\ntorch.Size([1, 2])\ntorch.float32\ncpu\n```\n:::\n:::\n\n\n### 장치 설정\n\n::: {#5aa2f082 .cell execution_count=2}\n``` {.python .cell-code}\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = torch.cuda.FloatTensor([1, 2, 3]) # GPU 가속 지정 방법 1. MAC에서는 지원이 안될 수도 있다.\ntensor = torch.rand((1, 1), device=device) # GPU 가속 지정 방법 2\nprint(device)\nprint(cpu)\nprint(gpu)\nprint(tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncuda\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([[0.8609]], device='cuda:0')\n```\n:::\n:::\n\n\n- cpu 텐서와 gpu 텐서는 상호 간 연산이 불가능하다.\n- numpy 배열은 cpu 텐서와의 연산만 가능\n\n### 장치 변환\n\n::: {#90ecbae4 .cell execution_count=3}\n``` {.python .cell-code}\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = cpu.cuda() # cpu -> gpu\ncpu2 = gpu.cpu() # gpu -> cpu\ngpu2 = cpu.to(\"cuda\") # cpu -> gpu 2 MAC에서도 지원이 되니까 이 방법으로 사용하자\nprint(cpu)\nprint(cpu2)\nprint(gpu)\nprint(gpu2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1., 2., 3.])\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([1., 2., 3.], device='cuda:0')\n```\n:::\n:::\n\n\n::: {#efc129a3 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\nndarray = np.array([1, 2, 3], dtype=np.uint8)\nyo = torch.from_numpy(ndarray)\nprint(yo)\nprint(yo.to(\"cuda\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1, 2, 3], dtype=torch.uint8)\ntensor([1, 2, 3], device='cuda:0', dtype=torch.uint8)\n```\n:::\n:::\n\n\n::: {#472aa121 .cell execution_count=5}\n``` {.python .cell-code}\nndarray = yo.detach().cpu().numpy() # detach: graph에서 분리된 새로운 텐서 반환\nprint(ndarray)\nprint(type(ndarray))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 3]\n<class 'numpy.ndarray'>\n```\n:::\n:::\n\n\n## 단순 선형회귀\n\n::: {#36734f8d .cell execution_count=6}\n``` {.python .cell-code}\nfrom torch import optim\n\nx = torch.FloatTensor([\n    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]\n])\ny = torch.FloatTensor([\n    [0.94], [2.05], [2.87], [4.10], [5.01], [6.15], [6.95], [8.12], [9.05], [10.11],\n    [11.03], [12.20], [12.89], [14.15], [15.02], [16.18], [16.95], [18.22], [19.10], [20.05],\n    [21.01], [22.20], [22.89], [24.10], [25.05], [26.15], [26.95], [28.12], [29.05], [30.10]\n])\n```\n:::\n\n\n::: {#3a04306d .cell execution_count=7}\n``` {.python .cell-code}\nweight = torch.zeros(1, requires_grad=True)\nbias = torch.zeros(1, requires_grad=True)\nlearning_rate = 0.001\n```\n:::\n\n\n::: {#a0e5d150 .cell execution_count=8}\n``` {.python .cell-code}\noptimizer = optim.SGD([weight, bias], lr=learning_rate)\n\nfor epoch in range(1000):\n    hypothesis = x * weight + bias\n    cost = torch.mean((hypothesis - y) ** 2)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost.item():.4f} | Weight: {weight.item():.4f} | Bias: {bias.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 100/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0482\nEpoch 200/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0473\nEpoch 300/1000 | Cost: 0.0089 | Weight: 1.0011 | Bias: 0.0463\nEpoch 400/1000 | Cost: 0.0088 | Weight: 1.0011 | Bias: 0.0455\nEpoch 500/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0446\nEpoch 600/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0438\nEpoch 700/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0430\nEpoch 800/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0423\nEpoch 900/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0416\nEpoch 1000/1000 | Cost: 0.0088 | Weight: 1.0014 | Bias: 0.0409\n```\n:::\n:::\n\n\n::: {#d9a4df01 .cell execution_count=9}\n``` {.python .cell-code}\nfrom torch import nn\n\nmodel = nn.Linear(1, 1, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(1000):\n    hypothesis = model(x)\n    cost = criterion(hypothesis, y)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 100/1000 | Cost: 0.0162 | Model: [Parameter containing:\ntensor([[1.0107]], requires_grad=True), Parameter containing:\ntensor([-0.1499], requires_grad=True)]\nEpoch 200/1000 | Cost: 0.0155 | Model: [Parameter containing:\ntensor([[1.0103]], requires_grad=True), Parameter containing:\ntensor([-0.1416], requires_grad=True)]\nEpoch 300/1000 | Cost: 0.0149 | Model: [Parameter containing:\ntensor([[1.0100]], requires_grad=True), Parameter containing:\ntensor([-0.1338], requires_grad=True)]\nEpoch 400/1000 | Cost: 0.0144 | Model: [Parameter containing:\ntensor([[1.0096]], requires_grad=True), Parameter containing:\ntensor([-0.1264], requires_grad=True)]\nEpoch 500/1000 | Cost: 0.0139 | Model: [Parameter containing:\ntensor([[1.0092]], requires_grad=True), Parameter containing:\ntensor([-0.1192], requires_grad=True)]\nEpoch 600/1000 | Cost: 0.0134 | Model: [Parameter containing:\ntensor([[1.0089]], requires_grad=True), Parameter containing:\ntensor([-0.1125], requires_grad=True)]\nEpoch 700/1000 | Cost: 0.0130 | Model: [Parameter containing:\ntensor([[1.0086]], requires_grad=True), Parameter containing:\ntensor([-0.1060], requires_grad=True)]\nEpoch 800/1000 | Cost: 0.0126 | Model: [Parameter containing:\ntensor([[1.0083]], requires_grad=True), Parameter containing:\ntensor([-0.0998], requires_grad=True)]\nEpoch 900/1000 | Cost: 0.0123 | Model: [Parameter containing:\ntensor([[1.0080]], requires_grad=True), Parameter containing:\ntensor([-0.0939], requires_grad=True)]\nEpoch 1000/1000 | Cost: 0.0119 | Model: [Parameter containing:\ntensor([[1.0077]], requires_grad=True), Parameter containing:\ntensor([-0.0883], requires_grad=True)]\n```\n:::\n:::\n\n\n## 데이터 로드\n\n::: {#fc313e59 .cell execution_count=10}\n``` {.python .cell-code}\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_x = torch.FloatTensor([\n    [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]\n])\ntrain_y = torch.FloatTensor([\n    [0.1, 1.5], [1, 2.8], [1.9, 4.1], [2.8, 5.4], [3.7, 6.7], [4.6, 8]\n])\ntrain_dataset = TensorDataset(train_x, train_y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n```\n:::\n\n\n::: {#d40c5ee2 .cell execution_count=11}\n``` {.python .cell-code}\nmodel = nn.Linear(2, 2, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(20000):\n    cost = 0\n    for batch in train_dataloader:\n        x, y = batch\n        output = model(x)\n        loss = criterion(output, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        cost += loss\n\n    cost = cost / len(train_dataloader)\n\n    if (epoch + 1) % 1000 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1000/1000 | Cost: 0.0147 | Model: [Parameter containing:\ntensor([[0.6787, 0.1290],\n        [0.9696, 0.3267]], requires_grad=True), Parameter containing:\ntensor([-0.5401, -0.1114], requires_grad=True)]\nEpoch 2000/1000 | Cost: 0.0038 | Model: [Parameter containing:\ntensor([[0.7724, 0.0804],\n        [0.9733, 0.3248]], requires_grad=True), Parameter containing:\ntensor([-0.6824, -0.1170], requires_grad=True)]\nEpoch 3000/1000 | Cost: 0.0010 | Model: [Parameter containing:\ntensor([[0.8202, 0.0558],\n        [0.9752, 0.3239]], requires_grad=True), Parameter containing:\ntensor([-0.7548, -0.1199], requires_grad=True)]\nEpoch 4000/1000 | Cost: 0.0003 | Model: [Parameter containing:\ntensor([[0.8446, 0.0432],\n        [0.9762, 0.3234]], requires_grad=True), Parameter containing:\ntensor([-0.7918, -0.1213], requires_grad=True)]\nEpoch 5000/1000 | Cost: 0.0001 | Model: [Parameter containing:\ntensor([[0.8570, 0.0368],\n        [0.9766, 0.3231]], requires_grad=True), Parameter containing:\ntensor([-0.8106, -0.1221], requires_grad=True)]\nEpoch 6000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8633, 0.0335],\n        [0.9769, 0.3230]], requires_grad=True), Parameter containing:\ntensor([-0.8202, -0.1225], requires_grad=True)]\nEpoch 7000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8665, 0.0319],\n        [0.9770, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8251, -0.1226], requires_grad=True)]\nEpoch 8000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8682, 0.0310],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8275, -0.1227], requires_grad=True)]\nEpoch 9000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8690, 0.0306],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8288, -0.1228], requires_grad=True)]\nEpoch 10000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8694, 0.0304],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8295, -0.1228], requires_grad=True)]\nEpoch 11000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8696, 0.0302],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8298, -0.1228], requires_grad=True)]\nEpoch 12000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8698, 0.0302],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8300, -0.1228], requires_grad=True)]\nEpoch 13000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8698, 0.0302],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8300, -0.1228], requires_grad=True)]\nEpoch 14000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8698, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 15000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8698, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 16000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8699, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 17000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8699, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 18000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8699, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 19000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8699, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\nEpoch 20000/1000 | Cost: 0.0000 | Model: [Parameter containing:\ntensor([[0.8699, 0.0301],\n        [0.9771, 0.3229]], requires_grad=True), Parameter containing:\ntensor([-0.8301, -0.1229], requires_grad=True)]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "00_files"
    ],
    "filters": [],
    "includes": {}
  }
}