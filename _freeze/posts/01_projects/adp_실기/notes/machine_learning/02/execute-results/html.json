{
  "hash": "e253abbb941481257e304cac526c8b61",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"분류 - 앙상블\"\ndate: 2025-07-27\ncategories: [\"머신 러닝\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## voting\n\n- 서로 다른 알고리즘이 결합. 분류에서는 voting[^1]으로 결정\n\n### Example\n\n::: {#676b41f6 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#73591d13 .cell execution_count=2}\n``` {.python .cell-code}\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n0.9210526315789473\n```\n:::\n:::\n\n\n::: {#9d408271 .cell execution_count=3}\n``` {.python .cell-code}\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogisticRegression 정확도: 0.9298\nKNeighborsClassifier 정확도: 0.9211\n```\n:::\n:::\n\n\n- 반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님\n\n## bagging\n\n- 같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting[^1]으로 결정\n\n### RandomForest\n\n::: {#86ae3c9a .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n```\n:::\n\n\n::: {#879b9f9b .cell execution_count=5}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n0.9178825924669155\n```\n:::\n:::\n\n\n[^1]: hard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.\n\n## boosting\n\n### GBM\n\n::: {#7d4d016f .cell execution_count=6}\n``` {.python .cell-code}\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n```\n:::\n\n\n0.939, 701.6343066692352초\n\n- 아주 오래 걸림.\n\n### XGBoost\n\n- 결손값을 자체 처리할 수 있다.\n- 조기 종료 기능이 있다.\n- 자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\n\n- python xgboost\n\n::: {#a5e0c29a .cell execution_count=7}\n``` {.python .cell-code}\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n```\n:::\n\n\n::: {#b61f8977 .cell execution_count=8}\n``` {.python .cell-code}\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n```\n:::\n\n\n::: {#fe3df619 .cell execution_count=9}\n``` {.python .cell-code}\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n```\n:::\n\n\n::: {#08be87e7 .cell execution_count=10}\n``` {.python .cell-code}\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\ttrain-logloss:0.61277\teval-logloss:0.58601\n[1]\ttrain-logloss:0.57664\teval-logloss:0.55582\n[2]\ttrain-logloss:0.54304\teval-logloss:0.52806\n[3]\ttrain-logloss:0.51255\teval-logloss:0.50298\n[4]\ttrain-logloss:0.48471\teval-logloss:0.47989\n[5]\ttrain-logloss:0.45884\teval-logloss:0.45674\n[6]\ttrain-logloss:0.43517\teval-logloss:0.43736\n[7]\ttrain-logloss:0.41363\teval-logloss:0.42013\n[8]\ttrain-logloss:0.39341\teval-logloss:0.40354\n[9]\ttrain-logloss:0.37494\teval-logloss:0.38841\n[10]\ttrain-logloss:0.35744\teval-logloss:0.37480\n[11]\ttrain-logloss:0.34054\teval-logloss:0.35955\n[12]\ttrain-logloss:0.32442\teval-logloss:0.34527\n[13]\ttrain-logloss:0.30963\teval-logloss:0.32976\n[14]\ttrain-logloss:0.29638\teval-logloss:0.32018\n[15]\ttrain-logloss:0.28306\teval-logloss:0.30855\n[16]\ttrain-logloss:0.27060\teval-logloss:0.29773\n[17]\ttrain-logloss:0.25894\teval-logloss:0.28763\n[18]\ttrain-logloss:0.24842\teval-logloss:0.27760\n[19]\ttrain-logloss:0.23861\teval-logloss:0.27116\n[20]\ttrain-logloss:0.22890\teval-logloss:0.26281\n[21]\ttrain-logloss:0.21995\teval-logloss:0.25627\n[22]\ttrain-logloss:0.21183\teval-logloss:0.25092\n[23]\ttrain-logloss:0.20363\teval-logloss:0.24393\n[24]\ttrain-logloss:0.19606\teval-logloss:0.23852\n[25]\ttrain-logloss:0.18881\teval-logloss:0.23108\n[26]\ttrain-logloss:0.18165\teval-logloss:0.22460\n[27]\ttrain-logloss:0.17502\teval-logloss:0.21895\n[28]\ttrain-logloss:0.16889\teval-logloss:0.21248\n[29]\ttrain-logloss:0.16323\teval-logloss:0.20904\n[30]\ttrain-logloss:0.15760\teval-logloss:0.20456\n[31]\ttrain-logloss:0.15228\teval-logloss:0.20042\n[32]\ttrain-logloss:0.14728\teval-logloss:0.19496\n[33]\ttrain-logloss:0.14254\teval-logloss:0.19157\n[34]\ttrain-logloss:0.13775\teval-logloss:0.18653\n[35]\ttrain-logloss:0.13340\teval-logloss:0.18317\n[36]\ttrain-logloss:0.12934\teval-logloss:0.17926\n[37]\ttrain-logloss:0.12562\teval-logloss:0.17500\n[38]\ttrain-logloss:0.12162\teval-logloss:0.17143\n[39]\ttrain-logloss:0.11812\teval-logloss:0.16801\n[40]\ttrain-logloss:0.11493\teval-logloss:0.16522\n[41]\ttrain-logloss:0.11171\teval-logloss:0.16259\n[42]\ttrain-logloss:0.10874\teval-logloss:0.16035\n[43]\ttrain-logloss:0.10593\teval-logloss:0.15740\n[44]\ttrain-logloss:0.10316\teval-logloss:0.15462\n[45]\ttrain-logloss:0.10017\teval-logloss:0.15140\n[46]\ttrain-logloss:0.09748\teval-logloss:0.14985\n[47]\ttrain-logloss:0.09515\teval-logloss:0.14732\n[48]\ttrain-logloss:0.09280\teval-logloss:0.14613\n[49]\ttrain-logloss:0.09039\teval-logloss:0.14424\n[50]\ttrain-logloss:0.08814\teval-logloss:0.14301\n[51]\ttrain-logloss:0.08583\teval-logloss:0.14055\n[52]\ttrain-logloss:0.08369\teval-logloss:0.13767\n[53]\ttrain-logloss:0.08167\teval-logloss:0.13494\n[54]\ttrain-logloss:0.07971\teval-logloss:0.13295\n[55]\ttrain-logloss:0.07782\teval-logloss:0.13025\n[56]\ttrain-logloss:0.07603\teval-logloss:0.12777\n[57]\ttrain-logloss:0.07431\teval-logloss:0.12528\n[58]\ttrain-logloss:0.07265\teval-logloss:0.12285\n[59]\ttrain-logloss:0.07107\teval-logloss:0.12062\n[60]\ttrain-logloss:0.06952\teval-logloss:0.11986\n[61]\ttrain-logloss:0.06804\teval-logloss:0.11877\n[62]\ttrain-logloss:0.06626\teval-logloss:0.11728\n[63]\ttrain-logloss:0.06490\teval-logloss:0.11527\n[64]\ttrain-logloss:0.06361\teval-logloss:0.11325\n[65]\ttrain-logloss:0.06205\teval-logloss:0.11093\n[66]\ttrain-logloss:0.06085\teval-logloss:0.10911\n[67]\ttrain-logloss:0.05957\teval-logloss:0.10839\n[68]\ttrain-logloss:0.05846\teval-logloss:0.10659\n[69]\ttrain-logloss:0.05701\teval-logloss:0.10541\n[70]\ttrain-logloss:0.05598\teval-logloss:0.10382\n[71]\ttrain-logloss:0.05487\teval-logloss:0.10325\n[72]\ttrain-logloss:0.05390\teval-logloss:0.10238\n[73]\ttrain-logloss:0.05262\teval-logloss:0.10137\n[74]\ttrain-logloss:0.05173\teval-logloss:0.09989\n[75]\ttrain-logloss:0.05078\teval-logloss:0.09905\n[76]\ttrain-logloss:0.04998\teval-logloss:0.09774\n[77]\ttrain-logloss:0.04902\teval-logloss:0.09725\n[78]\ttrain-logloss:0.04813\teval-logloss:0.09723\n[79]\ttrain-logloss:0.04728\teval-logloss:0.09558\n[80]\ttrain-logloss:0.04655\teval-logloss:0.09499\n[81]\ttrain-logloss:0.04558\teval-logloss:0.09360\n[82]\ttrain-logloss:0.04481\teval-logloss:0.09289\n[83]\ttrain-logloss:0.04411\teval-logloss:0.09233\n[84]\ttrain-logloss:0.04323\teval-logloss:0.09104\n[85]\ttrain-logloss:0.04244\teval-logloss:0.09051\n[86]\ttrain-logloss:0.04163\teval-logloss:0.08929\n[87]\ttrain-logloss:0.04105\teval-logloss:0.08824\n[88]\ttrain-logloss:0.04029\teval-logloss:0.08709\n[89]\ttrain-logloss:0.03970\teval-logloss:0.08667\n[90]\ttrain-logloss:0.03908\teval-logloss:0.08651\n[91]\ttrain-logloss:0.03840\teval-logloss:0.08554\n[92]\ttrain-logloss:0.03790\teval-logloss:0.08459\n[93]\ttrain-logloss:0.03717\teval-logloss:0.08382\n[94]\ttrain-logloss:0.03655\teval-logloss:0.08279\n[95]\ttrain-logloss:0.03609\teval-logloss:0.08246\n[96]\ttrain-logloss:0.03551\teval-logloss:0.08162\n[97]\ttrain-logloss:0.03503\teval-logloss:0.08062\n[98]\ttrain-logloss:0.03438\teval-logloss:0.07993\n[99]\ttrain-logloss:0.03390\teval-logloss:0.07963\n[100]\ttrain-logloss:0.03329\teval-logloss:0.07899\n[101]\ttrain-logloss:0.03284\teval-logloss:0.07873\n[102]\ttrain-logloss:0.03245\teval-logloss:0.07871\n[103]\ttrain-logloss:0.03202\teval-logloss:0.07846\n[104]\ttrain-logloss:0.03158\teval-logloss:0.07822\n[105]\ttrain-logloss:0.03122\teval-logloss:0.07799\n[106]\ttrain-logloss:0.03076\teval-logloss:0.07690\n[107]\ttrain-logloss:0.03032\teval-logloss:0.07710\n[108]\ttrain-logloss:0.02993\teval-logloss:0.07759\n[109]\ttrain-logloss:0.02950\teval-logloss:0.07750\n[110]\ttrain-logloss:0.02908\teval-logloss:0.07647\n[111]\ttrain-logloss:0.02867\teval-logloss:0.07550\n[112]\ttrain-logloss:0.02831\teval-logloss:0.07529\n[113]\ttrain-logloss:0.02787\teval-logloss:0.07401\n[114]\ttrain-logloss:0.02750\teval-logloss:0.07395\n[115]\ttrain-logloss:0.02712\teval-logloss:0.07300\n[116]\ttrain-logloss:0.02674\teval-logloss:0.07235\n[117]\ttrain-logloss:0.02635\teval-logloss:0.07196\n[118]\ttrain-logloss:0.02599\teval-logloss:0.07107\n[119]\ttrain-logloss:0.02565\teval-logloss:0.07043\n[120]\ttrain-logloss:0.02536\teval-logloss:0.07095\n[121]\ttrain-logloss:0.02505\teval-logloss:0.07092\n[122]\ttrain-logloss:0.02473\teval-logloss:0.07007\n[123]\ttrain-logloss:0.02444\teval-logloss:0.07007\n[124]\ttrain-logloss:0.02418\teval-logloss:0.07058\n[125]\ttrain-logloss:0.02393\teval-logloss:0.07069\n[126]\ttrain-logloss:0.02363\teval-logloss:0.07066\n[127]\ttrain-logloss:0.02333\teval-logloss:0.06986\n[128]\ttrain-logloss:0.02305\teval-logloss:0.06984\n[129]\ttrain-logloss:0.02277\teval-logloss:0.06906\n[130]\ttrain-logloss:0.02252\teval-logloss:0.06911\n[131]\ttrain-logloss:0.02224\teval-logloss:0.06825\n[132]\ttrain-logloss:0.02198\teval-logloss:0.06751\n[133]\ttrain-logloss:0.02175\teval-logloss:0.06699\n[134]\ttrain-logloss:0.02155\teval-logloss:0.06748\n[135]\ttrain-logloss:0.02138\teval-logloss:0.06752\n[136]\ttrain-logloss:0.02114\teval-logloss:0.06747\n[137]\ttrain-logloss:0.02096\teval-logloss:0.06682\n[138]\ttrain-logloss:0.02075\teval-logloss:0.06686\n[139]\ttrain-logloss:0.02057\teval-logloss:0.06663\n[140]\ttrain-logloss:0.02032\teval-logloss:0.06654\n[141]\ttrain-logloss:0.02013\teval-logloss:0.06599\n[142]\ttrain-logloss:0.01995\teval-logloss:0.06647\n[143]\ttrain-logloss:0.01972\teval-logloss:0.06640\n[144]\ttrain-logloss:0.01950\teval-logloss:0.06636\n[145]\ttrain-logloss:0.01925\teval-logloss:0.06568\n[146]\ttrain-logloss:0.01910\teval-logloss:0.06597\n[147]\ttrain-logloss:0.01891\teval-logloss:0.06518\n[148]\ttrain-logloss:0.01876\teval-logloss:0.06547\n[149]\ttrain-logloss:0.01854\teval-logloss:0.06481\n[150]\ttrain-logloss:0.01838\teval-logloss:0.06530\n[151]\ttrain-logloss:0.01824\teval-logloss:0.06490\n[152]\ttrain-logloss:0.01806\teval-logloss:0.06506\n[153]\ttrain-logloss:0.01789\teval-logloss:0.06519\n[154]\ttrain-logloss:0.01771\teval-logloss:0.06496\n[155]\ttrain-logloss:0.01762\teval-logloss:0.06516\n[156]\ttrain-logloss:0.01742\teval-logloss:0.06457\n[157]\ttrain-logloss:0.01729\teval-logloss:0.06484\n[158]\ttrain-logloss:0.01716\teval-logloss:0.06408\n[159]\ttrain-logloss:0.01698\teval-logloss:0.06389\n[160]\ttrain-logloss:0.01679\teval-logloss:0.06333\n[161]\ttrain-logloss:0.01671\teval-logloss:0.06355\n[162]\ttrain-logloss:0.01657\teval-logloss:0.06357\n[163]\ttrain-logloss:0.01645\teval-logloss:0.06321\n[164]\ttrain-logloss:0.01631\teval-logloss:0.06317\n[165]\ttrain-logloss:0.01621\teval-logloss:0.06322\n[166]\ttrain-logloss:0.01604\teval-logloss:0.06270\n[167]\ttrain-logloss:0.01594\teval-logloss:0.06232\n[168]\ttrain-logloss:0.01587\teval-logloss:0.06253\n[169]\ttrain-logloss:0.01572\teval-logloss:0.06206\n[170]\ttrain-logloss:0.01564\teval-logloss:0.06167\n[171]\ttrain-logloss:0.01554\teval-logloss:0.06097\n[172]\ttrain-logloss:0.01547\teval-logloss:0.06117\n[173]\ttrain-logloss:0.01534\teval-logloss:0.06110\n[174]\ttrain-logloss:0.01526\teval-logloss:0.06115\n[175]\ttrain-logloss:0.01516\teval-logloss:0.06047\n[176]\ttrain-logloss:0.01502\teval-logloss:0.06018\n[177]\ttrain-logloss:0.01493\teval-logloss:0.06022\n[178]\ttrain-logloss:0.01482\teval-logloss:0.06012\n[179]\ttrain-logloss:0.01475\teval-logloss:0.05975\n[180]\ttrain-logloss:0.01468\teval-logloss:0.05968\n[181]\ttrain-logloss:0.01461\teval-logloss:0.05988\n[182]\ttrain-logloss:0.01454\teval-logloss:0.05952\n[183]\ttrain-logloss:0.01447\teval-logloss:0.05945\n[184]\ttrain-logloss:0.01437\teval-logloss:0.05952\n[185]\ttrain-logloss:0.01428\teval-logloss:0.05933\n[186]\ttrain-logloss:0.01420\teval-logloss:0.05926\n[187]\ttrain-logloss:0.01410\teval-logloss:0.05917\n[188]\ttrain-logloss:0.01404\teval-logloss:0.05883\n[189]\ttrain-logloss:0.01397\teval-logloss:0.05843\n[190]\ttrain-logloss:0.01389\teval-logloss:0.05825\n[191]\ttrain-logloss:0.01382\teval-logloss:0.05821\n[192]\ttrain-logloss:0.01372\teval-logloss:0.05829\n[193]\ttrain-logloss:0.01364\teval-logloss:0.05811\n[194]\ttrain-logloss:0.01358\teval-logloss:0.05808\n[195]\ttrain-logloss:0.01352\teval-logloss:0.05823\n[196]\ttrain-logloss:0.01346\teval-logloss:0.05829\n[197]\ttrain-logloss:0.01340\teval-logloss:0.05823\n[198]\ttrain-logloss:0.01331\teval-logloss:0.05832\n[199]\ttrain-logloss:0.01324\teval-logloss:0.05813\n[200]\ttrain-logloss:0.01317\teval-logloss:0.05811\n[201]\ttrain-logloss:0.01312\teval-logloss:0.05769\n[202]\ttrain-logloss:0.01305\teval-logloss:0.05754\n[203]\ttrain-logloss:0.01296\teval-logloss:0.05764\n[204]\ttrain-logloss:0.01289\teval-logloss:0.05749\n[205]\ttrain-logloss:0.01284\teval-logloss:0.05756\n[206]\ttrain-logloss:0.01279\teval-logloss:0.05772\n[207]\ttrain-logloss:0.01273\teval-logloss:0.05768\n[208]\ttrain-logloss:0.01268\teval-logloss:0.05783\n[209]\ttrain-logloss:0.01263\teval-logloss:0.05752\n[210]\ttrain-logloss:0.01258\teval-logloss:0.05711\n[211]\ttrain-logloss:0.01251\teval-logloss:0.05697\n[212]\ttrain-logloss:0.01243\teval-logloss:0.05662\n[213]\ttrain-logloss:0.01237\teval-logloss:0.05671\n[214]\ttrain-logloss:0.01231\teval-logloss:0.05659\n[215]\ttrain-logloss:0.01226\teval-logloss:0.05629\n[216]\ttrain-logloss:0.01219\teval-logloss:0.05595\n[217]\ttrain-logloss:0.01214\teval-logloss:0.05591\n[218]\ttrain-logloss:0.01208\teval-logloss:0.05580\n[219]\ttrain-logloss:0.01201\teval-logloss:0.05606\n[220]\ttrain-logloss:0.01196\teval-logloss:0.05592\n[221]\ttrain-logloss:0.01190\teval-logloss:0.05601\n[222]\ttrain-logloss:0.01185\teval-logloss:0.05608\n[223]\ttrain-logloss:0.01181\teval-logloss:0.05569\n[224]\ttrain-logloss:0.01176\teval-logloss:0.05559\n[225]\ttrain-logloss:0.01169\teval-logloss:0.05585\n[226]\ttrain-logloss:0.01164\teval-logloss:0.05576\n[227]\ttrain-logloss:0.01158\teval-logloss:0.05549\n[228]\ttrain-logloss:0.01153\teval-logloss:0.05521\n[229]\ttrain-logloss:0.01149\teval-logloss:0.05509\n[230]\ttrain-logloss:0.01145\teval-logloss:0.05493\n[231]\ttrain-logloss:0.01140\teval-logloss:0.05507\n[232]\ttrain-logloss:0.01136\teval-logloss:0.05469\n[233]\ttrain-logloss:0.01132\teval-logloss:0.05500\n[234]\ttrain-logloss:0.01128\teval-logloss:0.05474\n[235]\ttrain-logloss:0.01124\teval-logloss:0.05472\n[236]\ttrain-logloss:0.01120\teval-logloss:0.05490\n[237]\ttrain-logloss:0.01115\teval-logloss:0.05503\n[238]\ttrain-logloss:0.01111\teval-logloss:0.05516\n[239]\ttrain-logloss:0.01107\teval-logloss:0.05524\n[240]\ttrain-logloss:0.01103\teval-logloss:0.05537\n[241]\ttrain-logloss:0.01099\teval-logloss:0.05536\n[242]\ttrain-logloss:0.01096\teval-logloss:0.05568\n[243]\ttrain-logloss:0.01090\teval-logloss:0.05543\n[244]\ttrain-logloss:0.01087\teval-logloss:0.05556\n[245]\ttrain-logloss:0.01083\teval-logloss:0.05519\n[246]\ttrain-logloss:0.01081\teval-logloss:0.05537\n[247]\ttrain-logloss:0.01077\teval-logloss:0.05536\n[248]\ttrain-logloss:0.01072\teval-logloss:0.05549\n[249]\ttrain-logloss:0.01068\teval-logloss:0.05562\n[250]\ttrain-logloss:0.01064\teval-logloss:0.05537\n[251]\ttrain-logloss:0.01061\teval-logloss:0.05555\n[252]\ttrain-logloss:0.01058\teval-logloss:0.05568\n[253]\ttrain-logloss:0.01054\teval-logloss:0.05532\n[254]\ttrain-logloss:0.01051\teval-logloss:0.05508\n[255]\ttrain-logloss:0.01049\teval-logloss:0.05525\n[256]\ttrain-logloss:0.01045\teval-logloss:0.05524\n[257]\ttrain-logloss:0.01042\teval-logloss:0.05537\n[258]\ttrain-logloss:0.01037\teval-logloss:0.05554\n[259]\ttrain-logloss:0.01033\teval-logloss:0.05563\n[260]\ttrain-logloss:0.01030\teval-logloss:0.05528\n[261]\ttrain-logloss:0.01028\teval-logloss:0.05546\n[262]\ttrain-logloss:0.01025\teval-logloss:0.05559\n[263]\ttrain-logloss:0.01022\teval-logloss:0.05558\n[264]\ttrain-logloss:0.01017\teval-logloss:0.05576\n[265]\ttrain-logloss:0.01014\teval-logloss:0.05593\n[266]\ttrain-logloss:0.01011\teval-logloss:0.05558\n[267]\ttrain-logloss:0.01008\teval-logloss:0.05571\n[268]\ttrain-logloss:0.01005\teval-logloss:0.05584\n[269]\ttrain-logloss:0.01002\teval-logloss:0.05561\n[270]\ttrain-logloss:0.00998\teval-logloss:0.05560\n[271]\ttrain-logloss:0.00997\teval-logloss:0.05577\n[272]\ttrain-logloss:0.00992\teval-logloss:0.05574\n[273]\ttrain-logloss:0.00989\teval-logloss:0.05540\n[274]\ttrain-logloss:0.00987\teval-logloss:0.05557\n[275]\ttrain-logloss:0.00984\teval-logloss:0.05570\n[276]\ttrain-logloss:0.00981\teval-logloss:0.05547\n[277]\ttrain-logloss:0.00978\teval-logloss:0.05546\n[278]\ttrain-logloss:0.00973\teval-logloss:0.05543\n[279]\ttrain-logloss:0.00970\teval-logloss:0.05556\n[280]\ttrain-logloss:0.00969\teval-logloss:0.05562\n[281]\ttrain-logloss:0.00966\teval-logloss:0.05528\n```\n:::\n:::\n\n\n::: {#8b810501 .cell execution_count=11}\n``` {.python .cell-code}\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x > 0.5 else 0 for x in pred_probs]\n```\n:::\n\n\n- sklearn xgboost\n\n::: {#82293978 .cell execution_count=12}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-logloss:0.61277\tvalidation_1-logloss:0.58601\n[1]\tvalidation_0-logloss:0.57664\tvalidation_1-logloss:0.55582\n[2]\tvalidation_0-logloss:0.54304\tvalidation_1-logloss:0.52806\n[3]\tvalidation_0-logloss:0.51255\tvalidation_1-logloss:0.50298\n[4]\tvalidation_0-logloss:0.48471\tvalidation_1-logloss:0.47989\n[5]\tvalidation_0-logloss:0.45884\tvalidation_1-logloss:0.45674\n[6]\tvalidation_0-logloss:0.43517\tvalidation_1-logloss:0.43736\n[7]\tvalidation_0-logloss:0.41363\tvalidation_1-logloss:0.42013\n[8]\tvalidation_0-logloss:0.39341\tvalidation_1-logloss:0.40354\n[9]\tvalidation_0-logloss:0.37494\tvalidation_1-logloss:0.38841\n[10]\tvalidation_0-logloss:0.35744\tvalidation_1-logloss:0.37480\n[11]\tvalidation_0-logloss:0.34054\tvalidation_1-logloss:0.35955\n[12]\tvalidation_0-logloss:0.32442\tvalidation_1-logloss:0.34527\n[13]\tvalidation_0-logloss:0.30963\tvalidation_1-logloss:0.32976\n[14]\tvalidation_0-logloss:0.29638\tvalidation_1-logloss:0.32018\n[15]\tvalidation_0-logloss:0.28306\tvalidation_1-logloss:0.30855\n[16]\tvalidation_0-logloss:0.27060\tvalidation_1-logloss:0.29773\n[17]\tvalidation_0-logloss:0.25894\tvalidation_1-logloss:0.28763\n[18]\tvalidation_0-logloss:0.24842\tvalidation_1-logloss:0.27760\n[19]\tvalidation_0-logloss:0.23861\tvalidation_1-logloss:0.27116\n[20]\tvalidation_0-logloss:0.22890\tvalidation_1-logloss:0.26281\n[21]\tvalidation_0-logloss:0.21995\tvalidation_1-logloss:0.25627\n[22]\tvalidation_0-logloss:0.21183\tvalidation_1-logloss:0.25092\n[23]\tvalidation_0-logloss:0.20363\tvalidation_1-logloss:0.24393\n[24]\tvalidation_0-logloss:0.19606\tvalidation_1-logloss:0.23852\n[25]\tvalidation_0-logloss:0.18881\tvalidation_1-logloss:0.23108\n[26]\tvalidation_0-logloss:0.18165\tvalidation_1-logloss:0.22460\n[27]\tvalidation_0-logloss:0.17502\tvalidation_1-logloss:0.21895\n[28]\tvalidation_0-logloss:0.16889\tvalidation_1-logloss:0.21248\n[29]\tvalidation_0-logloss:0.16323\tvalidation_1-logloss:0.20904\n[30]\tvalidation_0-logloss:0.15760\tvalidation_1-logloss:0.20456\n[31]\tvalidation_0-logloss:0.15228\tvalidation_1-logloss:0.20042\n[32]\tvalidation_0-logloss:0.14728\tvalidation_1-logloss:0.19496\n[33]\tvalidation_0-logloss:0.14254\tvalidation_1-logloss:0.19157\n[34]\tvalidation_0-logloss:0.13775\tvalidation_1-logloss:0.18653\n[35]\tvalidation_0-logloss:0.13340\tvalidation_1-logloss:0.18317\n[36]\tvalidation_0-logloss:0.12934\tvalidation_1-logloss:0.17926\n[37]\tvalidation_0-logloss:0.12562\tvalidation_1-logloss:0.17500\n[38]\tvalidation_0-logloss:0.12162\tvalidation_1-logloss:0.17143\n[39]\tvalidation_0-logloss:0.11812\tvalidation_1-logloss:0.16801\n[40]\tvalidation_0-logloss:0.11493\tvalidation_1-logloss:0.16522\n[41]\tvalidation_0-logloss:0.11171\tvalidation_1-logloss:0.16259\n[42]\tvalidation_0-logloss:0.10874\tvalidation_1-logloss:0.16035\n[43]\tvalidation_0-logloss:0.10593\tvalidation_1-logloss:0.15740\n[44]\tvalidation_0-logloss:0.10316\tvalidation_1-logloss:0.15462\n[45]\tvalidation_0-logloss:0.10017\tvalidation_1-logloss:0.15140\n[46]\tvalidation_0-logloss:0.09748\tvalidation_1-logloss:0.14985\n[47]\tvalidation_0-logloss:0.09515\tvalidation_1-logloss:0.14732\n[48]\tvalidation_0-logloss:0.09280\tvalidation_1-logloss:0.14613\n[49]\tvalidation_0-logloss:0.09039\tvalidation_1-logloss:0.14424\n[50]\tvalidation_0-logloss:0.08814\tvalidation_1-logloss:0.14301\n[51]\tvalidation_0-logloss:0.08583\tvalidation_1-logloss:0.14055\n[52]\tvalidation_0-logloss:0.08369\tvalidation_1-logloss:0.13767\n[53]\tvalidation_0-logloss:0.08167\tvalidation_1-logloss:0.13494\n[54]\tvalidation_0-logloss:0.07971\tvalidation_1-logloss:0.13295\n[55]\tvalidation_0-logloss:0.07782\tvalidation_1-logloss:0.13025\n[56]\tvalidation_0-logloss:0.07603\tvalidation_1-logloss:0.12777\n[57]\tvalidation_0-logloss:0.07431\tvalidation_1-logloss:0.12528\n[58]\tvalidation_0-logloss:0.07265\tvalidation_1-logloss:0.12285\n[59]\tvalidation_0-logloss:0.07107\tvalidation_1-logloss:0.12062\n[60]\tvalidation_0-logloss:0.06952\tvalidation_1-logloss:0.11986\n[61]\tvalidation_0-logloss:0.06804\tvalidation_1-logloss:0.11877\n[62]\tvalidation_0-logloss:0.06626\tvalidation_1-logloss:0.11728\n[63]\tvalidation_0-logloss:0.06490\tvalidation_1-logloss:0.11527\n[64]\tvalidation_0-logloss:0.06361\tvalidation_1-logloss:0.11325\n[65]\tvalidation_0-logloss:0.06205\tvalidation_1-logloss:0.11093\n[66]\tvalidation_0-logloss:0.06085\tvalidation_1-logloss:0.10911\n[67]\tvalidation_0-logloss:0.05957\tvalidation_1-logloss:0.10839\n[68]\tvalidation_0-logloss:0.05846\tvalidation_1-logloss:0.10659\n[69]\tvalidation_0-logloss:0.05701\tvalidation_1-logloss:0.10541\n[70]\tvalidation_0-logloss:0.05598\tvalidation_1-logloss:0.10382\n[71]\tvalidation_0-logloss:0.05487\tvalidation_1-logloss:0.10325\n[72]\tvalidation_0-logloss:0.05390\tvalidation_1-logloss:0.10238\n[73]\tvalidation_0-logloss:0.05262\tvalidation_1-logloss:0.10137\n[74]\tvalidation_0-logloss:0.05173\tvalidation_1-logloss:0.09989\n[75]\tvalidation_0-logloss:0.05078\tvalidation_1-logloss:0.09905\n[76]\tvalidation_0-logloss:0.04998\tvalidation_1-logloss:0.09774\n[77]\tvalidation_0-logloss:0.04902\tvalidation_1-logloss:0.09725\n[78]\tvalidation_0-logloss:0.04813\tvalidation_1-logloss:0.09723\n[79]\tvalidation_0-logloss:0.04728\tvalidation_1-logloss:0.09558\n[80]\tvalidation_0-logloss:0.04655\tvalidation_1-logloss:0.09499\n[81]\tvalidation_0-logloss:0.04558\tvalidation_1-logloss:0.09360\n[82]\tvalidation_0-logloss:0.04481\tvalidation_1-logloss:0.09289\n[83]\tvalidation_0-logloss:0.04411\tvalidation_1-logloss:0.09233\n[84]\tvalidation_0-logloss:0.04323\tvalidation_1-logloss:0.09104\n[85]\tvalidation_0-logloss:0.04244\tvalidation_1-logloss:0.09051\n[86]\tvalidation_0-logloss:0.04163\tvalidation_1-logloss:0.08929\n[87]\tvalidation_0-logloss:0.04105\tvalidation_1-logloss:0.08824\n[88]\tvalidation_0-logloss:0.04029\tvalidation_1-logloss:0.08709\n[89]\tvalidation_0-logloss:0.03970\tvalidation_1-logloss:0.08667\n[90]\tvalidation_0-logloss:0.03908\tvalidation_1-logloss:0.08651\n[91]\tvalidation_0-logloss:0.03840\tvalidation_1-logloss:0.08554\n[92]\tvalidation_0-logloss:0.03790\tvalidation_1-logloss:0.08459\n[93]\tvalidation_0-logloss:0.03717\tvalidation_1-logloss:0.08382\n[94]\tvalidation_0-logloss:0.03655\tvalidation_1-logloss:0.08279\n[95]\tvalidation_0-logloss:0.03609\tvalidation_1-logloss:0.08246\n[96]\tvalidation_0-logloss:0.03551\tvalidation_1-logloss:0.08162\n[97]\tvalidation_0-logloss:0.03503\tvalidation_1-logloss:0.08062\n[98]\tvalidation_0-logloss:0.03438\tvalidation_1-logloss:0.07993\n[99]\tvalidation_0-logloss:0.03390\tvalidation_1-logloss:0.07963\n[100]\tvalidation_0-logloss:0.03329\tvalidation_1-logloss:0.07899\n[101]\tvalidation_0-logloss:0.03284\tvalidation_1-logloss:0.07873\n[102]\tvalidation_0-logloss:0.03245\tvalidation_1-logloss:0.07871\n[103]\tvalidation_0-logloss:0.03202\tvalidation_1-logloss:0.07846\n[104]\tvalidation_0-logloss:0.03158\tvalidation_1-logloss:0.07822\n[105]\tvalidation_0-logloss:0.03122\tvalidation_1-logloss:0.07799\n[106]\tvalidation_0-logloss:0.03076\tvalidation_1-logloss:0.07690\n[107]\tvalidation_0-logloss:0.03032\tvalidation_1-logloss:0.07710\n[108]\tvalidation_0-logloss:0.02993\tvalidation_1-logloss:0.07759\n[109]\tvalidation_0-logloss:0.02950\tvalidation_1-logloss:0.07750\n[110]\tvalidation_0-logloss:0.02908\tvalidation_1-logloss:0.07647\n[111]\tvalidation_0-logloss:0.02867\tvalidation_1-logloss:0.07550\n[112]\tvalidation_0-logloss:0.02831\tvalidation_1-logloss:0.07529\n[113]\tvalidation_0-logloss:0.02787\tvalidation_1-logloss:0.07401\n[114]\tvalidation_0-logloss:0.02750\tvalidation_1-logloss:0.07395\n[115]\tvalidation_0-logloss:0.02712\tvalidation_1-logloss:0.07300\n[116]\tvalidation_0-logloss:0.02674\tvalidation_1-logloss:0.07235\n[117]\tvalidation_0-logloss:0.02635\tvalidation_1-logloss:0.07196\n[118]\tvalidation_0-logloss:0.02599\tvalidation_1-logloss:0.07107\n[119]\tvalidation_0-logloss:0.02565\tvalidation_1-logloss:0.07043\n[120]\tvalidation_0-logloss:0.02536\tvalidation_1-logloss:0.07095\n[121]\tvalidation_0-logloss:0.02505\tvalidation_1-logloss:0.07092\n[122]\tvalidation_0-logloss:0.02473\tvalidation_1-logloss:0.07007\n[123]\tvalidation_0-logloss:0.02444\tvalidation_1-logloss:0.07007\n[124]\tvalidation_0-logloss:0.02418\tvalidation_1-logloss:0.07058\n[125]\tvalidation_0-logloss:0.02393\tvalidation_1-logloss:0.07069\n[126]\tvalidation_0-logloss:0.02363\tvalidation_1-logloss:0.07066\n[127]\tvalidation_0-logloss:0.02333\tvalidation_1-logloss:0.06986\n[128]\tvalidation_0-logloss:0.02305\tvalidation_1-logloss:0.06984\n[129]\tvalidation_0-logloss:0.02277\tvalidation_1-logloss:0.06906\n[130]\tvalidation_0-logloss:0.02252\tvalidation_1-logloss:0.06911\n[131]\tvalidation_0-logloss:0.02224\tvalidation_1-logloss:0.06825\n[132]\tvalidation_0-logloss:0.02198\tvalidation_1-logloss:0.06751\n[133]\tvalidation_0-logloss:0.02175\tvalidation_1-logloss:0.06699\n[134]\tvalidation_0-logloss:0.02155\tvalidation_1-logloss:0.06748\n[135]\tvalidation_0-logloss:0.02138\tvalidation_1-logloss:0.06752\n[136]\tvalidation_0-logloss:0.02114\tvalidation_1-logloss:0.06747\n[137]\tvalidation_0-logloss:0.02096\tvalidation_1-logloss:0.06682\n[138]\tvalidation_0-logloss:0.02075\tvalidation_1-logloss:0.06686\n[139]\tvalidation_0-logloss:0.02057\tvalidation_1-logloss:0.06663\n[140]\tvalidation_0-logloss:0.02032\tvalidation_1-logloss:0.06654\n[141]\tvalidation_0-logloss:0.02013\tvalidation_1-logloss:0.06599\n[142]\tvalidation_0-logloss:0.01995\tvalidation_1-logloss:0.06647\n[143]\tvalidation_0-logloss:0.01972\tvalidation_1-logloss:0.06640\n[144]\tvalidation_0-logloss:0.01950\tvalidation_1-logloss:0.06636\n[145]\tvalidation_0-logloss:0.01925\tvalidation_1-logloss:0.06568\n[146]\tvalidation_0-logloss:0.01910\tvalidation_1-logloss:0.06597\n[147]\tvalidation_0-logloss:0.01891\tvalidation_1-logloss:0.06518\n[148]\tvalidation_0-logloss:0.01876\tvalidation_1-logloss:0.06547\n[149]\tvalidation_0-logloss:0.01854\tvalidation_1-logloss:0.06481\n[150]\tvalidation_0-logloss:0.01838\tvalidation_1-logloss:0.06530\n[151]\tvalidation_0-logloss:0.01824\tvalidation_1-logloss:0.06490\n[152]\tvalidation_0-logloss:0.01806\tvalidation_1-logloss:0.06506\n[153]\tvalidation_0-logloss:0.01789\tvalidation_1-logloss:0.06519\n[154]\tvalidation_0-logloss:0.01771\tvalidation_1-logloss:0.06496\n[155]\tvalidation_0-logloss:0.01762\tvalidation_1-logloss:0.06516\n[156]\tvalidation_0-logloss:0.01742\tvalidation_1-logloss:0.06457\n[157]\tvalidation_0-logloss:0.01729\tvalidation_1-logloss:0.06484\n[158]\tvalidation_0-logloss:0.01716\tvalidation_1-logloss:0.06408\n[159]\tvalidation_0-logloss:0.01698\tvalidation_1-logloss:0.06389\n[160]\tvalidation_0-logloss:0.01679\tvalidation_1-logloss:0.06333\n[161]\tvalidation_0-logloss:0.01671\tvalidation_1-logloss:0.06355\n[162]\tvalidation_0-logloss:0.01657\tvalidation_1-logloss:0.06357\n[163]\tvalidation_0-logloss:0.01645\tvalidation_1-logloss:0.06321\n[164]\tvalidation_0-logloss:0.01631\tvalidation_1-logloss:0.06317\n[165]\tvalidation_0-logloss:0.01621\tvalidation_1-logloss:0.06322\n[166]\tvalidation_0-logloss:0.01604\tvalidation_1-logloss:0.06270\n[167]\tvalidation_0-logloss:0.01594\tvalidation_1-logloss:0.06232\n[168]\tvalidation_0-logloss:0.01587\tvalidation_1-logloss:0.06253\n[169]\tvalidation_0-logloss:0.01572\tvalidation_1-logloss:0.06206\n[170]\tvalidation_0-logloss:0.01564\tvalidation_1-logloss:0.06167\n[171]\tvalidation_0-logloss:0.01554\tvalidation_1-logloss:0.06097\n[172]\tvalidation_0-logloss:0.01547\tvalidation_1-logloss:0.06117\n[173]\tvalidation_0-logloss:0.01534\tvalidation_1-logloss:0.06110\n[174]\tvalidation_0-logloss:0.01526\tvalidation_1-logloss:0.06115\n[175]\tvalidation_0-logloss:0.01516\tvalidation_1-logloss:0.06047\n[176]\tvalidation_0-logloss:0.01502\tvalidation_1-logloss:0.06018\n[177]\tvalidation_0-logloss:0.01493\tvalidation_1-logloss:0.06022\n[178]\tvalidation_0-logloss:0.01482\tvalidation_1-logloss:0.06012\n[179]\tvalidation_0-logloss:0.01475\tvalidation_1-logloss:0.05975\n[180]\tvalidation_0-logloss:0.01468\tvalidation_1-logloss:0.05968\n[181]\tvalidation_0-logloss:0.01461\tvalidation_1-logloss:0.05988\n[182]\tvalidation_0-logloss:0.01454\tvalidation_1-logloss:0.05952\n[183]\tvalidation_0-logloss:0.01447\tvalidation_1-logloss:0.05945\n[184]\tvalidation_0-logloss:0.01437\tvalidation_1-logloss:0.05952\n[185]\tvalidation_0-logloss:0.01428\tvalidation_1-logloss:0.05933\n[186]\tvalidation_0-logloss:0.01420\tvalidation_1-logloss:0.05926\n[187]\tvalidation_0-logloss:0.01410\tvalidation_1-logloss:0.05917\n[188]\tvalidation_0-logloss:0.01404\tvalidation_1-logloss:0.05883\n[189]\tvalidation_0-logloss:0.01397\tvalidation_1-logloss:0.05843\n[190]\tvalidation_0-logloss:0.01389\tvalidation_1-logloss:0.05825\n[191]\tvalidation_0-logloss:0.01382\tvalidation_1-logloss:0.05821\n[192]\tvalidation_0-logloss:0.01372\tvalidation_1-logloss:0.05829\n[193]\tvalidation_0-logloss:0.01364\tvalidation_1-logloss:0.05811\n[194]\tvalidation_0-logloss:0.01358\tvalidation_1-logloss:0.05808\n[195]\tvalidation_0-logloss:0.01352\tvalidation_1-logloss:0.05823\n[196]\tvalidation_0-logloss:0.01346\tvalidation_1-logloss:0.05829\n[197]\tvalidation_0-logloss:0.01340\tvalidation_1-logloss:0.05823\n[198]\tvalidation_0-logloss:0.01331\tvalidation_1-logloss:0.05832\n[199]\tvalidation_0-logloss:0.01324\tvalidation_1-logloss:0.05813\n[200]\tvalidation_0-logloss:0.01317\tvalidation_1-logloss:0.05811\n[201]\tvalidation_0-logloss:0.01312\tvalidation_1-logloss:0.05769\n[202]\tvalidation_0-logloss:0.01305\tvalidation_1-logloss:0.05754\n[203]\tvalidation_0-logloss:0.01296\tvalidation_1-logloss:0.05764\n[204]\tvalidation_0-logloss:0.01289\tvalidation_1-logloss:0.05749\n[205]\tvalidation_0-logloss:0.01284\tvalidation_1-logloss:0.05756\n[206]\tvalidation_0-logloss:0.01279\tvalidation_1-logloss:0.05772\n[207]\tvalidation_0-logloss:0.01273\tvalidation_1-logloss:0.05768\n[208]\tvalidation_0-logloss:0.01268\tvalidation_1-logloss:0.05783\n[209]\tvalidation_0-logloss:0.01263\tvalidation_1-logloss:0.05752\n[210]\tvalidation_0-logloss:0.01258\tvalidation_1-logloss:0.05711\n[211]\tvalidation_0-logloss:0.01251\tvalidation_1-logloss:0.05697\n[212]\tvalidation_0-logloss:0.01243\tvalidation_1-logloss:0.05662\n[213]\tvalidation_0-logloss:0.01237\tvalidation_1-logloss:0.05671\n[214]\tvalidation_0-logloss:0.01231\tvalidation_1-logloss:0.05659\n[215]\tvalidation_0-logloss:0.01226\tvalidation_1-logloss:0.05629\n[216]\tvalidation_0-logloss:0.01219\tvalidation_1-logloss:0.05595\n[217]\tvalidation_0-logloss:0.01214\tvalidation_1-logloss:0.05591\n[218]\tvalidation_0-logloss:0.01208\tvalidation_1-logloss:0.05580\n[219]\tvalidation_0-logloss:0.01201\tvalidation_1-logloss:0.05606\n[220]\tvalidation_0-logloss:0.01196\tvalidation_1-logloss:0.05592\n[221]\tvalidation_0-logloss:0.01190\tvalidation_1-logloss:0.05601\n[222]\tvalidation_0-logloss:0.01185\tvalidation_1-logloss:0.05608\n[223]\tvalidation_0-logloss:0.01181\tvalidation_1-logloss:0.05569\n[224]\tvalidation_0-logloss:0.01176\tvalidation_1-logloss:0.05559\n[225]\tvalidation_0-logloss:0.01169\tvalidation_1-logloss:0.05585\n[226]\tvalidation_0-logloss:0.01164\tvalidation_1-logloss:0.05576\n[227]\tvalidation_0-logloss:0.01158\tvalidation_1-logloss:0.05549\n[228]\tvalidation_0-logloss:0.01153\tvalidation_1-logloss:0.05521\n[229]\tvalidation_0-logloss:0.01149\tvalidation_1-logloss:0.05509\n[230]\tvalidation_0-logloss:0.01145\tvalidation_1-logloss:0.05493\n[231]\tvalidation_0-logloss:0.01140\tvalidation_1-logloss:0.05507\n[232]\tvalidation_0-logloss:0.01136\tvalidation_1-logloss:0.05469\n[233]\tvalidation_0-logloss:0.01132\tvalidation_1-logloss:0.05500\n[234]\tvalidation_0-logloss:0.01128\tvalidation_1-logloss:0.05474\n[235]\tvalidation_0-logloss:0.01124\tvalidation_1-logloss:0.05472\n[236]\tvalidation_0-logloss:0.01120\tvalidation_1-logloss:0.05490\n[237]\tvalidation_0-logloss:0.01115\tvalidation_1-logloss:0.05503\n[238]\tvalidation_0-logloss:0.01111\tvalidation_1-logloss:0.05516\n[239]\tvalidation_0-logloss:0.01107\tvalidation_1-logloss:0.05524\n[240]\tvalidation_0-logloss:0.01103\tvalidation_1-logloss:0.05537\n[241]\tvalidation_0-logloss:0.01099\tvalidation_1-logloss:0.05536\n[242]\tvalidation_0-logloss:0.01096\tvalidation_1-logloss:0.05568\n[243]\tvalidation_0-logloss:0.01090\tvalidation_1-logloss:0.05543\n[244]\tvalidation_0-logloss:0.01087\tvalidation_1-logloss:0.05556\n[245]\tvalidation_0-logloss:0.01083\tvalidation_1-logloss:0.05519\n[246]\tvalidation_0-logloss:0.01081\tvalidation_1-logloss:0.05537\n[247]\tvalidation_0-logloss:0.01077\tvalidation_1-logloss:0.05536\n[248]\tvalidation_0-logloss:0.01072\tvalidation_1-logloss:0.05549\n[249]\tvalidation_0-logloss:0.01068\tvalidation_1-logloss:0.05562\n[250]\tvalidation_0-logloss:0.01064\tvalidation_1-logloss:0.05537\n[251]\tvalidation_0-logloss:0.01061\tvalidation_1-logloss:0.05555\n[252]\tvalidation_0-logloss:0.01058\tvalidation_1-logloss:0.05568\n[253]\tvalidation_0-logloss:0.01054\tvalidation_1-logloss:0.05532\n[254]\tvalidation_0-logloss:0.01051\tvalidation_1-logloss:0.05508\n[255]\tvalidation_0-logloss:0.01049\tvalidation_1-logloss:0.05525\n[256]\tvalidation_0-logloss:0.01045\tvalidation_1-logloss:0.05524\n[257]\tvalidation_0-logloss:0.01042\tvalidation_1-logloss:0.05537\n[258]\tvalidation_0-logloss:0.01037\tvalidation_1-logloss:0.05554\n[259]\tvalidation_0-logloss:0.01033\tvalidation_1-logloss:0.05563\n[260]\tvalidation_0-logloss:0.01030\tvalidation_1-logloss:0.05528\n[261]\tvalidation_0-logloss:0.01028\tvalidation_1-logloss:0.05546\n[262]\tvalidation_0-logloss:0.01025\tvalidation_1-logloss:0.05559\n[263]\tvalidation_0-logloss:0.01022\tvalidation_1-logloss:0.05558\n[264]\tvalidation_0-logloss:0.01017\tvalidation_1-logloss:0.05576\n[265]\tvalidation_0-logloss:0.01014\tvalidation_1-logloss:0.05593\n[266]\tvalidation_0-logloss:0.01011\tvalidation_1-logloss:0.05558\n[267]\tvalidation_0-logloss:0.01008\tvalidation_1-logloss:0.05571\n[268]\tvalidation_0-logloss:0.01005\tvalidation_1-logloss:0.05584\n[269]\tvalidation_0-logloss:0.01002\tvalidation_1-logloss:0.05561\n[270]\tvalidation_0-logloss:0.00998\tvalidation_1-logloss:0.05560\n[271]\tvalidation_0-logloss:0.00997\tvalidation_1-logloss:0.05577\n[272]\tvalidation_0-logloss:0.00992\tvalidation_1-logloss:0.05574\n[273]\tvalidation_0-logloss:0.00989\tvalidation_1-logloss:0.05540\n[274]\tvalidation_0-logloss:0.00987\tvalidation_1-logloss:0.05557\n[275]\tvalidation_0-logloss:0.00984\tvalidation_1-logloss:0.05570\n[276]\tvalidation_0-logloss:0.00981\tvalidation_1-logloss:0.05547\n[277]\tvalidation_0-logloss:0.00978\tvalidation_1-logloss:0.05546\n[278]\tvalidation_0-logloss:0.00973\tvalidation_1-logloss:0.05543\n[279]\tvalidation_0-logloss:0.00970\tvalidation_1-logloss:0.05556\n[280]\tvalidation_0-logloss:0.00969\tvalidation_1-logloss:0.05562\n[281]\tvalidation_0-logloss:0.00966\tvalidation_1-logloss:0.05528\n```\n:::\n:::\n\n\n### LightGBM\n\n- 성능은 xgboost랑 별로 차이가 없음.\n- 1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\n- one hot 인코딩 필요 없음\n\n- python lightgbm\n\n::: {#7232fa09 .cell execution_count=13}\n``` {.python .cell-code}\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[LightGBM] [Info] Number of positive: 262, number of negative: 147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4092\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.640587 -> initscore=0.577912\n[LightGBM] [Info] Start training from score 0.577912\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[159]\ttraining's binary_logloss: 0.00195741\tvalid_1's binary_logloss: 0.0442418\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02_files/figure-html/cell-14-output-2.png){width=642 height=449}\n:::\n:::\n\n\n## stacking\n\n::: {#f9259961 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n```\n:::\n\n\n::: {#707b60f8 .cell execution_count=15}\n``` {.python .cell-code}\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n```\n:::\n\n\n::: {#7479cba6 .cell execution_count=16}\n``` {.python .cell-code}\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.991\n```\n:::\n:::\n\n\n- test 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n### CV 세트 기반 stacking\n\n::: {#49118349 .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n```\n:::\n\n\n::: {#ca76c460 .cell execution_count=18}\n``` {.python .cell-code}\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n```\n:::\n\n\n::: {#a8e87524 .cell execution_count=19}\n``` {.python .cell-code}\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.982\n```\n:::\n:::\n\n\n## Baysian Optimization\n\n- Grid search로는 시간이 너무 오래 걸리는 경우\n\n- 목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\n- Surrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\n- acquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n::: {#450bc93c .cell execution_count=20}\n``` {.python .cell-code}\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]\r100%|██████████| 20/20 [00:00<00:00, 1705.00trial/s, best loss: -284.0]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n{'x': 4.0, 'y': 15.0}\n```\n:::\n:::\n\n\n### XGBoost 하이퍼파라미터 최적화\n\n::: {#1177ac1f .cell execution_count=21}\n``` {.python .cell-code}\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n```\n:::\n\n\n::: {#c7698d8f .cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\r  2%|▏         | 1/50 [00:00<00:15,  3.22trial/s, best loss: -0.9582752410828395]\r  4%|▍         | 2/50 [00:00<00:14,  3.38trial/s, best loss: -0.9714186127570582]\r  6%|▌         | 3/50 [00:00<00:15,  3.13trial/s, best loss: -0.9714186127570582]\r  8%|▊         | 4/50 [00:01<00:16,  2.82trial/s, best loss: -0.9714186127570582]\r 10%|█         | 5/50 [00:01<00:13,  3.43trial/s, best loss: -0.9714186127570582]\r 12%|█▏        | 6/50 [00:01<00:10,  4.12trial/s, best loss: -0.9714186127570582]\r 14%|█▍        | 7/50 [00:01<00:08,  4.92trial/s, best loss: -0.9714186127570582]\r 16%|█▌        | 8/50 [00:01<00:07,  5.39trial/s, best loss: -0.9714186127570582]\r 18%|█▊        | 9/50 [00:02<00:07,  5.79trial/s, best loss: -0.9714186127570582]\r 20%|██        | 10/50 [00:02<00:06,  5.88trial/s, best loss: -0.9714186127570582]\r 22%|██▏       | 11/50 [00:02<00:05,  6.59trial/s, best loss: -0.9714186127570582]\r 24%|██▍       | 12/50 [00:02<00:05,  6.58trial/s, best loss: -0.9714186127570582]\r 26%|██▌       | 13/50 [00:02<00:05,  7.31trial/s, best loss: -0.9714186127570582]\r 28%|██▊       | 14/50 [00:02<00:04,  7.28trial/s, best loss: -0.9714186127570582]\r 30%|███       | 15/50 [00:02<00:04,  7.17trial/s, best loss: -0.9714186127570582]\r 38%|███▊      | 19/50 [00:03<00:02, 14.61trial/s, best loss: -0.9714186127570582]\r 42%|████▏     | 21/50 [00:03<00:02, 10.53trial/s, best loss: -0.9714186127570582]\r 46%|████▌     | 23/50 [00:03<00:02,  9.43trial/s, best loss: -0.9714186127570582]\r 50%|█████     | 25/50 [00:03<00:02,  8.95trial/s, best loss: -0.9736261182758219]\r 54%|█████▍    | 27/50 [00:04<00:02,  8.87trial/s, best loss: -0.9736261182758219]\r 56%|█████▌    | 28/50 [00:04<00:02,  8.90trial/s, best loss: -0.9736261182758219]\r 60%|██████    | 30/50 [00:04<00:02,  8.95trial/s, best loss: -0.9736261182758219]\r 62%|██████▏   | 31/50 [00:04<00:02,  8.95trial/s, best loss: -0.9736261182758219]\r 64%|██████▍   | 32/50 [00:04<00:02,  7.08trial/s, best loss: -0.9736261182758219]\r 66%|██████▌   | 33/50 [00:05<00:02,  5.92trial/s, best loss: -0.9736261182758219]\r 68%|██████▊   | 34/50 [00:05<00:03,  5.20trial/s, best loss: -0.9736261182758219]\r 70%|███████   | 35/50 [00:05<00:03,  3.97trial/s, best loss: -0.9736261182758219]\r 72%|███████▏  | 36/50 [00:05<00:03,  3.84trial/s, best loss: -0.9736261182758219]\r 74%|███████▍  | 37/50 [00:06<00:03,  3.79trial/s, best loss: -0.9736261182758219]\r 76%|███████▌  | 38/50 [00:06<00:02,  4.05trial/s, best loss: -0.9736261182758219]\r 78%|███████▊  | 39/50 [00:06<00:02,  4.46trial/s, best loss: -0.9736261182758219]\r 80%|████████  | 40/50 [00:06<00:01,  5.31trial/s, best loss: -0.9736261182758219]\r 82%|████████▏ | 41/50 [00:06<00:01,  6.03trial/s, best loss: -0.9736261182758219]\r 84%|████████▍ | 42/50 [00:06<00:01,  6.79trial/s, best loss: -0.9736261182758219]\r 86%|████████▌ | 43/50 [00:07<00:01,  6.73trial/s, best loss: -0.9736261182758219]\r 88%|████████▊ | 44/50 [00:07<00:00,  7.04trial/s, best loss: -0.9736261182758219]\r 90%|█████████ | 45/50 [00:07<00:00,  7.43trial/s, best loss: -0.9736261182758219]\r 92%|█████████▏| 46/50 [00:07<00:00,  7.02trial/s, best loss: -0.9736261182758219]\r 94%|█████████▍| 47/50 [00:07<00:00,  7.14trial/s, best loss: -0.9736261182758219]\r 96%|█████████▌| 48/50 [00:07<00:00,  6.67trial/s, best loss: -0.9736261182758219]\r 98%|█████████▊| 49/50 [00:07<00:00,  7.19trial/s, best loss: -0.9736261182758219]\r100%|██████████| 50/50 [00:08<00:00,  7.52trial/s, best loss: -0.9736261182758219]\r100%|██████████| 50/50 [00:08<00:00,  6.22trial/s, best loss: -0.9736261182758219]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\n{'colsample_bytree': 0.8065561529248224,\n 'learning_rate': 0.1128018538935688,\n 'max_depth': 18.0,\n 'min_child_weight': 2.0}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}