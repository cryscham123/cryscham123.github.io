{
  "hash": "ddcf72637474d46e0d8bb7f81e27eba3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"전처리 템플릿\"\ndate: 2025-10-05\ncategories: [\"데이터 분석\"]\n---\n\n\n\n\n- 참고: 이상치, 결측치, 불균형 처리 모두 원칙적으로는 train set에서만 fit 해야함.\n\n## 결측치 처리\n\n- 결측치가 발생하는 원인과 처리 전략\n    - 무작위 결측(Missing Completely at Random, MCAR): 결측치가 발생할 확률이 다른 모든 변수와 무관\n        - 예: 설문지 작성 중 무작위로 일부 페이지가 누락된 경우\n        - 이 경우, 결측치를 제거하거나 단순 대체해도 편향이 발생하지 않음\n    - 조건부 무작위 결측(Missing at Random, MAR): 결측치가 발생할 확률이 관측된 데이터에만 의존\n        - 예: 고소득자일수록 소득 공개를 꺼려하는 경우 (교육수준이 높을수록 소득 결측 확률이 높음)\n        - 실무에서 가장 일반적인 가정: 대부분의 실제 데이터에서 결측 패턴은 관측된 변수들로 어느 정도 설명 가능\n        - 관측된 데이터를 활용한 예측 기반 대치법이 효과적\n    - 비무작위 결측(Missing Not at Random, MNAR): 결측치가 발생할 확률이 결측된 값 자체와 관련\n        - 예: 극도로 낮은 소득자가 소득 공개를 꺼리는 경우\n        - 통계적 방법만으로는 해결이 어려우며, 도메인 지식이나 외부 정보가 필요\n        - **실무에서는 MAR 가정 하에 처리 후, 민감도 분석을 통해 결측 메커니즘과 처리 방법의 적절성을 확인**\n            - 민감도 분석: 다양한 결측치 처리 방법을 적용하여 결과를 비교\n                - 단순 방법(평균/중앙값 대치) vs 고급 방법(KNN, MICE)\n                - 단순 방법과 고급 방법의 결과가 비슷하면 → MCAR 가능성 높음, 단순 방법으로도 충분\n                - 고급 방법이 더 나은 성능을 보이면 → MAR 가능성 높음, 고급 방법 선택\n                - 어떤 방법을 써도 결과가 불안정하면 → MNAR 가능성, 도메인 지식과 추가 정보 필요\n\n- 결측치 처리 방법\n    - 제거: 결측치가 적거나 무작위 결측일 때 사용\n    - 대치(대체):\n        - 일반적인 방법\n            - 시계열 데이터 o: 이전 값, 이후 값, 선형 보간법\n            - 시계열 데이터 x: 평균, 중앙값, 최빈값\n        - 고급 대치법(과적합 발생 가능성 유의)\n            - KNN 대치: 유사한 관측치의 값을 사용하여 결측치를 대체. 결측치가 없는 데이터로 예측\n            - 다변량 대치: 결측치를 다른 변수들의 값으로 예측하여 대체.\n\n### 다변량 대치법\n\n::: {#a2710298 .cell execution_count=1}\n``` {.python .cell-code}\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\ndef multi_impute(df, categorical, max_iter=40):\n    df_imp = df.copy()\n    num_cols = [col for col in df.columns if col not in categorical]\n\n    # 결측치가 많은 column 우선 처리\n    null_counts = df_imp.isnull().sum()\n    null_cols = null_counts[null_counts > 0].sort_values().index.tolist()\n\n    # 초기값 대체\n    for col in categorical:\n        df_imp[col] = df_imp[col].astype('category')\n        mode = df_imp[col].mode(dropna=True)\n        df_imp[col].fillna(mode[0], inplace=True)\n\n    for col in num_cols:\n        mean = df_imp[col].mean()\n        df_imp[col].fillna(mean, inplace=True)\n\n    # 반복 임퓨팅\n    for _ in range(max_iter):\n        prev = df_imp.copy()\n\n        for col in null_cols:\n            idx_missing = df[col].isnull()\n            idx_obs = ~idx_missing\n            predictors = [c for c in df.columns if c != col]\n\n            X_obs = df_imp.loc[idx_obs, predictors]\n            y_obs = df_imp.loc[idx_obs, col]\n            X_mis = df_imp.loc[idx_missing, predictors]\n\n            # column type에 따라 다른 모델 선택\n            # LightGBM으로 randomforest를 사용한 이유: sklearn의 RandomForest는 categorical 변수를 직접 처리하지 못함\n            if col in categorical:\n                model = LGBMClassifier(\n                    boosting_type=\"rf\", n_estimators=5,\n                    bagging_fraction=0.8, bagging_freq=1\n                )\n            else:\n                model = LGBMRegressor(\n                    boosting_type=\"rf\", n_estimators=5,\n                    bagging_fraction=0.8, bagging_freq=1\n                )\n            model.fit(X_obs, y_obs)\n            y_pred = model.predict(X_mis)\n            df_imp.loc[idx_missing, col] = y_pred\n        if df_imp.equals(prev):\n            break\n    return df_imp\n```\n:::\n\n\n- 현재 결측치 처리 방법 중 가장 성능이 좋은걸로 알려져 있는 missing forest를 모방한 방법.\n    - missforest 라이브러리는 adp 환경에서 설치 불가\n- 다중 대치를 안 하고 있지만, sklearn 공식 문서에 따르면 대부분 single 대치로도 충분하다고 한다.\n- 참고로 이 방법은 missforest의 정확한 구현은 아니기 때문에, 시험에서는 다변량 대치법을 사용했다고만 쓰자.\n- 만약 train set에서만 fit을 시키고 싶다면, 이 방법은 그냥 안 쓰는걸 추천.\n    - 그렇게 하려면 class 형태로 바꿔야 하는데, too much인가 싶은 느낌이 슬슬 나기 시작.\n\n::: {#d831e548 .cell execution_count=2}\n``` {.python .cell-code}\nclass MultiImputer:\n    def __init__(self, categorical=[], max_iter=40, n_estimators=5):\n        self.categorical = categorical\n        self.max_iter = max_iter\n        self.n_estimators = n_estimators\n        \n        self.models_ = {}\n        self.initial_fill_ = {}\n        self.null_cols_ = []\n        self.num_cols_ = []\n        \n    def fit(self, X, y=None):\n        df = X.copy()\n        self.num_cols_ = [col for col in df.columns if col not in self.categorical]\n        \n        null_counts = df.isnull().sum()\n        self.null_cols_ = null_counts[null_counts > 0].sort_values().index.tolist()\n        \n        # 초기값 계산 및 저장 (Train의 통계량만 사용!)\n        for col in self.categorical:\n            df[col] = df[col].astype('category')\n            mode = df[col].mode(dropna=True)\n            self.initial_fill_[col] = mode[0]\n            df[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for col in self.num_cols_:\n            mean = df[col].mean()\n            self.initial_fill_[col] = mean\n            df[col].fillna(mean, inplace=True)\n        \n        for _ in range(self.max_iter):\n            prev = df.copy()\n            \n            for col in self.null_cols_:\n                idx_missing = X[col].isnull()\n                idx_obs = ~idx_missing\n                    \n                predictors = [c for c in df.columns if c != col]\n                X_obs = df.loc[idx_obs, predictors]\n                y_obs = df.loc[idx_obs, col]\n                X_mis = df.loc[idx_missing, predictors]\n                \n                # 모델 선택 및 학습\n                if col in self.categorical:\n                    model = LGBMClassifier(\n                        boosting_type=\"rf\", \n                        n_estimators=self.n_estimators,\n                        bagging_fraction=0.8, \n                        bagging_freq=1,\n                        verbose=-1\n                    )\n                else:\n                    model = LGBMRegressor(\n                        boosting_type=\"rf\", \n                        n_estimators=self.n_estimators,\n                        bagging_fraction=0.8, \n                        bagging_freq=1,\n                        verbose=-1\n                    )\n                \n                model.fit(X_obs, y_obs)\n                \n                y_pred = model.predict(X_mis)\n                df.loc[idx_missing, col] = y_pred\n                \n                self.models_[col] = model\n            \n            if df.equals(prev):\n                break\n        \n        return self\n    \n    def transform(self, X):\n        df_imp = X.copy()\n        \n        for col in self.categorical:\n            df_imp[col] = df_imp[col].astype('category')\n            df_imp[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for col in self.num_cols_:\n            df_imp[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for _ in range(self.max_iter):\n            prev = df_imp.copy()\n            \n            for col in self.null_cols_:                   \n                idx_missing = X[col].isnull()\n                \n                predictors = [c for c in df_imp.columns if c != col]\n                X_mis = df_imp.loc[idx_missing, predictors]\n                \n                model = self.models_[col]\n                y_pred = model.predict(X_mis)\n                df_imp.loc[idx_missing, col] = y_pred\n            \n            if df_imp.equals(prev):\n                break\n        \n        return df_imp\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\n```\n:::\n\n\n- 혹시몰라서 class 형태로 만든 다변량 대치법\n- 이걸 언제 다 적고 있을까\n\n## 이상치 처리\n\n- 이상치 탐지는 EDA 참고\n- 처리는 알아서 잘 하자.\n\n## 불균형 처리\n\n- 잘 알려져 있는 방법 대충 잘 선택해서 사용.\n- 딱히 SOTA(가장 좋은 방법)가 있지 않음.\n\n### 성능 비교\n\n- 각각의 처리법에 대해서 어떤게 제일 좋은지 비교\n\n::: {#e6cb2883 .cell execution_count=3}\n``` {.python .cell-code}\nfrom lightgbm import LGBMRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import cross_val_score\n\ndf[cat_cols] = df[cat_cols].astype('category')\n\ndf1 = df.dropna()\ndf2 = df[num_cols].fillna(df[num_cols].mean())\nfor col in cat_cols:\n    df2[col] = target[col].fillna(target[col].mode()[0])\ndf3 = multi_impute(df, categorical=cat_cols)\n\ncandis = [\n    ('Nothing', df.drop('y', axis=1), df['dead']),\n    ('Just Delete', df1.drop('dead', axis=1), df1['dead']),\n    ('Simple Impute', df2.drop('dead', axis=1), df2['dead']),\n    ('MultiImputer', df3.drop('dead', axis=1), df3['dead'])\n]\n\nresult = pd.DataFrame()\nfor name, X, y in candis:\n    rf = LGBMRegressor(boosting_type=\"rf\", n_estimators=100, bagging_fraction=0.8, bagging_freq=1)\n    result[name] = cross_val_score(rf, X, y, scoring='neg_mean_squared_error') # classifier인 경우 'accuracy' 등등\n\nfig, ax = plt.subplots(figsize=(13, 6))\n\nmeans = -result.mean() # regressor인 경우, classifier인 경우 양수\nerrors = result.std()\n\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_yticks(np.arange(means.shape[0]))\nax.set_yticklabels(means.index)\nplt.show()\n```\n:::\n\n\n## Feature Selection\n\n### Filter Method\n\n1. basic methods\n\n- 하나의 값만 가지는 변수 혹은 분산이 너무 낮은 변수는 제거\n\n::: {#b79c3147 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=0.01)\n\nselected_cols = df.columns[sel.get_support()]\ndf_selected = df[selected_cols]\n```\n:::\n\n\n2. Univariate selection methods\n\n::: {#3bc0ade7 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n\nX_new = SelectKBest(chi2, k=2).fit_transform(X, y) # 최상위 2개\n\nX_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y) # 상위 10%\n```\n:::\n\n\n- 카이제곱 검정량이 가장 높은 변수들만 선택.\n- 연속형 변수에 대해서는 KBinsDiscretizer 작업 필요.\n\n::: {#42a40fdd .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif, mutual_info_regression\n\nX_new = SelectKBest(mutual_info_classif, k=2).fit_transform(X, y) # 최상위 2개\nX_new = SelectPercentile(mutual_info_regression, percentile=10).fit_transform(X, y) # 상위 10%\n```\n:::\n\n\n- EDA 파트 mutual info 참고\n- 추가: 상관계수, ANOVA F-value 등등 사용 가능\n\n### Wrapper Method\n\n- forward, backward, 등등\n\n### Embedded Method\n\n- L1, L2, Elasticnet 등등\n\n",
    "supporting": [
      "03_files"
    ],
    "filters": [],
    "includes": {}
  }
}