{
  "hash": "533c7a8e3fa091809b8f2c24d8c2678e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"비지도 학습 템플릿\"\ndate: 2025-10-05\ncategories: [\"데이터 분석\"]\n---\n\n\n\n\n## 군집 분석\n\n### Distance-based methods\n\n- Partitioning methods\n\n```{.python filename=\"k-means\"}\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # or RobustScaler\ndf_scaled = scaler.fit_transform(df)\n\n# Elbow method\nI = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i) # sklearn은 기본적으로 k-means++\n    kmeans.fit(df_scaled)\n    I.append(kmeans.inertia_)\nplt.plot(range(1, 11), I, marker='o')\n\n# k 선택 후 군집화\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df_scaled)\ndf['cluster'] = kmeans.labels_\n\n# 군집 중심값 정보\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\ncenters_df = pd.DataFrame(centers, columns=df.columns[:-1], index=[f'cluster_{i}' for i in range(centers.shape[0])])\ndisplay(centers_df)\n```\n\n1. Kmeans\n    - polinominal 시간 안에 해결 가능\n    - noise, outlier에 민감함\n    - 수치형만 처리 가능\n2. k-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\n3. k-prototype: 범주형, 수치형 섞인거 처리 가능\n4. k-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n    - PAM: Partitioning Around Medoids\n        - scalability 문제 있음\n    - CLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n        - 샘플링 과정에서 biased될 수 있음\n    - CLARANS: medoid 후보를 랜덤하게 선택함\n\n> k-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함\n> ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)\n\n- Hierarchical methods\n\n1. top-down: divisive, dia\n2. bottom-up: agglomerative\n\n```{.python filename=\"agglomerative\"}\nfrom scipy.cluster.hierarchy import dendrogram, linkage. cut_tree\n\nz = linkage(df_scaled, method='ward') # 'single', 'complete', 'average', 'ward' 등등\n\nresult = cut_tree(z, n_clusters=3).flatten()\n\nd = dendogram(z, labels=list(df.index))\nplt.show()\n```\n\n```{.python filename=\"합쳐지는 거리\"}\nthr = pd.DataFrame(d['dcoord'])\nthr # (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이\nthr[thr[2] > 70].sort_values(2)[[2]] # 70 이상의 높이에서 합쳐지는 군집들의 높이\n```\n\n- 거리기반 군집의 단점:\n    - 군집의 모양이 구형이 아닐 경우 찾기 어려움\n    - 군집의 갯수 결정하기 어려움\n    - 군집의 밀도가 높아야함\n\n### Density-based methods\n\n- 다양한 모양의 군집을 찾을 수 있음\n- noise, outlier에 강함\n\n1. DBSCAN: 잡음 포인트는 군집에서 제외\n    1. core point를 찾음(eps 이내에 minPts 이상 있는 점)\n    1. core point를 중심으로 군집을 확장\n        - core point가 아닌 경우 확장 종료\n    - 고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n    - 군집간 계층관계를 인식하기 어렵다\n    - 대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯\n\n```{.python filename=\"DBSCAN eps 결정\"}\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = 5\ndf_scaled = scaler.fit_transform(df)\n\nneigh = NearestNeighbors(n_neighbors=MIN_SAMPLES)\nneigh.fit(df_scaled)\ndistances, indices = neigh.kneighbors(df_scaled)\n\nk_distances = np.sort(distances[:, MIN_SAMPLES-1])[::-1]\n\n# k-dist plot 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(k_distances)\nplt.xlabel('Data Points sorted by distance')\nplt.ylabel(f'{MIN_SAMPLES-1}-th Nearest Neighbor Distance') # 자기 자신을 제외한 거리\nplt.title('k-distance Graph')\nplt.grid(True)\nplt.show()\n```\n\n- min samples 기준\n    - 2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.\n    - 이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기\n- eps 기준\n    - k-dist plot에서 급격히 꺾이는 지점\n\n```{.python filename=\"DBSCAN\"}\nfrom sklearn.cluster import DBSCAN\n\n# k-dist plot에서 찾은 값으로 DBSCAN 적용\neps_value = 18  # k-dist plot에서 결정한 값\n\ndb = DBSCAN(eps=eps_value, min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = db.labels_\n```\n\n2. OPTICS: DBSCAN의 단점을 보완\n    - 군집의 밀도가 다를 때도 잘 처리함\n    - 군집의 계층 구조를 인식할 수 있음\n    - minPts 파라미터가 필요함\n    - 얘로도 이상치 탐지 가능\n\n```{.python filename=\"OPTICS\"}\nfrom sklearn.cluster import OPTICS\n\noptics = OPTICS(min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = optics.labels_\n```\n\n### 평가\n\n- silhuette score: $\\frac{\\sum_{i=1}^{n} s(i)}{n}$\n    - s(i): $\\frac{b(i) - a(i)}{max((a(i), b(i)))}$\n        - a(i): 군집 내 노드간의 평균 거리\n        - b(i): 가장 가까운 군집과의 노드 간 평균 거리\n    - 1에 가까울 수록 좋음\n- 그 외 sklearn metrics 참고\n\n## 차원 축소\n\n- PCA, LSA, t-SNE, UMAP, ICA, MDS, NMF 등등\n    - 정말 많은 방법들이 있다.\n- 요인분석에서 확인적 요인분석은 python에서 제공하는 library가 없는걸로 알고 있음\n\n## 연관 분석\n\n::: {#d0b281f6 .cell execution_count=1}\n``` {.python .cell-code}\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\ntransactions = {}\nte = TransactionEncoder()\n\n# 전처리\ndata = df['target'].str.split(', ').values\nte_ary = te.fit_transform(data)\ntransactions[name] = pd.DataFrame(te_ary, columns=te.columns_)\n\n# 빈발패턴 생성\nfset = apriori(t, min_support=0.6, use_colnames=True, verbose=False)\nif fset.shape[0] == 0:\n    print(\"빈발패턴이 존재하지 않습니다.\")\nelse:\n    # 연관규칙 생성\n    rule = association_rules(fset, metric=\"confidence\", min_threshold=0.7)\n    rule['len_ant'] = rule['antecedents'].apply(lambda x: len(x))\n    rule['len_con'] = rule['consequents'].apply(lambda x: len(x))\n    display(rule[(rule['len_con'] == 1) & (rule['lift'] >= 1.2)].reset_index(drop=True))\n```\n:::\n\n\n- 지지도: 전체 거래에서 특정 항목 집합이 나타나는 비율\n- 신뢰도: 특정 항목 집합이 주어졌을 때 다른 항목\n- 향상도: 두 항목 집합이 독립적인 경우에 비해 함께 나타날 가능성\n\n",
    "supporting": [
      "06_files"
    ],
    "filters": [],
    "includes": {}
  }
}