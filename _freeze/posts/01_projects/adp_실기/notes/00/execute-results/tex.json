{
  "hash": "8666aa95362406583ff16a0be1ae29da",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: ADP 정리 노트\ncategories: [\"자격증\"]\ndate: last-modified\nhighlight-style: github\nformat: \n  pdf:\n    fig-pos: 'H'\n    fig-width: 6\n    fig-height: 4\n    documentclass: report\n    papersize: a4\n    top-level-division: chapter\n    fontsize: 17pt\n    toc: false\n    geometry:\n      - top=20mm\n    indent-size: 2\ninclude-in-header: \n  text: |\n    \\date{}\n    \\usepackage{fontspec}\n    \\setmainfont{Noto Sans KR}\n    \\usepackage{titlesec}\n    \\titleformat{\\chapter}{\\normalfont\\huge\\bfseries}{\\thechapter.}{0.8em}{\\huge}\n    \\titleformat{\\subsection}[block]{\\normalfont\\large\\bfseries}{}{0pt}{}\n    \\titleformat{\\subsubsection}[block]{\\normalfont\\normalsize\\bfseries}{}{0pt}{}\n    \\titlespacing*{\\chapter}{0pt}{-20pt}{5pt}\n    \\titlespacing*{\\section}{0pt}{10pt}{10pt}\n    \\newcommand{\\chapterbreak}{\\clearpage}\n    \\usepackage{setspace}\n    \\setstretch{1.5}\n    \\usepackage{tabularx}\n    \\newcolumntype{Y}{>{\\centering\\arraybackslash}X}\n    \\usepackage[bottom]{footmisc}\n    \\usepackage{setspace}\n    \\renewcommand{\\footnotesize}{\\setstretch{1.2}\\fontsize{9pt}{11pt}\\selectfont}\n    \\setlength{\\skip\\footins}{10pt}\n    \\setlength{\\footnotesep}{8pt}\n    \\renewcommand{\\footnoterule}{\\vspace{1em}\\hrule width 0.5\\linewidth\\vspace{1em}}\n    \\makeatletter\n    \\renewcommand{\\@makefntext}[1]{\\noindent\\makebox[1.5em][r]{\\@makefnmark}\\hspace{0.5em}#1}\n    \\makeatother\n    \\usepackage{float}\n    \\usepackage{multirow}\n    \\usepackage{booktabs}\n    \\usepackage{makecell}\n\nexecute:\n  echo: true\n  warning: false\n  message: false\n\n---\n\n\n\n\n# 확률과 통계\n\n## 통계학\n\n- **불확실한 상황** 하에서 데이터에 근거하여 **과학적인 의사결정**을 도출하기 위한 이론과 방법의 체계\n- **모집단**으로 부터 수집된 **데이터**(sample)를 기반으로 모집단의 **특성을 추론**하는 것을 목표로 한다.\n\n![통계적 의사결정 과정](img/2025-03-08-12-28-23.png)\n\n## 확률\n\n- 고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n- 통계적 의미: 특정 사건이 발생하는 **상대도수의 극한**\n    - 각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.\n \n## 확률 분포 정의 단계\n\n![](img/2025-03-08-13-00-48.png)\n\n- **Experiment(확률실험)**: 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\n- **Sample space(표본공간)**: 모든 simple event의 집합\n- **Event(사건)**: 실험에서 발생하는 결과 (부분 집합)\n- **Simple event(단순사건)**: 원소가 하나인 사건\n- **확률 변수**: 확률실험의 결과를 수치로 나타낸 변수\n\n## 확률 분포\n\n### 이산 확률 분포\n\n이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n- **베르누이 시행**: 각 시행은 서로 **독립적**이고, 실패와 성공 **두 가지 결과만 존재**.\n    - 단 **모집단의 크기가 충분히 크고**, **표본(시행)의 크기가 충분히 작다면** **비복원 추출**에서도 **유효**\n    - 평균: p\n    - 분산: p(1-p)\n- **이항 분포**: n번의 독립적인 **베르누이 시행**을 수행하여 성공 횟수를 측정\n    - X ~ B(n, p), $f(x) = \\binom{n}{x} p^x (1-p)^{n-x}$\n    - 평균: np\n    - 분산: np(1-p)\n    - n이 매우 크고, p가 매우 작을 때, **포아송 분포로 근사**할 수 있다. (λ = np)\n- **음이항 분포**\n    - 정의: n번의 독립적인 **베르누이 시행**을 수행하여 k번 성공하고, r번 실패한 경우 (n = k + r)\n        1. r번의 실패가 나오기 전까지, 성공한 횟수 x\n            - X ~ NB(r, p), $f(x) = \\binom{x+r-1}{x} p^x (1-p)^r$\n            - 평균: $\\frac{rp}{1-p}$\n            - 분산: $\\frac{rp}{(1-p)^2}$\n        2. r번의 실패가 나오기 전까지, 시행한 횟수 x\n            - 4번에서 성공을 실패로 바꿈\n        3. k번의 성공이 나오기 전까지, 실패한 횟수 x\n            - 1번에서 실패를 성공으로 바꿈\n        4. k번의 성공이 나오기 전까지, 시행한 횟수 x\n            - $f(x) = \\binom{x-1}{k-1} p^k (1-p)^{x-k}$\n            - k가 1일 때 기하분포와 동일\n        5. n번의 시행 횟수에서, k번 성공 또는 r번 실패한 경우: 이항분포\n- **기하 분포**: \n    - 정의:\n        1. 성공 확률이 p인 **베르누이 시행**에서 첫 성공까지의 시행 횟수\n            - X ~ G(p), $f(x) = (1-p)^{x-1} p, x = 1, 2, 3, ...$\n            - 평균: $\\frac{1}{p}$\n            - 분산: $\\frac{1-p}{p^2}$\n        2. 성공 확률이 p인 **베르누이 시행**에서 첫 성공까지의 실패 횟수\n            - X ~ G(p), $f(x) = (1-p)^x p, x = 0, 1, 2, ...$\n            - 평균: $\\frac{1-p}{p}$\n            - 분산: $\\frac{1-p}{p^2}$\n    - **비기억 특성**: $P(X > n+k | X > n) = P(X > k)$\n- **초기하 분포**: **베르누이 시행이 아닌 시행**에서 성공하는 횟수\n    - X ~ H(n, N, k), $f(x) = \\frac{\\binom{K}{x} \\binom{N-K}{n-x}}{\\binom{N}{n}}$\n    - 평균: $\\frac{nK}{N}$\n    - 분산: $\\frac{nK(N-K)(N-n)}{N^2(N-1)}$\n- **포아송 분포**: **임의의 기간**동안 **어떤 사건이 간헐적**으로 발생할 때, 동일한 길이의 기간동안 실제 사건이 발생하는 횟수\n    - X ~ Poisson(λ), $f(x) = \\frac{e^{-λ} λ^x}{x!}, λ > 0$\n    - 평균: λ\n    - 분산: λ\n\n### 연속 확률 분포\n\n연속 표본 공간에서 정의 가능\n\n- **균일 분포**\n    - $f(x) = \\frac{1}{b-a}, a ≤ x ≤ b$\n    - 평균: $\\frac{a+b}{2}$\n    - 분산: $\\frac{(b-a)^2}{12}$\n- **정규 분포**\n    - $X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)$\n    - 선형 변환: $Y = aX + b \\sim N(aμ + b, a^2σ^2)$\n- **t 분포**\n    - 자유도가 커질수록 표준 정규분포에 근사함.\n    - $\\frac{Z}{\\sqrt{V/n}} \\sim t(n)$, Z: 표준정규분포, V: 자유도가 n인 카이제곱분포\n- **f 분포**\n    - $F = \\frac{X_1/ν_1}{X_2/ν_2}$, $X_1 \\sim χ^2(ν_1)$, $X_2 \\sim χ^2(ν_2)$, X1과 X2는 서로 독립\n- **감마 분포**\n    - α: 분포의 형태 결정, θ: 분포의 크기 결정\n    - 평균: αθ\n    - 분산: αθ²\n    - **카이제곱 분포**: α = v/2, θ = 2 인 감마분포\n        - $Z_i \\sim N(0,1)$일 때, $Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)$\n        - $X_i$가 서로 독립이고, 자유도가 $ν_i$인 카이제곱분포를 따른다면, $X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)$\n        - 자유도가 커질수록 기댓값을 중심으로 모이고, 대칭에 가까워진다.\n    - **지수 분포**: α = 1, θ = 1/λ 인 감마분포\n        - $X ~ Exp(λ = \\frac{1}{θ})$, f(x) = $λe^{-λx}, x > 0$\n        - θ: 평균 사건 발생 간격, λ: 단위 시간당 사건 발생 횟수\n        - 포아송 분포에서 사건 발생 간격의 분포\n        - $\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)$, $θ = 1/λ$\n        - 비기억 특성을 가진다: $p(X > s + t | X > s) = p(X > t) = e^{-λt}$\n        - 독립적으로 동일한 지수분포를 따르는 확률변수 n개의 합은 $α = n, θ = \\frac{1}{λ}$인 감마분포를 따른다.\n\n### 다변량 분포\n\n- **다항 분포**: n번의 독립적인 **베르누이 시행**을 수행하여 k개의 범주로 분류\n    - X ~ M(n, p1, p2, ..., pk), $f(x_1, x_2, ..., x_k) = \\frac{n!}{x_1! x_2! ... x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}$\n    - 평균: $[np_1, np_2, ..., np_k]$\n    - 분산: $[np_1(1-p_1), np_2(1-p_2), ..., np_k(1-p_k)]$\n    - 공분산: $-np_ip_j (i ≠ j)$\n    - 독립인 변수의 갯수는 k-1개 (k개의 사건)\n\n### 샘플링\n\n### 분포의 동질성 검정\n\n- 연속형\n    - 이표본 검정: 콜모고로프-스미르노프 검정 사용\n    - 일표본 검정:\n        - 정규분포, 지수분포: 앤더슨-달링 검정 사용\n        - 그 외: 몬테카를로 방법 사용\n- 이산형\n    - 이표본: 카이제곱 독립성 검정\n    - 일표본: 카이제곱 동질성 검정\n\n## 표본의 분포\n\n- 샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n\n- 통계량: 표본의 특성을 나타내는 값\n- 추정량: 아래의 조건을 만족하는 통계량\n    - 불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n    - 효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n### 표본 평균의 분포\n\n- 모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 $σ^2$이면, $\\bar{X}$의 평균은 μ이고, 분산은 $σ^2/n$인 정규분포를 따른다.\n    - 단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리[^1])\n- 만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n    - 단 이때는 **모집단이 정규분포를 따라야 한다.**\n\n[^1]: 모집단의 분포와 상관 없이, 표본의 평균은 정규분포에 수렴한다는 정리. 이항분포의 경우, P(X=c) ~ P(c - 0.5 < X < c + 0.5)로 근사 가능하다는 라플라스의 정리를 일반화한 것\n\n### 표본 분산의 분포\n\n- **정규 모집단으로 부터 나온 표본**의 분산 S에 대하여, $\\frac{(n-1)S^2}{σ^2}$은 자유도가 n-1인 카이제곱 분포를 따른다.\n    - 모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n- 두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.\n\n## 추정\n\n- 통계적 추론: **모집단에서 추출된 표본**의 **통계량**으로부터 **모수**를 추론하는 것\n    - 추정\n        - 점추정\n        - 구간추정\n    - 가설 검정\n\n### 점 추정\n\n- 불편성\n    - $E(\\hat{\\theta}) = θ$\n    - bias = $E(\\hat{\\theta}) - \\theta$\n        - 보통 sample size가 커질수록 bias는 0에 수렴\n    - $\\bar{X}, X_n$은 μ의 불편추정량이다.\n- 최소분산\n    - $Var(\\bar{X})$가 $Var(X_n)$보다 분산이 작아서 더 좋은 추정량\n    - $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2$\n        - 큰 오차에 더 큰 페널티를 주기 위해 제곱\n\n![대표적인 불편추정량](img/2025-09-06-20-37-05.png)\n\n- **전부 중심극한의정리를 적용**할 수 있다. (비율은 0과 1의 평균이므로)\n- 모평균, 모비율의 차이는 서로 독립이라는 가정이 필요하다.\n\n### 구간 추정\n\n- α: 유의수준\n- 1 - α: 신뢰수준[^2]\n- (θ_L, θ_U) = (1 - α) × 100% 신뢰구간\n\n1. ($θ_L, θ_U$) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다\n1. 구간이 충분히 좁아야 한다\n    - 표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.\n    - 고로 신뢰구간이 대칭임\n\n[^2]: 샘플링을 무한히 반복했을 때, **이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함**한다. 즉, 구간이 확률 변수이다.\n\n### 표본의 크기 결정\n\n특정 오차 아래로 하는 표본의 수 구하는 법\n\n- 그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.\n- **모비율**을 모를 때는 일단 **0.5로 보수적으로 놓고 계산**\n\n## 모분산 추정\n\n- 카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음\n    - 그냥 쉽게 구하기 위해 $(x^2_{α/2}, x^2_{1-α/2})$를 사용\n- 모분산의 신뢰구간: $(\\frac{(n-1)s^2}{x^2_{(1-\\alpha)/2}(n-1)}, \\frac{(n-1)s^2}{x^2_{\\alpha/2}(n-1)})$\n- 표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.\n\n## 가설 검정\n\n![검정 방법 선택 기준](img/2025-09-07-10-39-35.png)\n\n![모수 검정 방법 선택 기준](img/2025-09-07-10-45-15.png)\n\n- 관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, ...), 각 케이스에 맞는 모형을 사용해야 함.\n\n### 정규성 검정\n\n- 표본이 정규분포를 따르는지 검정.\n- 따르지 않더라도 중심극한정리에 의해 **표본의 크기가 충분히 크면** 모수 검정을 사용할 수 있다.\n\n- **shapiro wilk 검정**: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음\n    - H0: 데이터가 정규분포를 따른다.\n    - H1: 데이터가 정규분포를 따르지 않는다.\n- **jarque-Bera**: 대표본에 사용.\n    - H0: 데이터가 정규분포를 따른다.\n    - H1: 데이터가 정규분포를 따르지 않는다.\n- **Q-Q plot**: x축이 이론적 분위수, y축이 표본 분위수\n\n```{.python filename=\"normality test\"}\nfrom scipy.stats import shapiro, jarque_bera, zscore, probplot\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nstat, p = shapiro(data)\n\nprint(f\"Shapiro-Wilk Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = jarque_bera(data)\nprint(f\"Jarque-Bera Test: stat={stat:.3f}, p={p:.3f}\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nzdata = zscore(data)\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\n\n(osm, odr), (slope, intercept, r)) = probplot(zdata, plot=ax[0])\nax[0].set_title(\"Q-Q Plot\")\n\nsns.histplot(data, kde=True, ax=ax[1])\nax[1].set_title(\"Histogram\")\n\nplt.show()\n```\n\n### 등분산성 검정\n\n- **Barlett 검정**: **정규성을 만족하는 경우에만** 사용 가능\n    - H0: $σ_1^2 = σ_2^2 = ... = σ_k^2$\n    - H1: $σ_i ≠ σ_j$ for some i, j\n- **Levene 검정**: **정규성을 만족하지 않는 경우에도** 사용 가능\n    - H0: $σ_1^2 = σ_2^2 = ... = σ_k^2$\n    - H1: $σ_i ≠ σ_j$ for some i, j\n\n```{.python filename=\"equal variance test\"}\nfrom scipy.stats import bartlett, levene\n\ngroup1 = [1, 2, 3, 4, 5]\ngroup2 = [2, 3, 4, 5, 6]\ngroup3 = [3, 4, 5, 6, 7]\n\nstat, p = bartlett(group1, group2, group3)\nprint(f\"Bartlett's Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = levene(group1, group2, group3)\nprint(f\"Levene's Test: stat={stat:.3f}, p={p:.3f}\")\n```\n\n# 분산분석\n\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\multirow{2}{*}{\\textbf{표본}} & \\multirow{2}{*}{\\textbf{개수}} & \\multicolumn{2}{c|}{\\textbf{비모수 검정}} & \\multicolumn{2}{c|}{\\textbf{모수 검정}} \\\\ \\cline{3-6}\n & & \\textbf{서열척도} & \\textbf{명목척도} & \\textbf{등분산성 o} & \\textbf{등분산성 x} \\\\ \\hline\n\\textbf{단일 표본} & 1개 & \\makecell{부호검정, \\\\ 부호순위검정} & \\makecell{적합성 검정, \\\\ Run 검정} & \\multicolumn{2}{c|}{일표본 t-검정} \\\\ \\hline\n\\multirow{2}{*}{\\textbf{대응 표본}} & 2개 & \\makecell{부호검정, \\\\ 부호순위검정} & McNemar 검정 & \\multicolumn{2}{c|}{대응표본 t-검정} \\\\ \\cline{2-6}\n & K개 & Friedman 검정 & Cochran Q 검정 & \\multicolumn{2}{c|}{반복측정 분산분석} \\\\ \\hline\n\\multirow{2}{*}{\\textbf{독립 표본}} & 2개 & \\makecell{순위합 검정, \\\\ 만위트니U 검정} & \\multirow{2}{*}{\\makecell{독립성 검정, \\\\ 동질성 검정}} & 독립표본 t-검정 & Welch's t-검정 \\\\ \\cline{2-3} \\cline{5-6}\n & K개 & Kruskal-Wallis 검정 & & 일원배치 분산분석 & Welch's ANOVA \\\\ \\hline\n\\end{tabular}\n\n# 회귀분석\n\n## 회귀분석을 위한 가정\n\n- **선형성**: 종속변수와 독립변수 간의 관계는 선형이다.\n- **정규성**: 종속변수 **잔차들의 분포**는 정규분포이다.\n- **등분산성**: 종속변수 **잔차들의 분포**는 동일한 분산을 갖는다.\n- **독립성**: 모든 **잔차값**은 서로 독립이다.\n\n### 검정\n\n- 영향치 처리: 레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차\n    - Cook's distance\n    - DFBETAS\n    - DFFITS\n    - Leverage\n- 다중공산성 검정\n- 선형성 검정\n    - 잔차도 검정: 잔차 vs 예측값 산점도\n    - 잔차도가 어떠한 패턴도 보이지 않아야 한다.\n- 정규성 검정: 분산분석이랑 동일\n- 등분산성 검정: \n    - 잔차도 검정: 잔차 vs 예측값 산점도\n    - 잔차도가 일정한 폭을 가져야 한다.\n- 독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다..\n    - 더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.\n\n```{.python filename=\"linear regression test\"}\nimpot statsmodels.api as sm\n\nXc = sm.add_constant(X)\nmodel = sm.OLS(y, Xc).fit()\nresid = model.resid\n\nprint(model.summary())\n```\n\n## 전처리\n\n1. 범주형 변수 처리:\n    - 더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현\n        - 각 범주의 회귀계수는 기준 범주와의 차이를 의미\n1. 이상치 / 영향점: 관측값 제거\n1. 선형성 위반: 독립변수 변환, GAM\n1. 정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM\n1. 다중공산성 위반: 다중공산성 파트 참고\n    - 혹은 변수 선택법을 사용\n\n- 가정 만족할 때까지 검정, 전처리 계속 반복\n\n### 변수 변환\n\n![회귀 모델 수정 λ](img/2025-09-20-18-38-36.png)\n\n![경험적인 적절한 λ](img/2025-09-20-18-55-55.png)\n\n- 최적의 λ는 최대 우도 추정법으로 구할 수 있다.\n- 변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.\n- 일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.\n    - 아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.\n    - 통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.\n\n```{.python filename=\"box cox transformation\"}\nfrom scipy.stats import boxcox\n\ny_transformed, best_lambda = boxcox(y)\nprint(f\"Best lambda: {best_lambda:.3f}\")\n```\n\n### 변수 선택법\n\n- 전진 선택법\n- 후진 선택법\n- 단계적 선택법\n- 최적조합 선택법: 모든 조합 다 해봄\n- 기준\n    - R2, Adj R2\n    - AIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법\n    - BIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌\n    - Mallows' Cp\n\n## 종류\n\n- 최대한 단순한 모델을 사용하는 것이 일반화에 좋다.\n\n### 단순\n\n- 그냥 선형 회귀\n\n### 규제 선형 회귀\n\n- 지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦\n- 독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)\n    - 일반적으로는 scale을 하든 안하든 r square에 차이가 없다.\n\n1. 릿지회귀\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma β_j^2$\n    - 회귀계수 절댓값을 0에 가깝게 함\n    - 하지만 0으로 만들지는 않음\n    - 작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.\n    - 회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음\n1. 라쏘회귀:\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma |β_j|$\n    - 회귀계수를 0으로 만들 수 있음\n    - 변수 선택 효과\n    - 릿지보다 해석이 쉬움\n    - 일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음\n1. 엘라스틱넷 회귀\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ_1\\Sigma |β_j| + λ_2\\Sigma β_j^2$\n    - 릿지와 라쏘의 장점을 모두 가짐\n    - 변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦\n    - 독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음\n\n### 일반화 선형 회귀(GLM)\n\n- 종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우\n    - 이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.\n    - 포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.\n    - 따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.\n- 종속변수가 범주형일 경우: logistic 회귀 분석\n    - $z = β_0 + β_1 x_1 + β_2 x_2 + ... + β_n x_n$\n    - $p = \\frac{1}{1 + e^{-z}}$\n    - 오즈: $\\frac{p}{1-p}$ = $e^z$\n    - 오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율\n        - $(e^{β_k})^k$\n\n- 종속변수가 count 데이터일 경우: 포아송 회귀 분석\n\n### 이상치에 강한 선형 회귀\n\n- Robust 회귀\n- Quantile 회귀\n\n# 상관분석\n\n## 상관계수\n\n- 두 변수 간의 선형적 관계의 강도와 방향을 나타내는 척도\n\n### 질적 변수\n\n- **스피어만** 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.\n- **켄달의 타우**: 서열척도 vs 서열척도.\n    - 둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.\n    - 샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.\n    - **두 변수의 크기는 같아야함**.\n- phi 계수: 명목척도 vs 명목척도\n    - 두 변인 모두 level이 2개일 때 사용\n    - 두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산\n- 크래머 v: 명목척도 vs 명목척도.\n    - 적어도 하나의 변수가 3개 이상의 level을 가지면 사용\n    - 범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.\n\n### 양적 변수\n\n- 피어슨 상관계수: 연속형 vs 연속형\n    - 두 변수 간의 선형적 관계를 측정\n    - -1 ~ 1 사이의 값\n    - 0: 독립, 1: 완전한 양의 상관관계, -1: 완전한 음의 상관관계\n    - 이상치에 민감\n\n## 다중 공산성\n\n- 독립 변수 집합 X 가 본래는 서로 독립적(직교적)이어야 한다는 가정에서 벗어난 정도.\n\n## 문제\n\n- 회귀계수 **추정치의 분산이 커지고**, 결과적으로 **추정이 매우 불안정**해짐.\n- 특정 독립변수가 설명하는 **효과**를 다른 변수와 **구분하기 어려워짐**.\n- 예측력이나 설명력이 높게 나올 수 있지만, **모형의 구조적 해석은 신뢰할 수 어려움**.\n- 물론 이런 문제들은 **중요한 변수**에 영향을 줄 때 생김.\n\n## 해결 방법\n\n### 잘못된 예시\n\n1. 그냥 둔다\n1. 직교화\n    - PCA(주성분 분석)나 요인분석 등을 사용하여 독립변수들을 직교화\n    - 요인이 해석 불가능한 경우가 아니면 좋지 않음\n1. 규칙 기반 접근: 상관계수가 0.8이 넘는걸 제거하거나, 종속변수의 상관계수보다 높은 변수 제거\n    - 직관에만 의존하고, 잘못된 결론을 낳을 수 있음\n\n### 좋은 예시\n\n다중 공산성의 문제를 세부적으로 진단하고 각각에 대해 해결 방법을 다음과 같이 제시한다.\n\n1. 전 변수 집합 대상:\n    - 독립변수의 전체 차원이 부족한 경우\n    - 표본을 더 모으거나 새로운 변수를 도입\n1. 개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)\n    - 특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우\n    - **VIF가 10**을 넘을 경우\n    - 덜 중요하다면 제거\n    - 중요하다면 변수에 대한 독립적 정보 보강(세분화, ...)\n1. 두 변수의 상관계수가 높은 경우\n    - 둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)\n    - 둘 중 하나를 제거\n\n```{.python filename=\"multicollinearity test\"}\ncor = df.corr()\ncond_num = np.linalg.cond(cor)\nprint(\"Condition Number:\", cond_num)\n```\n\n- **30을 초과**하면 다중공선성이 높다고 판단한다.\n- 선형 종속 가능성을 봄.\n- scaling이 선행되어야 함.\n\n```{.python filename=\"VIF test\"}\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef check_vif(X, y):\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    model.fit()\n    vif_df = pd.DataFrame(columns=['feature', 'VIF'])\n    for i in range(1, len(model.exog_names)):\n        vif_df.loc[i, 'feature'] = model.exog_names[i]\n        vif_df.loc[i, 'VIF'] = variance_inflation_factor(model.exog, i)\n    return vif_df.sort_values(by='VIF', ascending=False)\n\nprint(check_vif(X, y))\n```\n\n```{.python filename=\"correlation heatmap\"}\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(cor, annot=True, ax=ax)\n```\n\n# 시계열 분석\n\n# 베이지안 통계\n\n# 머신러닝\n\n## 전처리\n\n### 결측치 처리\n\n- 결측치가 발생하는 원인\n    - **무작위 결측**(Missing Completely at Random, MCAR): 결측치가 발생할 확률이 관측된 데이터와 무관\n        - 이 경우, 결측치를 제거하거나 대체해도 무방\n    - **조건부 무작위 결측**(Missing at Random, MAR): 결측치가 발생할 확률이 관측된 데이터에만 의존\n        - 이 경우, 관측된 데이터로 결측치를 예측하여 대체하는 방법이 유용\n    - **비무작위 결측**(Missing Not at Random, MNAR): 결측치가 발생할 확률이 관측된 데이터와도 관련이 없음\n        - 통계적 방법으로 해결하기 어려움\n\n- 결측치 처리 방법\n    - 제거: 결측치가 적거나 무작위 결측일 때 사용\n    - 대치(대체):\n        - 일반적인 방법\n            - 시계열 데이터 o: 이전 값, 이후 값, 선형 보간법\n            - 시계열 데이터 x: 평균, 중앙값, 최빈값\n        - 고급 대치법(과적합 발생 가능성 유의)\n            - KNN 대치: 유사한 관측치의 값을 사용하여 결측치를 대체. 결측치가 없는 데이터로 예측\n            - 다중 대치: 결측치를 여러 번 대체하여 불확실성을 반영\n\n```{.python filename=\"imputer\"}\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n# KNN \nimputer = KNNImputer(n_neighbors=5)\nimputed_X = imputer.fit_transform(X)\n\n# MICE\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimputer = IterativeImputer(max_iter=10)\nimputed_X = imputer.fit_transform(X)\n```\n\n### 이상치 처리\n\n\n### 불균형 처리\n\n## ensemble\n\n- stacking:\n    - 여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.\n    - 메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.\n- voting, averaging:\n    - 여러 모델을 학습시킨 후, 각 모델의 예측 결과를 투표(voting)하거나 평균(averaging)하여 최종 예측을 수행한다.\n    - 분류 문제에서는 다수결 투표(hard voting) 또는 확률 평균(soft voting)을 사용하고, 회귀 문제에서는 단순 평균 또는 가중 평균을 사용한다.\n- bagging\n    - vs cross validation:\n        - cross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님\n        - bagging은 분산을 줄이기 위해 사용함\n    - **bagging의 voting, averaging은 unsupervised learning**\n- boosting: sequentially 학습\n    - 이전 모델의 오차를 보완하는 방식으로 학습한다.\n\n```{.python filename=\"voting\"}\n#| eval: false\n\nfrom sklearn.ensemble import VotingClassifier, VotingRegressor\n\nvoting_lf = VotingClassifier(estimators=[\n    ('lr', LogisticRegression()),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier())\n], voting='soft') # or hard\n\n\nvoting_rf = VotingRegressor(estimators=[\n    ('lr', LinearRegression()),\n    ('dt', DecisionTreeRegressor()),\n    ('rf', RandomForestRegressor())\n], weights=[1, 1, 2])\n\n\nvoting_lf.fit(X_train, y_train)\nvoting_rf.fit(X_train, y_train)\nvoting_lf.predict(X_test)\nvoting_rf.predict(X_test)\n```\n\n## 군집분석\n\n### 전제 조건\n\n1. scalability\n1. 다양한 타입의 속성을 처리해야 함\n    - k-means는 수치형만 처리 가능\n1. 인위적인 형상의 군집도 발견할 수 있어야 함\n    - k-means는 non-convex 형태는 잘 못찾음\n1. 파라미터 설정에 전문지식을 요하지 않아야함\n1. noise와 outliers를 처리해야 함\n1. 데이터가 입력되는 순서에 민감하면 안됨\n1. 차원 수가 높아도 잘 처리할 수 있어야함\n1. 사용자 정의 제약조건도 수용할 수 있어야함\n1. 해석과 사용이 용이해야함\n1. scaling, one hot encoding 등의 전처리가 필요하다.\n\n```\n### model\n\n- Distance-based methods\n  - Partitioning methods\n    - k-means:\n      - polinominal 시간 안에 해결 가능\n      - noise, outlier에 민감함\n      - 수치형만 처리 가능\n      - non-convex 형태는 잘 못찾음\n    - k-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\n    - k-prototype: 범주형, 수치형 섞인거 처리 가능\n    - k-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n      - PAM: Partitioning Around Medoids\n        - scalability 문제 있음\n      - CLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n        - 샘플링 과정에서 biased될 수 있음\n      - CLARANS: medoid 후보를 랜덤하게 선택함\n    - k-means++: 초기 centroids를 더 잘 잡음\n  - Hierarchical methods\n    - top-down: divisive, dia\n    - bottom-up: agglomerative\n        - ward's distance: 군집 간의 거리 계산을 군집 내의 분산을 최소화하는 방식으로 계산\n            - ESS: 각 군집의 중심으로 부터의 거리 제곱합\n- Density-based methods\n    - 다양한 모양의 군집을 찾을 수 있음\n    - noise, outlier에 강함\n    - DBSCAN: 잡음 포인트는 군집에서 제외\n        1. core point를 찾음(eps 이내에 minPts 이상 있는 점)\n        1. core point를 중심으로 군집을 확장\n            - core point가 아닌 경우 확장 종료\n        - 고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n        - 군집간 계층관계를 인식하기 어렵다\n    - OPTICS: DBSCAN의 단점을 보완\n        - 군집의 밀도가 다를 때도 잘 처리함\n        - 군집의 계층 구조를 인식할 수 있음\n        - eps, minPts 파라미터가 필요함\n- Grid-based methods: 대표만(각 grid를 대표) 가지고 군집분석 하는거\n    - 속도와 메모리 측면에서 효율적\n- Model-based clustering methods\n\n- 거리기반 군집의 단점:\n    - 군집의 모양이 구형이 아닐 경우 찾기 어려움\n    - 군집의 갯수 결정하기 어려움\n    - 군집의 밀도가 높아야함\n\n### 평가\n\n- silhuette score: $\\frac{\\sum_{i=1}^{n} s(i)}{n}$\n  - s(i): $\\frac{b(i) - a(i)}{max((a(i), b(i)))}$\n    - a(i): 군집 내 노드간의 평균 거리\n    - b(i): 가장 가까운 군집과의 노드 간 평균 거리\n  - 1에 가까울 수록 좋음\n```\n\n# 생존분석\n\n\n\n# OR\n\n- pulp 이용해서 푼다.\n- 제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯\n\n```{.python filename=\"pulp example\"}\nimport pulp\n\nprob = pulp.LpProblem(\"Problem_name\", pulp.LpMinimize) # 문제에 맞게 설정\n\n# 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)\nx_A1 = pulp.LpVariable(\"A_to_1\", 0, None, pulp.LpInteger)\nx_A2 = pulp.LpVariable(\"A_to_2\", 0, None, pulp.LpInteger)\nx_A3 = pulp.LpVariable(\"A_to_3\", 0, None, pulp.LpInteger)\nx_B1 = pulp.LpVariable(\"B_to_1\", 0, None, pulp.LpInteger)\nx_B2 = pulp.LpVariable(\"B_to_2\", 0, None, pulp.LpInteger)\nx_B3 = pulp.LpVariable(\"B_to_3\", 0, None, pulp.LpInteger)\n\n# 3. 목표 함수 정의 (총 운송 비용)\nprob += 2*x_A1 + 4*x_A2 + 5*x_A3 + 3*x_B1 + 1*x_B2 + 6*x_B3, \"obj_name\"\n\n# 4. 제약 조건 정의\nprob += x_A1 + x_A2 + x_A3 <= 100, \"Factory_A_Supply\"\nprob += x_B1 + x_B2 + x_B3 <= 200, \"Factory_B_Supply\"\nprob += x_A1 + x_B1 == 80, \"Warehouse_1_Demand\"\nprob += x_A2 + x_B2 == 90, \"Warehouse_2_Demand\"\nprob += x_A3 + x_B3 == 130, \"Warehouse_3_Demand\"\n\n# 5. 문제 풀이\nprob.solve()\n\n# 6. 결과 확인\nprint(\"Status:\", pulp.LpStatus[prob.status])\nprint(\"\\nOptimal Transportation Plan:\")\nfor v in prob.variables():\n    print(v.name, \"=\", v.varValue)\n\nprint(\"\\nTotal Minimum Cost = \", pulp.value(prob.objective))\n```\n\n",
    "supporting": [
      "00_files"
    ],
    "filters": []
  }
}