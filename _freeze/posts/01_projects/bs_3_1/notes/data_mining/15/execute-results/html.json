{
  "hash": "8b8887e1b79265c5bee9d56cfdad265f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"analysis\"\ndate: 2025-05-22\ncategories: [\"data mining\"]\n---\n\n\n\n\n![](/img/human-thumb.jpg){.post-thumbnail}\n\n## 데이터 load\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(survey)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: grid\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoading required package: survival\n\nAttaching package: 'survey'\n\nThe following object is masked from 'package:graphics':\n\n    dotchart\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(semPlot)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:survival':\n\n    cluster\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1234)\n\nX_train <- read.csv(\"_data/train_data.csv\")\nX_test <- read.csv(\"_data/test_data.csv\")\ny_train <- factor(X_train$y)\ny_test <- factor(X_test$y)\nweights_train <- X_train$weights\nweights_test <- X_test$weights\nX_train <- X_train %>% select(-y, -weights)\nX_test <- X_test %>% select(-y, -weights)\n```\n:::\n\n\n\n\n## 무작위 분류기 (Random Classifier)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 훈련 데이터에서 클래스 비율 계산\nclass_proportions <- table(y_train) / length(y_train)\n\n# 테스트 데이터에 대한 무작위 예측 생성\n# 클래스 비율에 맞게 무작위로 샘플링\nrandom_predictions <- sample(levels(y_test), \n                            size = length(y_test), \n                            replace = TRUE, \n                            prob = class_proportions)\nrandom_predictions <- factor(random_predictions, levels = levels(y_test))\n\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_random <- matrix(0,\n                                          nrow = length(levels(y_test)),\n                                          ncol = length(levels(y_test)),\n                                          dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat <- as.character(y_test[i])\n  predicted_cat <- as.character(random_predictions[i])\n  weight <- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_random[actual_cat, predicted_cat] <- weighted_confusion_matrix_random[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== 무작위 분류기 가중 혼동 행렬 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 무작위 분류기 가중 혼동 행렬 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_confusion_matrix_random, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Predicted\nActual      active  passive\n  active  57687.49 58084.02\n  passive 30544.02 28027.14\n```\n\n\n:::\n\n```{.r .cell-code}\n# 무작위 분류기 가중 성능 지표 계산\ntotal_weighted_sum_random <- sum(weighted_confusion_matrix_random)\nweighted_accuracy_random <- sum(diag(weighted_confusion_matrix_random)) / total_weighted_sum_random\ncat(\"\\n=== 무작위 분류기 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 무작위 분류기 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"가중 정확도:\", round(weighted_accuracy_random, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n가중 정확도: 0.4916 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_random <- numeric(length(levels(y_test)))\nweighted_recall_random <- numeric(length(levels(y_test)))\nweighted_f1_score_random <- numeric(length(levels(y_test)))\nnames(weighted_precision_random) <- names(weighted_recall_random) <- names(weighted_f1_score_random) <- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP <- weighted_confusion_matrix_random[cat, cat]\n  FP <- sum(weighted_confusion_matrix_random[, cat]) - TP\n  FN <- sum(weighted_confusion_matrix_random[cat, ]) - TP\n\n  weighted_precision_random[cat] <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_random[cat] <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_random[cat] <- ifelse((weighted_precision_random[cat] + weighted_recall_random[cat]) == 0, 0,\n                                        2 * (weighted_precision_random[cat] * weighted_recall_random[cat]) / (weighted_precision_random[cat] + weighted_recall_random[cat]))\n}\n\ncat(\"\\n무작위 분류기 범주별 가중 정밀도:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n무작위 분류기 범주별 가중 정밀도:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_precision_random, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.6538  0.3255 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n무작위 분류기 범주별 가중 재현율:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n무작위 분류기 범주별 가중 재현율:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_recall_random, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.4983  0.4785 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n무작위 분류기 범주별 가중 F1-score:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n무작위 분류기 범주별 가중 F1-score:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_f1_score_random, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.5656  0.3874 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Random Forest 모델 학습 및 평가\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- randomForest(\n  x = X_train,\n  y = y_train,\n  weights = weights_train,\n  ntree = 500,\n  mtry = floor(sqrt(ncol(X_train))),\n  importance = TRUE,\n  sampsize = table(y_train),\n  replace = TRUE\n)\nvarImpPlot(rf_model)\n```\n\n::: {.cell-output-display}\n![](15_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ny_pred <- predict(rf_model, X_test)\nactual_classes <- y_test\npredicted_classes <- y_pred\ntest_weights <- weights_test\noutcome_levels <- levels(actual_classes)\nweighted_confusion_matrix <- matrix(0,\n                                   nrow = length(outcome_levels),\n                                   ncol = length(outcome_levels),\n                                   dimnames = list(Actual = outcome_levels, Predicted = outcome_levels))\nfor (i in 1:length(actual_classes)) {\n  actual_cat <- as.character(actual_classes[i])\n  predicted_cat <- as.character(predicted_classes[i])\n  weight <- test_weights[i]\n  weighted_confusion_matrix[actual_cat, predicted_cat] <- weighted_confusion_matrix[actual_cat, predicted_cat] + weight\n}\nprint(round(weighted_confusion_matrix, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Predicted\nActual      active  passive\n  active  89806.77 25964.74\n  passive 37069.58 21501.58\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal_weighted_sum <- sum(weighted_confusion_matrix)\nweighted_accuracy <- sum(diag(weighted_confusion_matrix)) / total_weighted_sum\ncat(\"\\n=== 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"가중 정확도:\", round(weighted_accuracy, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n가중 정확도: 0.6384 \n```\n\n\n:::\n\n```{.r .cell-code}\nweighted_precision <- numeric(length(outcome_levels))\nweighted_recall <- numeric(length(outcome_levels))\nweighted_f1_score <- numeric(length(outcome_levels))\nnames(weighted_precision) <- names(weighted_recall) <- names(weighted_f1_score) <- outcome_levels\n\nfor (cat in outcome_levels) {\n  TP <- weighted_confusion_matrix[cat, cat]\n  FP <- sum(weighted_confusion_matrix[, cat]) - TP # 해당 열의 합 - TP\n  FN <- sum(weighted_confusion_matrix[cat, ]) - TP # 해당 행의 합 - TP\n  weighted_precision[cat] <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall[cat] <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score[cat] <- ifelse((weighted_precision[cat] + weighted_recall[cat]) == 0, 0,\n                                    2 * (weighted_precision[cat] * weighted_recall[cat]) / (weighted_precision[cat] + weighted_recall[cat]))\n}\ncat(\"\\n범주별 가중 정밀도:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n범주별 가중 정밀도:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_precision, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7078  0.4530 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n범주별 가중 재현율:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n범주별 가중 재현율:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_recall, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7757  0.3671 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n범주별 가중 F1-score:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n범주별 가중 F1-score:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_f1_score, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7402  0.4055 \n```\n\n\n:::\n\n```{.r .cell-code}\ny_pred_prob <- predict(rf_model, X_test, type = \"prob\")\ncat(\"\\n비가중 ROC 곡선 (참고용):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n비가중 ROC 곡선 (참고용):\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_roc <- function() {\n  num_classes <- length(levels(y_test))\n  if (num_classes <= 4 && num_classes > 0) { # 범주가 1개 이하이면 플롯 불가능\n    par(mfrow = c(2, ceiling(num_classes/2)))\n  } else if (num_classes > 4) {\n    par(mfrow = c(2, 2)) # 범주가 많으면 일부만 표시하거나 레이아웃 조정 필요\n    warning(\"범주가 4개 이상입니다. 일부 ROC 곡선만 표시될 수 있습니다. 레이아웃을 조정하거나 플롯 코드 를 수정하세요.\")\n  } else {\n    cat(\"ROC 곡선을 그릴 범주가 부족합니다.\\n\")\n    return(numeric(0)) # 빈 numeric 반환\n  }\n  auc_values <- numeric(num_classes)\n  names(auc_values) <- levels(y_test)\n  for (i in 1:num_classes) {\n    class_label <- levels(y_test)[i]\n    if(class_label %in% unique(y_test) && class_label %in% colnames(y_pred_prob)) {\n      roc_obj <- roc(response = ifelse(y_test == class_label, 1, 0), predictor = y_pred_prob[, class_label])\n      auc_values[i] <- auc(roc_obj)\n      plot(roc_obj, main = paste(\"ROC for\", class_label, \"(Unweighted)\"), col = \"blue\", lwd = 2)\n      abline(a = 0, b = 1, lty = 2, col = \"gray\")\n      text(0.5, 0.3, paste(\"AUC =\", round(auc_values[i], 3)), col = \"red\")\n      } else {\n        cat(\"클래스\", class_label, \"에 대한 ROC 곡선을 그릴 수 없습니다 (데이터 부족).\\n\")\n        auc_values[i] <- NA # AUC 값에 NA 할당\n        plot.new() # 빈 플롯 생성\n        text(0.5, 0.5, paste(\"No ROC for\", class_label))\n      }\n  }\n  par(mfrow = c(1, 1))\n  return(auc_values)\n}\nauc_values_unweighted <- plot_roc()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](15_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\ncat(\"\\n각 클래스별 비가중 AUC:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n각 클래스별 비가중 AUC:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.frame(Class = levels(y_test), AUC_Unweighted = auc_values_unweighted))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Class AUC_Unweighted\nactive   active      0.6220194\npassive passive      0.6220194\n```\n\n\n:::\n:::\n\n\n\n\n## 모델 튜닝\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(\n  method = \"cv\",\n  number = 5,\n  savePredictions = \"final\",\n  classProbs = TRUE,\n  summaryFunction = multiClassSummary\n)\nparam_grid <- expand.grid(\n  mtry = floor(sqrt(ncol(X_train)))\n)\nif (ncol(X_train) == 1) {\n  param_grid <- expand.grid(mtry = 1)\n}\nrf_tuned <- train(\n  x = X_train,\n  y = y_train,\n  method = \"rf\",\n  metric = \"Accuracy\",\n  weights = weights_train,\n  trControl = ctrl,\n  tuneGrid = param_grid,\n  importance = TRUE,\n  ntree = 500\n)\nprint(rf_tuned)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n3412 samples\n  51 predictor\n   2 classes: 'active', 'passive' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 2730, 2730, 2730, 2729, 2729 \nResampling results:\n\n  logLoss    AUC       prAUC      Accuracy   Kappa      F1         Sensitivity\n  0.6219635  0.740729  0.7297194  0.6787714  0.3564413  0.6590399  0.6340299  \n  Specificity  Pos_Pred_Value  Neg_Pred_Value  Precision  Recall   \n  0.7219467    0.6875098       0.6726023       0.6875098  0.6340299\n  Detection_Rate  Balanced_Accuracy\n  0.311242        0.6779883        \n\nTuning parameter 'mtry' was held constant at a value of 7\n```\n\n\n:::\n\n```{.r .cell-code}\ny_pred_tuned <- predict(rf_tuned, X_test)\nactual_classes_tuned <- y_test\npredicted_classes_tuned <- y_pred_tuned\ntest_weights_tuned <- weights_test\nweighted_confusion_matrix_tuned <- matrix(0,\n                                         nrow = length(levels(actual_classes_tuned)),\n                                         ncol = length(levels(actual_classes_tuned)),\n                                         dimnames = list(Actual = levels(actual_classes_tuned), Predicted = levels(actual_classes_tuned)))\n\nfor (i in 1:length(actual_classes_tuned)) {\n  actual_cat <- as.character(actual_classes_tuned[i])\n  predicted_cat <- as.character(predicted_classes_tuned[i])\n  weight <- test_weights_tuned[i]\n   if (actual_cat %in% levels(actual_classes_tuned) && predicted_cat %in% levels(actual_classes_tuned)) {\n      weighted_confusion_matrix_tuned[actual_cat, predicted_cat] <- weighted_confusion_matrix_tuned[actual_cat, predicted_cat] + weight\n  } else {\n      warning(paste(\"튜닝 모델: 유효하지 않은 범주 발견: 실제 =\", actual_cat, \", 예측 =\", predicted_cat, \"인 개체 (인덱스:\", i, \")\"))\n  }\n}\ncat(\"\\n=== 튜닝된 모델 가중 혼동 행렬 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 튜닝된 모델 가중 혼동 행렬 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_confusion_matrix_tuned, 2)) # 보기 좋게 반올림하여 출력\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Predicted\nActual      active  passive\n  active  89635.01 26136.50\n  passive 37763.88 20807.29\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal_weighted_sum_tuned <- sum(weighted_confusion_matrix_tuned)\nweighted_accuracy_tuned <- sum(diag(weighted_confusion_matrix_tuned)) / total_weighted_sum_tuned\ncat(\"\\n=== 튜닝된 모델 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 튜닝된 모델 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"가중 정확도:\", round(weighted_accuracy_tuned, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n가중 정확도: 0.6335 \n```\n\n\n:::\n\n```{.r .cell-code}\nweighted_precision_tuned <- numeric(length(levels(actual_classes_tuned)))\nweighted_recall_tuned <- numeric(length(levels(actual_classes_tuned)))\nweighted_f1_score_tuned <- numeric(length(levels(actual_classes_tuned)))\nnames(weighted_precision_tuned) <- names(weighted_recall_tuned) <- names(weighted_f1_score_tuned) <- levels(actual_classes_tuned)\nfor (cat in levels(actual_classes_tuned)) {\n  TP <- weighted_confusion_matrix_tuned[cat, cat]\n  FP <- sum(weighted_confusion_matrix_tuned[, cat]) - TP\n  FN <- sum(weighted_confusion_matrix_tuned[cat, ]) - TP\n\n  weighted_precision_tuned[cat] <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_tuned[cat] <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_tuned[cat] <- ifelse((weighted_precision_tuned[cat] + weighted_recall_tuned[cat]) == 0, 0,\n                                    2 * (weighted_precision_tuned[cat] * weighted_recall_tuned[cat]) / (weighted_precision_tuned[cat] + weighted_recall_tuned[cat]))\n}\n\ncat(\"\\n튜닝된 모델 범주별 가중 정밀도:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n튜닝된 모델 범주별 가중 정밀도:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_precision_tuned, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7036  0.4432 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n튜닝된 모델 범주별 가중 재현율:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n튜닝된 모델 범주별 가중 재현율:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_recall_tuned, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7742  0.3552 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n튜닝된 모델 범주별 가중 F1-score:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n튜닝된 모델 범주별 가중 F1-score:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_f1_score_tuned, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7372  0.3944 \n```\n\n\n:::\n\n```{.r .cell-code}\nimportance_obj_tuned <- varImp(rf_tuned)\n\nif(\"importance\" %in% names(importance_obj_tuned)) {\n  imp_df_tuned <- importance_obj_tuned$importance\n} else {\n  imp_df_tuned <- importance_obj_tuned\n}\n\nvar_names_tuned <- rownames(imp_df_tuned)\nif(\"Overall\" %in% colnames(imp_df_tuned)) {\n  var_importance_tuned <- imp_df_tuned$Overall\n} else {\n  var_importance_tuned <- imp_df_tuned[,1]\n}\n\nimportance_df_tuned <- data.frame(\n  Variable = var_names_tuned,\n  Importance = var_importance_tuned\n)\nimportance_df_tuned <- importance_df_tuned[order(importance_df_tuned$Importance, decreasing = TRUE),]\n\n# 상위 10개 변수 출력 (튜닝된 모델)\ntop_vars_tuned <- head(importance_df_tuned$Variable, 10)\ncat(\"\\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n튜닝된 모델에서 가장 중요한 변수 상위 10개:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(top_vars_tuned)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"parent_monitoring_w5\" \"parent_attachment_w5\" \"friend_stress_w4\"    \n [4] \"deviant_esteem_w3\"    \"academic_stress_w4\"   \"friend_stress_w2\"    \n [7] \"parent_monitoring_w3\" \"parent_monitoring_w2\" \"parent_attachment_w4\"\n[10] \"parent_attachment_w2\"\n```\n\n\n:::\n:::\n\n\n\n\n## XGBoost 모델\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# XGBoost 모델 학습 및 평가\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'xgboost'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n\n\n:::\n\n```{.r .cell-code}\n# 데이터 변환 (XGBoost는 DMatrix 형식을 사용)\n# 먼저 factor를 수치형으로 변환\ny_train_numeric <- as.integer(y_train) - 1  # 0부터 시작하는 인덱스로 변환\ny_test_numeric <- as.integer(y_test) - 1\n\n# XGBoost DMatrix 생성\ndtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train_numeric, weight = weights_train)\ndtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test_numeric, weight = weights_test)\n\n# 모델 파라미터 설정\nxgb_params <- list(\n  objective = \"multi:softprob\",\n  eval_metric = \"mlogloss\",\n  num_class = length(levels(y_train)),\n  eta = 0.3,               # 학습률\n  max_depth = 6,           # 트리 최대 깊이\n  min_child_weight = 1,    # 최소 자식 노드 가중치 합\n  subsample = 0.8,         # 샘플링 비율\n  colsample_bytree = 0.8   # 특성 샘플링 비율\n)\n\n# XGBoost 모델 학습\ncat(\"\\nXGBoost 모델 학습 중...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 모델 학습 중...\n```\n\n\n:::\n\n```{.r .cell-code}\nxgb_model <- xgb.train(\n  params = xgb_params,\n  data = dtrain,\n  nrounds = 100,           # 부스팅 반복 횟수\n  verbose = 0\n)\ncat(\"XGBoost 모델 학습 완료.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nXGBoost 모델 학습 완료.\n```\n\n\n:::\n\n```{.r .cell-code}\n# 변수 중요도 확인\nxgb_importance <- xgb.importance(model = xgb_model)\nif (nrow(xgb_importance) > 0) {\n  cat(\"\\nXGBoost 변수 중요도 상위 10개:\\n\")\n  print(head(xgb_importance, 10))\n  xgb.plot.importance(xgb_importance, top_n = 10)\n} else {\n  cat(\"\\nXGBoost 변수 중요도를 계산할 수 없습니다.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 변수 중요도 상위 10개:\n                        Feature       Gain      Cover  Frequency\n                         <char>      <num>      <num>      <num>\n 1:        parent_monitoring_w5 0.03182668 0.03658157 0.02176108\n 2:            friend_stress_w4 0.03088689 0.04049583 0.02153674\n 3:        parent_attachment_w5 0.02946235 0.02678660 0.01940550\n 4: higher_school_dependence_w5 0.02714116 0.02699829 0.02007852\n 5:            desire_stress_w1 0.02705206 0.02603461 0.02602356\n 6:        parent_attachment_w4 0.02674024 0.02234054 0.02097588\n 7:          academic_stress_w3 0.02657457 0.03015163 0.01985418\n 8:          academic_stress_w4 0.02626054 0.03414376 0.01962984\n 9:               neg_esteem_w1 0.02609752 0.02822353 0.02288278\n10:            friend_stress_w2 0.02527457 0.02162330 0.02153674\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](15_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# XGBoost 모델로 테스트 세트 예측\ncat(\"\\nXGBoost 모델로 테스트 세트 예측 중...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 모델로 테스트 세트 예측 중...\n```\n\n\n:::\n\n```{.r .cell-code}\nxgb_pred_probs <- predict(xgb_model, dtest)\n# 예측 확률을 행렬로 변환 (각 클래스별 확률값)\nxgb_pred_probs_matrix <- matrix(xgb_pred_probs, nrow = length(y_test), byrow = TRUE)\n# 가장 높은 확률을 가진 클래스를 예측값으로 선택\ny_pred_xgb_idx <- apply(xgb_pred_probs_matrix, 1, which.max) - 1  # 0-based 인덱스\n# 다시 factor로 변환\ny_pred_xgb <- factor(levels(y_test)[y_pred_xgb_idx + 1], levels = levels(y_test))\ncat(\"예측 완료.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n예측 완료.\n```\n\n\n:::\n\n```{.r .cell-code}\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_xgb <- matrix(0,\n                                       nrow = length(levels(y_test)),\n                                       ncol = length(levels(y_test)),\n                                       dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat <- as.character(y_test[i])\n  predicted_cat <- as.character(y_pred_xgb[i])\n  weight <- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_xgb[actual_cat, predicted_cat] <- weighted_confusion_matrix_xgb[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== XGBoost 모델 가중 혼동 행렬 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== XGBoost 모델 가중 혼동 행렬 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_confusion_matrix_xgb, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Predicted\nActual      active  passive\n  active  94434.78 21336.73\n  passive 44765.10 13806.06\n```\n\n\n:::\n\n```{.r .cell-code}\n# XGBoost 모델 가중 성능 지표 계산\ntotal_weighted_sum_xgb <- sum(weighted_confusion_matrix_xgb)\nweighted_accuracy_xgb <- sum(diag(weighted_confusion_matrix_xgb)) / total_weighted_sum_xgb\ncat(\"\\n=== XGBoost 모델 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== XGBoost 모델 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"가중 정확도:\", round(weighted_accuracy_xgb, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n가중 정확도: 0.6209 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_xgb <- numeric(length(levels(y_test)))\nweighted_recall_xgb <- numeric(length(levels(y_test)))\nweighted_f1_score_xgb <- numeric(length(levels(y_test)))\nnames(weighted_precision_xgb) <- names(weighted_recall_xgb) <- names(weighted_f1_score_xgb) <- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP <- weighted_confusion_matrix_xgb[cat, cat]\n  FP <- sum(weighted_confusion_matrix_xgb[, cat]) - TP\n  FN <- sum(weighted_confusion_matrix_xgb[cat, ]) - TP\n\n  weighted_precision_xgb[cat] <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_xgb[cat] <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_xgb[cat] <- ifelse((weighted_precision_xgb[cat] + weighted_recall_xgb[cat]) == 0, 0,\n                                     2 * (weighted_precision_xgb[cat] * weighted_recall_xgb[cat]) / (weighted_precision_xgb[cat] + weighted_recall_xgb[cat]))\n}\n\ncat(\"\\nXGBoost 모델 범주별 가중 정밀도:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 모델 범주별 가중 정밀도:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_precision_xgb, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.6784  0.3929 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nXGBoost 모델 범주별 가중 재현율:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 모델 범주별 가중 재현율:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_recall_xgb, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.8157  0.2357 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nXGBoost 모델 범주별 가중 F1-score:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nXGBoost 모델 범주별 가중 F1-score:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_f1_score_xgb, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7407  0.2946 \n```\n\n\n:::\n:::\n\n\n\n\n## 로지스틱 회귀 모델\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 다항 로지스틱 회귀 모델 학습 및 평가\nlibrary(nnet)\n\n# 다항 로지스틱 회귀 모델 학습 (가중치 적용)\ncat(\"\\n다항 로지스틱 회귀 모델 학습 중...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n다항 로지스틱 회귀 모델 학습 중...\n```\n\n\n:::\n\n```{.r .cell-code}\n# X_train에 열 이름이 없으면 추가\nif(is.null(colnames(X_train))) {\n  colnames(X_train) <- paste0(\"V\", 1:ncol(X_train))\n}\n\n# 학습 데이터를 데이터프레임으로 변환\ntrain_df <- as.data.frame(X_train)\ntrain_df$y <- y_train\n\n# 다항 로지스틱 회귀 모델 학습\nlogistic_model <- multinom(\n  y ~ .,\n  data = train_df,\n  weights = weights_train,\n  trace = FALSE\n)\ncat(\"다항 로지스틱 회귀 모델 학습 완료.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n다항 로지스틱 회귀 모델 학습 완료.\n```\n\n\n:::\n\n```{.r .cell-code}\n# 모델 요약\nprint(summary(logistic_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nmultinom(formula = y ~ ., data = train_df, weights = weights_train, \n    trace = FALSE)\n\nCoefficients:\n                                   Values   Std. Err.\n(Intercept)                 -1.747743e-02 0.002352613\nparent_attachment_w1         8.629500e-02 0.003520811\ndeviant_esteem_w1            2.455207e-02 0.003349262\nparent_stress_w1             1.581364e-02 0.002926812\nparent_monitoring_w1        -3.621483e-02 0.002691047\ndesire_stress_w1            -1.654925e-01 0.002964678\nfriend_stress_w1            -1.484645e-02 0.002712385\nself_confidence_w1          -1.833542e-02 0.003513107\nhigher_school_dependence_w1  5.676484e-02 0.001927342\nneg_esteem_w1               -1.340150e-01 0.003292447\nacademic_stress_w1           1.402940e-02 0.002968398\nparent_attachment_w2        -1.597988e-01 0.003607662\ndeviant_esteem_w2           -3.201644e-03 0.003073032\nparent_stress_w2            -3.385004e-03 0.002958998\nparent_monitoring_w2        -5.046192e-02 0.002743269\ndesire_stress_w2            -7.252106e-02 0.002767975\nfriend_stress_w2             1.161718e-01 0.002658148\nself_confidence_w2          -3.121364e-02 0.003129254\nhigher_school_dependence_w2  2.883447e-02 0.001971023\nneg_esteem_w2               -4.552701e-02 0.002822384\nacademic_stress_w2           5.587968e-02 0.002906613\nparent_attachment_w3        -1.932719e-01 0.003799447\ndeviant_esteem_w3            1.982736e-01 0.003650501\nparent_stress_w3             4.636885e-03 0.003426560\nparent_monitoring_w3        -5.904395e-02 0.002631096\ndesire_stress_w3            -5.003618e-02 0.002935088\nfriend_stress_w3             3.303750e-02 0.003050699\nself_confidence_w3           9.940994e-03 0.003381305\nhigher_school_dependence_w3  5.753226e-04 0.002152310\nneg_esteem_w3                6.789459e-02 0.003165765\nacademic_stress_w3          -7.253990e-03 0.002785823\nparent_attachment_w4        -1.965769e-02 0.003727261\ndeviant_esteem_w4            1.507238e-02 0.003228654\nparent_stress_w4             3.484563e-02 0.003474462\nparent_monitoring_w4        -4.877685e-02 0.002696410\ndesire_stress_w4            -4.753889e-02 0.002757239\nfriend_stress_w4             1.109145e-01 0.003010243\nself_confidence_w4           2.145545e-01 0.003658857\nhigher_school_dependence_w4 -2.398239e-02 0.002350664\nneg_esteem_w4               -1.394140e-01 0.003148682\nacademic_stress_w4          -8.451687e-02 0.002843888\nparent_attachment_w5        -1.940178e-01 0.003516885\ndeviant_esteem_w5           -9.225044e-02 0.003343494\nparent_stress_w5             7.638213e-02 0.003335598\nparent_monitoring_w5        -1.537289e-01 0.002660936\ndesire_stress_w5             2.553032e-05 0.002858326\nfriend_stress_w5             8.165718e-02 0.003068119\nself_confidence_w5           9.089148e-02 0.003390435\nhigher_school_dependence_w5 -7.819745e-02 0.002162491\nneg_esteem_w5                7.233250e-02 0.003152148\nacademic_stress_w5          -1.152715e-01 0.002639858\n\nResidual Deviance: 1055167 \nAIC: 1055269 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 테스트 세트 예측\ncat(\"\\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n다항 로지스틱 회귀 모델로 테스트 세트 예측 중...\n```\n\n\n:::\n\n```{.r .cell-code}\n# X_test에 열 이름이 없으면 추가\nif(is.null(colnames(X_test))) {\n  colnames(X_test) <- paste0(\"V\", 1:ncol(X_test))\n}\ny_pred_logistic <- predict(logistic_model, newdata = as.data.frame(X_test))\ncat(\"예측 완료.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n예측 완료.\n```\n\n\n:::\n\n```{.r .cell-code}\n# 가중 혼동 행렬 계산\nweighted_confusion_matrix_logistic <- matrix(0,\n                                           nrow = length(levels(y_test)),\n                                           ncol = length(levels(y_test)),\n                                           dimnames = list(Actual = levels(y_test), Predicted = levels(y_test)))\n\nfor (i in 1:length(y_test)) {\n  actual_cat <- as.character(y_test[i])\n  predicted_cat <- as.character(y_pred_logistic[i])\n  weight <- weights_test[i]\n  if (actual_cat %in% levels(y_test) && predicted_cat %in% levels(y_test)) {\n    weighted_confusion_matrix_logistic[actual_cat, predicted_cat] <- weighted_confusion_matrix_logistic[actual_cat, predicted_cat] + weight\n  }\n}\n\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 다항 로지스틱 회귀 모델 가중 혼동 행렬 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_confusion_matrix_logistic, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Predicted\nActual      active  passive\n  active  70274.82 45496.69\n  passive 23412.35 35158.82\n```\n\n\n:::\n\n```{.r .cell-code}\n# 다항 로지스틱 회귀 모델 가중 성능 지표 계산\ntotal_weighted_sum_logistic <- sum(weighted_confusion_matrix_logistic)\nweighted_accuracy_logistic <- sum(diag(weighted_confusion_matrix_logistic)) / total_weighted_sum_logistic\ncat(\"\\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 다항 로지스틱 회귀 모델 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"가중 정확도:\", round(weighted_accuracy_logistic, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n가중 정확도: 0.6047 \n```\n\n\n:::\n\n```{.r .cell-code}\n# 각 범주별 가중 정밀도, 재현율, F1-score 계산\nweighted_precision_logistic <- numeric(length(levels(y_test)))\nweighted_recall_logistic <- numeric(length(levels(y_test)))\nweighted_f1_score_logistic <- numeric(length(levels(y_test)))\nnames(weighted_precision_logistic) <- names(weighted_recall_logistic) <- names(weighted_f1_score_logistic) <- levels(y_test)\n\nfor (cat in levels(y_test)) {\n  TP <- weighted_confusion_matrix_logistic[cat, cat]\n  FP <- sum(weighted_confusion_matrix_logistic[, cat]) - TP\n  FN <- sum(weighted_confusion_matrix_logistic[cat, ]) - TP\n\n  weighted_precision_logistic[cat] <- ifelse((TP + FP) == 0, 0, TP / (TP + FP))\n  weighted_recall_logistic[cat] <- ifelse((TP + FN) == 0, 0, TP / (TP + FN))\n  weighted_f1_score_logistic[cat] <- ifelse((weighted_precision_logistic[cat] + weighted_recall_logistic[cat]) == 0, 0,\n                                          2 * (weighted_precision_logistic[cat] * weighted_recall_logistic[cat]) / (weighted_precision_logistic[cat] + weighted_recall_logistic[cat]))\n}\n\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n다항 로지스틱 회귀 모델 범주별 가중 정밀도:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_precision_logistic, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.7501  0.4359 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n다항 로지스틱 회귀 모델 범주별 가중 재현율:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_recall_logistic, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.6070  0.6003 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n다항 로지스틱 회귀 모델 범주별 가중 F1-score:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(round(weighted_f1_score_logistic, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n active passive \n 0.6710  0.5051 \n```\n\n\n:::\n:::\n\n\n\n\n## 최종 모델 성능 비교\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 모든 모델의 가중 정확도 비교\nfinal_metrics <- data.frame(\n  Metric = c(\"가중 정확도\"),\n  RandomForest = c(round(weighted_accuracy_tuned * 100, 2)),\n  # C50_Tree = c(round(weighted_accuracy_c50 * 100, 2)),\n  XGBoost = c(round(weighted_accuracy_xgb * 100, 2)),\n  # SVM = c(round(weighted_accuracy_svm * 100, 2)),\n  # KNN = c(round(weighted_accuracy_knn * 100, 2)),\n  LogisticRegression = c(round(weighted_accuracy_logistic * 100, 2))\n)\n\n# 결과 출력\ncat(\"\\n=== 최종 모델 가중 정확도 비교 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 최종 모델 가중 정확도 비교 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(kable(final_metrics, caption = \"원본 모델 vs 튜닝 모델 가중 정확도 비교\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nTable: 원본 모델 vs 튜닝 모델 가중 정확도 비교\n\n|Metric      | RandomForest| XGBoost| LogisticRegression|\n|:-----------|------------:|-------:|------------------:|\n|가중 정확도 |        63.35|   62.09|              60.47|\n```\n\n\n:::\n\n```{.r .cell-code}\n# 클래스별 가중 성능 지표 (튜닝된 모델 결과 사용 권장)\ncat(\"\\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 튜닝된 모델 범주별 가중 성능 지표 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nclass_weighted_metrics_tuned <- data.frame(\n  Class = levels(y_test), # y_test 또는 actual_classes_tuned 사용 가능\n  Weighted_Precision = round(weighted_precision_tuned, 4),\n  Weighted_Recall = round(weighted_recall_tuned, 4),\n  Weighted_F1 = round(weighted_f1_score_tuned, 4)\n)\nprint(kable(class_weighted_metrics_tuned, caption = \"튜닝된 모델 클래스별 가중 성능 지표\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nTable: 튜닝된 모델 클래스별 가중 성능 지표\n\n|        |Class   | Weighted_Precision| Weighted_Recall| Weighted_F1|\n|:-------|:-------|------------------:|---------------:|-----------:|\n|active  |active  |             0.7036|          0.7742|      0.7372|\n|passive |passive |             0.4432|          0.3552|      0.3944|\n```\n\n\n:::\n\n```{.r .cell-code}\n# 최종 변수 중요도 요약 (튜닝된 모델 결과 사용)\ncat(\"\\n=== 튜닝된 모델 변수 중요도 ===\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=== 튜닝된 모델 변수 중요도 ===\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(kable(head(importance_df_tuned, 10), caption = \"튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nTable: 튜닝된 모델 상위 10개 중요 변수 (가중치 고려 학습 결과)\n\n|   |Variable             | Importance|\n|:--|:--------------------|----------:|\n|44 |parent_monitoring_w5 |  100.00000|\n|41 |parent_attachment_w5 |   98.03124|\n|36 |friend_stress_w4     |   91.75903|\n|22 |deviant_esteem_w3    |   66.68785|\n|40 |academic_stress_w4   |   66.10126|\n|16 |friend_stress_w2     |   61.13150|\n|24 |parent_monitoring_w3 |   60.13058|\n|14 |parent_monitoring_w2 |   57.68542|\n|31 |parent_attachment_w4 |   51.69019|\n|11 |parent_attachment_w2 |   51.56372|\n```\n\n\n:::\n:::\n",
    "supporting": [
      "15_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}