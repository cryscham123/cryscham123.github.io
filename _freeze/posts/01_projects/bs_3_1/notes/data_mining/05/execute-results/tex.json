{
  "hash": "5ee32b7b2b86ccdf65451eeb35b44d89",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 데이터마이닝 1차 팀과제 script\ncategories: [\"보고서\", \"데이터 마이닝\"]\ndate: last-modified\nformat: \n  pdf:\n    fig-pos: 'H'\n    fig-width: 6\n    fig-height: 4\n    css: styles.css\n    documentclass: report\n    papersize: a4\n    number-sections: true\n    number-depth: 3\n    top-level-division: chapter\n    toc: false\n    fontsize: 17pt\n    geometry:\n      - top=20mm\n    indent-size: 2\ninclude-in-header: \n  text: |\n    \\date{}\n    \\usepackage{fontspec}\n    \\setmainfont{Noto Sans KR}\n    \\usepackage{titlesec}\n    \\titleformat{\\chapter}{\\normalfont\\huge\\bfseries}{\\thechapter.}{0.8em}{\\huge}\n    \\titleformat{\\subsection}[block]{\\normalfont\\large\\bfseries}{}{0pt}{}\n    \\titleformat{\\subsubsection}[block]{\\normalfont\\normalsize\\bfseries}{}{0pt}{}\n    \\titlespacing*{\\chapter}{0pt}{-20pt}{5pt}\n    \\titlespacing*{\\section}{0pt}{10pt}{10pt}\n    \\newcommand{\\chapterbreak}{\\clearpage}\n    \\usepackage{setspace}\n    \\setstretch{1.5}\n    \\usepackage{tabularx}\n    \\newcolumntype{Y}{>{\\centering\\arraybackslash}X}\n    \\usepackage[bottom]{footmisc}\n    \\usepackage{setspace}\n    \\renewcommand{\\footnotesize}{\\setstretch{1.2}\\fontsize{9pt}{11pt}\\selectfont}\n    \\setlength{\\skip\\footins}{10pt}\n    \\setlength{\\footnotesep}{8pt}\n    \\renewcommand{\\footnoterule}{\\vspace{1em}\\hrule width 0.5\\linewidth\\vspace{1em}}\n    \\makeatletter\n    \\renewcommand{\\@makefntext}[1]{\\noindent\\makebox[1.5em][r]{\\@makefnmark}\\hspace{0.5em}#1}\n    \\makeatother\n    \\usepackage{float}\n    \\usepackage{multirow}\n    \\usepackage{booktabs}\n\nexecute:\n  echo: false\n  warning: false\n  message: false\n\n---\n\n\n\n\n# Airflow 소개\n\n## 개요\n\n데이터 분석 및 엔지니어링 분야에서는 복잡한 데이터 처리 작업을 자동화하고 관리하기 위해 워크플로우 관리 도구를 도입합니다.\n특히 대규모 데이터 처리 환경에서는 거의 필수적으로 사용하죠.\n\nApache Airflow는 현재 가장 널리 사용되는 워크플로우 관리 도구 중 하나로, Python으로 작업 흐름을 정의하고 스케줄링, 모니터링, 오류 처리 등의 기능을 제공합니다.\n\n## workflow tool이란?\n\n워크플로우 툴(Workflow Tool)은 복잡한 작업 과정을 자동화하고 관리하는 소프트웨어를 칭입니다.\n쉽게 말해, 여러 단계의 작업을 순서대로 실행하고 모니터링할 수 있게 도와주는 도구죠.\n\n예를 들어보겠습니다:\n\n- 매일 아침 데이터베이스에서 정보를 가져와\n- 이를 정리하고 분석한 다음\n- 분석 결과를 보고서로 만들어\n- 이메일로 팀원들에게 자동 발송하는 과정\n\n이런 반복적인 작업을 수동으로 진행하면 시간도 많이 소요되고 실수할 가능성도 높습니다.\n워크플로우 툴은 이런 과정을 코드로 작성하여 자동화함으로써 사람의 개입 없이도 정확하게 작업이 수행되도록 합니다.\n\n데이터 분석 분야에서 주로 사용되는 워크플로우 툴로는 Apache Airflow, Prefect, Dagster 등이 있으며, 이들은 작업 실패 시 자동 재시도, 작업 간 의존성 관리, 스케줄링, 모니터링 등의 기능을 제공합니다.\n\n## 데이터 분석 분야에서의 workflow tool\n\n데이터 분석 및 엔지니어링 분야에서 워크플로우 관리 도구의 중요성은 지속적으로 대두되고 있습니다.\n이는 데이터의 규모와 복잡성이 증가하면서 수동적인 작업 관리로는 한계가 있기 때문입니다.\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![데이터 팀이 직면한 문제 (Unravel 설문조사)](05_files/figure-pdf/cell-2-output-1.pdf){}\n:::\n:::\n\n\n- [unravel 2022 dataops 설문조사](https://www.unraveldata.com/resources/key-findings-of-the-2022-dataops-unleashed-survey/)에 따르면, 데이터 팀이 가장 많이 겪는 문제는 환경 전체적인 가시성 부족입니다. 이는 데이터 파이프라인이 복잡해지면서 작업의 흐름을 파악하기 어려워지고, 오류 발생 시 대응이 늦어지는 문제를 의미합니다. Apache Airflow와 같은 워크플로우 관리 도구는 작업 자동화, 스케줄링, 모니터링을 통해 데이터 팀의 생산성을 향상시키고, 오류를 최소화하는 데 큰 도움을 줍니다. (최신 자료는 잘 안보이네요. 2022년 자료긴 하지만 뭐.. 이정도도 나쁘지 않죠)\n\n- 또한 [Gartner의 2025 트렌드](https://www.gartner.com/en/newsroom/press-releases/2025-03-05-gartner-identifies-top-trends-in-data-and-analytics-for-2025)에 따르면 데이터 분석 전략에서 주목해야 할 최신 트렌드로 **다양한 형태의 데이터를 통합 관리하는 시스템**과 **쉽게 활용 가능한 데이터 상품화**를 강조하고 있습니다. 이러한 트렌드를 실현하기 위해서는 데이터를 수집하고 처리하는 과정을 자동화하고 효율적으로 관리하는 것이 필수적인데, Apache Airflow와 같은 작업 자동화 도구가 바로 이런 필요성을 충족시키는 핵심 기술이 될 수 있습니다.\n\n## workflow 활용 사례\n\n### 국내 기업\n\n추가 예정\n\n### 해외 기업\n\n추가 예정\n\n## Workflow Tool 다운로드 통계\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![2024.03~2025.02 데이터 관련 Workflow tool 다운로드 수 비교](05_files/figure-pdf/cell-3-output-1.pdf){}\n:::\n:::\n\n\n- PyPI 다운로드 통계를 통해 경쟁 기술과 비교한 상대적인 Airflow의 사용 비율을 조사해 보았습니다.\n- Airflow는 데이터 분석 workflow 툴 중에서도 가장 많은 다운로드 수를 기록하고 있습니다.\n- 물론 다운로드가 반드시 실사용으로 이어진다고 보장할 수는 없지만 다른 tool에 비해 상대적으로 많은 관심을 받고 있다는 것을 알 수 있습니다.\n\n## Airflow github star 수\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![workflow tool github star 수 변화](05_files/figure-pdf/cell-4-output-1.pdf){}\n:::\n:::\n\n\n- GitHub Star 수는 오픈소스 프로젝트의 인기도와 커뮤니티 활성화 정도를 보여주는 중요한 지표입니다.\n- Apache Airflow는 2015년 출시 이후 꾸준히 성장하여 2025년까지 약 32,000개의 Star를 기록했습니다.\n- 후발 주자인 Prefect와 Dagster도 성장세를 보이고 있으나, Airflow가 여전히 시장을 선도하고 있습니다.\n- 특히 2021년 이후 Airflow의 Star 수 증가 속도가 더욱 가속화되는 추세를 보이고 있어, 워크플로우 도구 시장에서의 입지가 더욱 공고해지고 있습니다.\n\n## Airflow를 사용하는 직종 비율\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![2024 Airflow 이용자 중 직종별 비율](05_files/figure-pdf/cell-5-output-1.pdf){}\n:::\n:::\n\n\n- [Airflow 공식 사이트 2024 설문조사 결과](https://airflow.apache.org/blog/airflow-survey-2024/)를 참고해서 작성했습니다.\n- airflow 기술을 요구하는 직종 중 **데이터 엔지니어** 직무에서 요구되는 비율이 높습니다.\n- 생각보다 data engineer의 비율이 아주 높지는 않습니다. Airflow 기술을 익힌다면 데이터 분야의 다양한 직무에서 활동할 수 있는 기회가 높아질 것으로 기대됩니다.\n\n## Airflow 사용자들의 만족도\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![2024 Airflow 사용자 만족도 조사 결과](05_files/figure-pdf/cell-6-output-1.pdf){}\n:::\n:::\n\n\n- [Airflow 공식 사이트 2024 설문조사 결과](https://airflow.apache.org/blog/airflow-survey-2024/)에 따르면, 93%의 사용자가 자신의 직무에서 Airflow가 중요하다고 답했습니다.\n- 같은 조사에서 91%의 사용자가 Airflow를 다른 사람에게 추천할 의향이 있다고 답했습니다.\n- 이처럼 높은 만족도는 Airflow가 실제 업무 환경에서 효과적으로 작동하고 있음을 보여줍니다.\n\n\\clearpage\n\n## 채용 시장에서의 Airflow 수요\n\n학생 입장에서 실제 기업에서 활용하고 있는 기술 현황에 대해 조사하기에는 채용 공고가 가장 직접적인 정보 제공원이라고 판단해서 이쪽으로 조사해봤습니다.\n\n뭔가 대규모로 분석해보고 싶었는데 볼 수 있는 정보가 길어봤자 2주가 끝이더라고요.\n그래서 가볍게 Job Description 예시 몇개 가져왔습니다.\n\n### 주요 기업 JD 발췌 예시\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-03-22-18-23-30.png}}\n\\caption{\\href{https://toss.im/career/job-detail?job\\_id=4071103003\\&company=\\%ED\\%86\\%A0\\%EC\\%8A\\%A4\\%EC\\%A6\\%9D\\%EA\\%B6\\%8C\\&detailedPosition=Infra}{토스 증권 Data Engineer JD}}\n\\label{fig:toss_jd}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-03-22-17-31-13.png}}\n\\caption{\\href{https://www.lgresearch.ai/careers/view?seq=228}{LG MLOps Engineer Internship}}\n\\label{fig:toss_jd}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-03-22-18-24-36.png}}\n\\caption{\\href{https://www.google.com/about/careers/applications/jobs/results/124798537199166150-senior-software-engineer-enterprise-data-and-engineering?q=airflow}{Google Senior Software Engineer, Enterprise Data and Engineering}}\n\\label{fig:toss_jd}\n\\end{center}\n\\end{figure}\n\n국내 뿐만 아니라 해외 주요 기업에서도 Airflow를 활용한 데이터 파이프라인 구축 및 관리 역할에 수요가 있습니다. (그래봤자 3개만 발췌했지만요)\n\n이상의 내용들을 통해 데이터 분석 직무에서 workflow tool에 대한 중요성은 대두되고 있고, Airflow는 그 중에서도 높은 위상을 가지며, 현재 다양한 데이터 분석 직무에서 Airflow 기술을 활용하고 있음을 알 수 있습니다.\n\n# Airflow 개념 & Demo\n\n## Airflow 개념\n\n### 동작 원리\n\n![동작 원리](img/2025-04-08-20-32-55.png)\n\n- Web server : Airflow의 웹 UI 서버 입니다.\n- Scheduler : 모든 DAG와 Task에 대하여 모니터링 및 관리하고, 실행해야할 Task를 스케줄링 해줍니다.\n- DAG : Directed Acyclic Graph로 개발자가 Python으로 작성한 워크플로우 입니다. Task들의 dependency를 정의합니다.\n- Worker : 실제 Task를 실행하는 주체입니다.\n- Operator: Task를 정의하는 객체입니다. Airflow 내장 및 커뮤니티 정의 Operator를 사용하여 다양한 작업을 수행할 수 있습니다.\n- Connection: 외부 시스템과의 연결을 정의합니다. 예를 들어, 데이터베이스, 클라우드 서비스 등과의 연결을 설정할 수 있습니다.\n- Hook: Operator와 Connection을 연결하는 객체입니다. Hook을 사용하여 외부 시스템과의 상호작용을 쉽게 처리할 수 있습니다.\n\n## Airflow Demo\n\n### 개요\n\nApache Airflow를 활용해 데이터 파이프라인을 구축해보겠습니다.\n아래와 같은 흐름으로 진행될 예정입니다.\n\n![파이프라인 프로세스](img/2025-04-08-20-28-50.png)\n\n데이터베이스에는 각각 아래와 같은 demo 데이터를 저정했습니다.\n\n```{.python filename=init-mysql1.sql}\nCREATE TABLE shopping_data (\n    amount FLOAT,\n    date DATE\n);\n\nINSERT INTO shopping_data (amount, date) VALUES\n    (300.00, '2025-04-01'),\n    (280.00, '2025-04-02'),\n    (290.00, '2025-04-03'),\n    (200.00, '2025-04-04'),\n    (null, '2025-04-05'),\n    (130.00, '2025-04-06'),\n    (140.00, '2025-04-07'),\n    (160.00, '2025-04-08'),\n    (270.00, '2025-04-09'),\n    (180.00, '2025-04-10'),\n    (120.00, '2025-04-11'),\n    (70.00,  '2025-04-12'),\n    (60.00,  '2025-04-13'),\n    (65.00,  '2025-04-14'),\n    (null, '2025-04-15'),\n    (110.00, '2025-04-16'),\n    (250.00, '2025-04-17'),\n    (260.00, '2025-04-18'),\n    (255.00, '2025-04-19'),\n    (220.00, '2025-04-20');\n```\n\n```{.python filename=init-mysql2.sql}\nCREATE TABLE factor_data (\n    date DATE,\n    staff_count INT,\n    operating_hours FLOAT,\n    product_variety INT,\n    event_frequency INT,\n    store_cleanliness INT,\n    training_hours FLOAT\n);\n\nINSERT INTO factor_data (date, staff_count, operating_hours, \\\n    product_variety, event_frequency, store_cleanliness, training_hours) VALUES\n    ('2025-04-01', 10, 12.0, 50, 2, 8, 5.0),\n    ('2025-04-02', 9,  11.5, 45, 1, 7, 4.5),\n    ('2025-04-03', 10, 12.0, 48, 2, 8, 5.0),\n    ('2025-04-04', 7,  10.0, 40, 1, 6, 3.0),\n    ('2025-04-05', 5,  9.0,  35, 0, 5, 2.0),\n    ('2025-04-06', 4,  8.5,  30, 0, 4, 1.5),\n    ('2025-04-07', 5,  9.0,  35, 1, 5, 2.0),\n    ('2025-04-08', 6,  10.0, 38, 1, 6, 3.0),\n    ('2025-04-09', 9,  11.5, 45, 2, 7, 4.0),\n    ('2025-04-10', 7,  10.5, 40, 1, 6, 3.5),\n    ('2025-04-11', 4,  8.0,  30, 0, 4, 1.0),\n    ('2025-04-12', 3,  7.5,  25, 0, 3, 0.5),\n    ('2025-04-13', 3,  7.0,  20, 0, 3, 0.5),\n    ('2025-04-14', 4,  8.0,  28, 0, 4, 1.0),\n    ('2025-04-15', 5,  9.0,  35, 1, 5, 2.0),\n    ('2025-04-16', 6,  9.5,  38, 1, 6, 2.5),\n    ('2025-04-17', 8,  11.0, 42, 2, 7, 4.0),\n    ('2025-04-18', 9,  11.5, 45, 2, 8, 4.5),\n    ('2025-04-19', 8,  11.0, 43, 2, 7, 4.0),\n    ('2025-04-20', 7,  10.5, 40, 1, 6, 3.5),\n    ('2025-04-21', 6,  10.0, 38, 1, 5, 3.0),\n    ('2025-04-22', 5,  9.0,  35, 1, 5, 2.0),\n    ('2025-04-23', 6,  9.5,  38, 1, 6, 2.5),\n    ('2025-04-24', 7,  10.0, 40, 1, 6, 3.0),\n    ('2025-04-25', 4,  8.0,  30, 0, 4, 1.0),\n    ('2025-04-26', 3,  7.5,  25, 0, 3, 0.5),\n    ('2025-04-27', 4,  8.0,  28, 0, 4, 1.0),\n    ('2025-04-28', 5,  9.0,  35, 1, 5, 2.0),\n    ('2025-04-29', 8,  11.0, 42, 2, 7, 4.0),\n    ('2025-04-30', 9,  11.5, 45, 2, 8, 4.5);\n```\n\n\\clearpage\n\n매 달 자동으로 위의 데이터를 불러와 판매량과 상관관계가 높은 요인을 분석하고, 그 결과를 이메일로 전송하는 파이프라인을 구축해보겠습니다.\n\n### Airflow 설치\n\n먼저 airflow를 설치합니다.\n아래의 명령어로 간단하게 세팅할 수 있습니다.\n\n```\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml'\n```\n\n그러면 airflow 세팅에 필요한 설정 파일이 자동으로 다운로드 됩니다.\n\n저는 여기서 이번 demo를 위해 몇가지 설정을 추가했습니다.\n\n```{.python filename=docker-compose.yml}\nAIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp\nAIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com\nAIRFLOW__SMTP__SMTP_PORT: ${SMTP_PORT:-587}\nAIRFLOW__SMTP__SMTP_USER: ${SMTP_USER}\nAIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD}\nAIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_FROM_MAIL}\n```\n\n먼저 메일 전송을 위한 설정을 추가해줍니다.\n\n```{.python filename=docker-compose.yml}\nservices:\n  mysql1:\n    image: mysql:8.0\n    environment:\n      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root}\n      MYSQL_DATABASE: ${SHOPPING_DB_NAME:-shopping_db}\n      MYSQL_USER: ${MYSQL_USER:-user}\n      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-secret}\n    volumes:\n      - ./mysql1-data:/var/lib/mysql\n      - ./init-mysql1.sql:/docker-entrypoint-initdb.d/init.sql\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \\\n              \"-prootpassword\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 10s\n\n  mysql2:\n    image: mysql:8.0\n    environment:\n      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root}\n      MYSQL_DATABASE: ${WEATHER_DB_NAME:-weather}\n      MYSQL_USER: ${MYSQL_USER:-user}\n      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-secret}\n    volumes:\n      - ./mysql2-data:/var/lib/mysql\n      - ./init-mysql2.sql:/docker-entrypoint-initdb.d/init.sql\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-u\", \"root\", \\\n              \"-prootpassword\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 10s\n```\n\ndatabase 역할을 해줄 환경 두 개도 설정했습니다.\n\n\\clearpage\n\n이제 아래의 명령어를 실행한 후 localhost:8080으로 접속하면 Airflow UI에 접속할 수 있습니다.\n\n```\ndocker compose up -d\n```\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-06-22-19-27.png}}\n\\caption{airflow 접속 화면}\n\\label{fig:toss_jd}\n\\end{center}\n\\end{figure}\n\n초기 id 와 password는 airflow로 설정되어 있습니다.\n\n### DAG 설정\n\n아래와 같이 파일을 작성해서 dags 폴더에 저장합니다.\n\n```{.python filename=dags/demo.py}\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nimport pandas as pd\nfrom datetime import datetime\nfrom airflow.providers.mysql.hooks.mysql import MySqlHook\nfrom airflow.operators.email import EmailOperator\n\nwith DAG(\n    dag_id='integrate_shopping_factor_data',\n    schedule='@monthly',\n    start_date=datetime(2024, 4, 4),\n    catchup=False,\n) as dag:\n\n    def extract_shopping_data():\n        hook = MySqlHook(mysql_conn_id='shopping_source')\n        conn = hook.get_sqlalchemy_engine().connect()\n        df = pd.read_sql(\"SELECT * FROM shopping_data\", conn)\n        df.to_csv('/tmp/shopping_data.csv', index=False)\n\n    def extract_factor_data():\n        hook = MySqlHook(mysql_conn_id='factor_source')\n        conn = hook.get_sqlalchemy_engine().connect()\n        df = pd.read_sql(\"SELECT * FROM factor_data\", conn)\n        df.to_csv('/tmp/factor_data.csv', index=False)\n\n    def preprocess_and_integrate():\n        shopping_df = pd.read_csv('/tmp/shopping_data.csv')\n        factor_df = pd.read_csv('/tmp/factor_data.csv')\n        shopping_df['amount'] = shopping_df['amount'].fillna( \\\n                                        shopping_df['amount'].mean())\n        integrated_df = pd.merge(shopping_df, factor_df, on='date', how='inner')\n        integrated_df.to_csv('/tmp/integrated_data.csv', index=False)\n        return \"Data preprocessed and integrated\"\n\n    def generate_report(**kwargs):\n        df = pd.read_csv('/tmp/integrated_data.csv')\n```\n\n\n```{.python filename=dags/demo.py}\n    extract_shopping_data_task = PythonOperator(\n        task_id='extract_shopping_data',\n        python_callable=extract_shopping_data,\n    )\n\n    extract_factor_data_task = PythonOperator(\n        task_id='extract_factor_data',\n        python_callable=extract_factor_data,\n    )\n\n    process_task = PythonOperator(\n        task_id='preprocess_and_integrate',\n        python_callable=preprocess_and_integrate,\n    )\n\n    report_task = PythonOperator(\n        task_id='generate_report',\n        python_callable=generate_report,\n        provide_context=True,\n    )\n\n    email_task = EmailOperator(\n        task_id='send_email_report',\n        to='cryscham123@naver.com',\n        subject='Monthly Sales Insights Report',\n        html_content=\"{{ ti.xcom_pull(task_ids='generate_report', \\\n                                      key='html_report') }}\",\n    )\n\n    [extract_shopping_data_task, extract_factor_data_task] >> process_task \\\n      >> report_task >> email_task\n```\n\n\\clearpage\n\n### Connections 설정\n\nAirflow UI에서 Connections 메뉴를 통해 데이터 소스를 위한 Connection을 설정합니다.\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-06-22-21-56.png}}\n\\caption{상단 메뉴의 Connections를 눌러 줍니다.}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-06-22-22-32.png}}\n\\caption{+ 기호를 선택}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-06-22-23-41.png}}\n\\caption{설정에 맞게 form을 작성한 후 save를 눌러줍니다.}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-08-21-08-26.png}}\n\\caption{나머지 db도 알맞게 설정해줍니다.}\n\\end{center}\n\\end{figure}\n\n### DAG 실행\n\n이후 DAG를 수동으로 실행하면 아래와 같은 결과를 확인할 수 있습니다.\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-08-19-59-58.png}}\n\\caption{Trigger DAG를 선택}\n\\end{center}\n\\end{figure}\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-08-20-06-03.png}}\n\\caption{결과화면에서 task간 의존 관계와 결과를 확인할 수 있습니다.}\n\\end{center}\n\\end{figure}\n\n\\clearpage\n\n### 결과\n\n\\begin{figure}[H]\n\\begin{center}\n\\fbox{\\includegraphics{img/2025-04-08-20-07-10.png}}\n\\caption{메일도 정상적으로 확인해볼 수 있습니다.}\n\\end{center}\n\\end{figure}\n\n",
    "supporting": [
      "05_files"
    ],
    "filters": []
  }
}