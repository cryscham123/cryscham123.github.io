{
  "hash": "6bbea0f77e33dc5e083d3b7010c10d6e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ëª¨ë¸ë§, í‰ê°€ í…œí”Œë¦¿\"\ndate: 2025-09-21\ncategories: [\"ë°ì´í„° ë¶„ì„\"]\ndirectories: [\"adp_ì‹¤ê¸°\"]\n---\n\n\n## ëª¨ë¸ ì •ì˜\n\n::: {#7bdddd3f .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), num_features), # outlierê°€ ë§ë‹¤ë©´ RobustScalerë¥¼ ê³ ë ¤í•˜ì\n    ('cat', OneHotEncoder(drop='first'), cat_features),\n    ('ord', OrdinalEncoder(), ord_features) # ìˆœì„œí˜• ë³€ìˆ˜. í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ê·¸ëƒ¥ DataFrame.map()ìœ¼ë¡œ ì§ì ‘ ì¸ì½”ë”©í•´ì£¼ëŠ”ê²Œ ë” ë‚˜ì„ë“¯\n])\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model_name', YourModel(random_state=42))\n])\n```\n:::\n\n\n- category í˜•ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¶ë‹¤ë©´, pipelineë³´ë‹¤ëŠ” LGBM, CatBoost, XGBoost ë“±ì˜ ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•˜ëŠ”ê²Œ ë” ë‚˜ìŒ.\n    - sklearnì€ ë°ì´í„°ë“¤ì„ numpy arrayë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì— category í˜•ì„ ìœ ì§€í•˜ì§€ ëª»í•¨\n\n## Grid Search & Bayesian Optimization\n\n::: {#355c7583 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\n# íŒŒë¼ë¯¸í„° ì´ë¦„ ì•ì— pipelineì—ì„œ ì‚¬ìš©í•œ 'ëª¨ë¸ì´ë¦„__'ì„ ë¶™ì—¬ì•¼ í•¨\nparams = {\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__max_depth': [10, 50, 100],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=pipeline, \n                           param_grid=params, \n                           cv=5, \n                           scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error\ngrid_search.fit(X_train, y_train)\nbest_model = grid_search.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\n# ============ ë¶„ë¥˜ scoring ============\n\ny_pred_proba = best_model.predict_proba(X_test)[:, 1] # ë‹¤ì¤‘í´ë˜ìŠ¤ì¸ ê²½ìš° [:, 1] ì œê±°\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒ\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multi_class='ovr', 'ovo' ì¤‘ ì„ íƒ\n\nprint(f\"test_accuracy: {test_acc:.4f}\")\nprint(f\"test_precision: {test_precision:.4f}\")\nprint(f\"test_recall: {test_recall:.4f}\")\nprint(f\"test_f1_score: {test_f1:.4f}\")\nprint(f\"test_auc: {test_auc:.4f}\")\n\n# ============ íšŒê·€ scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")\n```\n:::\n\n\n- ğŸ‘‡ ì‹œí—˜ì—ì„œ ì‚¬ìš© ë¶ˆê°€. í•˜ì§€ë§Œ kaggleì´ë‚˜ daconì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë¨\n\n::: {#a88aa2cd .cell execution_count=3}\n``` {.python .cell-code}\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\ndef objective(trial):\n    # ëª¨ë¸ì— ì í•©í•œ íŒŒë¼ë¯¸í„°ì— ë§ê²Œ ìˆ˜ì •\n    params = {\n        'classifier__n_estimators': trial.suggest_int(\"n_estimators\", 100, 500, 100),\n        'classifier__max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'classifier__max_depth': trial.suggest_int(\"max_depth\", 10, 110, 20),\n        'classifier__min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 10, 2),\n        'classifier__min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 1, 4, 1)\n    }\n    pipeline.set_params(**params)\n\n    cv_score = cross_val_score(pipeline,\n                               X_train,\n                               y_train,\n                               cv=5,\n                               scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error\n    mean_cv_accuracy = cv_score.mean()\n    return mean_cv_accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\npipeline.set_params(**study.best_params)\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\n# ============ ë¶„ë¥˜ scoring ============\n\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multiclass='ovr', 'ovo' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©\n\nprint(\"test_accuracy:\", test_acc)\nprint(\"test_precision:\", test_precision)\nprint(\"test_recall:\", test_recall)\nprint(\"test_f1_score:\", test_f1)\nprint(\"test_auc:\", test_auc)\n\n# ============ íšŒê·€ scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")\n```\n:::\n\n\n- ğŸ‘‡ ì‹œí—˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n\n::: {#1e895895 .cell execution_count=4}\n``` {.python .cell-code}\nfrom hyperopt import hp, STATUS_OK, fmin, tpe, Trials\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\n# íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜\nsearch_space = {\n    'classifier__n_estimators': hp.quniform('n_estimators', 100, 500, 100),\n    'classifier__max_features': hp.choice('max_features', ['sqrt', 'log2']),\n    'classifier__max_depth': hp.quniform('max_depth', 10, 110, 20),\n    'classifier__min_samples_split': hp.quniform('min_samples_split', 2, 10, 2),\n    'classifier__min_samples_leaf': hp.quniform('min_samples_leaf', 1, 4, 1)\n}\n\ndef objective_func(params):\n    # íŒŒë¼ë¯¸í„° íƒ€ì… ë³€í™˜ (hyperoptëŠ” floatë¡œ ë°˜í™˜í•˜ë¯€ë¡œ intë¡œ ë³€í™˜ í•„ìš”)\n    params_int = {\n        'classifier__n_estimators': int(params['classifier__n_estimators']),\n        'classifier__max_features': params['classifier__max_features'],\n        'classifier__max_depth': int(params['classifier__max_depth']),\n        'classifier__min_samples_split': int(params['classifier__min_samples_split']),\n        'classifier__min_samples_leaf': int(params['classifier__min_samples_leaf'])\n    }\n    pipeline.set_params(**params_int)\n    \n    cv_score = cross_val_score(pipeline,\n                               X_train,\n                               y_train,\n                               cv=5,\n                               scoring='accuracy') # ë¶„ë¥˜: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, íšŒê·€: neg_root_mean_squared_error, r2, neg_mean_absolute_error\n    mean_cv_score = cv_score.mean()\n    \n    # hyperoptëŠ” ìµœì†Œí™”í•˜ë¯€ë¡œ ìŒìˆ˜ ë°˜í™˜ (ìµœëŒ€í™”í•˜ë ¤ëŠ” ê²½ìš°)\n    return {'loss': -mean_cv_score, 'status': STATUS_OK}\n\n# ìµœì í™” ì‹¤í–‰\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\n\n# ìµœì  íŒŒë¼ë¯¸í„° ì ìš© (íƒ€ì… ë³€í™˜ í¬í•¨)\nbest_params = {\n    'classifier__n_estimators': int(best['n_estimators']),\n    'classifier__max_features': ['sqrt', 'log2'][int(best['max_features'])],\n    'classifier__max_depth': int(best['max_depth']),\n    'classifier__min_samples_split': int(best['min_samples_split']),\n    'classifier__min_samples_leaf': int(best['min_samples_leaf'])\n}\n\npipeline.set_params(**best_params)\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\n# ============ ë¶„ë¥˜ scoring ============\n\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° 'macro', 'micro', 'weighted' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš° multiclass='ovr', 'ovo' ì¤‘ ì„ íƒí•´ì„œ ì‚¬ìš©\n\nprint(\"test_accuracy:\", test_acc)\nprint(\"test_precision:\", test_precision)\nprint(\"test_recall:\", test_recall)\nprint(\"test_f1_score:\", test_f1)\nprint(\"test_auc:\", test_auc)\n\n# ============ íšŒê·€ scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")\n```\n:::\n\n\n## ëª¨ë¸ í‰ê°€ visualization\n\n### ROC AUC\n\n::: {#b8677d33 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# ROC ê³¡ì„  ê³„ì‚°\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ \nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n\n# ROC ê³¡ì„  ì‹œê°í™”\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n```\n:::\n\n\n### Feature Importance\n\n::: {#8a6edba3 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimportances = pipeline.named_steps['classifier'].feature_importances_\nftr_importance = pd.Series(importances, index=X.columns)\nftr_top = ftr_importance.sort_values(ascending=False)[:20] # ì›í•˜ëŠ” ìˆ˜ ë§Œí¼ ì„¤ì •\nsns.barplot(ftr_top20, y=ftr_top.index)\nplt.show()\n```\n:::\n\n\n### Drop Column importance\n\n::: {#b624a287 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.base import clone\n\nfor col in X_train.columns:\n    X_train_dropped = X_train.drop(columns=[col])\n    X_test_dropped = X_test.drop(columns=[col])\n\n    # ì‹œê°„ì˜ ë‹¨ì¶•ì„ ìœ„í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n    # ì—„ë°€í•˜ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´ ê° iterationë§ˆë‹¤ ë‹¤ì‹œ íŠœë‹\n    model_dropped = clone(best_model)\n    model_dropped.fit(X_train_dropped, y_train)\n\n    score = model_dropped.score(X_test_dropped, y_test)\n    print(f\"{col}'s difference:\", best_model.score(X_test, y_test) - score) # í˜¹ì€ ë‹¤ë¥¸ í‰ê°€ ì§€í‘œ ì‚¬ìš©\n```\n:::\n\n\n### Permutation importance\n\n::: {#4fb9e689 .cell execution_count=8}\n``` {.python .cell-code}\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(best_model).fit(X_test, y_test)\neli5.show_weights(perm, feature_names=X_test.columns.tolist())\n```\n:::\n\n\n- ìŒìˆ˜ ê°’ì€ ì œê±°í•´ë„ ëœë‹¤ëŠ” ëœ»\n- ë‹¨ì : ìƒê´€ê´€ê³„ê°€ ë†’ì€ featureì— ëŒ€í•´ì„œ ë¹„í˜„ì‹¤ì ì¸ ë°ì´í„° ì¡°í•©ì´ ìƒì„±ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\n    - (ex. shuffleì„ í†µí•´ í‚¤ 180cm, ëª¸ë¬´ê²Œ 30kgì˜ ë°ì´í„°ê°€ ì¡°í•©ë˜ëŠ” ê²½ìš°)\n\n## ensemble\n\n- stacking:\n    - ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„, ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë©”íƒ€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤.\n    - ë©”íƒ€ ëª¨ë¸ì€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.\n- voting, averaging:\n    - ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„, ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íˆ¬í‘œ(voting)í•˜ê±°ë‚˜ í‰ê· (averaging)í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.\n    - ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ë‹¤ìˆ˜ê²° íˆ¬í‘œ(hard voting) ë˜ëŠ” í™•ë¥  í‰ê· (soft voting)ì„ ì‚¬ìš©í•˜ê³ , íšŒê·€ ë¬¸ì œì—ì„œëŠ” ë‹¨ìˆœ í‰ê·  ë˜ëŠ” ê°€ì¤‘ í‰ê· ì„ ì‚¬ìš©í•œë‹¤.\n- bagging\n    - vs cross validation:\n        - cross validationì€ ì´ë¯¸ ìƒì„±ëœ ëª¨ë¸ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ë°©ë²•. ëª¨ë¸ êµ¬ì¶• ë°©ë²•ì€ ì•„ë‹˜\n        - baggingì€ ë¶„ì‚°ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•¨\n- boosting: sequentially í•™ìŠµ\n    - ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.\n\n::: {#3935adb7 .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.ensemble import StackingClassifier, StackingRegressor\n\nstacking_lf = StackingClassifier(estimators=[\n    ('lr', LogisticRegression()),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier())\n], final_estimator=LogisticRegression())\n\nstacking_rf = StackingRegressor(estimators=[\n    ('lr', LinearRegression()),\n    ('dt', DecisionTreeRegressor()),\n    ('rf', RandomForestRegressor())\n], final_estimator=LinearRegression())\n\nstacking_lf.fit(X_train, y_train)\nstacking_rf.fit(X_train, y_train)\nstacking_lf.predict(X_test)\nstacking_rf.predict(X_test)\n```\n:::\n\n\n::: {#a677250f .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.ensemble import VotingClassifier, VotingRegressor\n\nvoting_lf = VotingClassifier(estimators=[\n    ('lr', LogisticRegression()),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier())\n], voting='soft') # or hard\n\n\nvoting_rf = VotingRegressor(estimators=[\n    ('lr', LinearRegression()),\n    ('dt', DecisionTreeRegressor()),\n    ('rf', RandomForestRegressor())\n], weights=[1, 1, 2])\n\n\nvoting_lf.fit(X_train, y_train)\nvoting_rf.fit(X_train, y_train)\nvoting_lf.predict(X_test)\nvoting_rf.predict(X_test)\n```\n:::\n\n\n## ëª¨ë¸ íŒŒë¼ë¯¸í„°\n\n- ì•„ë˜ë¶€í„°ëŠ” aiì˜ ë„ì›€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\n- ì ë‹¹íˆ ëª‡ ê°œë§Œ ê³¨ë¼ì„œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n\n### Logistic íšŒê·€ë¶„ì„\n\n::: {#30210c7e .cell execution_count=11}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__penalty\": ['l1', 'l2', 'elasticnet', 'none'],  # ì •ê·œí™” ë°©ë²•\n    \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],  # ì •ê·œí™” ê°•ë„ (ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™”)\n    \"classifier__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],  # elasticnetì¼ ë•Œë§Œ ì‚¬ìš©\n    \"classifier__solver\": ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜\n    \"classifier__max_iter\": [100, 500, 1000, 2000]  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__penalty\": hp.choice(\"penalty\", ['l1', 'l2', 'elasticnet']),\n    \"classifier__C\": hp.loguniform(\"C\", np.log(0.001), np.log(100)),\n    \"classifier__l1_ratio\": hp.uniform(\"l1_ratio\", 0.1, 0.9),\n    \"classifier__solver\": hp.choice(\"solver\", ['liblinear', 'saga']),\n    \"classifier__max_iter\": hp.quniform(\"max_iter\", 100, 2000, 100)\n}\n```\n:::\n\n\n### KNN\n\n::: {#0714bab7 .cell execution_count=12}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__n_neighbors\": [3, 5, 7, 9, 11, 15, 21],  # kê°’ (ì´ì›ƒì˜ ìˆ˜)\n    \"classifier__weights\": ['uniform', 'distance'],  # ê°€ì¤‘ì¹˜ ë°©ë²•\n    \"classifier__algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],  # ì•Œê³ ë¦¬ì¦˜\n    \"classifier__leaf_size\": [20, 30, 40, 50],  # ë¦¬í”„ í¬ê¸° (ball_tree, kd_treeìš©)\n    \"classifier__p\": [1, 2],  # ê±°ë¦¬ ì¸¡ë„ (1: ë§¨í•˜íƒ„, 2: ìœ í´ë¦¬ë“œ)\n    \"classifier__metric\": ['minkowski', 'euclidean', 'manhattan']  # ê±°ë¦¬ í•¨ìˆ˜\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__n_neighbors\": hp.quniform(\"n_neighbors\", 3, 21, 2),\n    \"classifier__weights\": hp.choice(\"weights\", ['uniform', 'distance']),\n    \"classifier__algorithm\": hp.choice(\"algorithm\", ['auto', 'ball_tree', 'kd_tree']),\n    \"classifier__leaf_size\": hp.quniform(\"leaf_size\", 20, 50, 10),\n    \"classifier__p\": hp.choice(\"p\", [1, 2]),\n    \"classifier__metric\": hp.choice(\"metric\", ['minkowski', 'euclidean', 'manhattan'])\n}\n```\n:::\n\n\n### Support Vector Machine\n\n::: {#12b49090 .cell execution_count=13}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # ì •ê·œí™” íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™”)\n    \"classifier__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],  # ì»¤ë„ í•¨ìˆ˜\n    \"classifier__degree\": [2, 3, 4, 5],  # poly ì»¤ë„ ì°¨ìˆ˜ (polyì¼ ë•Œë§Œ ì‚¬ìš©)\n    \"classifier__gamma\": ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],  # ì»¤ë„ ê³„ìˆ˜ (rbf, poly, sigmoidìš©)\n    \"classifier__coef0\": [0.0, 0.1, 0.5, 1.0],  # ì»¤ë„ì˜ ë…ë¦½í•­ (poly, sigmoidìš©)\n    \"classifier__shrinking\": [True, False],  # ìˆ˜ì¶• íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš© ì—¬ë¶€\n    \"classifier__tol\": [1e-5, 1e-4, 1e-3],  # ì •ì§€ ê¸°ì¤€ í—ˆìš© ì˜¤ì°¨\n    \"classifier__max_iter\": [100, 500, 1000, 2000, -1]  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (-1: ì œí•œ ì—†ìŒ)\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__C\": hp.loguniform(\"C\", np.log(0.001), np.log(1000)),\n    \"classifier__kernel\": hp.choice(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n    \"classifier__degree\": hp.quniform(\"degree\", 2, 5, 1),  # polyì¼ ë•Œë§Œ\n    \"classifier__gamma\": hp.choice(\"gamma\", ['scale', 'auto'] + [hp.loguniform(\"gamma_float\", np.log(0.001), np.log(10))]),\n    \"classifier__coef0\": hp.uniform(\"coef0\", 0.0, 1.0),\n    \"classifier__shrinking\": hp.choice(\"shrinking\", [True, False]),\n    \"classifier__tol\": hp.loguniform(\"tol\", np.log(1e-5), np.log(1e-3)),\n    \"classifier__max_iter\": hp.quniform(\"max_iter\", 100, 2000, 100)\n}\n```\n:::\n\n\n### Random Forest\n\n::: {#e1188a1d .cell execution_count=14}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # íŠ¸ë¦¬ ê°œìˆ˜\n    \"classifier__criterion\": ['gini', 'entropy', 'log_loss'],  # ë¶ˆìˆœë„ ì¸¡ì • ê¸°ì¤€\n    \"classifier__max_depth\": [None, 5, 10, 20, 30, 50],  # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (None: ì œí•œ ì—†ìŒ)\n    \"classifier__min_samples_split\": [2, 5, 10, 20],  # ë‚´ë¶€ ë…¸ë“œ ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n    \"classifier__min_samples_leaf\": [1, 2, 4, 10],  # ë¦¬í”„ ë…¸ë“œì— í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n    \"classifier__min_weight_fraction_leaf\": [0.0, 0.1, 0.2],  # ë¦¬í”„ ë…¸ë“œì— í•„ìš”í•œ ìµœì†Œ ê°€ì¤‘ì¹˜ ë¹„ìœ¨\n    \"classifier__max_features\": ['sqrt', 'log2', None, 0.3, 0.5, 0.7],  # ê° ë¶„í• ì—ì„œ ê³ ë ¤í•  í”¼ì²˜ ìˆ˜\n    \"classifier__max_leaf_nodes\": [None, 50, 100, 200],  # ìµœëŒ€ ë¦¬í”„ ë…¸ë“œ ìˆ˜\n    \"classifier__min_impurity_decrease\": [0.0, 0.01, 0.05, 0.1],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ë¶ˆìˆœë„ ê°ì†ŒëŸ‰\n    \"classifier__bootstrap\": [True, False],  # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‚¬ìš© ì—¬ë¶€\n    \"classifier__oob_score\": [True, False],  # OOB ìŠ¤ì½”ì–´ ê³„ì‚° ì—¬ë¶€\n    \"classifier__max_samples\": [None, 0.5, 0.7, 0.9],  # ê° íŠ¸ë¦¬ì— ì‚¬ìš©í•  ìƒ˜í”Œ ë¹„ìœ¨\n    \"classifier__ccp_alpha\": [0.0, 0.01, 0.05, 0.1]  # ë³µì¡ë„ ê°€ì§€ì¹˜ê¸° íŒŒë¼ë¯¸í„°\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__criterion\": hp.choice(\"criterion\", ['gini', 'entropy']),\n    \"classifier__max_depth\": hp.choice(\"max_depth\", [None, hp.quniform(\"max_depth_val\", 5, 50, 5)]),\n    \"classifier__min_samples_split\": hp.quniform(\"min_samples_split\", 2, 20, 2),\n    \"classifier__min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 10, 1),\n    \"classifier__max_features\": hp.choice(\"max_features\", ['sqrt', 'log2', None]),\n    \"classifier__min_impurity_decrease\": hp.uniform(\"min_impurity_decrease\", 0.0, 0.1),\n    \"classifier__bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n    \"classifier__max_samples\": hp.choice(\"max_samples\", [None, hp.uniform(\"max_samples_val\", 0.5, 1.0)]),\n    \"classifier__ccp_alpha\": hp.uniform(\"ccp_alpha\", 0.0, 0.1)\n}\n```\n:::\n\n\n### XGBOOST\n\n::: {#4b0eec5e .cell execution_count=15}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥  (eta)\n    \"classifier__max_depth\": [3, 4, 5, 6, 7, 8],  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´\n    \"classifier__min_child_weight\": [1, 3, 5, 7],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜ í•©\n    \"classifier__gamma\": [0, 0.1, 0.2, 0.3, 0.4],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨\n    \"classifier__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (íŠ¸ë¦¬ë³„)\n    \"classifier__colsample_bylevel\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (ë ˆë²¨ë³„)\n    \"classifier__colsample_bynode\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨ (ë…¸ë“œë³„)\n    \"classifier__reg_alpha\": [0, 0.01, 0.1, 1, 10],  # L1 ì •ê·œí™” íŒŒë¼ë¯¸í„°\n    \"classifier__reg_lambda\": [0, 0.01, 0.1, 1, 10],  # L2 ì •ê·œí™” íŒŒë¼ë¯¸í„°\n    \"classifier__max_delta_step\": [0, 1, 2, 5, 10],  # ê° íŠ¸ë¦¬ ê°€ì¤‘ì¹˜ ë³€í™”ì˜ ìµœëŒ€ê°’\n    \"classifier__scale_pos_weight\": [1, 2, 3, 5]  # ì–‘ì„± í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë°ì´í„°ìš©)\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__max_depth\": hp.quniform(\"max_depth\", 3, 8, 1),\n    \"classifier__min_child_weight\": hp.quniform(\"min_child_weight\", 1, 7, 1),\n    \"classifier__gamma\": hp.uniform(\"gamma\", 0, 0.4),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 1.0),\n    \"classifier__colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.6, 1.0),\n    \"classifier__colsample_bynode\": hp.uniform(\"colsample_bynode\", 0.6, 1.0),\n    \"classifier__reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.01), np.log(10)),\n    \"classifier__reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10)),\n    \"classifier__max_delta_step\": hp.quniform(\"max_delta_step\", 0, 10, 1),\n    \"classifier__scale_pos_weight\": hp.quniform(\"scale_pos_weight\", 1, 5, 1)\n}\n```\n:::\n\n\n### LightGBM\n\n::: {#a4954082 .cell execution_count=16}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥ \n    \"classifier__max_depth\": [3, 5, 7, 10, -1],  # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ (-1: ì œí•œ ì—†ìŒ)\n    \"classifier__num_leaves\": [15, 31, 63, 127, 255],  # ë¦¬í”„ ë…¸ë“œ ìˆ˜ (2^max_depth - 1ë³´ë‹¤ ì‘ê²Œ)\n    \"classifier__min_child_samples\": [10, 20, 30, 50],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n    \"classifier__min_child_weight\": [1e-3, 1e-2, 1e-1, 1],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ê°€ì¤‘ì¹˜ í•©\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨\n    \"classifier__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨\n    \"classifier__reg_alpha\": [0, 0.01, 0.1, 1, 10],  # L1 ì •ê·œí™”\n    \"classifier__reg_lambda\": [0, 0.01, 0.1, 1, 10],  # L2 ì •ê·œí™”\n    \"classifier__min_split_gain\": [0, 0.01, 0.1, 0.5],  # ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ gain\n    \"classifier__min_data_in_leaf\": [10, 20, 50, 100],  # ë¦¬í”„ì˜ ìµœì†Œ ë°ì´í„° ìˆ˜\n    \"classifier__boosting_type\": ['gbdt', 'dart', 'goss'],  # ë¶€ìŠ¤íŒ… íƒ€ì…\n    \"classifier__feature_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],  # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨\n    \"classifier__bagging_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨\n    \"classifier__bagging_freq\": [0, 1, 3, 5]  # ë°°ê¹… ë¹ˆë„\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__max_depth\": hp.choice(\"max_depth\", [-1, hp.quniform(\"max_depth_val\", 3, 10, 1)]),\n    \"classifier__num_leaves\": hp.quniform(\"num_leaves\", 15, 255, 16),\n    \"classifier__min_child_samples\": hp.quniform(\"min_child_samples\", 10, 50, 10),\n    \"classifier__min_child_weight\": hp.loguniform(\"min_child_weight\", np.log(1e-3), np.log(1)),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 1.0),\n    \"classifier__reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.01), np.log(10)),\n    \"classifier__reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10)),\n    \"classifier__min_split_gain\": hp.uniform(\"min_split_gain\", 0, 0.5),\n    \"classifier__min_data_in_leaf\": hp.quniform(\"min_data_in_leaf\", 10, 100, 10),\n    \"classifier__boosting_type\": hp.choice(\"boosting_type\", ['gbdt', 'dart', 'goss']),\n    \"classifier__feature_fraction\": hp.uniform(\"feature_fraction\", 0.6, 1.0),\n    \"classifier__bagging_fraction\": hp.uniform(\"bagging_fraction\", 0.6, 1.0),\n    \"classifier__bagging_freq\": hp.quniform(\"bagging_freq\", 0, 5, 1)\n}\n```\n:::\n\n\n### Catboost\n\n::: {#143e8f73 .cell execution_count=17}\n``` {.python .cell-code}\n# Grid Searchìš©\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # ë¶€ìŠ¤íŒ… ë¼ìš´ë“œ ìˆ˜\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # í•™ìŠµë¥ \n    \"classifier__depth\": [3, 4, 5, 6, 7, 8],  # íŠ¸ë¦¬ ê¹Šì´ (max_depth ëŒ€ì‹  depth ì‚¬ìš©)\n    \"classifier__l2_leaf_reg\": [1, 3, 5, 10, 20],  # L2 ì •ê·œí™” íŒŒë¼ë¯¸í„°\n    \"classifier__border_count\": [32, 64, 128, 255],  # ìˆ˜ì¹˜í˜• í”¼ì²˜ì˜ ë¶„í• ì  ê°œìˆ˜\n    \"classifier__bagging_temperature\": [0, 0.5, 1, 2, 5],  # ë°°ê¹… ì˜¨ë„ (0: ë¹„í™œì„±í™”)\n    \"classifier__random_strength\": [1, 2, 5, 10],  # íŠ¸ë¦¬ êµ¬ì¡°ì˜ ë¬´ì‘ìœ„ì„±\n    \"classifier__od_type\": ['IncToDec', 'Iter'],  # ì¡°ê¸° ì¢…ë£Œ íƒ€ì…\n    \"classifier__od_wait\": [10, 20, 50],  # ì¡°ê¸° ì¢…ë£Œ ëŒ€ê¸° ë¼ìš´ë“œ\n    \"classifier__bootstrap_type\": ['Bayesian', 'Bernoulli', 'MVS', 'Poisson'],  # ë¶€íŠ¸ìŠ¤íŠ¸ë© íƒ€ì…\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # ìƒ˜í”Œë§ ë¹„ìœ¨ (Bernoulliì¼ ë•Œë§Œ)\n    \"classifier__rsm\": [0.5, 0.7, 0.9, 1.0],  # ë¬´ì‘ìœ„ ì„œë¸ŒìŠ¤í˜ì´ìŠ¤ ë°©ë²• (í”¼ì²˜ ìƒ˜í”Œë§)\n    \"classifier__leaf_estimation_iterations\": [1, 3, 5, 10],  # ë¦¬í”„ ê°’ ì¶”ì • ë°˜ë³µ íšŸìˆ˜\n    \"classifier__grow_policy\": ['SymmetricTree', 'Depthwise', 'Lossguide'],  # íŠ¸ë¦¬ ì„±ì¥ ì •ì±…\n    \"classifier__min_data_in_leaf\": [1, 5, 10, 20],  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜\n    \"classifier__max_leaves\": [16, 31, 64, 127]  # ìµœëŒ€ ë¦¬í”„ ìˆ˜ (Lossguideì¼ ë•Œë§Œ)\n}\n\n# Bayesian Optimizationìš© (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__depth\": hp.quniform(\"depth\", 3, 8, 1),\n    \"classifier__l2_leaf_reg\": hp.quniform(\"l2_leaf_reg\", 1, 20, 1),\n    \"classifier__border_count\": hp.choice(\"border_count\", [32, 64, 128, 255]),\n    \"classifier__bagging_temperature\": hp.uniform(\"bagging_temperature\", 0, 5),\n    \"classifier__random_strength\": hp.quniform(\"random_strength\", 1, 10, 1),\n    \"classifier__od_type\": hp.choice(\"od_type\", ['IncToDec', 'Iter']),\n    \"classifier__od_wait\": hp.quniform(\"od_wait\", 10, 50, 10),\n    \"classifier__bootstrap_type\": hp.choice(\"bootstrap_type\", ['Bayesian', 'Bernoulli', 'MVS']),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__rsm\": hp.uniform(\"rsm\", 0.5, 1.0),\n    \"classifier__leaf_estimation_iterations\": hp.quniform(\"leaf_estimation_iterations\", 1, 10, 1),\n    \"classifier__grow_policy\": hp.choice(\"grow_policy\", ['SymmetricTree', 'Depthwise', 'Lossguide']),\n    \"classifier__min_data_in_leaf\": hp.quniform(\"min_data_in_leaf\", 1, 20, 1),\n    \"classifier__max_leaves\": hp.quniform(\"max_leaves\", 16, 127, 1)\n}\n\n# ì£¼ìš” íŠ¹ì§•:\n# - GPU ì§€ì› (gpu_device_id=-1ë¡œ ì„¤ì •í•˜ë©´ GPU ì‚¬ìš©)\n# - ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬ (cat_features íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •)\n# - ë‚´ì¥ëœ êµì°¨ê²€ì¦ ë° ì¡°ê¸° ì¢…ë£Œ\n# - í…ìŠ¤íŠ¸ ë° ì„ë² ë”© í”¼ì²˜ ì§€ì›\n```\n:::\n\n\n",
    "supporting": [
      "01_files"
    ],
    "filters": [],
    "includes": {}
  }
}