{
  "hash": "30684d3b26464ec6a34770f8f970e1a5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"EDA 템플릿\"\ndate: 2025-09-29\ncategories: [\"데이터 분석\"]\n---\n\n\n## Load Library\n\n::: {#60bf8af1 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport platform\nimport warnings\n```\n:::\n\n\n## Settings\n\n::: {#0b5d4d0c .cell execution_count=2}\n``` {.python .cell-code}\nwarnings.filterwarnings('ignore')\n\nif platform.system() == 'Darwin': #맥\n        plt.rc('font', family='AppleGothic') \nelif platform.system() == 'Windows': #윈도우\n        plt.rc('font', family='Malgun Gothic') \nelif platform.system() == 'Linux': #리눅스 (구글 콜랩)\n        #!wget \"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\"\n        #!mv malgun.ttf /usr/share/fonts/truetype/\n        #import matplotlib.font_manager as fm \n        #fm._rebuild() \n        plt.rc('font', family='Malgun Gothic') \nplt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\nwarnings.filterwarnings('ignore')\n```\n:::\n\n\n## Load Data\n\n::: {#96064f80 .cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.read_csv('df.csv')\n\nprint(f\"df shape: {df.shape}\")\n```\n:::\n\n\n## Missing Values\n\n::: {#ae61edc6 .cell execution_count=4}\n``` {.python .cell-code}\nprint(df.isnull().sum())\n\nprint(df[df.isnull().any(axis=1)])\n```\n:::\n\n\n- 결측치 처리는 전처리 참조\n\n## 단일 column 분석\n\n::: {#8bfd6f1c .cell execution_count=5}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n::: {#6188a52a .cell execution_count=6}\n``` {.python .cell-code}\ndf.info()\n```\n:::\n\n\n- 순서형 변수는 연속형으로 처리하던가 범주형으로 처리하던가 알아서 정하면 됨.\n- 순서형 그 자체로 보고 분석할 수 있는 방법들도 있긴 있음.\n\n### 범주형\n\n::: {#659537c4 .cell execution_count=7}\n``` {.python .cell-code}\ndf.describe(include='object')\n```\n:::\n\n\n::: {#e18d2f94 .cell execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\ncat_cols = []\nnum_cols = []\n\nfor col in cat_cols:\n    target_counts = df[col].value_counts().sort_index()\n    target_ratio = df[col].value_counts(normalize=True).sort_index()\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    target_counts.plot.bar(ax=ax[0])\n    ax[0].set_title(f\"{col}'s Count\")\n    ax[0].set_xlabel(f\"{col}'s Level\")\n    ax[0].set_ylabel('count')\n\n    wedges, texts, autotexts = ax[1].pie(target_ratio, autopct='%1.1f%%')\n    ax[1].set_title(f\"{col}'s Distribution\", fontsize=14, pad=20)\n    plt.show()\n\n    summary = pd.concat([target_counts, target_ratio], axis=1)\n    summary.columns = ['counts', 'probs']\n    summary = summary.sort_values('counts', ascending=False)\n    display(summary)\n```\n:::\n\n\n### 연속형\n\n```{.python filname=\"basic statistic\"}\ndetailed_stats = []\nfor col in num_cols:\n    stats_dict = {\n        'Mean': df[col].mean(),\n        'Median': df[col].median(),\n        'Min': df[col].min(),\n        'Max': df[col].max(),\n        'Std Dev': df[col].std(),\n        'IQR': df[col].quantile(0.75) - df[col].quantile(0.25),\n        'Skewness': df[col].skew(),\n        'Kurtosis': df[col].kurtosis(),\n        'CV (%)': df[col].std() / df[col].mean() * 100\n    }\n    detailed_stats.append(pd.Series(stats_dict, name=col))\n\ndisplay(pd.concat(detailed_stats, axis=1).round(2).T)\n```\n\n```{.python filename=\"distribution\"}\nfrom scipy.stats import probplot\n\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    # 히스토그램과 KDE\n    df[col].hist(ax=ax[0], color='skyblue', density=True)\n    sns.kdeplot(df[col], color='red', linewidth=2, label='KDE', ax=ax[0])\n    ax[0].set_title(f'{col} Distribution')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Density')\n\n    mean_val = df[col].mean()\n    median_val = df[col].median()\n    ax[0].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.1f}')\n    ax[0].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.1f}')\n    ax[0].legend()\n\n    # 박스플롯\n    sns.boxplot(y=df[col], ax=ax[1], color='lightgreen')\n\n    # Q-Q 플롯\n    probplot(df[col], plot=ax[2])\n\n    plt.show()\n```\n\n```{.python filename=\"normality\"}\nfrom scipy.stats import shapiro, anderson, jarque_bera, normaltest\n\nfor col in num_cols:\n    # 소표본 적합\n    stat, p = shapiro(df[col].dropna())\n    print(f\"Shapiro-Wilk Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    result = anderson(df[col].dropna())\n    print(f\"Anderson-Darling Test: stat={result.statistic:.4f}, critical values={result.critical_values}, significance levels={result.significance_level}\")\n\n    # 왜도, 첨도 기준 판별. 대표본 적합\n    stat, p = jarque_bera(df[col].dropna())\n    print(f\"Jarque-Bera Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    # 왜도, 첨도 기준 판별. 범용적\n    stat, p = normaltest(df[col].dropna())\n    print(f\"D'Agostino's K-squared Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    print()\n```\n\n```{.python filename=\"outliers - IQR\"}\noutliers_info = []\noutliers_mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n\nfor col in num_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    out_idx = (df[col] < lower_bound) | (df[col] > upper_bound)\n    outliers_mask[col] |= out_idx\n    outliers = df[out_idx]\n    \n    outliers_info.append(pd.Series({\n        'Q1': Q1,\n        'Q3': Q3,\n        'IQR': IQR,\n        'Lower Bound': lower_bound,\n        'Upper Bound': upper_bound,\n        'Outlier Count': len(outliers),\n        'Outlier %': len(outliers) / len(df) * 100\n    }, name=col))\noutliers_info = pd.concat(outliers_info, axis=1)\noutliers = df[outliers_mask.any(axis=1)]\noutliers['reason'] = outliers_mask[outliers_mask.any(axis=1)].apply(lambda x: ', '.join(x.index[x]), axis=1)\nnormal = df.loc[(~outliers_mask).all(axis=1)]\n\ndisplay(outliers_info.round(2).T)\ndisplay(outliers)\n```\n\n```{.python filename=\"outliers - Z-score\"}\noutliers_info = []\noutliers_mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n\nfor col in num_cols:\n    mean = df[col].mean()\n    std = df[col].std()\n    z_scores = (df[col] - mean) / std\n    out_idx = (z_scores.abs() > 3)\n    outliers_mask[col] |= out_idx\n    outliers = df[out_idx]\n\n    outliers_info.append(pd.Series({\n        'Mean': mean,\n        'Std Dev': std,\n        'Outlier Count': len(outliers),\n        'Outlier %': len(outliers) / len(df) * 100\n    }, name=col))\noutliers_info = pd.concat(outliers_info, axis=1)\noutliers = df[outliers_mask.any(axis=1)]\noutliers['reason'] = outliers_mask[outliers_mask.any(axis=1)].apply(lambda x: ', '.join(x.index[x]), axis=1)\nnormal = df.loc[(~outliers_mask).all(axis=1)]\n\ndisplay(outliers_info.round(2).T)\ndisplay(outliers)\n```\n\n- 이상치 처리는 전처리 참조\n- HUOE(Heterogeneous Univariate Outlier Ensembles) 방법도 있음 (하지만 이를 위한 library는 없는듯)\n\n## 변수간 관계\n\n### 연속, 연속\n\n- 산점도로 시각화\n- spearman 상관계수 확인\n- spearman - pearson 차이로 비선형 관계 확인 가능\n    - 0.1 이상 차이가 나는지 확인. (공식 기준 아님)\n\n### 범주, 연속\n\n::: {#e9a2122f .cell execution_count=9}\n``` {.python .cell-code}\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    for target_val in df['target'].unique():\n        subset = df[df['target'] == target_val][col]\n        subset.hist(ax=ax[0], label=target_val, density=True)\n    ax[0].set_title(f'{col} Distribution')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Density')\n\n    plt.show()\n```\n:::\n\n\n- 이런 식으로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.\n\n- 분포 확인 후, ANOVA나 Kruskal-Wallis 등으로 그룹 간 차이 검정 진행\n- t 검정을 사용했다면, Cohen's d 등으로 효과 크기 확인 가능\n- 자세한건 분산 분석 파트 참조\n\n### 범주, 범주\n\n- 시각화는 마찬가지로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.\n- chi-square 검정 등으로 독립성 검정 진행\n- Cramer's V 등으로 연관성 정도 확인\n    - 두 변수가 level이 2개면 phi 계수로 확인 가능 (Cramer's V와 동일한 값 나옴)\n\n::: {#6b3a194d .cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import association\n\ncontingency_table = pd.crosstab(df['var1'], df['var2'])\nchi2, p, dof, expected = chi2_contingency(contingency_table, correction=False)\ncramers_v = association(contingency_table, method='cramer')\n\nprint(f\"Chi-square Test: chi2={chi2:.4f}, p-value={p:.4f}\")\nprint(f\"Cramer's V: {cramers_v:.4f}\")\n```\n:::\n\n\n## 독립변수 vs 종속변수\n\n- 간단한 트리 모델링 이후 feature importance 확인 가능\n- 0.01이 넘는 변수들만 따로 분석 진행\n\n::: {#eb845751 .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.feature_selection import mutual_info_regression\n\nmi_score = mutual_info_regression(df[num_cols], df['target']) # df['target']이 범주형이면 mutual_info_classif 사용\nmi_result = pd.Series(mi_score, index=num_cols).sort_values(ascending=False)\nmi_result\n```\n:::\n\n\n- mutual information로 선형 + 비선형 관계 확인 가능\n- 구체적으로 어떤 관계인지는 scatter plot으로 확인 (연속 + 연속이면)\n\n::: {#6f15e907 .cell execution_count=12}\n``` {.python .cell-code}\nimport pingouin as pg\n\npcorr = pg.partial_corr(data=df, x='var1', y='var2', covar='var3', method='')\npcorr\n```\n:::\n\n\n- 하나의 변수를 통제했을 때, 두 변수 간의 상관관계를 분석\n- 내부적으로는 회귀분석을 사용\n- 도메인 지식으로 통제변수 선정해서 진행\n    - 혹은 EDA 과정에서 발견한 관계로 선정 (한 마디로 본인이 알아서 잘 선정하기)\n- 인과분석을 하면 이 과정은 필요 없으나, ADP 환경에서 인과분석 라이브러리는 제공 안하는듯\n\n### Interation Effect\n\n- 두 개 이상의 독립변수가 결합하여 종속변수에 미치는 영향\n- 유효한 상호작용 효과가 있다면 새로운 변수를 만들어서 분석에 포함 (예: new_var = var1 * var2)\n- 자세한건 분산분석 파트 참조\n\n### Polinorminal Relationships\n\n- 회귀분석 파트 참조\n\n## 다변량 분석\n\n### 차원 축소\n\n::: {#3e2875c8 .cell execution_count=13}\n``` {.python .cell-code}\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\nplt.xlabel(f'PC1 ({explained_var_ratio[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({explained_var_ratio[1]:.1%} variance)')\nplt.title('PCA Projection by Support Level')\nplt.colorbar(scatter, label='Support Level')\n```\n:::\n\n\n::: {#2fd420fa .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\ntsne_result = tsne.fit_transform(df[num_cols])\n\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=df['target'])\n```\n:::\n\n\n- UMAP이 전역적으로는 구조를 더 잘 보존하고, 속도도 빠르다.\n- ADP 환경에서 UMAP 라이브러리는 제공 안하는듯\n- 애초에 2차원으로 정사영해서 plot한 이 자료들로 의미있는 해석을 더 할 수 있는지 의문\n\n### DBSCAN, Isolation Forest 등으로 이상치 탐지 가능\n\n- DBSCAN은 비지도 학습 파트 군집분석 참조\n\n::: {#4f020bbc .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import IsolationForest\n\niso_forest = IsolationForest()\noe = OrdinalEncoder()\n\ndf_encoded = df.copy()\ndf_encoded[cat_cols] = oe.fit_transform(df[cat_cols])\noutlier_labels = iso_forest.fit_predict(df_encoded)\noutliers = df[outlier_labels == -1]\nnormal = df[outlier_labels != -1]\ndisplay(outliers)\n```\n:::\n\n\n- 해당 결과들을 voting 등으로 종합해서 이상치로 판단할 수도 있음.\n- 자세한건 modeling 파트의 voting 부분 참조\n- 개인적인 생각: 지워야 하는 이상치는 측정 과정에서의 오류 등인데, 이런 종합적인 방법은 그런 이상치를 찾는데 유용하지는 않은듯 하다.\n    - 그래서 이상치 분석 목적이 아니라면 이 방법은 굳이 안써도 되지 않을까\n\n```{.python filename=\"outlier 연속형 분포\"}\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    # 박스플롯\n    # (이상치 변수 생성 필요)\n    sns.boxplot(x=df['이상치'], y=normal[col], ax=ax[0], data=df)\n    ax[0].set_title(f'{col} Boxplot by Outlier Status')\n\n    sns.kdeplot(normal[col], color='blue', linewidth=2, label='Normal', ax=ax[1])\n    sns.kdeplot(outliers[col], color='orange', linewidth=2, label='Outliers', ax=ax[1])\n    ax[1].set_title(f'{col} Distribution by Outlier Status')\n\n    plt.show()\n```\n\n```{.python filename=\"outlier 범주형 분포\"}\nfor col in cat_cols:\n    normal_counts = normal[col].value_counts().sort_index()\n    outlier_counts = outliers[col].value_counts().sort_index()\n\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    normal_counts.plot.bar(ax=ax[0])\n    ax[0].set_title(f\"{col}'s Count\")\n    ax[0].set_xlabel(f\"{col}'s Level\")\n    ax[0].set_ylabel('count')\n\n    outlier_counts.plot.bar(ax=ax[1], color='orange')\n    ax[1].set_title(f\"{col}'s Count (Outliers)\")\n    ax[1].set_xlabel(f\"{col}'s Level\")\n    ax[1].set_ylabel('count')\n\n    combined_df = pd.DataFrame({'Normal': normal_counts, 'Outliers': outlier_counts}).fillna(0)\n    combined_df.plot.bar(ax=ax[2], stacked=True)\n    ax[2].set_title(f\"{col}'s Count Comparison\")\n    ax[2].set_xlabel(f\"{col}'s Level\")\n    ax[2].set_ylabel('count')\n    plt.show()\n```\n\n- 그 외 profiling 분석이나 맨 휘트니, 카이제곱 독립성 검정 등으로 이상치와 정상치의 차이 분석 가능\n\n",
    "supporting": [
      "02_files"
    ],
    "filters": [],
    "includes": {}
  }
}