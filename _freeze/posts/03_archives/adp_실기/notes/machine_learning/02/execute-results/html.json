{
  "hash": "e253abbb941481257e304cac526c8b61",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"분류 - 앙상블\"\ndate: 2025-07-27\ncategories: [\"머신 러닝\"]\n---\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## voting\n\n- 서로 다른 알고리즘이 결합. 분류에서는 voting[^1]으로 결정\n\n### Example\n\n::: {#c414b35e .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#7616309c .cell execution_count=2}\n``` {.python .cell-code}\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0.9649122807017544\n```\n:::\n:::\n\n\n::: {#973855e5 .cell execution_count=3}\n``` {.python .cell-code}\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogisticRegression 정확도: 0.9649\nKNeighborsClassifier 정확도: 0.9737\n```\n:::\n:::\n\n\n- 반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님\n\n## bagging\n\n- 같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting[^1]으로 결정\n\n### RandomForest\n\n::: {#032f98ff .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n```\n:::\n\n\n::: {#20098dd2 .cell execution_count=5}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.9202578893790295\n```\n:::\n:::\n\n\n[^1]: hard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.\n\n## boosting\n\n### GBM\n\n::: {#0ba1932c .cell execution_count=6}\n``` {.python .cell-code}\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n```\n:::\n\n\n0.939, 701.6343066692352초\n\n- 아주 오래 걸림.\n\n### XGBoost\n\n- 결손값을 자체 처리할 수 있다.\n- 조기 종료 기능이 있다.\n- 자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\n\n- python xgboost\n\n::: {#197a24a7 .cell execution_count=7}\n``` {.python .cell-code}\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n```\n:::\n\n\n::: {#3b50c681 .cell execution_count=8}\n``` {.python .cell-code}\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n```\n:::\n\n\n::: {#cfb6211e .cell execution_count=9}\n``` {.python .cell-code}\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n```\n:::\n\n\n::: {#79c031eb .cell execution_count=10}\n``` {.python .cell-code}\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\ttrain-logloss:0.61609\teval-logloss:0.61857\n[1]\ttrain-logloss:0.58040\teval-logloss:0.57937\n[2]\ttrain-logloss:0.54699\teval-logloss:0.54726\n[3]\ttrain-logloss:0.51661\teval-logloss:0.51558\n[4]\ttrain-logloss:0.48886\teval-logloss:0.48902\n[5]\ttrain-logloss:0.46341\teval-logloss:0.46252\n[6]\ttrain-logloss:0.43863\teval-logloss:0.43778\n[7]\ttrain-logloss:0.41582\teval-logloss:0.41512\n[8]\ttrain-logloss:0.39639\teval-logloss:0.39426\n[9]\ttrain-logloss:0.37669\teval-logloss:0.37501\n[10]\ttrain-logloss:0.35842\teval-logloss:0.35718\n[11]\ttrain-logloss:0.34194\teval-logloss:0.33963\n[12]\ttrain-logloss:0.32601\teval-logloss:0.32432\n[13]\ttrain-logloss:0.31117\teval-logloss:0.31015\n[14]\ttrain-logloss:0.29765\teval-logloss:0.29571\n[15]\ttrain-logloss:0.28477\teval-logloss:0.28348\n[16]\ttrain-logloss:0.27287\teval-logloss:0.27074\n[17]\ttrain-logloss:0.26163\teval-logloss:0.25986\n[18]\ttrain-logloss:0.25122\teval-logloss:0.24828\n[19]\ttrain-logloss:0.24091\teval-logloss:0.23881\n[20]\ttrain-logloss:0.23125\teval-logloss:0.22994\n[21]\ttrain-logloss:0.22256\teval-logloss:0.22082\n[22]\ttrain-logloss:0.21394\teval-logloss:0.21303\n[23]\ttrain-logloss:0.20620\teval-logloss:0.20532\n[24]\ttrain-logloss:0.19864\teval-logloss:0.19827\n[25]\ttrain-logloss:0.19130\teval-logloss:0.19174\n[26]\ttrain-logloss:0.18439\teval-logloss:0.18519\n[27]\ttrain-logloss:0.17818\teval-logloss:0.17781\n[28]\ttrain-logloss:0.17203\teval-logloss:0.17230\n[29]\ttrain-logloss:0.16598\teval-logloss:0.16751\n[30]\ttrain-logloss:0.16056\teval-logloss:0.16282\n[31]\ttrain-logloss:0.15508\teval-logloss:0.15768\n[32]\ttrain-logloss:0.14991\teval-logloss:0.15328\n[33]\ttrain-logloss:0.14502\teval-logloss:0.14914\n[34]\ttrain-logloss:0.14051\teval-logloss:0.14535\n[35]\ttrain-logloss:0.13599\teval-logloss:0.14163\n[36]\ttrain-logloss:0.13171\teval-logloss:0.13731\n[37]\ttrain-logloss:0.12783\teval-logloss:0.13417\n[38]\ttrain-logloss:0.12411\teval-logloss:0.13047\n[39]\ttrain-logloss:0.12061\teval-logloss:0.12729\n[40]\ttrain-logloss:0.11703\teval-logloss:0.12348\n[41]\ttrain-logloss:0.11363\teval-logloss:0.12054\n[42]\ttrain-logloss:0.11022\teval-logloss:0.11668\n[43]\ttrain-logloss:0.10700\teval-logloss:0.11463\n[44]\ttrain-logloss:0.10424\teval-logloss:0.11221\n[45]\ttrain-logloss:0.10149\teval-logloss:0.10911\n[46]\ttrain-logloss:0.09863\teval-logloss:0.10583\n[47]\ttrain-logloss:0.09583\teval-logloss:0.10387\n[48]\ttrain-logloss:0.09352\teval-logloss:0.10191\n[49]\ttrain-logloss:0.09097\teval-logloss:0.10054\n[50]\ttrain-logloss:0.08866\teval-logloss:0.09796\n[51]\ttrain-logloss:0.08631\teval-logloss:0.09640\n[52]\ttrain-logloss:0.08408\teval-logloss:0.09550\n[53]\ttrain-logloss:0.08184\teval-logloss:0.09424\n[54]\ttrain-logloss:0.07985\teval-logloss:0.09255\n[55]\ttrain-logloss:0.07779\teval-logloss:0.09145\n[56]\ttrain-logloss:0.07597\teval-logloss:0.08994\n[57]\ttrain-logloss:0.07390\teval-logloss:0.08903\n[58]\ttrain-logloss:0.07207\teval-logloss:0.08814\n[59]\ttrain-logloss:0.07046\teval-logloss:0.08702\n[60]\ttrain-logloss:0.06864\teval-logloss:0.08658\n[61]\ttrain-logloss:0.06707\teval-logloss:0.08636\n[62]\ttrain-logloss:0.06540\teval-logloss:0.08605\n[63]\ttrain-logloss:0.06378\teval-logloss:0.08493\n[64]\ttrain-logloss:0.06225\teval-logloss:0.08455\n[65]\ttrain-logloss:0.06081\teval-logloss:0.08363\n[66]\ttrain-logloss:0.05949\teval-logloss:0.08388\n[67]\ttrain-logloss:0.05811\teval-logloss:0.08365\n[68]\ttrain-logloss:0.05689\teval-logloss:0.08205\n[69]\ttrain-logloss:0.05568\teval-logloss:0.08171\n[70]\ttrain-logloss:0.05457\teval-logloss:0.08016\n[71]\ttrain-logloss:0.05351\teval-logloss:0.08038\n[72]\ttrain-logloss:0.05239\teval-logloss:0.07976\n[73]\ttrain-logloss:0.05134\teval-logloss:0.08018\n[74]\ttrain-logloss:0.05037\teval-logloss:0.07902\n[75]\ttrain-logloss:0.04942\teval-logloss:0.07762\n[76]\ttrain-logloss:0.04839\teval-logloss:0.07766\n[77]\ttrain-logloss:0.04749\teval-logloss:0.07717\n[78]\ttrain-logloss:0.04658\teval-logloss:0.07767\n[79]\ttrain-logloss:0.04577\teval-logloss:0.07592\n[80]\ttrain-logloss:0.04486\teval-logloss:0.07606\n[81]\ttrain-logloss:0.04411\teval-logloss:0.07518\n[82]\ttrain-logloss:0.04309\teval-logloss:0.07454\n[83]\ttrain-logloss:0.04233\teval-logloss:0.07458\n[84]\ttrain-logloss:0.04165\teval-logloss:0.07374\n[85]\ttrain-logloss:0.04100\teval-logloss:0.07286\n[86]\ttrain-logloss:0.04036\teval-logloss:0.07287\n[87]\ttrain-logloss:0.03968\teval-logloss:0.07180\n[88]\ttrain-logloss:0.03895\teval-logloss:0.07202\n[89]\ttrain-logloss:0.03829\teval-logloss:0.07149\n[90]\ttrain-logloss:0.03770\teval-logloss:0.07164\n[91]\ttrain-logloss:0.03708\teval-logloss:0.07075\n[92]\ttrain-logloss:0.03654\teval-logloss:0.07093\n[93]\ttrain-logloss:0.03594\teval-logloss:0.06992\n[94]\ttrain-logloss:0.03533\teval-logloss:0.07021\n[95]\ttrain-logloss:0.03470\teval-logloss:0.06922\n[96]\ttrain-logloss:0.03416\teval-logloss:0.06869\n[97]\ttrain-logloss:0.03356\teval-logloss:0.06927\n[98]\ttrain-logloss:0.03304\teval-logloss:0.06885\n[99]\ttrain-logloss:0.03257\teval-logloss:0.06830\n[100]\ttrain-logloss:0.03212\teval-logloss:0.06852\n[101]\ttrain-logloss:0.03160\teval-logloss:0.06760\n[102]\ttrain-logloss:0.03113\teval-logloss:0.06781\n[103]\ttrain-logloss:0.03065\teval-logloss:0.06694\n[104]\ttrain-logloss:0.03013\teval-logloss:0.06669\n[105]\ttrain-logloss:0.02980\teval-logloss:0.06668\n[106]\ttrain-logloss:0.02941\teval-logloss:0.06706\n[107]\ttrain-logloss:0.02899\teval-logloss:0.06699\n[108]\ttrain-logloss:0.02858\teval-logloss:0.06723\n[109]\ttrain-logloss:0.02823\teval-logloss:0.06683\n[110]\ttrain-logloss:0.02781\teval-logloss:0.06724\n[111]\ttrain-logloss:0.02740\teval-logloss:0.06759\n[112]\ttrain-logloss:0.02703\teval-logloss:0.06747\n[113]\ttrain-logloss:0.02667\teval-logloss:0.06686\n[114]\ttrain-logloss:0.02629\teval-logloss:0.06722\n[115]\ttrain-logloss:0.02602\teval-logloss:0.06677\n[116]\ttrain-logloss:0.02565\teval-logloss:0.06717\n[117]\ttrain-logloss:0.02522\teval-logloss:0.06700\n[118]\ttrain-logloss:0.02492\teval-logloss:0.06728\n[119]\ttrain-logloss:0.02467\teval-logloss:0.06681\n[120]\ttrain-logloss:0.02436\teval-logloss:0.06677\n[121]\ttrain-logloss:0.02409\teval-logloss:0.06706\n[122]\ttrain-logloss:0.02384\teval-logloss:0.06667\n[123]\ttrain-logloss:0.02352\teval-logloss:0.06583\n[124]\ttrain-logloss:0.02322\teval-logloss:0.06562\n[125]\ttrain-logloss:0.02294\teval-logloss:0.06558\n[126]\ttrain-logloss:0.02270\teval-logloss:0.06588\n[127]\ttrain-logloss:0.02249\teval-logloss:0.06545\n[128]\ttrain-logloss:0.02221\teval-logloss:0.06507\n[129]\ttrain-logloss:0.02194\teval-logloss:0.06535\n[130]\ttrain-logloss:0.02166\teval-logloss:0.06575\n[131]\ttrain-logloss:0.02134\teval-logloss:0.06563\n[132]\ttrain-logloss:0.02119\teval-logloss:0.06541\n[133]\ttrain-logloss:0.02094\teval-logloss:0.06570\n[134]\ttrain-logloss:0.02071\teval-logloss:0.06568\n[135]\ttrain-logloss:0.02056\teval-logloss:0.06571\n[136]\ttrain-logloss:0.02031\teval-logloss:0.06611\n[137]\ttrain-logloss:0.02012\teval-logloss:0.06624\n[138]\ttrain-logloss:0.01997\teval-logloss:0.06603\n[139]\ttrain-logloss:0.01975\teval-logloss:0.06630\n[140]\ttrain-logloss:0.01955\teval-logloss:0.06577\n[141]\ttrain-logloss:0.01937\teval-logloss:0.06553\n[142]\ttrain-logloss:0.01919\teval-logloss:0.06584\n[143]\ttrain-logloss:0.01907\teval-logloss:0.06568\n[144]\ttrain-logloss:0.01888\teval-logloss:0.06596\n[145]\ttrain-logloss:0.01869\teval-logloss:0.06544\n[146]\ttrain-logloss:0.01853\teval-logloss:0.06589\n[147]\ttrain-logloss:0.01838\teval-logloss:0.06601\n[148]\ttrain-logloss:0.01820\teval-logloss:0.06595\n[149]\ttrain-logloss:0.01803\teval-logloss:0.06545\n[150]\ttrain-logloss:0.01792\teval-logloss:0.06524\n[151]\ttrain-logloss:0.01777\teval-logloss:0.06548\n[152]\ttrain-logloss:0.01762\teval-logloss:0.06535\n[153]\ttrain-logloss:0.01752\teval-logloss:0.06519\n[154]\ttrain-logloss:0.01733\teval-logloss:0.06510\n[155]\ttrain-logloss:0.01716\teval-logloss:0.06473\n[156]\ttrain-logloss:0.01706\teval-logloss:0.06453\n[157]\ttrain-logloss:0.01696\teval-logloss:0.06470\n[158]\ttrain-logloss:0.01682\teval-logloss:0.06440\n[159]\ttrain-logloss:0.01673\teval-logloss:0.06457\n[160]\ttrain-logloss:0.01658\teval-logloss:0.06443\n[161]\ttrain-logloss:0.01649\teval-logloss:0.06425\n[162]\ttrain-logloss:0.01638\teval-logloss:0.06397\n[163]\ttrain-logloss:0.01624\teval-logloss:0.06350\n[164]\ttrain-logloss:0.01614\teval-logloss:0.06333\n[165]\ttrain-logloss:0.01602\teval-logloss:0.06321\n[166]\ttrain-logloss:0.01584\teval-logloss:0.06334\n[167]\ttrain-logloss:0.01572\teval-logloss:0.06377\n[168]\ttrain-logloss:0.01564\teval-logloss:0.06361\n[169]\ttrain-logloss:0.01552\teval-logloss:0.06333\n[170]\ttrain-logloss:0.01541\teval-logloss:0.06355\n[171]\ttrain-logloss:0.01534\teval-logloss:0.06367\n[172]\ttrain-logloss:0.01525\teval-logloss:0.06338\n[173]\ttrain-logloss:0.01516\teval-logloss:0.06361\n[174]\ttrain-logloss:0.01504\teval-logloss:0.06384\n[175]\ttrain-logloss:0.01497\teval-logloss:0.06401\n[176]\ttrain-logloss:0.01488\teval-logloss:0.06423\n[177]\ttrain-logloss:0.01482\teval-logloss:0.06428\n[178]\ttrain-logloss:0.01475\teval-logloss:0.06435\n[179]\ttrain-logloss:0.01464\teval-logloss:0.06455\n[180]\ttrain-logloss:0.01457\teval-logloss:0.06438\n[181]\ttrain-logloss:0.01442\teval-logloss:0.06440\n[182]\ttrain-logloss:0.01436\teval-logloss:0.06420\n[183]\ttrain-logloss:0.01426\teval-logloss:0.06440\n[184]\ttrain-logloss:0.01416\teval-logloss:0.06431\n[185]\ttrain-logloss:0.01410\teval-logloss:0.06436\n[186]\ttrain-logloss:0.01398\teval-logloss:0.06434\n[187]\ttrain-logloss:0.01385\teval-logloss:0.06436\n[188]\ttrain-logloss:0.01379\teval-logloss:0.06418\n[189]\ttrain-logloss:0.01371\teval-logloss:0.06440\n[190]\ttrain-logloss:0.01365\teval-logloss:0.06445\n[191]\ttrain-logloss:0.01359\teval-logloss:0.06451\n[192]\ttrain-logloss:0.01350\teval-logloss:0.06442\n[193]\ttrain-logloss:0.01343\teval-logloss:0.06463\n[194]\ttrain-logloss:0.01331\teval-logloss:0.06466\n[195]\ttrain-logloss:0.01325\teval-logloss:0.06477\n[196]\ttrain-logloss:0.01319\teval-logloss:0.06469\n[197]\ttrain-logloss:0.01313\teval-logloss:0.06455\n[198]\ttrain-logloss:0.01302\teval-logloss:0.06458\n[199]\ttrain-logloss:0.01297\teval-logloss:0.06463\n[200]\ttrain-logloss:0.01289\teval-logloss:0.06454\n[201]\ttrain-logloss:0.01284\teval-logloss:0.06440\n[202]\ttrain-logloss:0.01277\teval-logloss:0.06461\n[203]\ttrain-logloss:0.01272\teval-logloss:0.06472\n[204]\ttrain-logloss:0.01262\teval-logloss:0.06472\n[205]\ttrain-logloss:0.01256\teval-logloss:0.06493\n[206]\ttrain-logloss:0.01250\teval-logloss:0.06486\n[207]\ttrain-logloss:0.01245\teval-logloss:0.06491\n[208]\ttrain-logloss:0.01237\teval-logloss:0.06479\n[209]\ttrain-logloss:0.01232\teval-logloss:0.06486\n[210]\ttrain-logloss:0.01227\teval-logloss:0.06506\n[211]\ttrain-logloss:0.01222\teval-logloss:0.06511\n[212]\ttrain-logloss:0.01218\teval-logloss:0.06501\n[213]\ttrain-logloss:0.01210\teval-logloss:0.06492\n[214]\ttrain-logloss:0.01205\teval-logloss:0.06512\n```\n:::\n:::\n\n\n::: {#bc1d5654 .cell execution_count=11}\n``` {.python .cell-code}\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x > 0.5 else 0 for x in pred_probs]\n```\n:::\n\n\n- sklearn xgboost\n\n::: {#966b1977 .cell execution_count=12}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-logloss:0.61609\tvalidation_1-logloss:0.61857\n[1]\tvalidation_0-logloss:0.58040\tvalidation_1-logloss:0.57937\n[2]\tvalidation_0-logloss:0.54699\tvalidation_1-logloss:0.54726\n[3]\tvalidation_0-logloss:0.51661\tvalidation_1-logloss:0.51558\n[4]\tvalidation_0-logloss:0.48886\tvalidation_1-logloss:0.48902\n[5]\tvalidation_0-logloss:0.46341\tvalidation_1-logloss:0.46252\n[6]\tvalidation_0-logloss:0.43863\tvalidation_1-logloss:0.43778\n[7]\tvalidation_0-logloss:0.41582\tvalidation_1-logloss:0.41512\n[8]\tvalidation_0-logloss:0.39639\tvalidation_1-logloss:0.39426\n[9]\tvalidation_0-logloss:0.37669\tvalidation_1-logloss:0.37501\n[10]\tvalidation_0-logloss:0.35842\tvalidation_1-logloss:0.35718\n[11]\tvalidation_0-logloss:0.34194\tvalidation_1-logloss:0.33963\n[12]\tvalidation_0-logloss:0.32601\tvalidation_1-logloss:0.32432\n[13]\tvalidation_0-logloss:0.31117\tvalidation_1-logloss:0.31015\n[14]\tvalidation_0-logloss:0.29765\tvalidation_1-logloss:0.29571\n[15]\tvalidation_0-logloss:0.28477\tvalidation_1-logloss:0.28348\n[16]\tvalidation_0-logloss:0.27287\tvalidation_1-logloss:0.27074\n[17]\tvalidation_0-logloss:0.26163\tvalidation_1-logloss:0.25986\n[18]\tvalidation_0-logloss:0.25122\tvalidation_1-logloss:0.24828\n[19]\tvalidation_0-logloss:0.24091\tvalidation_1-logloss:0.23881\n[20]\tvalidation_0-logloss:0.23125\tvalidation_1-logloss:0.22994\n[21]\tvalidation_0-logloss:0.22256\tvalidation_1-logloss:0.22082\n[22]\tvalidation_0-logloss:0.21394\tvalidation_1-logloss:0.21303\n[23]\tvalidation_0-logloss:0.20620\tvalidation_1-logloss:0.20532\n[24]\tvalidation_0-logloss:0.19864\tvalidation_1-logloss:0.19827\n[25]\tvalidation_0-logloss:0.19130\tvalidation_1-logloss:0.19174\n[26]\tvalidation_0-logloss:0.18439\tvalidation_1-logloss:0.18519\n[27]\tvalidation_0-logloss:0.17818\tvalidation_1-logloss:0.17781\n[28]\tvalidation_0-logloss:0.17203\tvalidation_1-logloss:0.17230\n[29]\tvalidation_0-logloss:0.16598\tvalidation_1-logloss:0.16751\n[30]\tvalidation_0-logloss:0.16056\tvalidation_1-logloss:0.16282\n[31]\tvalidation_0-logloss:0.15508\tvalidation_1-logloss:0.15768\n[32]\tvalidation_0-logloss:0.14991\tvalidation_1-logloss:0.15328\n[33]\tvalidation_0-logloss:0.14502\tvalidation_1-logloss:0.14914\n[34]\tvalidation_0-logloss:0.14051\tvalidation_1-logloss:0.14535\n[35]\tvalidation_0-logloss:0.13599\tvalidation_1-logloss:0.14163\n[36]\tvalidation_0-logloss:0.13171\tvalidation_1-logloss:0.13731\n[37]\tvalidation_0-logloss:0.12783\tvalidation_1-logloss:0.13417\n[38]\tvalidation_0-logloss:0.12411\tvalidation_1-logloss:0.13047\n[39]\tvalidation_0-logloss:0.12061\tvalidation_1-logloss:0.12729\n[40]\tvalidation_0-logloss:0.11703\tvalidation_1-logloss:0.12348\n[41]\tvalidation_0-logloss:0.11363\tvalidation_1-logloss:0.12054\n[42]\tvalidation_0-logloss:0.11022\tvalidation_1-logloss:0.11668\n[43]\tvalidation_0-logloss:0.10700\tvalidation_1-logloss:0.11463\n[44]\tvalidation_0-logloss:0.10424\tvalidation_1-logloss:0.11221\n[45]\tvalidation_0-logloss:0.10149\tvalidation_1-logloss:0.10911\n[46]\tvalidation_0-logloss:0.09863\tvalidation_1-logloss:0.10583\n[47]\tvalidation_0-logloss:0.09583\tvalidation_1-logloss:0.10387\n[48]\tvalidation_0-logloss:0.09352\tvalidation_1-logloss:0.10191\n[49]\tvalidation_0-logloss:0.09097\tvalidation_1-logloss:0.10054\n[50]\tvalidation_0-logloss:0.08866\tvalidation_1-logloss:0.09796\n[51]\tvalidation_0-logloss:0.08631\tvalidation_1-logloss:0.09640\n[52]\tvalidation_0-logloss:0.08408\tvalidation_1-logloss:0.09550\n[53]\tvalidation_0-logloss:0.08184\tvalidation_1-logloss:0.09424\n[54]\tvalidation_0-logloss:0.07985\tvalidation_1-logloss:0.09255\n[55]\tvalidation_0-logloss:0.07779\tvalidation_1-logloss:0.09145\n[56]\tvalidation_0-logloss:0.07597\tvalidation_1-logloss:0.08994\n[57]\tvalidation_0-logloss:0.07390\tvalidation_1-logloss:0.08903\n[58]\tvalidation_0-logloss:0.07207\tvalidation_1-logloss:0.08814\n[59]\tvalidation_0-logloss:0.07046\tvalidation_1-logloss:0.08702\n[60]\tvalidation_0-logloss:0.06864\tvalidation_1-logloss:0.08658\n[61]\tvalidation_0-logloss:0.06707\tvalidation_1-logloss:0.08636\n[62]\tvalidation_0-logloss:0.06540\tvalidation_1-logloss:0.08605\n[63]\tvalidation_0-logloss:0.06378\tvalidation_1-logloss:0.08493\n[64]\tvalidation_0-logloss:0.06225\tvalidation_1-logloss:0.08455\n[65]\tvalidation_0-logloss:0.06081\tvalidation_1-logloss:0.08363\n[66]\tvalidation_0-logloss:0.05949\tvalidation_1-logloss:0.08388\n[67]\tvalidation_0-logloss:0.05811\tvalidation_1-logloss:0.08365\n[68]\tvalidation_0-logloss:0.05689\tvalidation_1-logloss:0.08205\n[69]\tvalidation_0-logloss:0.05568\tvalidation_1-logloss:0.08171\n[70]\tvalidation_0-logloss:0.05457\tvalidation_1-logloss:0.08016\n[71]\tvalidation_0-logloss:0.05351\tvalidation_1-logloss:0.08038\n[72]\tvalidation_0-logloss:0.05239\tvalidation_1-logloss:0.07976\n[73]\tvalidation_0-logloss:0.05134\tvalidation_1-logloss:0.08018\n[74]\tvalidation_0-logloss:0.05037\tvalidation_1-logloss:0.07902\n[75]\tvalidation_0-logloss:0.04942\tvalidation_1-logloss:0.07762\n[76]\tvalidation_0-logloss:0.04839\tvalidation_1-logloss:0.07766\n[77]\tvalidation_0-logloss:0.04749\tvalidation_1-logloss:0.07717\n[78]\tvalidation_0-logloss:0.04658\tvalidation_1-logloss:0.07767\n[79]\tvalidation_0-logloss:0.04577\tvalidation_1-logloss:0.07592\n[80]\tvalidation_0-logloss:0.04486\tvalidation_1-logloss:0.07606\n[81]\tvalidation_0-logloss:0.04411\tvalidation_1-logloss:0.07518\n[82]\tvalidation_0-logloss:0.04309\tvalidation_1-logloss:0.07454\n[83]\tvalidation_0-logloss:0.04233\tvalidation_1-logloss:0.07458\n[84]\tvalidation_0-logloss:0.04165\tvalidation_1-logloss:0.07374\n[85]\tvalidation_0-logloss:0.04100\tvalidation_1-logloss:0.07286\n[86]\tvalidation_0-logloss:0.04036\tvalidation_1-logloss:0.07287\n[87]\tvalidation_0-logloss:0.03968\tvalidation_1-logloss:0.07180\n[88]\tvalidation_0-logloss:0.03895\tvalidation_1-logloss:0.07202\n[89]\tvalidation_0-logloss:0.03829\tvalidation_1-logloss:0.07149\n[90]\tvalidation_0-logloss:0.03770\tvalidation_1-logloss:0.07164\n[91]\tvalidation_0-logloss:0.03708\tvalidation_1-logloss:0.07075\n[92]\tvalidation_0-logloss:0.03654\tvalidation_1-logloss:0.07093\n[93]\tvalidation_0-logloss:0.03594\tvalidation_1-logloss:0.06992\n[94]\tvalidation_0-logloss:0.03533\tvalidation_1-logloss:0.07021\n[95]\tvalidation_0-logloss:0.03470\tvalidation_1-logloss:0.06922\n[96]\tvalidation_0-logloss:0.03416\tvalidation_1-logloss:0.06869\n[97]\tvalidation_0-logloss:0.03356\tvalidation_1-logloss:0.06927\n[98]\tvalidation_0-logloss:0.03304\tvalidation_1-logloss:0.06885\n[99]\tvalidation_0-logloss:0.03257\tvalidation_1-logloss:0.06830\n[100]\tvalidation_0-logloss:0.03212\tvalidation_1-logloss:0.06852\n[101]\tvalidation_0-logloss:0.03160\tvalidation_1-logloss:0.06760\n[102]\tvalidation_0-logloss:0.03113\tvalidation_1-logloss:0.06781\n[103]\tvalidation_0-logloss:0.03065\tvalidation_1-logloss:0.06694\n[104]\tvalidation_0-logloss:0.03013\tvalidation_1-logloss:0.06669\n[105]\tvalidation_0-logloss:0.02980\tvalidation_1-logloss:0.06668\n[106]\tvalidation_0-logloss:0.02941\tvalidation_1-logloss:0.06706\n[107]\tvalidation_0-logloss:0.02899\tvalidation_1-logloss:0.06699\n[108]\tvalidation_0-logloss:0.02858\tvalidation_1-logloss:0.06723\n[109]\tvalidation_0-logloss:0.02823\tvalidation_1-logloss:0.06683\n[110]\tvalidation_0-logloss:0.02781\tvalidation_1-logloss:0.06724\n[111]\tvalidation_0-logloss:0.02740\tvalidation_1-logloss:0.06759\n[112]\tvalidation_0-logloss:0.02703\tvalidation_1-logloss:0.06747\n[113]\tvalidation_0-logloss:0.02667\tvalidation_1-logloss:0.06686\n[114]\tvalidation_0-logloss:0.02629\tvalidation_1-logloss:0.06722\n[115]\tvalidation_0-logloss:0.02602\tvalidation_1-logloss:0.06677\n[116]\tvalidation_0-logloss:0.02565\tvalidation_1-logloss:0.06717\n[117]\tvalidation_0-logloss:0.02522\tvalidation_1-logloss:0.06700\n[118]\tvalidation_0-logloss:0.02492\tvalidation_1-logloss:0.06728\n[119]\tvalidation_0-logloss:0.02467\tvalidation_1-logloss:0.06681\n[120]\tvalidation_0-logloss:0.02436\tvalidation_1-logloss:0.06677\n[121]\tvalidation_0-logloss:0.02409\tvalidation_1-logloss:0.06706\n[122]\tvalidation_0-logloss:0.02384\tvalidation_1-logloss:0.06667\n[123]\tvalidation_0-logloss:0.02352\tvalidation_1-logloss:0.06583\n[124]\tvalidation_0-logloss:0.02322\tvalidation_1-logloss:0.06562\n[125]\tvalidation_0-logloss:0.02294\tvalidation_1-logloss:0.06558\n[126]\tvalidation_0-logloss:0.02270\tvalidation_1-logloss:0.06588\n[127]\tvalidation_0-logloss:0.02249\tvalidation_1-logloss:0.06545\n[128]\tvalidation_0-logloss:0.02221\tvalidation_1-logloss:0.06507\n[129]\tvalidation_0-logloss:0.02194\tvalidation_1-logloss:0.06535\n[130]\tvalidation_0-logloss:0.02166\tvalidation_1-logloss:0.06575\n[131]\tvalidation_0-logloss:0.02134\tvalidation_1-logloss:0.06563\n[132]\tvalidation_0-logloss:0.02119\tvalidation_1-logloss:0.06541\n[133]\tvalidation_0-logloss:0.02094\tvalidation_1-logloss:0.06570\n[134]\tvalidation_0-logloss:0.02071\tvalidation_1-logloss:0.06568\n[135]\tvalidation_0-logloss:0.02056\tvalidation_1-logloss:0.06571\n[136]\tvalidation_0-logloss:0.02031\tvalidation_1-logloss:0.06611\n[137]\tvalidation_0-logloss:0.02012\tvalidation_1-logloss:0.06624\n[138]\tvalidation_0-logloss:0.01997\tvalidation_1-logloss:0.06603\n[139]\tvalidation_0-logloss:0.01975\tvalidation_1-logloss:0.06630\n[140]\tvalidation_0-logloss:0.01955\tvalidation_1-logloss:0.06577\n[141]\tvalidation_0-logloss:0.01937\tvalidation_1-logloss:0.06553\n[142]\tvalidation_0-logloss:0.01919\tvalidation_1-logloss:0.06584\n[143]\tvalidation_0-logloss:0.01907\tvalidation_1-logloss:0.06568\n[144]\tvalidation_0-logloss:0.01888\tvalidation_1-logloss:0.06596\n[145]\tvalidation_0-logloss:0.01869\tvalidation_1-logloss:0.06544\n[146]\tvalidation_0-logloss:0.01853\tvalidation_1-logloss:0.06589\n[147]\tvalidation_0-logloss:0.01838\tvalidation_1-logloss:0.06601\n[148]\tvalidation_0-logloss:0.01820\tvalidation_1-logloss:0.06595\n[149]\tvalidation_0-logloss:0.01803\tvalidation_1-logloss:0.06545\n[150]\tvalidation_0-logloss:0.01792\tvalidation_1-logloss:0.06524\n[151]\tvalidation_0-logloss:0.01777\tvalidation_1-logloss:0.06548\n[152]\tvalidation_0-logloss:0.01762\tvalidation_1-logloss:0.06535\n[153]\tvalidation_0-logloss:0.01752\tvalidation_1-logloss:0.06519\n[154]\tvalidation_0-logloss:0.01733\tvalidation_1-logloss:0.06510\n[155]\tvalidation_0-logloss:0.01716\tvalidation_1-logloss:0.06473\n[156]\tvalidation_0-logloss:0.01706\tvalidation_1-logloss:0.06453\n[157]\tvalidation_0-logloss:0.01696\tvalidation_1-logloss:0.06470\n[158]\tvalidation_0-logloss:0.01682\tvalidation_1-logloss:0.06440\n[159]\tvalidation_0-logloss:0.01673\tvalidation_1-logloss:0.06457\n[160]\tvalidation_0-logloss:0.01658\tvalidation_1-logloss:0.06443\n[161]\tvalidation_0-logloss:0.01649\tvalidation_1-logloss:0.06425\n[162]\tvalidation_0-logloss:0.01638\tvalidation_1-logloss:0.06397\n[163]\tvalidation_0-logloss:0.01624\tvalidation_1-logloss:0.06350\n[164]\tvalidation_0-logloss:0.01614\tvalidation_1-logloss:0.06333\n[165]\tvalidation_0-logloss:0.01602\tvalidation_1-logloss:0.06321\n[166]\tvalidation_0-logloss:0.01584\tvalidation_1-logloss:0.06334\n[167]\tvalidation_0-logloss:0.01572\tvalidation_1-logloss:0.06377\n[168]\tvalidation_0-logloss:0.01564\tvalidation_1-logloss:0.06361\n[169]\tvalidation_0-logloss:0.01552\tvalidation_1-logloss:0.06333\n[170]\tvalidation_0-logloss:0.01541\tvalidation_1-logloss:0.06355\n[171]\tvalidation_0-logloss:0.01534\tvalidation_1-logloss:0.06367\n[172]\tvalidation_0-logloss:0.01525\tvalidation_1-logloss:0.06338\n[173]\tvalidation_0-logloss:0.01516\tvalidation_1-logloss:0.06361\n[174]\tvalidation_0-logloss:0.01504\tvalidation_1-logloss:0.06384\n[175]\tvalidation_0-logloss:0.01497\tvalidation_1-logloss:0.06401\n[176]\tvalidation_0-logloss:0.01488\tvalidation_1-logloss:0.06423\n[177]\tvalidation_0-logloss:0.01482\tvalidation_1-logloss:0.06428\n[178]\tvalidation_0-logloss:0.01475\tvalidation_1-logloss:0.06435\n[179]\tvalidation_0-logloss:0.01464\tvalidation_1-logloss:0.06455\n[180]\tvalidation_0-logloss:0.01457\tvalidation_1-logloss:0.06438\n[181]\tvalidation_0-logloss:0.01442\tvalidation_1-logloss:0.06440\n[182]\tvalidation_0-logloss:0.01436\tvalidation_1-logloss:0.06420\n[183]\tvalidation_0-logloss:0.01426\tvalidation_1-logloss:0.06440\n[184]\tvalidation_0-logloss:0.01416\tvalidation_1-logloss:0.06431\n[185]\tvalidation_0-logloss:0.01410\tvalidation_1-logloss:0.06436\n[186]\tvalidation_0-logloss:0.01398\tvalidation_1-logloss:0.06434\n[187]\tvalidation_0-logloss:0.01385\tvalidation_1-logloss:0.06436\n[188]\tvalidation_0-logloss:0.01379\tvalidation_1-logloss:0.06418\n[189]\tvalidation_0-logloss:0.01371\tvalidation_1-logloss:0.06440\n[190]\tvalidation_0-logloss:0.01365\tvalidation_1-logloss:0.06445\n[191]\tvalidation_0-logloss:0.01359\tvalidation_1-logloss:0.06451\n[192]\tvalidation_0-logloss:0.01350\tvalidation_1-logloss:0.06442\n[193]\tvalidation_0-logloss:0.01343\tvalidation_1-logloss:0.06463\n[194]\tvalidation_0-logloss:0.01331\tvalidation_1-logloss:0.06466\n[195]\tvalidation_0-logloss:0.01325\tvalidation_1-logloss:0.06477\n[196]\tvalidation_0-logloss:0.01319\tvalidation_1-logloss:0.06469\n[197]\tvalidation_0-logloss:0.01313\tvalidation_1-logloss:0.06455\n[198]\tvalidation_0-logloss:0.01302\tvalidation_1-logloss:0.06458\n[199]\tvalidation_0-logloss:0.01297\tvalidation_1-logloss:0.06463\n[200]\tvalidation_0-logloss:0.01289\tvalidation_1-logloss:0.06454\n[201]\tvalidation_0-logloss:0.01284\tvalidation_1-logloss:0.06440\n[202]\tvalidation_0-logloss:0.01277\tvalidation_1-logloss:0.06461\n[203]\tvalidation_0-logloss:0.01272\tvalidation_1-logloss:0.06472\n[204]\tvalidation_0-logloss:0.01262\tvalidation_1-logloss:0.06472\n[205]\tvalidation_0-logloss:0.01256\tvalidation_1-logloss:0.06493\n[206]\tvalidation_0-logloss:0.01250\tvalidation_1-logloss:0.06486\n[207]\tvalidation_0-logloss:0.01245\tvalidation_1-logloss:0.06491\n[208]\tvalidation_0-logloss:0.01237\tvalidation_1-logloss:0.06479\n[209]\tvalidation_0-logloss:0.01232\tvalidation_1-logloss:0.06486\n[210]\tvalidation_0-logloss:0.01227\tvalidation_1-logloss:0.06506\n[211]\tvalidation_0-logloss:0.01222\tvalidation_1-logloss:0.06511\n[212]\tvalidation_0-logloss:0.01218\tvalidation_1-logloss:0.06501\n[213]\tvalidation_0-logloss:0.01210\tvalidation_1-logloss:0.06492\n[214]\tvalidation_0-logloss:0.01205\tvalidation_1-logloss:0.06512\n```\n:::\n:::\n\n\n### LightGBM\n\n- 성능은 xgboost랑 별로 차이가 없음.\n- 1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\n- one hot 인코딩 필요 없음\n\n- python lightgbm\n\n::: {#4df79238 .cell execution_count=13}\n``` {.python .cell-code}\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[LightGBM] [Info] Number of positive: 260, number of negative: 149\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4098\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.635697 -> initscore=0.556735\n[LightGBM] [Info] Start training from score 0.556735\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[328]\ttraining's binary_logloss: 1.55197e-05\tvalid_1's binary_logloss: 0.0124765\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02_files/figure-html/cell-14-output-2.png){width=648 height=449}\n:::\n:::\n\n\n## stacking\n\n::: {#66ad411a .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n```\n:::\n\n\n::: {#42ce8b0f .cell execution_count=15}\n``` {.python .cell-code}\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n```\n:::\n\n\n::: {#f6c9f9d4 .cell execution_count=16}\n``` {.python .cell-code}\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.956\n```\n:::\n:::\n\n\n- test 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n### CV 세트 기반 stacking\n\n::: {#b83d55a0 .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n```\n:::\n\n\n::: {#a3f558c7 .cell execution_count=18}\n``` {.python .cell-code}\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n```\n:::\n\n\n::: {#0308e5df .cell execution_count=19}\n``` {.python .cell-code}\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.956\n```\n:::\n:::\n\n\n## Baysian Optimization\n\n- Grid search로는 시간이 너무 오래 걸리는 경우\n\n- 목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\n- Surrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\n- acquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n::: {#533e6aaa .cell execution_count=20}\n``` {.python .cell-code}\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]\r100%|██████████| 20/20 [00:00<00:00, 1833.21trial/s, best loss: -279.0]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n{'x': -1.0, 'y': 14.0}\n```\n:::\n:::\n\n\n### XGBoost 하이퍼파라미터 최적화\n\n::: {#3e95d4b3 .cell execution_count=21}\n``` {.python .cell-code}\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n```\n:::\n\n\n::: {#ac1cefb2 .cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\r  2%|▏         | 1/50 [00:00<00:18,  2.67trial/s, best loss: -0.964868711513884]\r  6%|▌         | 3/50 [00:00<00:07,  6.18trial/s, best loss: -0.9736551644010688]\r 10%|█         | 5/50 [00:00<00:05,  8.07trial/s, best loss: -0.9736551644010688]\r 12%|█▏        | 6/50 [00:00<00:05,  8.29trial/s, best loss: -0.9736551644010688]\r 14%|█▍        | 7/50 [00:01<00:05,  7.33trial/s, best loss: -0.9736551644010688]\r 16%|█▌        | 8/50 [00:01<00:05,  7.31trial/s, best loss: -0.9736551644010688]\r 18%|█▊        | 9/50 [00:02<00:21,  1.90trial/s, best loss: -0.9736551644010688]\r 20%|██        | 10/50 [00:02<00:18,  2.14trial/s, best loss: -0.9736551644010688]\r 22%|██▏       | 11/50 [00:03<00:15,  2.45trial/s, best loss: -0.9736551644010688]\r 24%|██▍       | 12/50 [00:03<00:14,  2.60trial/s, best loss: -0.9736551644010688]\r 26%|██▌       | 13/50 [00:03<00:12,  3.00trial/s, best loss: -0.9736551644010688]\r 28%|██▊       | 14/50 [00:03<00:10,  3.36trial/s, best loss: -0.9736551644010688]\r 30%|███       | 15/50 [00:04<00:09,  3.69trial/s, best loss: -0.9736551644010688]\r 32%|███▏      | 16/50 [00:04<00:09,  3.44trial/s, best loss: -0.9736551644010688]\r 34%|███▍      | 17/50 [00:04<00:08,  3.85trial/s, best loss: -0.9736551644010688]\r 36%|███▌      | 18/50 [00:04<00:07,  4.23trial/s, best loss: -0.9736551644010688]\r 38%|███▊      | 19/50 [00:05<00:07,  4.41trial/s, best loss: -0.9736551644010688]\r 40%|████      | 20/50 [00:05<00:06,  4.29trial/s, best loss: -0.9736551644010688]\r 42%|████▏     | 21/50 [00:05<00:06,  4.21trial/s, best loss: -0.9736551644010688]\r 44%|████▍     | 22/50 [00:05<00:06,  4.51trial/s, best loss: -0.9736696874636923]\r 46%|████▌     | 23/50 [00:05<00:05,  4.85trial/s, best loss: -0.9736696874636923]\r 48%|████▊     | 24/50 [00:06<00:05,  5.06trial/s, best loss: -0.9736696874636923]\r 50%|█████     | 25/50 [00:06<00:04,  5.05trial/s, best loss: -0.9736696874636923]\r 52%|█████▏    | 26/50 [00:06<00:04,  4.85trial/s, best loss: -0.9736696874636923]\r 54%|█████▍    | 27/50 [00:06<00:04,  4.85trial/s, best loss: -0.9736696874636923]\r 56%|█████▌    | 28/50 [00:06<00:04,  4.77trial/s, best loss: -0.9736696874636923]\r 58%|█████▊    | 29/50 [00:07<00:04,  5.08trial/s, best loss: -0.9736696874636923]\r 60%|██████    | 30/50 [00:07<00:03,  5.12trial/s, best loss: -0.9736696874636923]\r 62%|██████▏   | 31/50 [00:07<00:03,  4.82trial/s, best loss: -0.9736696874636923]\r 64%|██████▍   | 32/50 [00:07<00:03,  4.79trial/s, best loss: -0.9736696874636923]\r 66%|██████▌   | 33/50 [00:07<00:03,  5.19trial/s, best loss: -0.9736696874636923]\r 68%|██████▊   | 34/50 [00:08<00:02,  5.48trial/s, best loss: -0.9736696874636923]\r 70%|███████   | 35/50 [00:08<00:02,  5.74trial/s, best loss: -0.9780556523759731]\r 72%|███████▏  | 36/50 [00:08<00:03,  4.59trial/s, best loss: -0.9780556523759731]\r 74%|███████▍  | 37/50 [00:08<00:03,  3.90trial/s, best loss: -0.9780556523759731]\r 76%|███████▌  | 38/50 [00:09<00:03,  3.39trial/s, best loss: -0.9780556523759731]\r 78%|███████▊  | 39/50 [00:09<00:03,  3.27trial/s, best loss: -0.9780556523759731]\r 80%|████████  | 40/50 [00:09<00:02,  3.42trial/s, best loss: -0.9780556523759731]\r 82%|████████▏ | 41/50 [00:10<00:02,  3.22trial/s, best loss: -0.9780556523759731]\r 84%|████████▍ | 42/50 [00:10<00:02,  2.82trial/s, best loss: -0.9780556523759731]\r 86%|████████▌ | 43/50 [00:10<00:02,  3.45trial/s, best loss: -0.9780556523759731]\r 88%|████████▊ | 44/50 [00:11<00:01,  3.42trial/s, best loss: -0.9780556523759731]\r 90%|█████████ | 45/50 [00:11<00:01,  2.97trial/s, best loss: -0.9780556523759731]\r 92%|█████████▏| 46/50 [00:11<00:01,  3.18trial/s, best loss: -0.9780556523759731]\r 94%|█████████▍| 47/50 [00:11<00:00,  3.86trial/s, best loss: -0.9780556523759731]\r 96%|█████████▌| 48/50 [00:12<00:00,  4.28trial/s, best loss: -0.9780556523759731]\r 98%|█████████▊| 49/50 [00:12<00:00,  4.80trial/s, best loss: -0.9780556523759731]\r100%|██████████| 50/50 [00:12<00:00,  5.32trial/s, best loss: -0.9780556523759731]\r100%|██████████| 50/50 [00:12<00:00,  4.02trial/s, best loss: -0.9780556523759731]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n{'colsample_bytree': 0.7215953516628759,\n 'learning_rate': 0.18583024903749928,\n 'max_depth': 11.0,\n 'min_child_weight': 1.0}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}