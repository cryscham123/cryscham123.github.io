{
  "hash": "e253abbb941481257e304cac526c8b61",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"분류 - 앙상블\"\ndate: 2025-07-27\ncategories: [\"머신 러닝\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## voting\n\n- 서로 다른 알고리즘이 결합. 분류에서는 voting[^1]으로 결정\n\n### Example\n\n::: {#a00b3050 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#5fe928ea .cell execution_count=2}\n``` {.python .cell-code}\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0.9385964912280702\n```\n:::\n:::\n\n\n::: {#b27cbc38 .cell execution_count=3}\n``` {.python .cell-code}\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogisticRegression 정확도: 0.9386\nKNeighborsClassifier 정확도: 0.9123\n```\n:::\n:::\n\n\n- 반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님\n\n## bagging\n\n- 같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting[^1]으로 결정\n\n### RandomForest\n\n::: {#35d63450 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n```\n:::\n\n\n::: {#768b9fc1 .cell execution_count=5}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.9216152019002375\n```\n:::\n:::\n\n\n[^1]: hard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.\n\n## boosting\n\n### GBM\n\n::: {#40043399 .cell execution_count=6}\n``` {.python .cell-code}\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n```\n:::\n\n\n0.939, 701.6343066692352초\n\n- 아주 오래 걸림.\n\n### XGBoost\n\n- 결손값을 자체 처리할 수 있다.\n- 조기 종료 기능이 있다.\n- 자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\n\n- python xgboost\n\n::: {#708cc494 .cell execution_count=7}\n``` {.python .cell-code}\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n```\n:::\n\n\n::: {#c059bf30 .cell execution_count=8}\n``` {.python .cell-code}\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n```\n:::\n\n\n::: {#3f73efd7 .cell execution_count=9}\n``` {.python .cell-code}\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n```\n:::\n\n\n::: {#cd68ce1c .cell execution_count=10}\n``` {.python .cell-code}\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\ttrain-logloss:0.62424\teval-logloss:0.59777\n[1]\ttrain-logloss:0.58779\teval-logloss:0.56156\n[2]\ttrain-logloss:0.55432\teval-logloss:0.53054\n[3]\ttrain-logloss:0.52351\teval-logloss:0.50160\n[4]\ttrain-logloss:0.49539\teval-logloss:0.47522\n[5]\ttrain-logloss:0.46757\teval-logloss:0.44992\n[6]\ttrain-logloss:0.44383\teval-logloss:0.42761\n[7]\ttrain-logloss:0.42005\teval-logloss:0.40613\n[8]\ttrain-logloss:0.39808\teval-logloss:0.38633\n[9]\ttrain-logloss:0.37870\teval-logloss:0.36842\n[10]\ttrain-logloss:0.35969\teval-logloss:0.35141\n[11]\ttrain-logloss:0.34290\teval-logloss:0.33598\n[12]\ttrain-logloss:0.32758\teval-logloss:0.32125\n[13]\ttrain-logloss:0.31287\teval-logloss:0.30786\n[14]\ttrain-logloss:0.29892\teval-logloss:0.29789\n[15]\ttrain-logloss:0.28558\teval-logloss:0.28766\n[16]\ttrain-logloss:0.27366\teval-logloss:0.27631\n[17]\ttrain-logloss:0.26184\teval-logloss:0.26744\n[18]\ttrain-logloss:0.25113\teval-logloss:0.25662\n[19]\ttrain-logloss:0.24069\teval-logloss:0.24660\n[20]\ttrain-logloss:0.23095\teval-logloss:0.24040\n[21]\ttrain-logloss:0.22133\teval-logloss:0.23252\n[22]\ttrain-logloss:0.21279\teval-logloss:0.22393\n[23]\ttrain-logloss:0.20420\teval-logloss:0.21701\n[24]\ttrain-logloss:0.19629\teval-logloss:0.20948\n[25]\ttrain-logloss:0.18899\teval-logloss:0.20315\n[26]\ttrain-logloss:0.18165\teval-logloss:0.19747\n[27]\ttrain-logloss:0.17512\teval-logloss:0.19161\n[28]\ttrain-logloss:0.16837\teval-logloss:0.18409\n[29]\ttrain-logloss:0.16213\teval-logloss:0.17747\n[30]\ttrain-logloss:0.15644\teval-logloss:0.17195\n[31]\ttrain-logloss:0.15069\teval-logloss:0.16564\n[32]\ttrain-logloss:0.14545\teval-logloss:0.16122\n[33]\ttrain-logloss:0.14039\teval-logloss:0.15579\n[34]\ttrain-logloss:0.13553\teval-logloss:0.15171\n[35]\ttrain-logloss:0.13076\teval-logloss:0.14631\n[36]\ttrain-logloss:0.12661\teval-logloss:0.14321\n[37]\ttrain-logloss:0.12243\teval-logloss:0.13868\n[38]\ttrain-logloss:0.11868\teval-logloss:0.13596\n[39]\ttrain-logloss:0.11490\teval-logloss:0.13172\n[40]\ttrain-logloss:0.11126\teval-logloss:0.12735\n[41]\ttrain-logloss:0.10781\teval-logloss:0.12351\n[42]\ttrain-logloss:0.10450\teval-logloss:0.11957\n[43]\ttrain-logloss:0.10131\teval-logloss:0.11603\n[44]\ttrain-logloss:0.09846\teval-logloss:0.11396\n[45]\ttrain-logloss:0.09545\teval-logloss:0.11227\n[46]\ttrain-logloss:0.09284\teval-logloss:0.11015\n[47]\ttrain-logloss:0.09020\teval-logloss:0.10715\n[48]\ttrain-logloss:0.08763\teval-logloss:0.10404\n[49]\ttrain-logloss:0.08535\teval-logloss:0.10262\n[50]\ttrain-logloss:0.08302\teval-logloss:0.10000\n[51]\ttrain-logloss:0.08075\teval-logloss:0.09821\n[52]\ttrain-logloss:0.07862\teval-logloss:0.09605\n[53]\ttrain-logloss:0.07660\teval-logloss:0.09381\n[54]\ttrain-logloss:0.07470\teval-logloss:0.09237\n[55]\ttrain-logloss:0.07281\teval-logloss:0.09051\n[56]\ttrain-logloss:0.07107\teval-logloss:0.08923\n[57]\ttrain-logloss:0.06933\teval-logloss:0.08746\n[58]\ttrain-logloss:0.06769\teval-logloss:0.08634\n[59]\ttrain-logloss:0.06609\teval-logloss:0.08479\n[60]\ttrain-logloss:0.06459\teval-logloss:0.08378\n[61]\ttrain-logloss:0.06299\teval-logloss:0.08323\n[62]\ttrain-logloss:0.06160\teval-logloss:0.08241\n[63]\ttrain-logloss:0.06027\teval-logloss:0.08159\n[64]\ttrain-logloss:0.05892\teval-logloss:0.08046\n[65]\ttrain-logloss:0.05770\teval-logloss:0.07975\n[66]\ttrain-logloss:0.05651\teval-logloss:0.07913\n[67]\ttrain-logloss:0.05529\teval-logloss:0.07875\n[68]\ttrain-logloss:0.05421\teval-logloss:0.07815\n[69]\ttrain-logloss:0.05280\teval-logloss:0.07674\n[70]\ttrain-logloss:0.05177\teval-logloss:0.07657\n[71]\ttrain-logloss:0.05074\teval-logloss:0.07522\n[72]\ttrain-logloss:0.04977\teval-logloss:0.07474\n[73]\ttrain-logloss:0.04878\teval-logloss:0.07455\n[74]\ttrain-logloss:0.04788\teval-logloss:0.07450\n[75]\ttrain-logloss:0.04706\teval-logloss:0.07308\n[76]\ttrain-logloss:0.04608\teval-logloss:0.07206\n[77]\ttrain-logloss:0.04528\teval-logloss:0.07178\n[78]\ttrain-logloss:0.04443\teval-logloss:0.07170\n[79]\ttrain-logloss:0.04341\teval-logloss:0.07047\n[80]\ttrain-logloss:0.04251\teval-logloss:0.06994\n[81]\ttrain-logloss:0.04184\teval-logloss:0.06921\n[82]\ttrain-logloss:0.04118\teval-logloss:0.06898\n[83]\ttrain-logloss:0.04037\teval-logloss:0.06854\n[84]\ttrain-logloss:0.03966\teval-logloss:0.06873\n[85]\ttrain-logloss:0.03903\teval-logloss:0.06884\n[86]\ttrain-logloss:0.03844\teval-logloss:0.06833\n[87]\ttrain-logloss:0.03779\teval-logloss:0.06856\n[88]\ttrain-logloss:0.03722\teval-logloss:0.06870\n[89]\ttrain-logloss:0.03648\teval-logloss:0.06859\n[90]\ttrain-logloss:0.03580\teval-logloss:0.06858\n[91]\ttrain-logloss:0.03525\teval-logloss:0.06808\n[92]\ttrain-logloss:0.03472\teval-logloss:0.06694\n[93]\ttrain-logloss:0.03410\teval-logloss:0.06672\n[94]\ttrain-logloss:0.03361\teval-logloss:0.06564\n[95]\ttrain-logloss:0.03307\teval-logloss:0.06545\n[96]\ttrain-logloss:0.03257\teval-logloss:0.06536\n[97]\ttrain-logloss:0.03205\teval-logloss:0.06465\n[98]\ttrain-logloss:0.03150\teval-logloss:0.06441\n[99]\ttrain-logloss:0.03105\teval-logloss:0.06357\n[100]\ttrain-logloss:0.03057\teval-logloss:0.06387\n[101]\ttrain-logloss:0.03006\teval-logloss:0.06370\n[102]\ttrain-logloss:0.02967\teval-logloss:0.06274\n[103]\ttrain-logloss:0.02925\teval-logloss:0.06244\n[104]\ttrain-logloss:0.02878\teval-logloss:0.06232\n[105]\ttrain-logloss:0.02836\teval-logloss:0.06265\n[106]\ttrain-logloss:0.02799\teval-logloss:0.06195\n[107]\ttrain-logloss:0.02764\teval-logloss:0.06176\n[108]\ttrain-logloss:0.02731\teval-logloss:0.06152\n[109]\ttrain-logloss:0.02690\teval-logloss:0.06142\n[110]\ttrain-logloss:0.02657\teval-logloss:0.06076\n[111]\ttrain-logloss:0.02626\teval-logloss:0.06059\n[112]\ttrain-logloss:0.02589\teval-logloss:0.06010\n[113]\ttrain-logloss:0.02553\teval-logloss:0.06003\n[114]\ttrain-logloss:0.02519\teval-logloss:0.05933\n[115]\ttrain-logloss:0.02480\teval-logloss:0.05840\n[116]\ttrain-logloss:0.02451\teval-logloss:0.05855\n[117]\ttrain-logloss:0.02414\teval-logloss:0.05766\n[118]\ttrain-logloss:0.02382\teval-logloss:0.05761\n[119]\ttrain-logloss:0.02348\teval-logloss:0.05676\n[120]\ttrain-logloss:0.02322\teval-logloss:0.05692\n[121]\ttrain-logloss:0.02290\teval-logloss:0.05610\n[122]\ttrain-logloss:0.02265\teval-logloss:0.05592\n[123]\ttrain-logloss:0.02243\teval-logloss:0.05622\n[124]\ttrain-logloss:0.02213\teval-logloss:0.05542\n[125]\ttrain-logloss:0.02189\teval-logloss:0.05473\n[126]\ttrain-logloss:0.02168\teval-logloss:0.05504\n[127]\ttrain-logloss:0.02140\teval-logloss:0.05427\n[128]\ttrain-logloss:0.02118\teval-logloss:0.05401\n[129]\ttrain-logloss:0.02093\teval-logloss:0.05346\n[130]\ttrain-logloss:0.02075\teval-logloss:0.05376\n[131]\ttrain-logloss:0.02049\teval-logloss:0.05298\n[132]\ttrain-logloss:0.02026\teval-logloss:0.05259\n[133]\ttrain-logloss:0.02003\teval-logloss:0.05251\n[134]\ttrain-logloss:0.01981\teval-logloss:0.05233\n[135]\ttrain-logloss:0.01962\teval-logloss:0.05210\n[136]\ttrain-logloss:0.01945\teval-logloss:0.05208\n[137]\ttrain-logloss:0.01925\teval-logloss:0.05146\n[138]\ttrain-logloss:0.01909\teval-logloss:0.05080\n[139]\ttrain-logloss:0.01891\teval-logloss:0.05059\n[140]\ttrain-logloss:0.01877\teval-logloss:0.05075\n[141]\ttrain-logloss:0.01862\teval-logloss:0.05036\n[142]\ttrain-logloss:0.01846\teval-logloss:0.05019\n[143]\ttrain-logloss:0.01826\teval-logloss:0.04959\n[144]\ttrain-logloss:0.01807\teval-logloss:0.04956\n[145]\ttrain-logloss:0.01791\teval-logloss:0.04940\n[146]\ttrain-logloss:0.01776\teval-logloss:0.04897\n[147]\ttrain-logloss:0.01761\teval-logloss:0.04879\n[148]\ttrain-logloss:0.01743\teval-logloss:0.04864\n[149]\ttrain-logloss:0.01730\teval-logloss:0.04864\n[150]\ttrain-logloss:0.01718\teval-logloss:0.04833\n[151]\ttrain-logloss:0.01703\teval-logloss:0.04792\n[152]\ttrain-logloss:0.01690\teval-logloss:0.04819\n[153]\ttrain-logloss:0.01677\teval-logloss:0.04801\n[154]\ttrain-logloss:0.01662\teval-logloss:0.04794\n[155]\ttrain-logloss:0.01646\teval-logloss:0.04764\n[156]\ttrain-logloss:0.01631\teval-logloss:0.04754\n[157]\ttrain-logloss:0.01619\teval-logloss:0.04778\n[158]\ttrain-logloss:0.01609\teval-logloss:0.04780\n[159]\ttrain-logloss:0.01598\teval-logloss:0.04805\n[160]\ttrain-logloss:0.01584\teval-logloss:0.04791\n[161]\ttrain-logloss:0.01575\teval-logloss:0.04757\n[162]\ttrain-logloss:0.01565\teval-logloss:0.04760\n[163]\ttrain-logloss:0.01555\teval-logloss:0.04782\n[164]\ttrain-logloss:0.01547\teval-logloss:0.04758\n[165]\ttrain-logloss:0.01532\teval-logloss:0.04775\n[166]\ttrain-logloss:0.01522\teval-logloss:0.04799\n[167]\ttrain-logloss:0.01514\teval-logloss:0.04821\n[168]\ttrain-logloss:0.01507\teval-logloss:0.04827\n[169]\ttrain-logloss:0.01499\teval-logloss:0.04803\n[170]\ttrain-logloss:0.01485\teval-logloss:0.04819\n[171]\ttrain-logloss:0.01475\teval-logloss:0.04788\n[172]\ttrain-logloss:0.01465\teval-logloss:0.04811\n[173]\ttrain-logloss:0.01456\teval-logloss:0.04763\n[174]\ttrain-logloss:0.01445\teval-logloss:0.04770\n[175]\ttrain-logloss:0.01432\teval-logloss:0.04777\n[176]\ttrain-logloss:0.01425\teval-logloss:0.04780\n[177]\ttrain-logloss:0.01418\teval-logloss:0.04809\n[178]\ttrain-logloss:0.01411\teval-logloss:0.04782\n[179]\ttrain-logloss:0.01405\teval-logloss:0.04787\n[180]\ttrain-logloss:0.01394\teval-logloss:0.04793\n[181]\ttrain-logloss:0.01387\teval-logloss:0.04771\n[182]\ttrain-logloss:0.01377\teval-logloss:0.04778\n[183]\ttrain-logloss:0.01371\teval-logloss:0.04738\n[184]\ttrain-logloss:0.01363\teval-logloss:0.04761\n[185]\ttrain-logloss:0.01353\teval-logloss:0.04763\n[186]\ttrain-logloss:0.01346\teval-logloss:0.04737\n[187]\ttrain-logloss:0.01340\teval-logloss:0.04761\n[188]\ttrain-logloss:0.01335\teval-logloss:0.04761\n[189]\ttrain-logloss:0.01329\teval-logloss:0.04757\n[190]\ttrain-logloss:0.01323\teval-logloss:0.04718\n[191]\ttrain-logloss:0.01318\teval-logloss:0.04718\n[192]\ttrain-logloss:0.01312\teval-logloss:0.04715\n[193]\ttrain-logloss:0.01306\teval-logloss:0.04690\n[194]\ttrain-logloss:0.01300\teval-logloss:0.04713\n[195]\ttrain-logloss:0.01295\teval-logloss:0.04713\n[196]\ttrain-logloss:0.01287\teval-logloss:0.04715\n[197]\ttrain-logloss:0.01282\teval-logloss:0.04691\n[198]\ttrain-logloss:0.01272\teval-logloss:0.04719\n[199]\ttrain-logloss:0.01266\teval-logloss:0.04742\n[200]\ttrain-logloss:0.01261\teval-logloss:0.04718\n[201]\ttrain-logloss:0.01256\teval-logloss:0.04697\n[202]\ttrain-logloss:0.01251\teval-logloss:0.04703\n[203]\ttrain-logloss:0.01243\teval-logloss:0.04704\n[204]\ttrain-logloss:0.01235\teval-logloss:0.04707\n[205]\ttrain-logloss:0.01230\teval-logloss:0.04694\n[206]\ttrain-logloss:0.01225\teval-logloss:0.04699\n[207]\ttrain-logloss:0.01218\teval-logloss:0.04693\n[208]\ttrain-logloss:0.01213\teval-logloss:0.04713\n[209]\ttrain-logloss:0.01208\teval-logloss:0.04689\n[210]\ttrain-logloss:0.01203\teval-logloss:0.04676\n[211]\ttrain-logloss:0.01195\teval-logloss:0.04645\n[212]\ttrain-logloss:0.01190\teval-logloss:0.04666\n[213]\ttrain-logloss:0.01185\teval-logloss:0.04634\n[214]\ttrain-logloss:0.01181\teval-logloss:0.04640\n[215]\ttrain-logloss:0.01176\teval-logloss:0.04627\n[216]\ttrain-logloss:0.01169\teval-logloss:0.04621\n[217]\ttrain-logloss:0.01164\teval-logloss:0.04611\n[218]\ttrain-logloss:0.01160\teval-logloss:0.04588\n[219]\ttrain-logloss:0.01155\teval-logloss:0.04576\n[220]\ttrain-logloss:0.01149\teval-logloss:0.04570\n[221]\ttrain-logloss:0.01145\teval-logloss:0.04594\n[222]\ttrain-logloss:0.01141\teval-logloss:0.04600\n[223]\ttrain-logloss:0.01137\teval-logloss:0.04573\n[224]\ttrain-logloss:0.01133\teval-logloss:0.04552\n[225]\ttrain-logloss:0.01128\teval-logloss:0.04539\n[226]\ttrain-logloss:0.01122\teval-logloss:0.04534\n[227]\ttrain-logloss:0.01118\teval-logloss:0.04554\n[228]\ttrain-logloss:0.01114\teval-logloss:0.04536\n[229]\ttrain-logloss:0.01109\teval-logloss:0.04550\n[230]\ttrain-logloss:0.01105\teval-logloss:0.04529\n[231]\ttrain-logloss:0.01100\teval-logloss:0.04516\n[232]\ttrain-logloss:0.01095\teval-logloss:0.04511\n[233]\ttrain-logloss:0.01091\teval-logloss:0.04516\n[234]\ttrain-logloss:0.01087\teval-logloss:0.04536\n[235]\ttrain-logloss:0.01083\teval-logloss:0.04514\n[236]\ttrain-logloss:0.01079\teval-logloss:0.04536\n[237]\ttrain-logloss:0.01075\teval-logloss:0.04515\n[238]\ttrain-logloss:0.01071\teval-logloss:0.04503\n[239]\ttrain-logloss:0.01066\teval-logloss:0.04498\n[240]\ttrain-logloss:0.01063\teval-logloss:0.04505\n[241]\ttrain-logloss:0.01058\teval-logloss:0.04519\n[242]\ttrain-logloss:0.01053\teval-logloss:0.04511\n[243]\ttrain-logloss:0.01049\teval-logloss:0.04499\n[244]\ttrain-logloss:0.01045\teval-logloss:0.04478\n[245]\ttrain-logloss:0.01041\teval-logloss:0.04500\n[246]\ttrain-logloss:0.01036\teval-logloss:0.04492\n[247]\ttrain-logloss:0.01032\teval-logloss:0.04506\n[248]\ttrain-logloss:0.01028\teval-logloss:0.04485\n[249]\ttrain-logloss:0.01025\teval-logloss:0.04473\n[250]\ttrain-logloss:0.01022\teval-logloss:0.04477\n[251]\ttrain-logloss:0.01016\teval-logloss:0.04469\n[252]\ttrain-logloss:0.01013\teval-logloss:0.04488\n[253]\ttrain-logloss:0.01009\teval-logloss:0.04477\n[254]\ttrain-logloss:0.01006\teval-logloss:0.04456\n[255]\ttrain-logloss:0.01003\teval-logloss:0.04436\n[256]\ttrain-logloss:0.01000\teval-logloss:0.04440\n[257]\ttrain-logloss:0.00995\teval-logloss:0.04433\n[258]\ttrain-logloss:0.00991\teval-logloss:0.04446\n[259]\ttrain-logloss:0.00987\teval-logloss:0.04435\n[260]\ttrain-logloss:0.00984\teval-logloss:0.04415\n[261]\ttrain-logloss:0.00981\teval-logloss:0.04398\n[262]\ttrain-logloss:0.00976\teval-logloss:0.04391\n[263]\ttrain-logloss:0.00973\teval-logloss:0.04380\n[264]\ttrain-logloss:0.00970\teval-logloss:0.04386\n[265]\ttrain-logloss:0.00968\teval-logloss:0.04365\n[266]\ttrain-logloss:0.00964\teval-logloss:0.04378\n[267]\ttrain-logloss:0.00961\teval-logloss:0.04383\n[268]\ttrain-logloss:0.00958\teval-logloss:0.04373\n[269]\ttrain-logloss:0.00955\teval-logloss:0.04356\n[270]\ttrain-logloss:0.00952\teval-logloss:0.04374\n[271]\ttrain-logloss:0.00947\teval-logloss:0.04369\n[272]\ttrain-logloss:0.00944\teval-logloss:0.04358\n[273]\ttrain-logloss:0.00942\teval-logloss:0.04364\n[274]\ttrain-logloss:0.00939\teval-logloss:0.04345\n[275]\ttrain-logloss:0.00936\teval-logloss:0.04362\n[276]\ttrain-logloss:0.00933\teval-logloss:0.04346\n[277]\ttrain-logloss:0.00929\teval-logloss:0.04359\n[278]\ttrain-logloss:0.00927\teval-logloss:0.04338\n[279]\ttrain-logloss:0.00924\teval-logloss:0.04342\n[280]\ttrain-logloss:0.00922\teval-logloss:0.04357\n[281]\ttrain-logloss:0.00917\teval-logloss:0.04352\n[282]\ttrain-logloss:0.00916\teval-logloss:0.04339\n[283]\ttrain-logloss:0.00914\teval-logloss:0.04321\n[284]\ttrain-logloss:0.00911\teval-logloss:0.04305\n[285]\ttrain-logloss:0.00907\teval-logloss:0.04317\n[286]\ttrain-logloss:0.00905\teval-logloss:0.04331\n[287]\ttrain-logloss:0.00903\teval-logloss:0.04311\n[288]\ttrain-logloss:0.00900\teval-logloss:0.04296\n[289]\ttrain-logloss:0.00899\teval-logloss:0.04299\n[290]\ttrain-logloss:0.00896\teval-logloss:0.04317\n[291]\ttrain-logloss:0.00894\teval-logloss:0.04307\n[292]\ttrain-logloss:0.00892\teval-logloss:0.04309\n[293]\ttrain-logloss:0.00890\teval-logloss:0.04327\n[294]\ttrain-logloss:0.00887\teval-logloss:0.04308\n[295]\ttrain-logloss:0.00885\teval-logloss:0.04286\n[296]\ttrain-logloss:0.00883\teval-logloss:0.04301\n[297]\ttrain-logloss:0.00882\teval-logloss:0.04288\n[298]\ttrain-logloss:0.00878\teval-logloss:0.04285\n[299]\ttrain-logloss:0.00876\teval-logloss:0.04266\n[300]\ttrain-logloss:0.00874\teval-logloss:0.04253\n[301]\ttrain-logloss:0.00873\teval-logloss:0.04244\n[302]\ttrain-logloss:0.00871\teval-logloss:0.04230\n[303]\ttrain-logloss:0.00869\teval-logloss:0.04233\n[304]\ttrain-logloss:0.00868\teval-logloss:0.04249\n[305]\ttrain-logloss:0.00867\teval-logloss:0.04236\n[306]\ttrain-logloss:0.00866\teval-logloss:0.04239\n[307]\ttrain-logloss:0.00865\teval-logloss:0.04255\n[308]\ttrain-logloss:0.00863\teval-logloss:0.04243\n[309]\ttrain-logloss:0.00862\teval-logloss:0.04234\n[310]\ttrain-logloss:0.00860\teval-logloss:0.04220\n[311]\ttrain-logloss:0.00857\teval-logloss:0.04237\n[312]\ttrain-logloss:0.00855\teval-logloss:0.04219\n[313]\ttrain-logloss:0.00854\teval-logloss:0.04223\n[314]\ttrain-logloss:0.00853\teval-logloss:0.04237\n[315]\ttrain-logloss:0.00850\teval-logloss:0.04223\n[316]\ttrain-logloss:0.00849\teval-logloss:0.04227\n[317]\ttrain-logloss:0.00848\teval-logloss:0.04215\n[318]\ttrain-logloss:0.00847\teval-logloss:0.04206\n[319]\ttrain-logloss:0.00844\teval-logloss:0.04223\n[320]\ttrain-logloss:0.00843\teval-logloss:0.04211\n[321]\ttrain-logloss:0.00842\teval-logloss:0.04226\n[322]\ttrain-logloss:0.00841\teval-logloss:0.04229\n[323]\ttrain-logloss:0.00840\teval-logloss:0.04245\n[324]\ttrain-logloss:0.00839\teval-logloss:0.04233\n[325]\ttrain-logloss:0.00838\teval-logloss:0.04216\n[326]\ttrain-logloss:0.00836\teval-logloss:0.04199\n[327]\ttrain-logloss:0.00835\teval-logloss:0.04202\n[328]\ttrain-logloss:0.00834\teval-logloss:0.04217\n[329]\ttrain-logloss:0.00831\teval-logloss:0.04220\n[330]\ttrain-logloss:0.00827\teval-logloss:0.04218\n[331]\ttrain-logloss:0.00826\teval-logloss:0.04206\n[332]\ttrain-logloss:0.00825\teval-logloss:0.04188\n[333]\ttrain-logloss:0.00823\teval-logloss:0.04182\n[334]\ttrain-logloss:0.00822\teval-logloss:0.04174\n[335]\ttrain-logloss:0.00820\teval-logloss:0.04168\n[336]\ttrain-logloss:0.00819\teval-logloss:0.04154\n[337]\ttrain-logloss:0.00818\teval-logloss:0.04171\n[338]\ttrain-logloss:0.00814\teval-logloss:0.04169\n[339]\ttrain-logloss:0.00813\teval-logloss:0.04158\n[340]\ttrain-logloss:0.00812\teval-logloss:0.04145\n[341]\ttrain-logloss:0.00811\teval-logloss:0.04129\n[342]\ttrain-logloss:0.00810\teval-logloss:0.04145\n[343]\ttrain-logloss:0.00809\teval-logloss:0.04149\n[344]\ttrain-logloss:0.00808\teval-logloss:0.04138\n[345]\ttrain-logloss:0.00807\teval-logloss:0.04152\n[346]\ttrain-logloss:0.00806\teval-logloss:0.04138\n[347]\ttrain-logloss:0.00805\teval-logloss:0.04145\n[348]\ttrain-logloss:0.00804\teval-logloss:0.04136\n[349]\ttrain-logloss:0.00803\teval-logloss:0.04120\n[350]\ttrain-logloss:0.00802\teval-logloss:0.04124\n[351]\ttrain-logloss:0.00801\teval-logloss:0.04138\n[352]\ttrain-logloss:0.00800\teval-logloss:0.04127\n[353]\ttrain-logloss:0.00799\teval-logloss:0.04114\n[354]\ttrain-logloss:0.00798\teval-logloss:0.04128\n[355]\ttrain-logloss:0.00797\teval-logloss:0.04116\n[356]\ttrain-logloss:0.00796\teval-logloss:0.04103\n[357]\ttrain-logloss:0.00795\teval-logloss:0.04110\n[358]\ttrain-logloss:0.00794\teval-logloss:0.04113\n[359]\ttrain-logloss:0.00793\teval-logloss:0.04108\n[360]\ttrain-logloss:0.00792\teval-logloss:0.04099\n[361]\ttrain-logloss:0.00791\teval-logloss:0.04084\n[362]\ttrain-logloss:0.00790\teval-logloss:0.04090\n[363]\ttrain-logloss:0.00789\teval-logloss:0.04077\n[364]\ttrain-logloss:0.00788\teval-logloss:0.04080\n[365]\ttrain-logloss:0.00787\teval-logloss:0.04095\n[366]\ttrain-logloss:0.00786\teval-logloss:0.04084\n[367]\ttrain-logloss:0.00785\teval-logloss:0.04075\n[368]\ttrain-logloss:0.00784\teval-logloss:0.04060\n[369]\ttrain-logloss:0.00783\teval-logloss:0.04055\n[370]\ttrain-logloss:0.00783\teval-logloss:0.04058\n[371]\ttrain-logloss:0.00782\teval-logloss:0.04072\n[372]\ttrain-logloss:0.00781\teval-logloss:0.04059\n[373]\ttrain-logloss:0.00780\teval-logloss:0.04065\n[374]\ttrain-logloss:0.00779\teval-logloss:0.04057\n[375]\ttrain-logloss:0.00778\teval-logloss:0.04046\n[376]\ttrain-logloss:0.00777\teval-logloss:0.04060\n[377]\ttrain-logloss:0.00776\teval-logloss:0.04063\n[378]\ttrain-logloss:0.00775\teval-logloss:0.04053\n[379]\ttrain-logloss:0.00774\teval-logloss:0.04066\n[380]\ttrain-logloss:0.00773\teval-logloss:0.04053\n[381]\ttrain-logloss:0.00772\teval-logloss:0.04048\n[382]\ttrain-logloss:0.00772\teval-logloss:0.04034\n[383]\ttrain-logloss:0.00771\teval-logloss:0.04037\n[384]\ttrain-logloss:0.00770\teval-logloss:0.04053\n[385]\ttrain-logloss:0.00769\teval-logloss:0.04045\n[386]\ttrain-logloss:0.00768\teval-logloss:0.04034\n[387]\ttrain-logloss:0.00767\teval-logloss:0.04020\n[388]\ttrain-logloss:0.00766\teval-logloss:0.04023\n[389]\ttrain-logloss:0.00765\teval-logloss:0.04037\n[390]\ttrain-logloss:0.00764\teval-logloss:0.04025\n[391]\ttrain-logloss:0.00764\teval-logloss:0.04038\n[392]\ttrain-logloss:0.00763\teval-logloss:0.04033\n[393]\ttrain-logloss:0.00762\teval-logloss:0.04022\n[394]\ttrain-logloss:0.00761\teval-logloss:0.04015\n[395]\ttrain-logloss:0.00760\teval-logloss:0.04018\n[396]\ttrain-logloss:0.00759\teval-logloss:0.04031\n[397]\ttrain-logloss:0.00758\teval-logloss:0.04021\n[398]\ttrain-logloss:0.00758\teval-logloss:0.04007\n[399]\ttrain-logloss:0.00757\teval-logloss:0.04022\n```\n:::\n:::\n\n\n::: {#a0664d29 .cell execution_count=11}\n``` {.python .cell-code}\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x > 0.5 else 0 for x in pred_probs]\n```\n:::\n\n\n- sklearn xgboost\n\n::: {#efcb80ff .cell execution_count=12}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-logloss:0.62424\tvalidation_1-logloss:0.59777\n[1]\tvalidation_0-logloss:0.58779\tvalidation_1-logloss:0.56156\n[2]\tvalidation_0-logloss:0.55432\tvalidation_1-logloss:0.53054\n[3]\tvalidation_0-logloss:0.52351\tvalidation_1-logloss:0.50160\n[4]\tvalidation_0-logloss:0.49539\tvalidation_1-logloss:0.47522\n[5]\tvalidation_0-logloss:0.46757\tvalidation_1-logloss:0.44992\n[6]\tvalidation_0-logloss:0.44383\tvalidation_1-logloss:0.42761\n[7]\tvalidation_0-logloss:0.42005\tvalidation_1-logloss:0.40613\n[8]\tvalidation_0-logloss:0.39808\tvalidation_1-logloss:0.38633\n[9]\tvalidation_0-logloss:0.37870\tvalidation_1-logloss:0.36842\n[10]\tvalidation_0-logloss:0.35969\tvalidation_1-logloss:0.35141\n[11]\tvalidation_0-logloss:0.34290\tvalidation_1-logloss:0.33598\n[12]\tvalidation_0-logloss:0.32758\tvalidation_1-logloss:0.32125\n[13]\tvalidation_0-logloss:0.31287\tvalidation_1-logloss:0.30786\n[14]\tvalidation_0-logloss:0.29892\tvalidation_1-logloss:0.29789\n[15]\tvalidation_0-logloss:0.28558\tvalidation_1-logloss:0.28766\n[16]\tvalidation_0-logloss:0.27366\tvalidation_1-logloss:0.27631\n[17]\tvalidation_0-logloss:0.26184\tvalidation_1-logloss:0.26744\n[18]\tvalidation_0-logloss:0.25113\tvalidation_1-logloss:0.25662\n[19]\tvalidation_0-logloss:0.24069\tvalidation_1-logloss:0.24660\n[20]\tvalidation_0-logloss:0.23095\tvalidation_1-logloss:0.24040\n[21]\tvalidation_0-logloss:0.22133\tvalidation_1-logloss:0.23252\n[22]\tvalidation_0-logloss:0.21279\tvalidation_1-logloss:0.22393\n[23]\tvalidation_0-logloss:0.20420\tvalidation_1-logloss:0.21701\n[24]\tvalidation_0-logloss:0.19629\tvalidation_1-logloss:0.20948\n[25]\tvalidation_0-logloss:0.18899\tvalidation_1-logloss:0.20315\n[26]\tvalidation_0-logloss:0.18165\tvalidation_1-logloss:0.19747\n[27]\tvalidation_0-logloss:0.17512\tvalidation_1-logloss:0.19161\n[28]\tvalidation_0-logloss:0.16837\tvalidation_1-logloss:0.18409\n[29]\tvalidation_0-logloss:0.16213\tvalidation_1-logloss:0.17747\n[30]\tvalidation_0-logloss:0.15644\tvalidation_1-logloss:0.17195\n[31]\tvalidation_0-logloss:0.15069\tvalidation_1-logloss:0.16564\n[32]\tvalidation_0-logloss:0.14545\tvalidation_1-logloss:0.16122\n[33]\tvalidation_0-logloss:0.14039\tvalidation_1-logloss:0.15579\n[34]\tvalidation_0-logloss:0.13553\tvalidation_1-logloss:0.15171\n[35]\tvalidation_0-logloss:0.13076\tvalidation_1-logloss:0.14631\n[36]\tvalidation_0-logloss:0.12661\tvalidation_1-logloss:0.14321\n[37]\tvalidation_0-logloss:0.12243\tvalidation_1-logloss:0.13868\n[38]\tvalidation_0-logloss:0.11868\tvalidation_1-logloss:0.13596\n[39]\tvalidation_0-logloss:0.11490\tvalidation_1-logloss:0.13172\n[40]\tvalidation_0-logloss:0.11126\tvalidation_1-logloss:0.12735\n[41]\tvalidation_0-logloss:0.10781\tvalidation_1-logloss:0.12351\n[42]\tvalidation_0-logloss:0.10450\tvalidation_1-logloss:0.11957\n[43]\tvalidation_0-logloss:0.10131\tvalidation_1-logloss:0.11603\n[44]\tvalidation_0-logloss:0.09846\tvalidation_1-logloss:0.11396\n[45]\tvalidation_0-logloss:0.09545\tvalidation_1-logloss:0.11227\n[46]\tvalidation_0-logloss:0.09284\tvalidation_1-logloss:0.11015\n[47]\tvalidation_0-logloss:0.09020\tvalidation_1-logloss:0.10715\n[48]\tvalidation_0-logloss:0.08763\tvalidation_1-logloss:0.10404\n[49]\tvalidation_0-logloss:0.08535\tvalidation_1-logloss:0.10262\n[50]\tvalidation_0-logloss:0.08302\tvalidation_1-logloss:0.10000\n[51]\tvalidation_0-logloss:0.08075\tvalidation_1-logloss:0.09821\n[52]\tvalidation_0-logloss:0.07862\tvalidation_1-logloss:0.09605\n[53]\tvalidation_0-logloss:0.07660\tvalidation_1-logloss:0.09381\n[54]\tvalidation_0-logloss:0.07470\tvalidation_1-logloss:0.09237\n[55]\tvalidation_0-logloss:0.07281\tvalidation_1-logloss:0.09051\n[56]\tvalidation_0-logloss:0.07107\tvalidation_1-logloss:0.08923\n[57]\tvalidation_0-logloss:0.06933\tvalidation_1-logloss:0.08746\n[58]\tvalidation_0-logloss:0.06769\tvalidation_1-logloss:0.08634\n[59]\tvalidation_0-logloss:0.06609\tvalidation_1-logloss:0.08479\n[60]\tvalidation_0-logloss:0.06459\tvalidation_1-logloss:0.08378\n[61]\tvalidation_0-logloss:0.06299\tvalidation_1-logloss:0.08323\n[62]\tvalidation_0-logloss:0.06160\tvalidation_1-logloss:0.08241\n[63]\tvalidation_0-logloss:0.06027\tvalidation_1-logloss:0.08159\n[64]\tvalidation_0-logloss:0.05892\tvalidation_1-logloss:0.08046\n[65]\tvalidation_0-logloss:0.05770\tvalidation_1-logloss:0.07975\n[66]\tvalidation_0-logloss:0.05651\tvalidation_1-logloss:0.07913\n[67]\tvalidation_0-logloss:0.05529\tvalidation_1-logloss:0.07875\n[68]\tvalidation_0-logloss:0.05421\tvalidation_1-logloss:0.07815\n[69]\tvalidation_0-logloss:0.05280\tvalidation_1-logloss:0.07674\n[70]\tvalidation_0-logloss:0.05177\tvalidation_1-logloss:0.07657\n[71]\tvalidation_0-logloss:0.05074\tvalidation_1-logloss:0.07522\n[72]\tvalidation_0-logloss:0.04977\tvalidation_1-logloss:0.07474\n[73]\tvalidation_0-logloss:0.04878\tvalidation_1-logloss:0.07455\n[74]\tvalidation_0-logloss:0.04788\tvalidation_1-logloss:0.07450\n[75]\tvalidation_0-logloss:0.04706\tvalidation_1-logloss:0.07308\n[76]\tvalidation_0-logloss:0.04608\tvalidation_1-logloss:0.07206\n[77]\tvalidation_0-logloss:0.04528\tvalidation_1-logloss:0.07178\n[78]\tvalidation_0-logloss:0.04443\tvalidation_1-logloss:0.07170\n[79]\tvalidation_0-logloss:0.04341\tvalidation_1-logloss:0.07047\n[80]\tvalidation_0-logloss:0.04251\tvalidation_1-logloss:0.06994\n[81]\tvalidation_0-logloss:0.04184\tvalidation_1-logloss:0.06921\n[82]\tvalidation_0-logloss:0.04118\tvalidation_1-logloss:0.06898\n[83]\tvalidation_0-logloss:0.04037\tvalidation_1-logloss:0.06854\n[84]\tvalidation_0-logloss:0.03966\tvalidation_1-logloss:0.06873\n[85]\tvalidation_0-logloss:0.03903\tvalidation_1-logloss:0.06884\n[86]\tvalidation_0-logloss:0.03844\tvalidation_1-logloss:0.06833\n[87]\tvalidation_0-logloss:0.03779\tvalidation_1-logloss:0.06856\n[88]\tvalidation_0-logloss:0.03722\tvalidation_1-logloss:0.06870\n[89]\tvalidation_0-logloss:0.03648\tvalidation_1-logloss:0.06859\n[90]\tvalidation_0-logloss:0.03580\tvalidation_1-logloss:0.06858\n[91]\tvalidation_0-logloss:0.03525\tvalidation_1-logloss:0.06808\n[92]\tvalidation_0-logloss:0.03472\tvalidation_1-logloss:0.06694\n[93]\tvalidation_0-logloss:0.03410\tvalidation_1-logloss:0.06672\n[94]\tvalidation_0-logloss:0.03361\tvalidation_1-logloss:0.06564\n[95]\tvalidation_0-logloss:0.03307\tvalidation_1-logloss:0.06545\n[96]\tvalidation_0-logloss:0.03257\tvalidation_1-logloss:0.06536\n[97]\tvalidation_0-logloss:0.03205\tvalidation_1-logloss:0.06465\n[98]\tvalidation_0-logloss:0.03150\tvalidation_1-logloss:0.06441\n[99]\tvalidation_0-logloss:0.03105\tvalidation_1-logloss:0.06357\n[100]\tvalidation_0-logloss:0.03057\tvalidation_1-logloss:0.06387\n[101]\tvalidation_0-logloss:0.03006\tvalidation_1-logloss:0.06370\n[102]\tvalidation_0-logloss:0.02967\tvalidation_1-logloss:0.06274\n[103]\tvalidation_0-logloss:0.02925\tvalidation_1-logloss:0.06244\n[104]\tvalidation_0-logloss:0.02878\tvalidation_1-logloss:0.06232\n[105]\tvalidation_0-logloss:0.02836\tvalidation_1-logloss:0.06265\n[106]\tvalidation_0-logloss:0.02799\tvalidation_1-logloss:0.06195\n[107]\tvalidation_0-logloss:0.02764\tvalidation_1-logloss:0.06176\n[108]\tvalidation_0-logloss:0.02731\tvalidation_1-logloss:0.06152\n[109]\tvalidation_0-logloss:0.02690\tvalidation_1-logloss:0.06142\n[110]\tvalidation_0-logloss:0.02657\tvalidation_1-logloss:0.06076\n[111]\tvalidation_0-logloss:0.02626\tvalidation_1-logloss:0.06059\n[112]\tvalidation_0-logloss:0.02589\tvalidation_1-logloss:0.06010\n[113]\tvalidation_0-logloss:0.02553\tvalidation_1-logloss:0.06003\n[114]\tvalidation_0-logloss:0.02519\tvalidation_1-logloss:0.05933\n[115]\tvalidation_0-logloss:0.02480\tvalidation_1-logloss:0.05840\n[116]\tvalidation_0-logloss:0.02451\tvalidation_1-logloss:0.05855\n[117]\tvalidation_0-logloss:0.02414\tvalidation_1-logloss:0.05766\n[118]\tvalidation_0-logloss:0.02382\tvalidation_1-logloss:0.05761\n[119]\tvalidation_0-logloss:0.02348\tvalidation_1-logloss:0.05676\n[120]\tvalidation_0-logloss:0.02322\tvalidation_1-logloss:0.05692\n[121]\tvalidation_0-logloss:0.02290\tvalidation_1-logloss:0.05610\n[122]\tvalidation_0-logloss:0.02265\tvalidation_1-logloss:0.05592\n[123]\tvalidation_0-logloss:0.02243\tvalidation_1-logloss:0.05622\n[124]\tvalidation_0-logloss:0.02213\tvalidation_1-logloss:0.05542\n[125]\tvalidation_0-logloss:0.02189\tvalidation_1-logloss:0.05473\n[126]\tvalidation_0-logloss:0.02168\tvalidation_1-logloss:0.05504\n[127]\tvalidation_0-logloss:0.02140\tvalidation_1-logloss:0.05427\n[128]\tvalidation_0-logloss:0.02118\tvalidation_1-logloss:0.05401\n[129]\tvalidation_0-logloss:0.02093\tvalidation_1-logloss:0.05346\n[130]\tvalidation_0-logloss:0.02075\tvalidation_1-logloss:0.05376\n[131]\tvalidation_0-logloss:0.02049\tvalidation_1-logloss:0.05298\n[132]\tvalidation_0-logloss:0.02026\tvalidation_1-logloss:0.05259\n[133]\tvalidation_0-logloss:0.02003\tvalidation_1-logloss:0.05251\n[134]\tvalidation_0-logloss:0.01981\tvalidation_1-logloss:0.05233\n[135]\tvalidation_0-logloss:0.01962\tvalidation_1-logloss:0.05210\n[136]\tvalidation_0-logloss:0.01945\tvalidation_1-logloss:0.05208\n[137]\tvalidation_0-logloss:0.01925\tvalidation_1-logloss:0.05146\n[138]\tvalidation_0-logloss:0.01909\tvalidation_1-logloss:0.05080\n[139]\tvalidation_0-logloss:0.01891\tvalidation_1-logloss:0.05059\n[140]\tvalidation_0-logloss:0.01877\tvalidation_1-logloss:0.05075\n[141]\tvalidation_0-logloss:0.01862\tvalidation_1-logloss:0.05036\n[142]\tvalidation_0-logloss:0.01846\tvalidation_1-logloss:0.05019\n[143]\tvalidation_0-logloss:0.01826\tvalidation_1-logloss:0.04959\n[144]\tvalidation_0-logloss:0.01807\tvalidation_1-logloss:0.04956\n[145]\tvalidation_0-logloss:0.01791\tvalidation_1-logloss:0.04940\n[146]\tvalidation_0-logloss:0.01776\tvalidation_1-logloss:0.04897\n[147]\tvalidation_0-logloss:0.01761\tvalidation_1-logloss:0.04879\n[148]\tvalidation_0-logloss:0.01743\tvalidation_1-logloss:0.04864\n[149]\tvalidation_0-logloss:0.01730\tvalidation_1-logloss:0.04864\n[150]\tvalidation_0-logloss:0.01718\tvalidation_1-logloss:0.04833\n[151]\tvalidation_0-logloss:0.01703\tvalidation_1-logloss:0.04792\n[152]\tvalidation_0-logloss:0.01690\tvalidation_1-logloss:0.04819\n[153]\tvalidation_0-logloss:0.01677\tvalidation_1-logloss:0.04801\n[154]\tvalidation_0-logloss:0.01662\tvalidation_1-logloss:0.04794\n[155]\tvalidation_0-logloss:0.01646\tvalidation_1-logloss:0.04764\n[156]\tvalidation_0-logloss:0.01631\tvalidation_1-logloss:0.04754\n[157]\tvalidation_0-logloss:0.01619\tvalidation_1-logloss:0.04778\n[158]\tvalidation_0-logloss:0.01609\tvalidation_1-logloss:0.04780\n[159]\tvalidation_0-logloss:0.01598\tvalidation_1-logloss:0.04805\n[160]\tvalidation_0-logloss:0.01584\tvalidation_1-logloss:0.04791\n[161]\tvalidation_0-logloss:0.01575\tvalidation_1-logloss:0.04757\n[162]\tvalidation_0-logloss:0.01565\tvalidation_1-logloss:0.04760\n[163]\tvalidation_0-logloss:0.01555\tvalidation_1-logloss:0.04782\n[164]\tvalidation_0-logloss:0.01547\tvalidation_1-logloss:0.04758\n[165]\tvalidation_0-logloss:0.01532\tvalidation_1-logloss:0.04775\n[166]\tvalidation_0-logloss:0.01522\tvalidation_1-logloss:0.04799\n[167]\tvalidation_0-logloss:0.01514\tvalidation_1-logloss:0.04821\n[168]\tvalidation_0-logloss:0.01507\tvalidation_1-logloss:0.04827\n[169]\tvalidation_0-logloss:0.01499\tvalidation_1-logloss:0.04803\n[170]\tvalidation_0-logloss:0.01485\tvalidation_1-logloss:0.04819\n[171]\tvalidation_0-logloss:0.01475\tvalidation_1-logloss:0.04788\n[172]\tvalidation_0-logloss:0.01465\tvalidation_1-logloss:0.04811\n[173]\tvalidation_0-logloss:0.01456\tvalidation_1-logloss:0.04763\n[174]\tvalidation_0-logloss:0.01445\tvalidation_1-logloss:0.04770\n[175]\tvalidation_0-logloss:0.01432\tvalidation_1-logloss:0.04777\n[176]\tvalidation_0-logloss:0.01425\tvalidation_1-logloss:0.04780\n[177]\tvalidation_0-logloss:0.01418\tvalidation_1-logloss:0.04809\n[178]\tvalidation_0-logloss:0.01411\tvalidation_1-logloss:0.04782\n[179]\tvalidation_0-logloss:0.01405\tvalidation_1-logloss:0.04787\n[180]\tvalidation_0-logloss:0.01394\tvalidation_1-logloss:0.04793\n[181]\tvalidation_0-logloss:0.01387\tvalidation_1-logloss:0.04771\n[182]\tvalidation_0-logloss:0.01377\tvalidation_1-logloss:0.04778\n[183]\tvalidation_0-logloss:0.01371\tvalidation_1-logloss:0.04738\n[184]\tvalidation_0-logloss:0.01363\tvalidation_1-logloss:0.04761\n[185]\tvalidation_0-logloss:0.01353\tvalidation_1-logloss:0.04763\n[186]\tvalidation_0-logloss:0.01346\tvalidation_1-logloss:0.04737\n[187]\tvalidation_0-logloss:0.01340\tvalidation_1-logloss:0.04761\n[188]\tvalidation_0-logloss:0.01335\tvalidation_1-logloss:0.04761\n[189]\tvalidation_0-logloss:0.01329\tvalidation_1-logloss:0.04757\n[190]\tvalidation_0-logloss:0.01323\tvalidation_1-logloss:0.04718\n[191]\tvalidation_0-logloss:0.01318\tvalidation_1-logloss:0.04718\n[192]\tvalidation_0-logloss:0.01312\tvalidation_1-logloss:0.04715\n[193]\tvalidation_0-logloss:0.01306\tvalidation_1-logloss:0.04690\n[194]\tvalidation_0-logloss:0.01300\tvalidation_1-logloss:0.04713\n[195]\tvalidation_0-logloss:0.01295\tvalidation_1-logloss:0.04713\n[196]\tvalidation_0-logloss:0.01287\tvalidation_1-logloss:0.04715\n[197]\tvalidation_0-logloss:0.01282\tvalidation_1-logloss:0.04691\n[198]\tvalidation_0-logloss:0.01272\tvalidation_1-logloss:0.04719\n[199]\tvalidation_0-logloss:0.01266\tvalidation_1-logloss:0.04742\n[200]\tvalidation_0-logloss:0.01261\tvalidation_1-logloss:0.04718\n[201]\tvalidation_0-logloss:0.01256\tvalidation_1-logloss:0.04697\n[202]\tvalidation_0-logloss:0.01251\tvalidation_1-logloss:0.04703\n[203]\tvalidation_0-logloss:0.01243\tvalidation_1-logloss:0.04704\n[204]\tvalidation_0-logloss:0.01235\tvalidation_1-logloss:0.04707\n[205]\tvalidation_0-logloss:0.01230\tvalidation_1-logloss:0.04694\n[206]\tvalidation_0-logloss:0.01225\tvalidation_1-logloss:0.04699\n[207]\tvalidation_0-logloss:0.01218\tvalidation_1-logloss:0.04693\n[208]\tvalidation_0-logloss:0.01213\tvalidation_1-logloss:0.04713\n[209]\tvalidation_0-logloss:0.01208\tvalidation_1-logloss:0.04689\n[210]\tvalidation_0-logloss:0.01203\tvalidation_1-logloss:0.04676\n[211]\tvalidation_0-logloss:0.01195\tvalidation_1-logloss:0.04645\n[212]\tvalidation_0-logloss:0.01190\tvalidation_1-logloss:0.04666\n[213]\tvalidation_0-logloss:0.01185\tvalidation_1-logloss:0.04634\n[214]\tvalidation_0-logloss:0.01181\tvalidation_1-logloss:0.04640\n[215]\tvalidation_0-logloss:0.01176\tvalidation_1-logloss:0.04627\n[216]\tvalidation_0-logloss:0.01169\tvalidation_1-logloss:0.04621\n[217]\tvalidation_0-logloss:0.01164\tvalidation_1-logloss:0.04611\n[218]\tvalidation_0-logloss:0.01160\tvalidation_1-logloss:0.04588\n[219]\tvalidation_0-logloss:0.01155\tvalidation_1-logloss:0.04576\n[220]\tvalidation_0-logloss:0.01149\tvalidation_1-logloss:0.04570\n[221]\tvalidation_0-logloss:0.01145\tvalidation_1-logloss:0.04594\n[222]\tvalidation_0-logloss:0.01141\tvalidation_1-logloss:0.04600\n[223]\tvalidation_0-logloss:0.01137\tvalidation_1-logloss:0.04573\n[224]\tvalidation_0-logloss:0.01133\tvalidation_1-logloss:0.04552\n[225]\tvalidation_0-logloss:0.01128\tvalidation_1-logloss:0.04539\n[226]\tvalidation_0-logloss:0.01122\tvalidation_1-logloss:0.04534\n[227]\tvalidation_0-logloss:0.01118\tvalidation_1-logloss:0.04554\n[228]\tvalidation_0-logloss:0.01114\tvalidation_1-logloss:0.04536\n[229]\tvalidation_0-logloss:0.01109\tvalidation_1-logloss:0.04550\n[230]\tvalidation_0-logloss:0.01105\tvalidation_1-logloss:0.04529\n[231]\tvalidation_0-logloss:0.01100\tvalidation_1-logloss:0.04516\n[232]\tvalidation_0-logloss:0.01095\tvalidation_1-logloss:0.04511\n[233]\tvalidation_0-logloss:0.01091\tvalidation_1-logloss:0.04516\n[234]\tvalidation_0-logloss:0.01087\tvalidation_1-logloss:0.04536\n[235]\tvalidation_0-logloss:0.01083\tvalidation_1-logloss:0.04514\n[236]\tvalidation_0-logloss:0.01079\tvalidation_1-logloss:0.04536\n[237]\tvalidation_0-logloss:0.01075\tvalidation_1-logloss:0.04515\n[238]\tvalidation_0-logloss:0.01071\tvalidation_1-logloss:0.04503\n[239]\tvalidation_0-logloss:0.01066\tvalidation_1-logloss:0.04498\n[240]\tvalidation_0-logloss:0.01063\tvalidation_1-logloss:0.04505\n[241]\tvalidation_0-logloss:0.01058\tvalidation_1-logloss:0.04519\n[242]\tvalidation_0-logloss:0.01053\tvalidation_1-logloss:0.04511\n[243]\tvalidation_0-logloss:0.01049\tvalidation_1-logloss:0.04499\n[244]\tvalidation_0-logloss:0.01045\tvalidation_1-logloss:0.04478\n[245]\tvalidation_0-logloss:0.01041\tvalidation_1-logloss:0.04500\n[246]\tvalidation_0-logloss:0.01036\tvalidation_1-logloss:0.04492\n[247]\tvalidation_0-logloss:0.01032\tvalidation_1-logloss:0.04506\n[248]\tvalidation_0-logloss:0.01028\tvalidation_1-logloss:0.04485\n[249]\tvalidation_0-logloss:0.01025\tvalidation_1-logloss:0.04473\n[250]\tvalidation_0-logloss:0.01022\tvalidation_1-logloss:0.04477\n[251]\tvalidation_0-logloss:0.01016\tvalidation_1-logloss:0.04469\n[252]\tvalidation_0-logloss:0.01013\tvalidation_1-logloss:0.04488\n[253]\tvalidation_0-logloss:0.01009\tvalidation_1-logloss:0.04477\n[254]\tvalidation_0-logloss:0.01006\tvalidation_1-logloss:0.04456\n[255]\tvalidation_0-logloss:0.01003\tvalidation_1-logloss:0.04436\n[256]\tvalidation_0-logloss:0.01000\tvalidation_1-logloss:0.04440\n[257]\tvalidation_0-logloss:0.00995\tvalidation_1-logloss:0.04433\n[258]\tvalidation_0-logloss:0.00991\tvalidation_1-logloss:0.04446\n[259]\tvalidation_0-logloss:0.00987\tvalidation_1-logloss:0.04435\n[260]\tvalidation_0-logloss:0.00984\tvalidation_1-logloss:0.04415\n[261]\tvalidation_0-logloss:0.00981\tvalidation_1-logloss:0.04398\n[262]\tvalidation_0-logloss:0.00976\tvalidation_1-logloss:0.04391\n[263]\tvalidation_0-logloss:0.00973\tvalidation_1-logloss:0.04380\n[264]\tvalidation_0-logloss:0.00970\tvalidation_1-logloss:0.04386\n[265]\tvalidation_0-logloss:0.00968\tvalidation_1-logloss:0.04365\n[266]\tvalidation_0-logloss:0.00964\tvalidation_1-logloss:0.04378\n[267]\tvalidation_0-logloss:0.00961\tvalidation_1-logloss:0.04383\n[268]\tvalidation_0-logloss:0.00958\tvalidation_1-logloss:0.04373\n[269]\tvalidation_0-logloss:0.00955\tvalidation_1-logloss:0.04356\n[270]\tvalidation_0-logloss:0.00952\tvalidation_1-logloss:0.04374\n[271]\tvalidation_0-logloss:0.00947\tvalidation_1-logloss:0.04369\n[272]\tvalidation_0-logloss:0.00944\tvalidation_1-logloss:0.04358\n[273]\tvalidation_0-logloss:0.00942\tvalidation_1-logloss:0.04364\n[274]\tvalidation_0-logloss:0.00939\tvalidation_1-logloss:0.04345\n[275]\tvalidation_0-logloss:0.00936\tvalidation_1-logloss:0.04362\n[276]\tvalidation_0-logloss:0.00933\tvalidation_1-logloss:0.04346\n[277]\tvalidation_0-logloss:0.00929\tvalidation_1-logloss:0.04359\n[278]\tvalidation_0-logloss:0.00927\tvalidation_1-logloss:0.04338\n[279]\tvalidation_0-logloss:0.00924\tvalidation_1-logloss:0.04342\n[280]\tvalidation_0-logloss:0.00922\tvalidation_1-logloss:0.04357\n[281]\tvalidation_0-logloss:0.00917\tvalidation_1-logloss:0.04352\n[282]\tvalidation_0-logloss:0.00916\tvalidation_1-logloss:0.04339\n[283]\tvalidation_0-logloss:0.00914\tvalidation_1-logloss:0.04321\n[284]\tvalidation_0-logloss:0.00911\tvalidation_1-logloss:0.04305\n[285]\tvalidation_0-logloss:0.00907\tvalidation_1-logloss:0.04317\n[286]\tvalidation_0-logloss:0.00905\tvalidation_1-logloss:0.04331\n[287]\tvalidation_0-logloss:0.00903\tvalidation_1-logloss:0.04311\n[288]\tvalidation_0-logloss:0.00900\tvalidation_1-logloss:0.04296\n[289]\tvalidation_0-logloss:0.00899\tvalidation_1-logloss:0.04299\n[290]\tvalidation_0-logloss:0.00896\tvalidation_1-logloss:0.04317\n[291]\tvalidation_0-logloss:0.00894\tvalidation_1-logloss:0.04307\n[292]\tvalidation_0-logloss:0.00892\tvalidation_1-logloss:0.04309\n[293]\tvalidation_0-logloss:0.00890\tvalidation_1-logloss:0.04327\n[294]\tvalidation_0-logloss:0.00887\tvalidation_1-logloss:0.04308\n[295]\tvalidation_0-logloss:0.00885\tvalidation_1-logloss:0.04286\n[296]\tvalidation_0-logloss:0.00883\tvalidation_1-logloss:0.04301\n[297]\tvalidation_0-logloss:0.00882\tvalidation_1-logloss:0.04288\n[298]\tvalidation_0-logloss:0.00878\tvalidation_1-logloss:0.04285\n[299]\tvalidation_0-logloss:0.00876\tvalidation_1-logloss:0.04266\n[300]\tvalidation_0-logloss:0.00874\tvalidation_1-logloss:0.04253\n[301]\tvalidation_0-logloss:0.00873\tvalidation_1-logloss:0.04244\n[302]\tvalidation_0-logloss:0.00871\tvalidation_1-logloss:0.04230\n[303]\tvalidation_0-logloss:0.00869\tvalidation_1-logloss:0.04233\n[304]\tvalidation_0-logloss:0.00868\tvalidation_1-logloss:0.04249\n[305]\tvalidation_0-logloss:0.00867\tvalidation_1-logloss:0.04236\n[306]\tvalidation_0-logloss:0.00866\tvalidation_1-logloss:0.04239\n[307]\tvalidation_0-logloss:0.00865\tvalidation_1-logloss:0.04255\n[308]\tvalidation_0-logloss:0.00863\tvalidation_1-logloss:0.04243\n[309]\tvalidation_0-logloss:0.00862\tvalidation_1-logloss:0.04234\n[310]\tvalidation_0-logloss:0.00860\tvalidation_1-logloss:0.04220\n[311]\tvalidation_0-logloss:0.00857\tvalidation_1-logloss:0.04237\n[312]\tvalidation_0-logloss:0.00855\tvalidation_1-logloss:0.04219\n[313]\tvalidation_0-logloss:0.00854\tvalidation_1-logloss:0.04223\n[314]\tvalidation_0-logloss:0.00853\tvalidation_1-logloss:0.04237\n[315]\tvalidation_0-logloss:0.00850\tvalidation_1-logloss:0.04223\n[316]\tvalidation_0-logloss:0.00849\tvalidation_1-logloss:0.04227\n[317]\tvalidation_0-logloss:0.00848\tvalidation_1-logloss:0.04215\n[318]\tvalidation_0-logloss:0.00847\tvalidation_1-logloss:0.04206\n[319]\tvalidation_0-logloss:0.00844\tvalidation_1-logloss:0.04223\n[320]\tvalidation_0-logloss:0.00843\tvalidation_1-logloss:0.04211\n[321]\tvalidation_0-logloss:0.00842\tvalidation_1-logloss:0.04226\n[322]\tvalidation_0-logloss:0.00841\tvalidation_1-logloss:0.04229\n[323]\tvalidation_0-logloss:0.00840\tvalidation_1-logloss:0.04245\n[324]\tvalidation_0-logloss:0.00839\tvalidation_1-logloss:0.04233\n[325]\tvalidation_0-logloss:0.00838\tvalidation_1-logloss:0.04216\n[326]\tvalidation_0-logloss:0.00836\tvalidation_1-logloss:0.04199\n[327]\tvalidation_0-logloss:0.00835\tvalidation_1-logloss:0.04202\n[328]\tvalidation_0-logloss:0.00834\tvalidation_1-logloss:0.04217\n[329]\tvalidation_0-logloss:0.00831\tvalidation_1-logloss:0.04220\n[330]\tvalidation_0-logloss:0.00827\tvalidation_1-logloss:0.04218\n[331]\tvalidation_0-logloss:0.00826\tvalidation_1-logloss:0.04206\n[332]\tvalidation_0-logloss:0.00825\tvalidation_1-logloss:0.04188\n[333]\tvalidation_0-logloss:0.00823\tvalidation_1-logloss:0.04182\n[334]\tvalidation_0-logloss:0.00822\tvalidation_1-logloss:0.04174\n[335]\tvalidation_0-logloss:0.00820\tvalidation_1-logloss:0.04168\n[336]\tvalidation_0-logloss:0.00819\tvalidation_1-logloss:0.04154\n[337]\tvalidation_0-logloss:0.00818\tvalidation_1-logloss:0.04171\n[338]\tvalidation_0-logloss:0.00814\tvalidation_1-logloss:0.04169\n[339]\tvalidation_0-logloss:0.00813\tvalidation_1-logloss:0.04158\n[340]\tvalidation_0-logloss:0.00812\tvalidation_1-logloss:0.04145\n[341]\tvalidation_0-logloss:0.00811\tvalidation_1-logloss:0.04129\n[342]\tvalidation_0-logloss:0.00810\tvalidation_1-logloss:0.04145\n[343]\tvalidation_0-logloss:0.00809\tvalidation_1-logloss:0.04149\n[344]\tvalidation_0-logloss:0.00808\tvalidation_1-logloss:0.04138\n[345]\tvalidation_0-logloss:0.00807\tvalidation_1-logloss:0.04152\n[346]\tvalidation_0-logloss:0.00806\tvalidation_1-logloss:0.04138\n[347]\tvalidation_0-logloss:0.00805\tvalidation_1-logloss:0.04145\n[348]\tvalidation_0-logloss:0.00804\tvalidation_1-logloss:0.04136\n[349]\tvalidation_0-logloss:0.00803\tvalidation_1-logloss:0.04120\n[350]\tvalidation_0-logloss:0.00802\tvalidation_1-logloss:0.04124\n[351]\tvalidation_0-logloss:0.00801\tvalidation_1-logloss:0.04138\n[352]\tvalidation_0-logloss:0.00800\tvalidation_1-logloss:0.04127\n[353]\tvalidation_0-logloss:0.00799\tvalidation_1-logloss:0.04114\n[354]\tvalidation_0-logloss:0.00798\tvalidation_1-logloss:0.04128\n[355]\tvalidation_0-logloss:0.00797\tvalidation_1-logloss:0.04116\n[356]\tvalidation_0-logloss:0.00796\tvalidation_1-logloss:0.04103\n[357]\tvalidation_0-logloss:0.00795\tvalidation_1-logloss:0.04110\n[358]\tvalidation_0-logloss:0.00794\tvalidation_1-logloss:0.04113\n[359]\tvalidation_0-logloss:0.00793\tvalidation_1-logloss:0.04108\n[360]\tvalidation_0-logloss:0.00792\tvalidation_1-logloss:0.04099\n[361]\tvalidation_0-logloss:0.00791\tvalidation_1-logloss:0.04084\n[362]\tvalidation_0-logloss:0.00790\tvalidation_1-logloss:0.04090\n[363]\tvalidation_0-logloss:0.00789\tvalidation_1-logloss:0.04077\n[364]\tvalidation_0-logloss:0.00788\tvalidation_1-logloss:0.04080\n[365]\tvalidation_0-logloss:0.00787\tvalidation_1-logloss:0.04095\n[366]\tvalidation_0-logloss:0.00786\tvalidation_1-logloss:0.04084\n[367]\tvalidation_0-logloss:0.00785\tvalidation_1-logloss:0.04075\n[368]\tvalidation_0-logloss:0.00784\tvalidation_1-logloss:0.04060\n[369]\tvalidation_0-logloss:0.00783\tvalidation_1-logloss:0.04055\n[370]\tvalidation_0-logloss:0.00783\tvalidation_1-logloss:0.04058\n[371]\tvalidation_0-logloss:0.00782\tvalidation_1-logloss:0.04072\n[372]\tvalidation_0-logloss:0.00781\tvalidation_1-logloss:0.04059\n[373]\tvalidation_0-logloss:0.00780\tvalidation_1-logloss:0.04065\n[374]\tvalidation_0-logloss:0.00779\tvalidation_1-logloss:0.04057\n[375]\tvalidation_0-logloss:0.00778\tvalidation_1-logloss:0.04046\n[376]\tvalidation_0-logloss:0.00777\tvalidation_1-logloss:0.04060\n[377]\tvalidation_0-logloss:0.00776\tvalidation_1-logloss:0.04063\n[378]\tvalidation_0-logloss:0.00775\tvalidation_1-logloss:0.04053\n[379]\tvalidation_0-logloss:0.00774\tvalidation_1-logloss:0.04066\n[380]\tvalidation_0-logloss:0.00773\tvalidation_1-logloss:0.04053\n[381]\tvalidation_0-logloss:0.00772\tvalidation_1-logloss:0.04048\n[382]\tvalidation_0-logloss:0.00772\tvalidation_1-logloss:0.04034\n[383]\tvalidation_0-logloss:0.00771\tvalidation_1-logloss:0.04037\n[384]\tvalidation_0-logloss:0.00770\tvalidation_1-logloss:0.04053\n[385]\tvalidation_0-logloss:0.00769\tvalidation_1-logloss:0.04045\n[386]\tvalidation_0-logloss:0.00768\tvalidation_1-logloss:0.04034\n[387]\tvalidation_0-logloss:0.00767\tvalidation_1-logloss:0.04020\n[388]\tvalidation_0-logloss:0.00766\tvalidation_1-logloss:0.04023\n[389]\tvalidation_0-logloss:0.00765\tvalidation_1-logloss:0.04037\n[390]\tvalidation_0-logloss:0.00764\tvalidation_1-logloss:0.04025\n[391]\tvalidation_0-logloss:0.00764\tvalidation_1-logloss:0.04038\n[392]\tvalidation_0-logloss:0.00763\tvalidation_1-logloss:0.04033\n[393]\tvalidation_0-logloss:0.00762\tvalidation_1-logloss:0.04022\n[394]\tvalidation_0-logloss:0.00761\tvalidation_1-logloss:0.04015\n[395]\tvalidation_0-logloss:0.00760\tvalidation_1-logloss:0.04018\n[396]\tvalidation_0-logloss:0.00759\tvalidation_1-logloss:0.04031\n[397]\tvalidation_0-logloss:0.00758\tvalidation_1-logloss:0.04021\n[398]\tvalidation_0-logloss:0.00758\tvalidation_1-logloss:0.04007\n[399]\tvalidation_0-logloss:0.00757\tvalidation_1-logloss:0.04022\n```\n:::\n:::\n\n\n### LightGBM\n\n- 성능은 xgboost랑 별로 차이가 없음.\n- 1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\n- one hot 인코딩 필요 없음\n\n- python lightgbm\n\n::: {#ce55cbdc .cell execution_count=13}\n``` {.python .cell-code}\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[LightGBM] [Info] Number of positive: 253, number of negative: 156\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4092\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.618582 -> initscore=0.483533\n[LightGBM] [Info] Start training from score 0.483533\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[291]\ttraining's binary_logloss: 2.39157e-05\tvalid_1's binary_logloss: 0.00285194\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02_files/figure-html/cell-14-output-2.png){width=647 height=449}\n:::\n:::\n\n\n## stacking\n\n::: {#a5261585 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n```\n:::\n\n\n::: {#ae607e40 .cell execution_count=15}\n``` {.python .cell-code}\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n```\n:::\n\n\n::: {#b392155c .cell execution_count=16}\n``` {.python .cell-code}\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.965\n```\n:::\n:::\n\n\n- test 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n### CV 세트 기반 stacking\n\n::: {#7926bb41 .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n```\n:::\n\n\n::: {#49ec2fa3 .cell execution_count=18}\n``` {.python .cell-code}\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n```\n:::\n\n\n::: {#006b595a .cell execution_count=19}\n``` {.python .cell-code}\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.965\n```\n:::\n:::\n\n\n## Baysian Optimization\n\n- Grid search로는 시간이 너무 오래 걸리는 경우\n\n- 목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\n- Surrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\n- acquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n::: {#ffe28377 .cell execution_count=20}\n``` {.python .cell-code}\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]\r100%|██████████| 20/20 [00:00<00:00, 1720.92trial/s, best loss: -224.0]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n{'x': -4.0, 'y': 12.0}\n```\n:::\n:::\n\n\n### XGBoost 하이퍼파라미터 최적화\n\n::: {#eb18291d .cell execution_count=21}\n``` {.python .cell-code}\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n```\n:::\n\n\n::: {#2cf23ee1 .cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\r  2%|▏         | 1/50 [00:00<00:07,  6.51trial/s, best loss: -0.9560241663762055]\r  4%|▍         | 2/50 [00:00<00:06,  7.84trial/s, best loss: -0.9560241663762055]\r  6%|▌         | 3/50 [00:00<00:06,  7.20trial/s, best loss: -0.9560241663762055]\r  8%|▊         | 4/50 [00:00<00:06,  7.19trial/s, best loss: -0.9560241663762055]\r 10%|█         | 5/50 [00:00<00:06,  6.92trial/s, best loss: -0.9560241663762055]\r 12%|█▏        | 6/50 [00:00<00:07,  5.84trial/s, best loss: -0.9560241663762055]\r 14%|█▍        | 7/50 [00:01<00:07,  5.90trial/s, best loss: -0.9560241663762055]\r 16%|█▌        | 8/50 [00:01<00:06,  6.59trial/s, best loss: -0.9603956082258627]\r 18%|█▊        | 9/50 [00:01<00:06,  6.20trial/s, best loss: -0.9603956082258627]\r 20%|██        | 10/50 [00:01<00:06,  6.43trial/s, best loss: -0.9626031137446264]\r 22%|██▏       | 11/50 [00:01<00:05,  6.50trial/s, best loss: -0.9626031137446264]\r 24%|██▍       | 12/50 [00:01<00:05,  6.64trial/s, best loss: -0.9626031137446264]\r 26%|██▌       | 13/50 [00:01<00:05,  7.00trial/s, best loss: -0.9626031137446264]\r 28%|██▊       | 14/50 [00:02<00:05,  7.00trial/s, best loss: -0.9626031137446264]\r 30%|███       | 15/50 [00:02<00:04,  7.28trial/s, best loss: -0.9626031137446264]\r 32%|███▏      | 16/50 [00:02<00:05,  6.69trial/s, best loss: -0.9626031137446264]\r 34%|███▍      | 17/50 [00:02<00:05,  6.60trial/s, best loss: -0.9626031137446264]\r 36%|███▌      | 18/50 [00:02<00:05,  6.27trial/s, best loss: -0.9626031137446264]\r 38%|███▊      | 19/50 [00:03<00:06,  4.83trial/s, best loss: -0.9626031137446264]\r 40%|████      | 20/50 [00:03<00:07,  3.98trial/s, best loss: -0.9626031137446264]\r 42%|████▏     | 21/50 [00:03<00:07,  3.99trial/s, best loss: -0.9626031137446264]\r 44%|████▍     | 22/50 [00:03<00:06,  4.04trial/s, best loss: -0.9626031137446264]\r 46%|████▌     | 23/50 [00:04<00:06,  4.19trial/s, best loss: -0.9626031137446264]\r 48%|████▊     | 24/50 [00:04<00:06,  3.93trial/s, best loss: -0.9626031137446264]\r 50%|█████     | 25/50 [00:04<00:06,  4.04trial/s, best loss: -0.9626031137446264]\r 52%|█████▏    | 26/50 [00:04<00:06,  3.90trial/s, best loss: -0.9626031137446264]\r 54%|█████▍    | 27/50 [00:05<00:05,  4.09trial/s, best loss: -0.9626031137446264]\r 56%|█████▌    | 28/50 [00:05<00:04,  4.52trial/s, best loss: -0.9647960962007668]\r 58%|█████▊    | 29/50 [00:05<00:04,  4.55trial/s, best loss: -0.9647960962007668]\r 60%|██████    | 30/50 [00:05<00:04,  4.54trial/s, best loss: -0.9647960962007668]\r 62%|██████▏   | 31/50 [00:05<00:04,  4.72trial/s, best loss: -0.9647960962007668]\r 64%|██████▍   | 32/50 [00:06<00:03,  5.13trial/s, best loss: -0.9647960962007668]\r 66%|██████▌   | 33/50 [00:06<00:03,  5.28trial/s, best loss: -0.9647960962007668]\r 68%|██████▊   | 34/50 [00:06<00:02,  5.54trial/s, best loss: -0.9647960962007668]\r 78%|███████▊  | 39/50 [00:06<00:00, 11.51trial/s, best loss: -0.9647960962007668]\r 82%|████████▏ | 41/50 [00:07<00:01,  8.29trial/s, best loss: -0.9647960962007668]\r 84%|████████▍ | 42/50 [00:07<00:01,  7.71trial/s, best loss: -0.9647960962007668]\r 86%|████████▌ | 43/50 [00:07<00:01,  6.58trial/s, best loss: -0.9647960962007668]\r 88%|████████▊ | 44/50 [00:07<00:00,  6.65trial/s, best loss: -0.9647960962007668]\r 90%|█████████ | 45/50 [00:07<00:00,  6.76trial/s, best loss: -0.9647960962007668]\r 92%|█████████▏| 46/50 [00:08<00:00,  5.00trial/s, best loss: -0.9647960962007668]\r 94%|█████████▍| 47/50 [00:08<00:00,  3.87trial/s, best loss: -0.9647960962007668]\r 96%|█████████▌| 48/50 [00:08<00:00,  3.48trial/s, best loss: -0.9647960962007668]\r 98%|█████████▊| 49/50 [00:09<00:00,  3.49trial/s, best loss: -0.9647960962007668]\r100%|██████████| 50/50 [00:09<00:00,  3.29trial/s, best loss: -0.9647960962007668]\r100%|██████████| 50/50 [00:09<00:00,  5.25trial/s, best loss: -0.9647960962007668]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n{'colsample_bytree': 0.8333015582921072,\n 'learning_rate': 0.181846600140329,\n 'max_depth': 11.0,\n 'min_child_weight': 1.0}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}