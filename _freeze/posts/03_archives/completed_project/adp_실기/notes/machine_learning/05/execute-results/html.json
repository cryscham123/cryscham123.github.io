{
  "hash": "e8b290ed50809a49d329f288a64274ee",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"회귀\"\ndate: 2025-07-28\ncategories: [\"머신 러닝\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## 경사하강법\n\n::: {#97d2e534 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = 2 * np.random.rand(100, 1)\ny = 6 + 4 * X + np.random.randn(100, 1)\nplt.scatter(X, y)\n```\n\n::: {.cell-output .cell-output-display}\n![](05_files/figure-html/cell-2-output-1.png){width=566 height=411}\n:::\n:::\n\n\n::: {#f25ebc09 .cell execution_count=2}\n``` {.python .cell-code}\ndef get_cost(y, y_pred):\n    N = len(y)\n    cost = np.sum(np.square(y - y_pred)) / N\n    return cost\n\ndef get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n    N = len(y)\n    w1_update = np.zeros_like(w1)\n    w0_update = np.zeros_like(w0)\n    y_pred = np.dot(X, w1.T) + w0\n    diff = y - y_pred\n\n    w1_update = -(2/N) * learning_rate * np.dot(X.T, diff)\n    w0_update = -(2/N) * learning_rate * np.sum(diff)\n\n    return w1_update, w0_update\n\ndef gradient_descent_steps(X, y, iters=10000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for _ in range(iters):\n        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n```\n:::\n\n\n::: {#50781e31 .cell execution_count=3}\n``` {.python .cell-code}\nw1, w0 = gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nw0: 6.394 w1: 3.611, total cost: 1.009\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](05_files/figure-html/cell-4-output-2.png){width=566 height=411}\n:::\n:::\n\n\n- 일반 경사하강법은 시간이 오래걸려서 잘 안씀\n\n## 미니 배치 확률적 경사 하강법\n\n::: {#89b39973 .cell execution_count=4}\n``` {.python .cell-code}\ndef stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for ind in range(iters):\n        stochastic_random_index = np.random.permutation(X.shape[0])\n        sample_X = X[stochastic_random_index[0:batch_size]]\n        sample_y = y[stochastic_random_index[0:batch_size]]\n\n        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n```\n:::\n\n\n::: {#3652a861 .cell execution_count=5}\n``` {.python .cell-code}\nw1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nw0: 6.372 w1: 3.619, total cost: 1.010\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](05_files/figure-html/cell-6-output-2.png){width=566 height=411}\n:::\n:::\n\n\n## 선형 회귀\n\n::: {#09b6ca21 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_boston\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nboston = load_boston()\n\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\ndf.head()\n```\n:::\n\n\n::: {#6d74b36e .cell execution_count=7}\n``` {.python .cell-code}\nlm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRAIO', 'LSTAT', 'RAD']\n\nfig, axs = plt.subplots(figsize=(16, 8), ncols=len(lm_features) // 2, nrows=2)\n\nfor i, feature in enumerate(lm_features):\n    row = i // 4\n    col = i % 4\n\n    sns.regplot(x=feature, y='price', data=df, ax=axs[row][col])\n```\n:::\n\n\nboston 데이터가 윤리적 문제로 사용 불가능하다고 한다.\n\n::: {#9266576f .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ny_target = df['price']\nX_data = df.drop(['price'], axis=1, inplace=False)\nlr = LinearRegression()\n\nneg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n```\n:::\n\n\ncross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함\n\n## 다항 회귀\n\n::: {#3cee10ca .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\ndef polynominal_func(X):\n    y = 1 + 2 * X[:, 0] + 3 * X[:, 0]**2 + 4 * X[:, 1]**3\n    return y\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=3)),\n                  ('linear', LinearRegression())])\nX = np.arange(4).reshape(2, 2)\ny = polynominal_func(X)\n\nmodel = model.fit(X, y)\n\nnp.round(model.named_steps['linear'].coef_, 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])\n```\n:::\n:::\n\n\n## 규제\n\n- L2 규제(Ridge): $min(RSS(W) + \\lambda ||W||^2)$\n- L1 규제(Lasso): $min(RSS(W) + \\lambda ||W||_1)$\n\n- λ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐\n- L1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음\n\n### 릿지\n\n::: {#70c12e31 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha = 10)\nneg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n```\n:::\n\n\n### 라쏘 엘라스틱넷\n\n::: {#37549c8d .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\ndef get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None):\n    coeff_df = pd.DataFrame()\n    for param in params:\n        if model_name == 'Ridge':\n            model = Ridge(alpha=param)\n        elif model_name == 'Lasso':\n            model = Lasso(alpha=param)\n        elif model_name == 'ElasticNet':\n            model = ElasticNet(alpha=param, l1_ratio=0.7)\n        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=\"neg_mean_squared_error\", cv=5)\n        rmse_scores = np.sqrt(-1 * neg_mse_scores)\n        avg_rmse = np.mean(rmse_scores)\n        print(f'{param}: {avg_rmse:.3f}')\n\n        model.fit(X_data_n, y_target_n)\n        coeff = pd.Series(data=model.coef_, index=X_data_n.columns)\n        colname = 'alpha:' + str(param)\n        coeff_df[colname] = coeff\n\n    return coeff_df\n```\n:::\n\n\n## 선형 회귀 모델을 위한 데이터 변환\n\n- 로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.\n\n::: {#8bcd77fe .cell execution_count=12}\n``` {.python .cell-code}\nnp.log1p(data)\n```\n:::\n\n\n",
    "supporting": [
      "05_files"
    ],
    "filters": [],
    "includes": {}
  }
}