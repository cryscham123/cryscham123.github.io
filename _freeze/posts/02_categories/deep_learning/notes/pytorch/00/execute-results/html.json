{
  "hash": "27e753306814cfa3517dcd37cc4765f3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"pytorch 기초\"\ndate: 2025-12-17\ncategories: [\"deep learning\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## 특징\n\n1. 동적 계산 그래프(Dynamic Computation Graph)\n   - 파이토치는 `동적 계산 그래프`를 사용하여 모델을 정의하고 수정할 수 있다. 모델의 구조를 실행 시점에 변경할 수 있다.\n2. GPU 가속 지원\n3. 직관적 인터페이스\n    - `실행모드`를 지원하고, 계산 그래프를 빌드하지 않고 코드를 실행할 수 있다.\n4. 제한된 프로덕션 지원\n    - 주로 연구 목적이다.\n\n## 텐서\n\n::: {#598e1bb2 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\ntensor = torch.rand(1, 2)\n\nprint(tensor)\nprint(tensor.shape) # 크기\nprint(tensor.dtype) # 자료형\nprint(tensor.device) # GPU 가속 여부\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.1930, 0.8300]])\ntorch.Size([1, 2])\ntorch.float32\ncpu\n```\n:::\n:::\n\n\n### 장치 설정\n\n::: {#551d41ed .cell execution_count=2}\n``` {.python .cell-code}\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = torch.cuda.FloatTensor([1, 2, 3]) # GPU 가속 지정 방법 1. MAC에서는 지원이 안될 수도 있다.\ntensor = torch.rand((1, 1), device=device) # GPU 가속 지정 방법 2\nprint(device)\nprint(cpu)\nprint(gpu)\nprint(tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncuda\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([[0.9628]], device='cuda:0')\n```\n:::\n:::\n\n\n- cpu 텐서와 gpu 텐서는 상호 간 연산이 불가능하다.\n- numpy 배열은 cpu 텐서와의 연산만 가능\n\n### 장치 변환\n\n::: {#e9ffbb14 .cell execution_count=3}\n``` {.python .cell-code}\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = cpu.cuda() # cpu -> gpu\ncpu2 = gpu.cpu() # gpu -> cpu\ngpu2 = cpu.to(\"cuda\") # cpu -> gpu 2 MAC에서도 지원이 되니까 이 방법으로 사용하자\nprint(cpu)\nprint(cpu2)\nprint(gpu)\nprint(gpu2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1., 2., 3.])\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([1., 2., 3.], device='cuda:0')\n```\n:::\n:::\n\n\n::: {#80e4aa41 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\nndarray = np.array([1, 2, 3], dtype=np.uint8)\nyo = torch.from_numpy(ndarray)\nprint(yo)\nprint(yo.to(\"cuda\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([1, 2, 3], dtype=torch.uint8)\ntensor([1, 2, 3], device='cuda:0', dtype=torch.uint8)\n```\n:::\n:::\n\n\n::: {#71fc3690 .cell execution_count=5}\n``` {.python .cell-code}\nndarray = yo.detach().cpu().numpy() # detach: graph에서 분리된 새로운 텐서 반환\nprint(ndarray)\nprint(type(ndarray))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 3]\n<class 'numpy.ndarray'>\n```\n:::\n:::\n\n\n## 단순 선형회귀\n\n::: {#35c8a4d3 .cell execution_count=6}\n``` {.python .cell-code}\nfrom torch import optim\n\nx = torch.FloatTensor([\n    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]\n])\ny = torch.FloatTensor([\n    [0.94], [2.05], [2.87], [4.10], [5.01], [6.15], [6.95], [8.12], [9.05], [10.11],\n    [11.03], [12.20], [12.89], [14.15], [15.02], [16.18], [16.95], [18.22], [19.10], [20.05],\n    [21.01], [22.20], [22.89], [24.10], [25.05], [26.15], [26.95], [28.12], [29.05], [30.10]\n])\n```\n:::\n\n\n::: {#d78fc933 .cell execution_count=7}\n``` {.python .cell-code}\nweight = torch.zeros(1, requires_grad=True)\nbias = torch.zeros(1, requires_grad=True)\nlearning_rate = 0.001\n```\n:::\n\n\n::: {#9164a4bc .cell execution_count=8}\n``` {.python .cell-code}\noptimizer = optim.SGD([weight, bias], lr=learning_rate)\n\nfor epoch in range(1000):\n    hypothesis = x * weight + bias\n    cost = torch.mean((hypothesis - y) ** 2)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost.item():.4f} | Weight: {weight.item():.4f} | Bias: {bias.item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 100/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0482\nEpoch 200/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0473\nEpoch 300/1000 | Cost: 0.0089 | Weight: 1.0011 | Bias: 0.0463\nEpoch 400/1000 | Cost: 0.0088 | Weight: 1.0011 | Bias: 0.0455\nEpoch 500/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0446\nEpoch 600/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0438\nEpoch 700/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0430\nEpoch 800/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0423\nEpoch 900/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0416\nEpoch 1000/1000 | Cost: 0.0088 | Weight: 1.0014 | Bias: 0.0409\n```\n:::\n:::\n\n\n::: {#6846ef88 .cell execution_count=9}\n``` {.python .cell-code}\nfrom torch import nn\n\nmodel = nn.Linear(1, 1, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(1000):\n    hypothesis = model(x)\n    cost = criterion(hypothesis, y)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 100/1000 | Cost: 0.0136 | Model: [Parameter containing:\ntensor([[0.9950]], requires_grad=True), Parameter containing:\ntensor([0.1702], requires_grad=True)]\nEpoch 200/1000 | Cost: 0.0132 | Model: [Parameter containing:\ntensor([[0.9953]], requires_grad=True), Parameter containing:\ntensor([0.1636], requires_grad=True)]\nEpoch 300/1000 | Cost: 0.0128 | Model: [Parameter containing:\ntensor([[0.9956]], requires_grad=True), Parameter containing:\ntensor([0.1573], requires_grad=True)]\nEpoch 400/1000 | Cost: 0.0124 | Model: [Parameter containing:\ntensor([[0.9959]], requires_grad=True), Parameter containing:\ntensor([0.1513], requires_grad=True)]\nEpoch 500/1000 | Cost: 0.0121 | Model: [Parameter containing:\ntensor([[0.9962]], requires_grad=True), Parameter containing:\ntensor([0.1455], requires_grad=True)]\nEpoch 600/1000 | Cost: 0.0118 | Model: [Parameter containing:\ntensor([[0.9965]], requires_grad=True), Parameter containing:\ntensor([0.1400], requires_grad=True)]\nEpoch 700/1000 | Cost: 0.0115 | Model: [Parameter containing:\ntensor([[0.9967]], requires_grad=True), Parameter containing:\ntensor([0.1348], requires_grad=True)]\nEpoch 800/1000 | Cost: 0.0113 | Model: [Parameter containing:\ntensor([[0.9970]], requires_grad=True), Parameter containing:\ntensor([0.1298], requires_grad=True)]\nEpoch 900/1000 | Cost: 0.0110 | Model: [Parameter containing:\ntensor([[0.9972]], requires_grad=True), Parameter containing:\ntensor([0.1251], requires_grad=True)]\nEpoch 1000/1000 | Cost: 0.0108 | Model: [Parameter containing:\ntensor([[0.9974]], requires_grad=True), Parameter containing:\ntensor([0.1205], requires_grad=True)]\n```\n:::\n:::\n\n\n## 데이터 로드\n\n::: {#f2f4e484 .cell execution_count=10}\n``` {.python .cell-code}\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_x = torch.FloatTensor([\n    [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]\n])\ntrain_y = torch.FloatTensor([\n    [0.1, 1.5], [1, 2.8], [1.9, 4.1], [2.8, 5.4], [3.7, 6.7], [4.6, 8]\n])\ntrain_dataset = TensorDataset(train_x, train_y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n```\n:::\n\n\n::: {#13a27265 .cell execution_count=11}\n``` {.python .cell-code}\nmodel = nn.Linear(2, 2, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(1000):\n    cost = 0\n    for batch in train_dataloader:\n        x, y = batch\n        output = model(x)\n        loss = criterion(output, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        cost += loss\n\n    cost = cost / len(train_dataloader)\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 100/1000 | Cost: 0.1810 | Model: [Parameter containing:\ntensor([[0.3308, 0.2977],\n        [0.8800, 0.2466]], requires_grad=True), Parameter containing:\ntensor([0.0446, 0.6826], requires_grad=True)]\nEpoch 200/1000 | Cost: 0.1563 | Model: [Parameter containing:\ntensor([[0.3674, 0.2787],\n        [0.9034, 0.2345]], requires_grad=True), Parameter containing:\ntensor([-0.0109,  0.6471], requires_grad=True)]\nEpoch 300/1000 | Cost: 0.1366 | Model: [Parameter containing:\ntensor([[0.4018, 0.2612],\n        [0.9254, 0.2234]], requires_grad=True), Parameter containing:\ntensor([-0.0628,  0.6140], requires_grad=True)]\nEpoch 400/1000 | Cost: 0.1195 | Model: [Parameter containing:\ntensor([[0.4339, 0.2448],\n        [0.9458, 0.2129]], requires_grad=True), Parameter containing:\ntensor([-0.1114,  0.5830], requires_grad=True)]\nEpoch 500/1000 | Cost: 0.1056 | Model: [Parameter containing:\ntensor([[0.4636, 0.2291],\n        [0.9649, 0.2029]], requires_grad=True), Parameter containing:\ntensor([-0.1568,  0.5540], requires_grad=True)]\nEpoch 600/1000 | Cost: 0.0913 | Model: [Parameter containing:\ntensor([[0.4916, 0.2147],\n        [0.9827, 0.1936]], requires_grad=True), Parameter containing:\ntensor([-0.1992,  0.5269], requires_grad=True)]\nEpoch 700/1000 | Cost: 0.0799 | Model: [Parameter containing:\ntensor([[0.5178, 0.2012],\n        [0.9994, 0.1851]], requires_grad=True), Parameter containing:\ntensor([-0.2389,  0.5016], requires_grad=True)]\nEpoch 800/1000 | Cost: 0.0704 | Model: [Parameter containing:\ntensor([[0.5421, 0.1884],\n        [1.0150, 0.1769]], requires_grad=True), Parameter containing:\ntensor([-0.2760,  0.4779], requires_grad=True)]\nEpoch 900/1000 | Cost: 0.0606 | Model: [Parameter containing:\ntensor([[0.5650, 0.1766],\n        [1.0296, 0.1694]], requires_grad=True), Parameter containing:\ntensor([-0.3107,  0.4558], requires_grad=True)]\nEpoch 1000/1000 | Cost: 0.0531 | Model: [Parameter containing:\ntensor([[0.5864, 0.1657],\n        [1.0432, 0.1624]], requires_grad=True), Parameter containing:\ntensor([-0.3431,  0.4351], requires_grad=True)]\n```\n:::\n:::\n\n\n### 모듈 클래스\n\n::: {#3d8d9628 .cell execution_count=12}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset\nimport pandas as pd\n\nclass CustomDataset(Dataset):\n    def __init__(self, file_path):\n        df = pd.read_csv(file_path)\n        self.x = df.iloc[:, 0].values\n        self.y = df.iloc[:, 1].values\n        self.length = len(df)\n\n    def __getitem__(self, index):\n        x = torch.FloatTensor([self.x[index] ** 2, self.x[index]])\n        y = torch.FloatTensor([self.y[index]])\n        return x, y\n\n    def __len__(self):\n        return self.length\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        return x\n```\n:::\n\n\n",
    "supporting": [
      "00_files"
    ],
    "filters": [],
    "includes": {}
  }
}