{
  "hash": "b68f8730e9db2e75103279decf6b846c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"회귀분석 템플릿\"\ndate: 2025-10-05\ncategories: [\"데이터 분석\"]\n---\n\n\n\n\n## 가정\n\n- **선형성**: 종속변수와 독립변수 간의 관계는 선형이다.\n- **정규성**: 종속변수 **잔차들의 분포**는 정규분포이다.\n- **등분산성**: 종속변수 **잔차들의 분포**는 동일한 분산을 갖는다.\n- **독립성**: 모든 **잔차값**은 서로 독립이다.\n\n## 가정 검정\n\n- 먼저 모델링을 진행한 후, 잔차를 통해 가정을 검정한다.\n\n```{.python filename=\"linear regression test\"}\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\nmodel = sm.OLS(y, Xc).fit()\nresid = model.resid\n\nprint(model.summary())\n```\n\n### 이상치, 영향치 처리\n\n- 레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차\n    - Cook's distance\n    - Leverage\n    - 그 외 DFFITS, DFBETAS 등\n\n::: {#71ac02f0 .cell execution_count=1}\n``` {.python .cell-code}\nmean = resid.mean()\nstd = resid.std()\noutlier_mask = (resid < mean - 3 * std) | (resid > mean + 3 * std)\n\ninfluence = model.get_influence()\ncooks = influence.cooks_distance[0]\ninfluence = cooks > 4 / (len(resid) - model.df_model - 1)\n\noutlier = resid[outlier_mask | influence].index\n\ny_log = y.drop(outlier)\nXc = sm.add_constant(X.drop(outlier))\nmodel = sm.OLS(y, Xc).fit()\nresid = model.resid\n\nprint(model.summary())\n```\n:::\n\n\n### 다중공선성 검정\n\n1. 전 변수 집합 대상\n\n```{.python filename=\"multicollinearity test\"}\ncor = df.corr()\ncond_num = np.linalg.cond(cor)\nprint(\"Condition Number:\", cond_num)\n```\n\n- **30을 초과**하면 다중공선성이 높다고 판단한다.\n- 선형 종속 가능성을 봄.\n- statsmodels의 ols를 사용해도 볼 수 있음\n- scaling이 선행되어야 함.\n- 초과 시 해석:\n    - 독립변수의 전체 차원이 부족한 경우\n    - 표본을 더 모으거나 새로운 변수를 도입\n\n2. 개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)\n\n```{.python filename=\"VIF test\"}\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef check_vif(X, y):\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    model.fit()\n    vif_df = pd.DataFrame(columns=['feature', 'VIF'])\n    for i in range(1, len(model.exog_names)):\n        vif_df.loc[i, 'feature'] = model.exog_names[i]\n        vif_df.loc[i, 'VIF'] = variance_inflation_factor(model.exog, i)\n    return vif_df.sort_values(by='VIF', ascending=False)\n\nprint(check_vif(X, y))\n```\n\n- 특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우\n- **VIF가 10**을 넘을 경우\n- 덜 중요하다면 제거\n- 중요하다면 변수에 대한 독립적 정보 보강(세분화, ...)\n\n3. 두 변수의 상관계수가 높은 경우\n\n```{.python filename=\"correlation heatmap\"}\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(cor, annot=True, ax=ax)\n```\n\n- 둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)\n- 둘 중 하나를 제거\n\n### 정규성\n\n- ols의 summary를 통해 확인 가능\n- 혹은 EDA 과정에서 사용한 정규성 검정 참조\n\n### 선형성, 등분산성 검정\n\n::: {#50e74334 .cell execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.residplot(x=model.fittedvalues, y=model.resid, lowess=True,\n              line_kws={'color': 'red', 'lw': 2})\nplt.show()\n```\n:::\n\n\n- 선형성:\n    - 잔차도가 어떠한 패턴도 보이지 않아야 한다.\n- 등분산성: \n    - 잔차도가 일정한 폭을 가져야 한다.\n\n::: {#d9f59650 .cell execution_count=3}\n``` {.python .cell-code}\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\n# Breusch-Pagan 검정\nlm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(resid, Xc)\nprint(f'Breusch-Pagan p-value: {lm_pvalue:.3f}')\n\n# 잔차의 정규성 만족시 lm_pvalue, 그 외 f_pvalue 사용\nif lm_pvalue > 0.05:\n    print(\"등분산성 가정을 만족합니다.\")\nelse:\n    print(\"등분산성 가정을 위반합니다.\")\n```\n:::\n\n\n### 독립성\n\n- 독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다.\n    - 더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.\n    - 2에 가까울수록 독립성 만족\n    - statsmodels의 ols로 확인 가능\n\n## 전처리\n\n1. 범주형 변수 처리:\n    - 더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현\n        - 각 범주의 회귀계수는 기준 범주와의 차이를 의미\n1. 이상치 / 영향점: 관측값 제거\n1. 선형성 위반: 독립변수 변환, GAM\n1. 정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM\n1. 다중공산성 위반: 다중공산성 파트 참고\n    - 혹은 변수 선택법을 사용\n\n- 가정 만족할 때까지 검정, 전처리 계속 반복\n\n### 변수 변환\n\n![회귀 모델 수정 λ](img/2025-09-20-18-38-36.png)\n\n![경험적인 적절한 λ](img/2025-09-20-18-55-55.png)\n\n- 최적의 λ는 최대 우도 추정법으로 구할 수 있다.\n- 변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.\n- 일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.\n    - 아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.\n    - 통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.\n\n```{.python filename=\"box cox transformation\"}\nfrom scipy.stats import boxcox\n\ny_transformed, best_lambda = boxcox(y)\nprint(f\"Best lambda: {best_lambda:.3f}\")\n```\n\n### 변수 선택법\n\n- 전진 선택법\n- 후진 선택법\n- 단계적 선택법\n- 최적조합 선택법: 모든 조합 다 해봄\n- 기준\n    - R2, Adj R2\n    - AIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법\n    - BIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌\n    - Mallows' Cp\n\n## 규제 선형 회귀\n\n- 지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦\n- 독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)\n    - 일반적으로는 scale을 하든 안하든 r square에 차이가 없다.\n\n1. 릿지회귀\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma β_j^2$\n    - 회귀계수 절댓값을 0에 가깝게 함\n    - 하지만 0으로 만들지는 않음\n    - 작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.\n    - 회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음\n1. 라쏘회귀:\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma |β_j|$\n    - 회귀계수를 0으로 만들 수 있음\n    - 변수 선택 효과\n    - 릿지보다 해석이 쉬움\n    - 일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음\n1. 엘라스틱넷 회귀\n    - $\\Sigma (y_i - \\hat{y_i})^2 + λ_1\\Sigma |β_j| + λ_2\\Sigma β_j^2$\n    - 릿지와 라쏘의 장점을 모두 가짐\n    - 변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦\n    - 독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음\n\n## 일반화 선형 회귀(GLM)\n\n- 종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우\n    - 이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.\n    - 포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.\n    - 따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.\n\n### Logistic 회귀\n\n- 종속변수가 범주형일 경우\n- $z = β_0 + β_1 x_1 + β_2 x_2 + ... + β_n x_n$\n- $p = \\frac{1}{1 + e^{-z}}$\n- 오즈: $\\frac{p}{1-p}$ = $e^z$\n- 오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율\n    - $(e^{β_k})^k$\n- LLR의 p-value가 낮다면, 모델이 통계적으로 유의미함을 의미\n- 잔차 검정은 안함\n\n### 포아송 회귀\n\n- 종속변수가 count 데이터일 경우\n\n::: {#ba8572d0 .cell execution_count=4}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nfitted = model.fit()\nprint(model.summary())\n```\n:::\n\n\n- $x_k$가 한 단위 증가할 때, 빈도 수가 $exp(β_k)$배 증가\n- 만약 관찰 시간이 다를 경우, offset로 np.log(df\\[관찰시간\\])을 넣어줘야 함\n- Deviance / DF_resid 가 1보다 크면 과산포, 작으면 과소산포\n    - 과산포: 사건발생 확률이 일정하지 않음\n    - 과산포 시 음이항 회귀 사용\n\n```{.python filename=\"음이항 회귀\"}\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\n\nmodel = sm.GLM(y, Xc, family=sm.families.NegativeBinomial())\nfitted = model.fit()\nprint(model.summary())\n```\n\n- Quasi-Poisson도 있지만, statsmodels에서는 제공하지 않음\n\n",
    "supporting": [
      "04_files"
    ],
    "filters": [],
    "includes": {}
  }
}