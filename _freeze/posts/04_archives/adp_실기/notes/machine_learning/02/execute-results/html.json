{
  "hash": "e253abbb941481257e304cac526c8b61",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"분류 - 앙상블\"\ndate: 2025-07-27\ncategories: [\"머신 러닝\"]\n---\n\n\n\n\n![](/img/stat-thumb.jpg){.post-thumbnail}\n\n## voting\n\n- 서로 다른 알고리즘이 결합. 분류에서는 voting[^1]으로 결정\n\n### Example\n\n::: {#b9d4647f .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#4fc4c882 .cell execution_count=2}\n``` {.python .cell-code}\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0.9473684210526315\n```\n:::\n:::\n\n\n::: {#5fe4a66b .cell execution_count=3}\n``` {.python .cell-code}\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogisticRegression 정확도: 0.9298\nKNeighborsClassifier 정확도: 0.9386\n```\n:::\n:::\n\n\n- 반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님\n\n## bagging\n\n- 같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting[^1]으로 결정\n\n### RandomForest\n\n::: {#2b0da1fe .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n```\n:::\n\n\n::: {#448c0def .cell execution_count=5}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.9121140142517815\n```\n:::\n:::\n\n\n[^1]: hard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.\n\n## boosting\n\n### GBM\n\n::: {#3bac995a .cell execution_count=6}\n``` {.python .cell-code}\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n```\n:::\n\n\n0.939, 701.6343066692352초\n\n- 아주 오래 걸림.\n\n### XGBoost\n\n- 결손값을 자체 처리할 수 있다.\n- 조기 종료 기능이 있다.\n- 자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\n\n- python xgboost\n\n::: {#a7d2284f .cell execution_count=7}\n``` {.python .cell-code}\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n```\n:::\n\n\n::: {#5fae0c89 .cell execution_count=8}\n``` {.python .cell-code}\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n```\n:::\n\n\n::: {#8e32046e .cell execution_count=9}\n``` {.python .cell-code}\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n```\n:::\n\n\n::: {#afdab235 .cell execution_count=10}\n``` {.python .cell-code}\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\ttrain-logloss:0.61998\teval-logloss:0.59703\n[1]\ttrain-logloss:0.58197\teval-logloss:0.57110\n[2]\ttrain-logloss:0.54757\teval-logloss:0.54592\n[3]\ttrain-logloss:0.51601\teval-logloss:0.52588\n[4]\ttrain-logloss:0.48728\teval-logloss:0.50815\n[5]\ttrain-logloss:0.46090\teval-logloss:0.48925\n[6]\ttrain-logloss:0.43663\teval-logloss:0.47570\n[7]\ttrain-logloss:0.41424\teval-logloss:0.45534\n[8]\ttrain-logloss:0.39307\teval-logloss:0.44195\n[9]\ttrain-logloss:0.37343\teval-logloss:0.42443\n[10]\ttrain-logloss:0.35524\teval-logloss:0.41319\n[11]\ttrain-logloss:0.33815\teval-logloss:0.39814\n[12]\ttrain-logloss:0.32204\teval-logloss:0.38904\n[13]\ttrain-logloss:0.30707\teval-logloss:0.37554\n[14]\ttrain-logloss:0.29311\teval-logloss:0.36850\n[15]\ttrain-logloss:0.27945\teval-logloss:0.35855\n[16]\ttrain-logloss:0.26705\teval-logloss:0.34728\n[17]\ttrain-logloss:0.25522\teval-logloss:0.34114\n[18]\ttrain-logloss:0.24369\teval-logloss:0.33369\n[19]\ttrain-logloss:0.23288\teval-logloss:0.32780\n[20]\ttrain-logloss:0.22312\teval-logloss:0.31902\n[21]\ttrain-logloss:0.21382\teval-logloss:0.31284\n[22]\ttrain-logloss:0.20471\teval-logloss:0.30783\n[23]\ttrain-logloss:0.19620\teval-logloss:0.30313\n[24]\ttrain-logloss:0.18827\teval-logloss:0.29949\n[25]\ttrain-logloss:0.18066\teval-logloss:0.29573\n[26]\ttrain-logloss:0.17345\teval-logloss:0.29148\n[27]\ttrain-logloss:0.16674\teval-logloss:0.28587\n[28]\ttrain-logloss:0.16041\teval-logloss:0.28159\n[29]\ttrain-logloss:0.15421\teval-logloss:0.27984\n[30]\ttrain-logloss:0.14846\teval-logloss:0.27685\n[31]\ttrain-logloss:0.14304\teval-logloss:0.27437\n[32]\ttrain-logloss:0.13789\teval-logloss:0.27101\n[33]\ttrain-logloss:0.13290\teval-logloss:0.26683\n[34]\ttrain-logloss:0.12805\teval-logloss:0.26610\n[35]\ttrain-logloss:0.12361\teval-logloss:0.26441\n[36]\ttrain-logloss:0.11950\teval-logloss:0.26142\n[37]\ttrain-logloss:0.11536\teval-logloss:0.25818\n[38]\ttrain-logloss:0.11152\teval-logloss:0.25687\n[39]\ttrain-logloss:0.10796\teval-logloss:0.25437\n[40]\ttrain-logloss:0.10438\teval-logloss:0.25147\n[41]\ttrain-logloss:0.10088\teval-logloss:0.25168\n[42]\ttrain-logloss:0.09770\teval-logloss:0.25166\n[43]\ttrain-logloss:0.09467\teval-logloss:0.24986\n[44]\ttrain-logloss:0.09178\teval-logloss:0.24949\n[45]\ttrain-logloss:0.08893\teval-logloss:0.24738\n[46]\ttrain-logloss:0.08629\teval-logloss:0.24574\n[47]\ttrain-logloss:0.08349\teval-logloss:0.24264\n[48]\ttrain-logloss:0.08106\teval-logloss:0.24329\n[49]\ttrain-logloss:0.07870\teval-logloss:0.24164\n[50]\ttrain-logloss:0.07645\teval-logloss:0.24162\n[51]\ttrain-logloss:0.07421\teval-logloss:0.23970\n[52]\ttrain-logloss:0.07196\teval-logloss:0.23703\n[53]\ttrain-logloss:0.06999\teval-logloss:0.23626\n[54]\ttrain-logloss:0.06804\teval-logloss:0.23466\n[55]\ttrain-logloss:0.06623\teval-logloss:0.23565\n[56]\ttrain-logloss:0.06445\teval-logloss:0.23413\n[57]\ttrain-logloss:0.06273\teval-logloss:0.23295\n[58]\ttrain-logloss:0.06093\teval-logloss:0.23110\n[59]\ttrain-logloss:0.05936\teval-logloss:0.23104\n[60]\ttrain-logloss:0.05792\teval-logloss:0.23144\n[61]\ttrain-logloss:0.05647\teval-logloss:0.23027\n[62]\ttrain-logloss:0.05509\teval-logloss:0.22936\n[63]\ttrain-logloss:0.05382\teval-logloss:0.22986\n[64]\ttrain-logloss:0.05253\teval-logloss:0.22886\n[65]\ttrain-logloss:0.05126\teval-logloss:0.22886\n[66]\ttrain-logloss:0.05008\teval-logloss:0.22796\n[67]\ttrain-logloss:0.04890\teval-logloss:0.22733\n[68]\ttrain-logloss:0.04781\teval-logloss:0.22840\n[69]\ttrain-logloss:0.04671\teval-logloss:0.22801\n[70]\ttrain-logloss:0.04567\teval-logloss:0.22754\n[71]\ttrain-logloss:0.04472\teval-logloss:0.22904\n[72]\ttrain-logloss:0.04384\teval-logloss:0.23060\n[73]\ttrain-logloss:0.04282\teval-logloss:0.23003\n[74]\ttrain-logloss:0.04202\teval-logloss:0.23156\n[75]\ttrain-logloss:0.04114\teval-logloss:0.23122\n[76]\ttrain-logloss:0.04031\teval-logloss:0.23064\n[77]\ttrain-logloss:0.03943\teval-logloss:0.23019\n[78]\ttrain-logloss:0.03866\teval-logloss:0.22972\n[79]\ttrain-logloss:0.03799\teval-logloss:0.22949\n[80]\ttrain-logloss:0.03721\teval-logloss:0.22910\n[81]\ttrain-logloss:0.03644\teval-logloss:0.22854\n[82]\ttrain-logloss:0.03575\teval-logloss:0.22762\n[83]\ttrain-logloss:0.03512\teval-logloss:0.22733\n[84]\ttrain-logloss:0.03450\teval-logloss:0.22755\n[85]\ttrain-logloss:0.03383\teval-logloss:0.22704\n[86]\ttrain-logloss:0.03324\teval-logloss:0.22625\n[87]\ttrain-logloss:0.03267\teval-logloss:0.22573\n[88]\ttrain-logloss:0.03207\teval-logloss:0.22487\n[89]\ttrain-logloss:0.03153\teval-logloss:0.22416\n[90]\ttrain-logloss:0.03095\teval-logloss:0.22480\n[91]\ttrain-logloss:0.03046\teval-logloss:0.22387\n[92]\ttrain-logloss:0.02991\teval-logloss:0.22381\n[93]\ttrain-logloss:0.02940\teval-logloss:0.22385\n[94]\ttrain-logloss:0.02887\teval-logloss:0.22266\n[95]\ttrain-logloss:0.02843\teval-logloss:0.22336\n[96]\ttrain-logloss:0.02796\teval-logloss:0.22344\n[97]\ttrain-logloss:0.02750\teval-logloss:0.22415\n[98]\ttrain-logloss:0.02703\teval-logloss:0.22302\n[99]\ttrain-logloss:0.02664\teval-logloss:0.22376\n[100]\ttrain-logloss:0.02625\teval-logloss:0.22443\n[101]\ttrain-logloss:0.02586\teval-logloss:0.22393\n[102]\ttrain-logloss:0.02546\teval-logloss:0.22413\n[103]\ttrain-logloss:0.02513\teval-logloss:0.22360\n[104]\ttrain-logloss:0.02471\teval-logloss:0.22419\n[105]\ttrain-logloss:0.02435\teval-logloss:0.22570\n[106]\ttrain-logloss:0.02401\teval-logloss:0.22527\n[107]\ttrain-logloss:0.02368\teval-logloss:0.22569\n[108]\ttrain-logloss:0.02333\teval-logloss:0.22634\n[109]\ttrain-logloss:0.02303\teval-logloss:0.22595\n[110]\ttrain-logloss:0.02274\teval-logloss:0.22652\n[111]\ttrain-logloss:0.02245\teval-logloss:0.22584\n[112]\ttrain-logloss:0.02212\teval-logloss:0.22487\n[113]\ttrain-logloss:0.02183\teval-logloss:0.22596\n[114]\ttrain-logloss:0.02155\teval-logloss:0.22743\n[115]\ttrain-logloss:0.02125\teval-logloss:0.22652\n[116]\ttrain-logloss:0.02099\teval-logloss:0.22771\n[117]\ttrain-logloss:0.02075\teval-logloss:0.22863\n[118]\ttrain-logloss:0.02053\teval-logloss:0.22753\n[119]\ttrain-logloss:0.02030\teval-logloss:0.22743\n[120]\ttrain-logloss:0.02007\teval-logloss:0.22719\n[121]\ttrain-logloss:0.01984\teval-logloss:0.22828\n[122]\ttrain-logloss:0.01958\teval-logloss:0.22620\n[123]\ttrain-logloss:0.01937\teval-logloss:0.22613\n[124]\ttrain-logloss:0.01917\teval-logloss:0.22586\n[125]\ttrain-logloss:0.01897\teval-logloss:0.22694\n[126]\ttrain-logloss:0.01876\teval-logloss:0.22699\n[127]\ttrain-logloss:0.01853\teval-logloss:0.22612\n[128]\ttrain-logloss:0.01835\teval-logloss:0.22640\n[129]\ttrain-logloss:0.01817\teval-logloss:0.22646\n[130]\ttrain-logloss:0.01797\teval-logloss:0.22760\n[131]\ttrain-logloss:0.01776\teval-logloss:0.22753\n[132]\ttrain-logloss:0.01755\teval-logloss:0.22671\n[133]\ttrain-logloss:0.01737\teval-logloss:0.22785\n[134]\ttrain-logloss:0.01719\teval-logloss:0.22794\n[135]\ttrain-logloss:0.01703\teval-logloss:0.22707\n[136]\ttrain-logloss:0.01686\teval-logloss:0.22629\n[137]\ttrain-logloss:0.01669\teval-logloss:0.22639\n[138]\ttrain-logloss:0.01653\teval-logloss:0.22753\n[139]\ttrain-logloss:0.01639\teval-logloss:0.22772\n[140]\ttrain-logloss:0.01624\teval-logloss:0.22731\n[141]\ttrain-logloss:0.01613\teval-logloss:0.22757\n[142]\ttrain-logloss:0.01599\teval-logloss:0.22767\n[143]\ttrain-logloss:0.01586\teval-logloss:0.22727\n```\n:::\n:::\n\n\n::: {#0e2c33cc .cell execution_count=11}\n``` {.python .cell-code}\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x > 0.5 else 0 for x in pred_probs]\n```\n:::\n\n\n- sklearn xgboost\n\n::: {#0fdb7923 .cell execution_count=12}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-logloss:0.61998\tvalidation_1-logloss:0.59703\n[1]\tvalidation_0-logloss:0.58197\tvalidation_1-logloss:0.57110\n[2]\tvalidation_0-logloss:0.54757\tvalidation_1-logloss:0.54592\n[3]\tvalidation_0-logloss:0.51601\tvalidation_1-logloss:0.52588\n[4]\tvalidation_0-logloss:0.48728\tvalidation_1-logloss:0.50815\n[5]\tvalidation_0-logloss:0.46090\tvalidation_1-logloss:0.48925\n[6]\tvalidation_0-logloss:0.43663\tvalidation_1-logloss:0.47570\n[7]\tvalidation_0-logloss:0.41424\tvalidation_1-logloss:0.45534\n[8]\tvalidation_0-logloss:0.39307\tvalidation_1-logloss:0.44195\n[9]\tvalidation_0-logloss:0.37343\tvalidation_1-logloss:0.42443\n[10]\tvalidation_0-logloss:0.35524\tvalidation_1-logloss:0.41319\n[11]\tvalidation_0-logloss:0.33815\tvalidation_1-logloss:0.39814\n[12]\tvalidation_0-logloss:0.32204\tvalidation_1-logloss:0.38904\n[13]\tvalidation_0-logloss:0.30707\tvalidation_1-logloss:0.37554\n[14]\tvalidation_0-logloss:0.29311\tvalidation_1-logloss:0.36850\n[15]\tvalidation_0-logloss:0.27945\tvalidation_1-logloss:0.35855\n[16]\tvalidation_0-logloss:0.26705\tvalidation_1-logloss:0.34728\n[17]\tvalidation_0-logloss:0.25522\tvalidation_1-logloss:0.34114\n[18]\tvalidation_0-logloss:0.24369\tvalidation_1-logloss:0.33369\n[19]\tvalidation_0-logloss:0.23288\tvalidation_1-logloss:0.32780\n[20]\tvalidation_0-logloss:0.22312\tvalidation_1-logloss:0.31902\n[21]\tvalidation_0-logloss:0.21382\tvalidation_1-logloss:0.31284\n[22]\tvalidation_0-logloss:0.20471\tvalidation_1-logloss:0.30783\n[23]\tvalidation_0-logloss:0.19620\tvalidation_1-logloss:0.30313\n[24]\tvalidation_0-logloss:0.18827\tvalidation_1-logloss:0.29949\n[25]\tvalidation_0-logloss:0.18066\tvalidation_1-logloss:0.29573\n[26]\tvalidation_0-logloss:0.17345\tvalidation_1-logloss:0.29148\n[27]\tvalidation_0-logloss:0.16674\tvalidation_1-logloss:0.28587\n[28]\tvalidation_0-logloss:0.16041\tvalidation_1-logloss:0.28159\n[29]\tvalidation_0-logloss:0.15421\tvalidation_1-logloss:0.27984\n[30]\tvalidation_0-logloss:0.14846\tvalidation_1-logloss:0.27685\n[31]\tvalidation_0-logloss:0.14304\tvalidation_1-logloss:0.27437\n[32]\tvalidation_0-logloss:0.13789\tvalidation_1-logloss:0.27101\n[33]\tvalidation_0-logloss:0.13290\tvalidation_1-logloss:0.26683\n[34]\tvalidation_0-logloss:0.12805\tvalidation_1-logloss:0.26610\n[35]\tvalidation_0-logloss:0.12361\tvalidation_1-logloss:0.26441\n[36]\tvalidation_0-logloss:0.11950\tvalidation_1-logloss:0.26142\n[37]\tvalidation_0-logloss:0.11536\tvalidation_1-logloss:0.25818\n[38]\tvalidation_0-logloss:0.11152\tvalidation_1-logloss:0.25687\n[39]\tvalidation_0-logloss:0.10796\tvalidation_1-logloss:0.25437\n[40]\tvalidation_0-logloss:0.10438\tvalidation_1-logloss:0.25147\n[41]\tvalidation_0-logloss:0.10088\tvalidation_1-logloss:0.25168\n[42]\tvalidation_0-logloss:0.09770\tvalidation_1-logloss:0.25166\n[43]\tvalidation_0-logloss:0.09467\tvalidation_1-logloss:0.24986\n[44]\tvalidation_0-logloss:0.09178\tvalidation_1-logloss:0.24949\n[45]\tvalidation_0-logloss:0.08893\tvalidation_1-logloss:0.24738\n[46]\tvalidation_0-logloss:0.08629\tvalidation_1-logloss:0.24574\n[47]\tvalidation_0-logloss:0.08349\tvalidation_1-logloss:0.24264\n[48]\tvalidation_0-logloss:0.08106\tvalidation_1-logloss:0.24329\n[49]\tvalidation_0-logloss:0.07870\tvalidation_1-logloss:0.24164\n[50]\tvalidation_0-logloss:0.07645\tvalidation_1-logloss:0.24162\n[51]\tvalidation_0-logloss:0.07421\tvalidation_1-logloss:0.23970\n[52]\tvalidation_0-logloss:0.07196\tvalidation_1-logloss:0.23703\n[53]\tvalidation_0-logloss:0.06999\tvalidation_1-logloss:0.23626\n[54]\tvalidation_0-logloss:0.06804\tvalidation_1-logloss:0.23466\n[55]\tvalidation_0-logloss:0.06623\tvalidation_1-logloss:0.23565\n[56]\tvalidation_0-logloss:0.06445\tvalidation_1-logloss:0.23413\n[57]\tvalidation_0-logloss:0.06273\tvalidation_1-logloss:0.23295\n[58]\tvalidation_0-logloss:0.06093\tvalidation_1-logloss:0.23110\n[59]\tvalidation_0-logloss:0.05936\tvalidation_1-logloss:0.23104\n[60]\tvalidation_0-logloss:0.05792\tvalidation_1-logloss:0.23144\n[61]\tvalidation_0-logloss:0.05647\tvalidation_1-logloss:0.23027\n[62]\tvalidation_0-logloss:0.05509\tvalidation_1-logloss:0.22936\n[63]\tvalidation_0-logloss:0.05382\tvalidation_1-logloss:0.22986\n[64]\tvalidation_0-logloss:0.05253\tvalidation_1-logloss:0.22886\n[65]\tvalidation_0-logloss:0.05126\tvalidation_1-logloss:0.22886\n[66]\tvalidation_0-logloss:0.05008\tvalidation_1-logloss:0.22796\n[67]\tvalidation_0-logloss:0.04890\tvalidation_1-logloss:0.22733\n[68]\tvalidation_0-logloss:0.04781\tvalidation_1-logloss:0.22840\n[69]\tvalidation_0-logloss:0.04671\tvalidation_1-logloss:0.22801\n[70]\tvalidation_0-logloss:0.04567\tvalidation_1-logloss:0.22754\n[71]\tvalidation_0-logloss:0.04472\tvalidation_1-logloss:0.22904\n[72]\tvalidation_0-logloss:0.04384\tvalidation_1-logloss:0.23060\n[73]\tvalidation_0-logloss:0.04282\tvalidation_1-logloss:0.23003\n[74]\tvalidation_0-logloss:0.04202\tvalidation_1-logloss:0.23156\n[75]\tvalidation_0-logloss:0.04114\tvalidation_1-logloss:0.23122\n[76]\tvalidation_0-logloss:0.04031\tvalidation_1-logloss:0.23064\n[77]\tvalidation_0-logloss:0.03943\tvalidation_1-logloss:0.23019\n[78]\tvalidation_0-logloss:0.03866\tvalidation_1-logloss:0.22972\n[79]\tvalidation_0-logloss:0.03799\tvalidation_1-logloss:0.22949\n[80]\tvalidation_0-logloss:0.03721\tvalidation_1-logloss:0.22910\n[81]\tvalidation_0-logloss:0.03644\tvalidation_1-logloss:0.22854\n[82]\tvalidation_0-logloss:0.03575\tvalidation_1-logloss:0.22762\n[83]\tvalidation_0-logloss:0.03512\tvalidation_1-logloss:0.22733\n[84]\tvalidation_0-logloss:0.03450\tvalidation_1-logloss:0.22755\n[85]\tvalidation_0-logloss:0.03383\tvalidation_1-logloss:0.22704\n[86]\tvalidation_0-logloss:0.03324\tvalidation_1-logloss:0.22625\n[87]\tvalidation_0-logloss:0.03267\tvalidation_1-logloss:0.22573\n[88]\tvalidation_0-logloss:0.03207\tvalidation_1-logloss:0.22487\n[89]\tvalidation_0-logloss:0.03153\tvalidation_1-logloss:0.22416\n[90]\tvalidation_0-logloss:0.03095\tvalidation_1-logloss:0.22480\n[91]\tvalidation_0-logloss:0.03046\tvalidation_1-logloss:0.22387\n[92]\tvalidation_0-logloss:0.02991\tvalidation_1-logloss:0.22381\n[93]\tvalidation_0-logloss:0.02940\tvalidation_1-logloss:0.22385\n[94]\tvalidation_0-logloss:0.02887\tvalidation_1-logloss:0.22266\n[95]\tvalidation_0-logloss:0.02843\tvalidation_1-logloss:0.22336\n[96]\tvalidation_0-logloss:0.02796\tvalidation_1-logloss:0.22344\n[97]\tvalidation_0-logloss:0.02750\tvalidation_1-logloss:0.22415\n[98]\tvalidation_0-logloss:0.02703\tvalidation_1-logloss:0.22302\n[99]\tvalidation_0-logloss:0.02664\tvalidation_1-logloss:0.22376\n[100]\tvalidation_0-logloss:0.02625\tvalidation_1-logloss:0.22443\n[101]\tvalidation_0-logloss:0.02586\tvalidation_1-logloss:0.22393\n[102]\tvalidation_0-logloss:0.02546\tvalidation_1-logloss:0.22413\n[103]\tvalidation_0-logloss:0.02513\tvalidation_1-logloss:0.22360\n[104]\tvalidation_0-logloss:0.02471\tvalidation_1-logloss:0.22419\n[105]\tvalidation_0-logloss:0.02435\tvalidation_1-logloss:0.22570\n[106]\tvalidation_0-logloss:0.02401\tvalidation_1-logloss:0.22527\n[107]\tvalidation_0-logloss:0.02368\tvalidation_1-logloss:0.22569\n[108]\tvalidation_0-logloss:0.02333\tvalidation_1-logloss:0.22634\n[109]\tvalidation_0-logloss:0.02303\tvalidation_1-logloss:0.22595\n[110]\tvalidation_0-logloss:0.02274\tvalidation_1-logloss:0.22652\n[111]\tvalidation_0-logloss:0.02245\tvalidation_1-logloss:0.22584\n[112]\tvalidation_0-logloss:0.02212\tvalidation_1-logloss:0.22487\n[113]\tvalidation_0-logloss:0.02183\tvalidation_1-logloss:0.22596\n[114]\tvalidation_0-logloss:0.02155\tvalidation_1-logloss:0.22743\n[115]\tvalidation_0-logloss:0.02125\tvalidation_1-logloss:0.22652\n[116]\tvalidation_0-logloss:0.02099\tvalidation_1-logloss:0.22771\n[117]\tvalidation_0-logloss:0.02075\tvalidation_1-logloss:0.22863\n[118]\tvalidation_0-logloss:0.02053\tvalidation_1-logloss:0.22753\n[119]\tvalidation_0-logloss:0.02030\tvalidation_1-logloss:0.22743\n[120]\tvalidation_0-logloss:0.02007\tvalidation_1-logloss:0.22719\n[121]\tvalidation_0-logloss:0.01984\tvalidation_1-logloss:0.22828\n[122]\tvalidation_0-logloss:0.01958\tvalidation_1-logloss:0.22620\n[123]\tvalidation_0-logloss:0.01937\tvalidation_1-logloss:0.22613\n[124]\tvalidation_0-logloss:0.01917\tvalidation_1-logloss:0.22586\n[125]\tvalidation_0-logloss:0.01897\tvalidation_1-logloss:0.22694\n[126]\tvalidation_0-logloss:0.01876\tvalidation_1-logloss:0.22699\n[127]\tvalidation_0-logloss:0.01853\tvalidation_1-logloss:0.22612\n[128]\tvalidation_0-logloss:0.01835\tvalidation_1-logloss:0.22640\n[129]\tvalidation_0-logloss:0.01817\tvalidation_1-logloss:0.22646\n[130]\tvalidation_0-logloss:0.01797\tvalidation_1-logloss:0.22760\n[131]\tvalidation_0-logloss:0.01776\tvalidation_1-logloss:0.22753\n[132]\tvalidation_0-logloss:0.01755\tvalidation_1-logloss:0.22671\n[133]\tvalidation_0-logloss:0.01737\tvalidation_1-logloss:0.22785\n[134]\tvalidation_0-logloss:0.01719\tvalidation_1-logloss:0.22794\n[135]\tvalidation_0-logloss:0.01703\tvalidation_1-logloss:0.22707\n[136]\tvalidation_0-logloss:0.01686\tvalidation_1-logloss:0.22629\n[137]\tvalidation_0-logloss:0.01669\tvalidation_1-logloss:0.22639\n[138]\tvalidation_0-logloss:0.01653\tvalidation_1-logloss:0.22753\n[139]\tvalidation_0-logloss:0.01639\tvalidation_1-logloss:0.22772\n[140]\tvalidation_0-logloss:0.01624\tvalidation_1-logloss:0.22731\n[141]\tvalidation_0-logloss:0.01613\tvalidation_1-logloss:0.22757\n[142]\tvalidation_0-logloss:0.01599\tvalidation_1-logloss:0.22767\n[143]\tvalidation_0-logloss:0.01586\tvalidation_1-logloss:0.22727\n[144]\tvalidation_0-logloss:0.01575\tvalidation_1-logloss:0.22753\n```\n:::\n:::\n\n\n### LightGBM\n\n- 성능은 xgboost랑 별로 차이가 없음.\n- 1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\n- one hot 인코딩 필요 없음\n\n- python lightgbm\n\n::: {#85d25a25 .cell execution_count=13}\n``` {.python .cell-code}\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[LightGBM] [Info] Number of positive: 255, number of negative: 154\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4092\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623472 -> initscore=0.504311\n[LightGBM] [Info] Start training from score 0.504311\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[85]\ttraining's binary_logloss: 0.0234098\tvalid_1's binary_logloss: 0.242924\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02_files/figure-html/cell-14-output-2.png){width=642 height=449}\n:::\n:::\n\n\n## stacking\n\n::: {#6adf48a1 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n```\n:::\n\n\n::: {#cabe384e .cell execution_count=15}\n``` {.python .cell-code}\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n```\n:::\n\n\n::: {#1a390dc8 .cell execution_count=16}\n``` {.python .cell-code}\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.982\n```\n:::\n:::\n\n\n- test 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n### CV 세트 기반 stacking\n\n::: {#234bc5b4 .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n```\n:::\n\n\n::: {#9c793f84 .cell execution_count=18}\n``` {.python .cell-code}\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n```\n:::\n\n\n::: {#880a82d9 .cell execution_count=19}\n``` {.python .cell-code}\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.974\n```\n:::\n:::\n\n\n## Baysian Optimization\n\n- Grid search로는 시간이 너무 오래 걸리는 경우\n\n- 목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\n- Surrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\n- acquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n::: {#516f05fd .cell execution_count=20}\n``` {.python .cell-code}\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]\r100%|██████████| 20/20 [00:00<00:00, 1860.70trial/s, best loss: -251.0]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n{'x': -3.0, 'y': 13.0}\n```\n:::\n:::\n\n\n### XGBoost 하이퍼파라미터 최적화\n\n::: {#3b9524ba .cell execution_count=21}\n``` {.python .cell-code}\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n```\n:::\n\n\n::: {#17ce695f .cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]\r  2%|▏         | 1/50 [00:00<00:23,  2.11trial/s, best loss: -0.9538892761705587]\r  4%|▍         | 2/50 [00:02<01:00,  1.25s/trial, best loss: -0.9604537004763566]\r  6%|▌         | 3/50 [00:03<01:06,  1.41s/trial, best loss: -0.9604537004763566]\r  8%|▊         | 4/50 [00:04<00:55,  1.22s/trial, best loss: -0.9604537004763566]\r 10%|█         | 5/50 [00:05<00:46,  1.04s/trial, best loss: -0.9604537004763566]\r 12%|█▏        | 6/50 [00:05<00:35,  1.25trial/s, best loss: -0.9604537004763566]\r 14%|█▍        | 7/50 [00:05<00:24,  1.74trial/s, best loss: -0.9604537004763566]\r 16%|█▌        | 8/50 [00:06<00:18,  2.32trial/s, best loss: -0.9604537004763566]\r 18%|█▊        | 9/50 [00:06<00:13,  3.00trial/s, best loss: -0.9604537004763566]\r 20%|██        | 10/50 [00:07<00:21,  1.88trial/s, best loss: -0.9604537004763566]\r 22%|██▏       | 11/50 [00:08<00:24,  1.58trial/s, best loss: -0.9604537004763566]\r 24%|██▍       | 12/50 [00:08<00:23,  1.60trial/s, best loss: -0.9604537004763566]\r 26%|██▌       | 13/50 [00:09<00:20,  1.77trial/s, best loss: -0.9626612059951203]\r 28%|██▊       | 14/50 [00:09<00:16,  2.14trial/s, best loss: -0.9626612059951203]\r 30%|███       | 15/50 [00:09<00:13,  2.65trial/s, best loss: -0.9626612059951203]\r 32%|███▏      | 16/50 [00:09<00:10,  3.24trial/s, best loss: -0.9626612059951203]\r 34%|███▍      | 17/50 [00:09<00:08,  3.75trial/s, best loss: -0.9626612059951203]\r 36%|███▌      | 18/50 [00:09<00:07,  4.33trial/s, best loss: -0.9626612059951203]\r 38%|███▊      | 19/50 [00:10<00:08,  3.85trial/s, best loss: -0.9626612059951203]\r 40%|████      | 20/50 [00:10<00:07,  4.18trial/s, best loss: -0.9626612059951203]\r 42%|████▏     | 21/50 [00:10<00:06,  4.38trial/s, best loss: -0.9626612059951203]\r 44%|████▍     | 22/50 [00:10<00:06,  4.56trial/s, best loss: -0.9626612059951203]\r 46%|████▌     | 23/50 [00:11<00:05,  4.67trial/s, best loss: -0.9626612059951203]\r 48%|████▊     | 24/50 [00:11<00:07,  3.60trial/s, best loss: -0.9648396653886371]\r 50%|█████     | 25/50 [00:11<00:07,  3.16trial/s, best loss: -0.9648396653886371]\r 52%|█████▏    | 26/50 [00:12<00:09,  2.62trial/s, best loss: -0.967047170907401] \r 54%|█████▍    | 27/50 [00:12<00:07,  2.96trial/s, best loss: -0.967047170907401]\r 56%|█████▌    | 28/50 [00:14<00:14,  1.53trial/s, best loss: -0.967047170907401]\r 58%|█████▊    | 29/50 [00:14<00:12,  1.65trial/s, best loss: -0.967047170907401]\r 60%|██████    | 30/50 [00:15<00:11,  1.76trial/s, best loss: -0.967047170907401]\r 62%|██████▏   | 31/50 [00:15<00:08,  2.25trial/s, best loss: -0.967047170907401]\r 64%|██████▍   | 32/50 [00:15<00:06,  2.67trial/s, best loss: -0.967047170907401]\r 66%|██████▌   | 33/50 [00:15<00:05,  3.29trial/s, best loss: -0.967047170907401]\r 68%|██████▊   | 34/50 [00:15<00:04,  3.81trial/s, best loss: -0.967047170907401]\r 70%|███████   | 35/50 [00:15<00:03,  3.95trial/s, best loss: -0.967047170907401]\r 72%|███████▏  | 36/50 [00:16<00:03,  4.66trial/s, best loss: -0.967047170907401]\r 74%|███████▍  | 37/50 [00:16<00:02,  5.08trial/s, best loss: -0.967047170907401]\r 76%|███████▌  | 38/50 [00:16<00:02,  4.49trial/s, best loss: -0.967047170907401]\r 78%|███████▊  | 39/50 [00:16<00:02,  4.89trial/s, best loss: -0.967047170907401]\r 80%|████████  | 40/50 [00:16<00:02,  4.87trial/s, best loss: -0.967047170907401]\r 82%|████████▏ | 41/50 [00:17<00:01,  5.07trial/s, best loss: -0.967047170907401]\r 84%|████████▍ | 42/50 [00:17<00:01,  4.76trial/s, best loss: -0.967047170907401]\r 86%|████████▌ | 43/50 [00:17<00:01,  4.88trial/s, best loss: -0.967047170907401]\r 88%|████████▊ | 44/50 [00:17<00:01,  4.96trial/s, best loss: -0.967047170907401]\r 90%|█████████ | 45/50 [00:17<00:01,  4.99trial/s, best loss: -0.967047170907401]\r 92%|█████████▏| 46/50 [00:18<00:00,  4.83trial/s, best loss: -0.967047170907401]\r 94%|█████████▍| 47/50 [00:18<00:00,  5.34trial/s, best loss: -0.967047170907401]\r 96%|█████████▌| 48/50 [00:18<00:00,  5.59trial/s, best loss: -0.967047170907401]\r 98%|█████████▊| 49/50 [00:18<00:00,  5.10trial/s, best loss: -0.967047170907401]\r100%|██████████| 50/50 [00:19<00:00,  2.70trial/s, best loss: -0.967047170907401]\r100%|██████████| 50/50 [00:19<00:00,  2.58trial/s, best loss: -0.967047170907401]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n{'colsample_bytree': 0.6859052847618307,\n 'learning_rate': 0.07174400225835986,\n 'max_depth': 9.0,\n 'min_child_weight': 2.0}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}