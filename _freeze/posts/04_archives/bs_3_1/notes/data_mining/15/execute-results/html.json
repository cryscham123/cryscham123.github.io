{
  "hash": "90777c43c940f7e562df52b91cf12b3a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"analysis\"\ndate: 2025-05-22\ncategories: [\"data mining\"]\n---\n\n\n\n\n![](/img/human-thumb.jpg){.post-thumbnail}\n\n## 데이터 load\n\n::: {#834ea1c9 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nRANDOM_STATE = 54321\nnp.random.seed(RANDOM_STATE)\n\nX_train_df = pd.read_csv(\"_data/train_data.csv\")\nX_test_df = pd.read_csv(\"_data/test_data.csv\")\ny_train = X_train_df['y'].astype('category')\ny_test = X_test_df['y'].astype('category')\nweights_train = X_train_df['weights']\nweights_test = X_test_df['weights']\nX_train = X_train_df.drop(columns=['y', 'weights'])\nX_test = X_test_df.drop(columns=['y', 'weights'])\n```\n:::\n\n\n## 무작위 분류\n\n::: {#aeba5c44 .cell execution_count=2}\n``` {.python .cell-code}\nclass_proportions_py = y_train.value_counts(normalize=True)\ntest_categories = list(y_test.cat.categories)\nprop_values_ordered = class_proportions_py.reindex(test_categories, fill_value=0).values\nrandom_predictions = np.random.choice(\n    a=test_categories,\n    size=len(y_test),\n    replace=True,\n    p=prop_values_ordered\n)\nrandom_predictions_cat = pd.Categorical(random_predictions, categories=test_categories, ordered=False)\ncm_random = confusion_matrix(y_test, random_predictions_cat, sample_weight=weights_test, labels=test_categories)\n\nprint(\"\\n=== 무작위 분류기 혼동 행렬 ===\\n\")\ncm_df_random = pd.DataFrame(cm_random, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_random.round(2))\n\naccuracy_random = accuracy_score(y_test, random_predictions_cat, sample_weight=weights_test)\nprecision_random_sk = precision_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_random_sk = recall_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_random_sk = f1_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== 무작위 분류기 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_random:.4f}\")\n\nprint(\"\\n무작위 분류기 범주별 정밀도\")\nprint(pd.Series(precision_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 재현율\")\nprint(pd.Series(recall_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 F1-score\")\nprint(pd.Series(f1_score_random_sk, index=test_categories).round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== 무작위 분류기 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37372.17           44867.14\nActual: stable                     41616.52           49155.97\n\n=== 무작위 분류기 성능 지표 ===\n\n정확도: 0.5001\n\n무작위 분류기 범주별 정밀도\nexplorative    0.4731\nstable         0.5228\ndtype: float64\n\n무작위 분류기 범주별 재현율\nexplorative    0.4544\nstable         0.5415\ndtype: float64\n\n무작위 분류기 범주별 F1-score\nexplorative    0.4636\nstable         0.5320\ndtype: float64\n```\n:::\n:::\n\n\n## Random Forest\n\n::: {#ca0479ec .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nbase_rf = RandomForestClassifier(\n    random_state=RANDOM_STATE,\n    oob_score=True,\n    class_weight='balanced_subsample',\n    n_jobs=-1\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'max_features': ['sqrt', 'log2', None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    estimator=base_rf,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nrf_model_py = grid_search.best_estimator_\nprint(f\"OOB Score: {rf_model_py.oob_score_:.4f}\")\n\nimportances = rf_model_py.feature_importances_\nfeature_names = X_train.columns\n\nmax_importance = importances.max()\nscaled_importances = (importances / max_importance) * 100\nforest_importances = pd.Series(scaled_importances, index=feature_names).sort_values(ascending=False)\n\nprint(\"\\n=== Random Forest 중요도 ===\")\ntop_10_importances = forest_importances\ntop_10_df = pd.DataFrame({'Feature': top_10_importances.index, 'Importance': top_10_importances.values})\nprint(top_10_df.round(2))\n\nsns.barplot(x=top_10_importances.values, y=top_10_importances.index)\nplt.title('Feature Importances (Random Forest)', fontsize=10)\nplt.xlabel('Importance Score', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('ran_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_rf = rf_model_py.predict(X_test)\ny_pred_proba_rf = rf_model_py.predict_proba(X_test)\n\ny_pred_rf_cat = pd.Categorical(y_pred_rf, categories=test_categories, ordered=False)\n\ncm_rf = confusion_matrix(y_test, y_pred_rf_cat, sample_weight=weights_test, labels=test_categories)\nprint(\"\\n=== Random Forest 혼동 행렬 ===\\n\")\ncm_df_rf = pd.DataFrame(cm_rf, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_rf.round(2))\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf_cat, sample_weight=weights_test)\nprecision_rf_sk = precision_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_rf_sk = recall_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_rf_sk = f1_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== Random Forest 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_rf:.4f}\")\nprint(\"\\nRandom Forest 범주별 정밀도:\")\nprint(pd.Series(precision_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 재현율:\")\nprint(pd.Series(recall_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 F1-score:\")\nprint(pd.Series(f1_score_rf_sk, index=test_categories).round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n최적의 하이퍼파라미터: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n최적 교차 검증 점수: 0.6129\nOOB Score: 0.5865\n\n=== Random Forest 중요도 ===\n                        Feature  Importance\n0            self_confidence_w4      100.00\n1              desire_stress_w1       89.19\n2          parent_attachment_w5       83.50\n3          parent_monitoring_w5       83.40\n4              friend_stress_w4       82.51\n5          parent_monitoring_w4       81.66\n6             deviant_esteem_w3       80.23\n7                 neg_esteem_w4       79.71\n8          parent_monitoring_w3       79.27\n9          parent_attachment_w4       78.51\n10  higher_school_dependence_w1       76.60\n11             desire_stress_w4       75.87\n12         parent_attachment_w3       74.15\n13  higher_school_dependence_w2       73.13\n14         parent_monitoring_w1       72.00\n15             friend_stress_w3       71.55\n16           self_confidence_w1       69.76\n17  higher_school_dependence_w4       67.74\n18             desire_stress_w2       67.66\n19            deviant_esteem_w4       66.96\n20           self_confidence_w3       66.83\n21  higher_school_dependence_w5       66.33\n22         parent_monitoring_w2       64.70\n23           self_confidence_w2       64.55\n24             parent_stress_w2       64.32\n25             friend_stress_w2       63.39\n26             parent_stress_w3       63.12\n27             parent_stress_w4       62.80\n28            deviant_esteem_w5       62.53\n29         parent_attachment_w1       62.00\n30             friend_stress_w1       61.78\n31                neg_esteem_w5       61.55\n32           academic_stress_w3       60.54\n33            deviant_esteem_w2       60.54\n34         parent_attachment_w2       60.46\n35  higher_school_dependence_w3       60.34\n36           academic_stress_w2       59.86\n37             parent_stress_w5       59.49\n38            deviant_esteem_w1       59.29\n39                neg_esteem_w3       58.54\n40           academic_stress_w4       58.47\n41           self_confidence_w5       57.98\n42                neg_esteem_w2       57.81\n43           academic_stress_w5       55.73\n44             desire_stress_w5       55.68\n45           academic_stress_w1       55.06\n46             friend_stress_w5       54.55\n47                neg_esteem_w1       52.13\n48             parent_stress_w1       52.07\n49             desire_stress_w3       51.97\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15_files/figure-html/cell-4-output-2.png){width=664 height=472}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== Random Forest 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37334.03           44905.28\nActual: stable                     27479.79           63292.70\n\n=== Random Forest 성능 지표 ===\n\n정확도: 0.5816\n\nRandom Forest 범주별 정밀도:\nexplorative    0.576\nstable         0.585\ndtype: float64\n\nRandom Forest 범주별 재현율:\nexplorative    0.4540\nstable         0.6973\ndtype: float64\n\nRandom Forest 범주별 F1-score:\nexplorative    0.5078\nstable         0.6362\ndtype: float64\n```\n:::\n:::\n\n\n## XGBoost\n\n::: {#074b3aa6 .cell execution_count=4}\n``` {.python .cell-code}\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_numeric = le.fit_transform(y_train)\ny_test_numeric = le.transform(y_test)\n\nclass_labels_ordered = list(le.classes_) \nnum_classes = len(class_labels_ordered)\n\nprint(f\"Label Encoder 클래스: {class_labels_ordered} -> {list(range(num_classes))}\")\n\nxgb_model = xgb.XGBClassifier(\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    num_class=num_classes,\n    seed=RANDOM_STATE,\n    use_label_encoder=False,\n    verbosity=0\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'learning_rate': [0.05, 0.1],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train_numeric, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nxgb_model_py = grid_search.best_estimator_\n\nimportance_scores = xgb_model_py.feature_importances_\nxgb_importance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': importance_scores\n})\n\nmax_importance = xgb_importance_df['Importance'].max()\nxgb_importance_df['Importance'] = (xgb_importance_df['Importance'] / max_importance) * 100\nxgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nXGBoost 변수 중요도\")\nprint(xgb_importance_df)\n\nsns.barplot(x='Importance', y='Feature', data=xgb_importance_df)\nplt.title('Feature Importances (XGBoost)', fontsize=10)\nplt.xlabel('Importance', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('xg_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_proba_xgb = xgb_model_py.predict_proba(X_test)\ny_pred_xgb_numeric = np.argmax(y_pred_proba_xgb, axis=1)\ny_pred_xgb = le.inverse_transform(y_pred_xgb_numeric)\ny_pred_xgb_cat = pd.Categorical(y_pred_xgb, categories=class_labels_ordered, ordered=False)\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb_cat, sample_weight=weights_test, labels=class_labels_ordered)\nprint(\"\\n=== XGBoost 모델 혼동 행렬 ===\\n\")\ncm_df_xgb = pd.DataFrame(cm_xgb, index=[f\"Actual: {cat}\" for cat in class_labels_ordered], columns=[f\"Predicted: {cat}\" for cat in class_labels_ordered])\nprint(cm_df_xgb.round(2))\n\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb_cat, sample_weight=weights_test)\nprecision_xgb_sk = precision_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nrecall_xgb_sk = recall_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nf1_score_xgb_sk = f1_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\n\nprint(\"\\n=== XGBoost 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_xgb:.4f}\")\nprint(\"\\nXGBoost 모델 범주별 정밀도:\")\nprint(pd.Series(precision_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 재현율:\")\nprint(pd.Series(recall_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_xgb_sk, index=class_labels_ordered).round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLabel Encoder 클래스: ['explorative', 'stable'] -> [0, 1]\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n최적의 하이퍼파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n최적 교차 검증 점수: nan\n\nXGBoost 변수 중요도\n                        Feature  Importance\n40         parent_attachment_w5  100.000000\n38                neg_esteem_w4   89.684937\n36           self_confidence_w4   87.710533\n33         parent_monitoring_w4   84.155891\n30         parent_attachment_w4   83.655487\n35             friend_stress_w4   83.593765\n23         parent_monitoring_w3   80.530685\n15             friend_stress_w2   80.180794\n12             parent_stress_w2   80.104950\n6            self_confidence_w1   78.258446\n17  higher_school_dependence_w2   77.525246\n21            deviant_esteem_w3   75.736404\n22             parent_stress_w3   74.421280\n4              desire_stress_w1   74.244553\n43         parent_monitoring_w5   74.036301\n14             desire_stress_w2   73.580856\n7   higher_school_dependence_w1   73.197128\n25             friend_stress_w3   72.865379\n47  higher_school_dependence_w5   72.309631\n31            deviant_esteem_w4   71.178528\n39           academic_stress_w4   69.950974\n48                neg_esteem_w5   69.804626\n32             parent_stress_w4   69.397591\n3          parent_monitoring_w1   68.932785\n46           self_confidence_w5   68.488029\n45             friend_stress_w5   68.380692\n26           self_confidence_w3   67.708191\n27  higher_school_dependence_w3   66.273537\n19           academic_stress_w2   65.726303\n41            deviant_esteem_w5   65.552612\n20         parent_attachment_w3   65.515610\n11            deviant_esteem_w2   65.489426\n9            academic_stress_w1   64.981789\n34             desire_stress_w4   63.373531\n49           academic_stress_w5   63.285591\n42             parent_stress_w5   62.527847\n13         parent_monitoring_w2   62.383938\n24             desire_stress_w3   62.092834\n18                neg_esteem_w2   61.555267\n29           academic_stress_w3   60.970982\n37  higher_school_dependence_w4   60.602821\n44             desire_stress_w5   59.860622\n5              friend_stress_w1   59.424625\n0          parent_attachment_w1   55.924370\n10         parent_attachment_w2   55.653919\n8                 neg_esteem_w1   52.370869\n28                neg_esteem_w3   50.224579\n2              parent_stress_w1   50.090385\n1             deviant_esteem_w1   49.659233\n16           self_confidence_w2   49.203815\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15_files/figure-html/cell-5-output-2.png){width=664 height=472}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== XGBoost 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                40510.53           41728.78\nActual: stable                     25198.53           65573.96\n\n=== XGBoost 모델 성능 지표 ===\n\n정확도: 0.6132\n\nXGBoost 모델 범주별 정밀도:\nexplorative    0.6165\nstable         0.6111\ndtype: float64\n\nXGBoost 모델 범주별 재현율:\nexplorative    0.4926\nstable         0.7224\ndtype: float64\n\nXGBoost 모델 범주별 F1-score:\nexplorative    0.5476\nstable         0.6621\ndtype: float64\n```\n:::\n:::\n\n\n## Logistic Regression\n\n::: {#0b9840c5 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\n\nbase_log_reg = LogisticRegression(\n    solver='lbfgs',\n    max_iter=5000,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nparam_grid = [\n    {\n        'penalty': ['l1'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['l2'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['lbfgs', 'liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['elasticnet'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['saga'],\n        'l1_ratio': [0.2, 0.5, 0.8],\n        'class_weight': ['balanced', None]\n    }\n]\n\ngrid_search = GridSearchCV(\n    estimator=base_log_reg,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nlog_reg_model_py = grid_search.best_estimator_\nclass_labels_logreg = log_reg_model_py.classes_\ncoef_series = pd.Series(log_reg_model_py.coef_[0], index=X_train.columns)\nsorted_coefs = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\ny_train_binary = (y_train == class_labels_logreg[1]).astype(int)\n\nlogit_model = sm.Logit(y_train_binary, X_train_sm)\nlogit_result = logit_model.fit(disp=0)\n\ncoef_summary = logit_result.summary2().tables[1]\ncoef_summary_df = pd.DataFrame(coef_summary)\ncoef_summary_sorted = coef_summary_df.sort_values('Coef.', key=abs, ascending=False)\nprint(\"\\n계수 및 p-value (절댓값이 큰 순서):\")\nprint(coef_summary_sorted[['Coef.', 'P>|z|']])\n\ny_pred_logistic_py = log_reg_model_py.predict(X_test)\n\ntest_categories_logreg = list(y_test.cat.categories) if hasattr(y_test, 'cat') else sorted(list(y_test.unique()))\ny_pred_logistic_cat = pd.Categorical(y_pred_logistic_py, categories=test_categories_logreg, ordered=False)\n\ncm_logistic = confusion_matrix(y_test, y_pred_logistic_cat, sample_weight=weights_test, labels=test_categories_logreg)\nprint(\"\\n=== 로지스틱 회귀 모델 혼동 행렬 ===\\n\")\ncm_df_logistic = pd.DataFrame(cm_logistic, index=[f\"Actual: {cat}\" for cat in test_categories_logreg], columns=[f\"Predicted: {cat}\" for cat in test_categories_logreg])\nprint(cm_df_logistic.round(2))\n\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic_cat, sample_weight=weights_test)\nprecision_logistic_sk = precision_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nrecall_logistic_sk = recall_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nf1_score_logistic_sk = f1_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\n\nprint(\"\\n=== 로지스틱 회귀 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_logistic:.4f}\")\nprint(\"\\n로지스틱 회귀 모델 범주별 정밀도:\")\nprint(pd.Series(precision_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 재현율:\")\nprint(pd.Series(recall_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_logistic_sk, index=test_categories_logreg).round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n최적의 하이퍼파라미터: {'C': 0.1, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n최적 교차 검증 점수: 0.6006\n\n계수 및 p-value (절댓값이 큰 순서):\n                                Coef.     P>|z|\ndesire_stress_w1             0.499608  0.000015\nself_confidence_w4          -0.332916  0.010294\nparent_stress_w3             0.271418  0.057369\nneg_esteem_w4                0.263964  0.031803\ndesire_stress_w2             0.255400  0.018759\nself_confidence_w3          -0.254064  0.030035\nfriend_stress_w4            -0.247249  0.024386\nconst                        0.245302  0.000002\ndeviant_esteem_w3           -0.237610  0.068700\nparent_monitoring_w3         0.211999  0.050412\nparent_attachment_w3         0.184476  0.274346\nparent_monitoring_w4         0.180603  0.106346\nhigher_school_dependence_w5 -0.164889  0.024659\nacademic_stress_w2          -0.156225  0.223868\nparent_attachment_w5         0.151586  0.345721\ndesire_stress_w3            -0.149660  0.181854\nself_confidence_w2          -0.140414  0.185298\ndeviant_esteem_w1           -0.138070  0.263940\nparent_stress_w4            -0.137391  0.383677\nfriend_stress_w1            -0.136979  0.134193\nacademic_stress_w5           0.118084  0.270881\nparent_monitoring_w1         0.113677  0.264741\nacademic_stress_w1          -0.112715  0.410824\ndesire_stress_w4             0.110917  0.295629\nself_confidence_w1           0.109299  0.356000\nparent_stress_w1            -0.104439  0.436255\nparent_stress_w2            -0.101390  0.460526\nparent_monitoring_w2        -0.098915  0.363357\ndesire_stress_w5             0.096226  0.359037\nneg_esteem_w1                0.096029  0.462255\nacademic_stress_w4           0.091028  0.451824\nhigher_school_dependence_w2 -0.086012  0.175999\nhigher_school_dependence_w1 -0.082377  0.182057\nparent_attachment_w4        -0.078615  0.648722\ndeviant_esteem_w4           -0.077995  0.525561\nneg_esteem_w5               -0.077245  0.532354\nparent_monitoring_w5         0.075278  0.495239\nfriend_stress_w5            -0.073577  0.475849\nfriend_stress_w2            -0.069986  0.466378\nhigher_school_dependence_w4 -0.059155  0.445485\nparent_stress_w5            -0.053443  0.710032\nneg_esteem_w3               -0.045219  0.687428\nparent_attachment_w1        -0.035101  0.822711\nhigher_school_dependence_w3  0.033887  0.623319\ndeviant_esteem_w5           -0.024602  0.843547\nneg_esteem_w2                0.021549  0.824973\nacademic_stress_w3          -0.017071  0.879205\nself_confidence_w5           0.015841  0.893445\nfriend_stress_w3            -0.010240  0.924892\nparent_attachment_w2         0.006187  0.967933\ndeviant_esteem_w2            0.002349  0.982333\n\n=== 로지스틱 회귀 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                42190.16           40049.15\nActual: stable                     26537.42           64235.07\n\n=== 로지스틱 회귀 모델 성능 지표 ===\n\n정확도: 0.6151\n\n로지스틱 회귀 모델 범주별 정밀도:\nexplorative    0.6139\nstable         0.6160\ndtype: float64\n\n로지스틱 회귀 모델 범주별 재현율:\nexplorative    0.5130\nstable         0.7076\ndtype: float64\n\n로지스틱 회귀 모델 범주별 F1-score:\nexplorative    0.5589\nstable         0.6586\ndtype: float64\n```\n:::\n:::\n\n\n## 모델 성능 비교\n\n::: {#12cedb01 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.cm as cm\n\nmodel_metrics_list = []\nclass_labels_ordered = list(y_test.cat.categories)\n\ndef compile_metrics(model_name, accuracy, precision_arr, recall_arr, f1_arr):\n    metrics = {'Model': model_name, 'Accuracy': accuracy}\n    for i, label in enumerate(class_labels_ordered):\n        metrics[f'Precision ({label})'] = precision_arr[i]\n        metrics[f'Recall ({label})'] = recall_arr[i]\n        metrics[f'F1-score ({label})'] = f1_arr[i]\n    return metrics\n\nmodel_metrics_list.append(compile_metrics(\n    \"Random Classifier\",\n    accuracy_random,\n    precision_random_sk,\n    recall_random_sk,\n    f1_score_random_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Random Forest\",\n    accuracy_rf,\n    precision_rf_sk,\n    recall_rf_sk,\n    f1_score_rf_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"XGBoost\",\n    accuracy_xgb,\n    precision_xgb_sk,\n    recall_xgb_sk,\n    f1_score_xgb_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Logistic Regression\",\n    accuracy_logistic,\n    precision_logistic_sk,\n    recall_logistic_sk,\n    f1_score_logistic_sk\n))\n\ncomparison_df = pd.DataFrame(model_metrics_list).set_index('Model')\nmodels = comparison_df.index.tolist()\nall_categories = ['Accuracy']\nfor label in class_labels_ordered:\n    all_categories.extend([\n        f'Precision ({label})',\n        f'Recall ({label})',\n        f'F1-score ({label})'\n    ])\nangles = np.linspace(0, 2*np.pi, len(all_categories), endpoint=False).tolist()\nangles += angles[:1]\nax = plt.subplot(111, polar=True)\ncolors = cm.tab10(np.linspace(0, 1, len(models)))\nfor i, model in enumerate(models):\n    values = comparison_df.loc[model, all_categories].values.flatten().tolist()\n    values += values[:1]\n    ax.plot(angles, values, 'o-', linewidth=2, color=colors[i], label=model, alpha=0.8)\n    ax.fill(angles, values, color=colors[i], alpha=0.1)\nax.set_xticks(angles[:-1])\nax.set_xticklabels(all_categories, fontsize=10)\n\nax.set_ylim(0, 1)\nax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\nax.grid(True, linestyle='-', alpha=0.3)\nplt.title('모델 성능 비교', size=15, y=1.1)\nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\nplt.savefig('model_met.png', dpi=300, bbox_inches='tight')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15_files/figure-html/cell-7-output-1.png){width=541 height=468}\n:::\n:::\n\n\n## ROC 커브\n\n::: {#0ce40a4b .cell execution_count=7}\n``` {.python .cell-code}\nrandom_pred_proba = np.zeros((len(y_test), len(class_labels_ordered)))\nfor i, cls in enumerate(class_labels_ordered):\n    random_pred_proba[:, i] = prop_values_ordered[i]\n\npred_probas = {\n    \"Random Classifier\": random_pred_proba,\n    \"Random Forest\": y_pred_proba_rf,\n    \"XGBoost\": y_pred_proba_xgb,\n    \"Logistic Regression\": log_reg_model_py.predict_proba(X_test)\n}\n\ncolors = {\n    \"Random Classifier\": \"grey\",\n    \"Random Forest\": \"forestgreen\",\n    \"XGBoost\": \"darkorange\",\n    \"Logistic Regression\": \"navy\"\n}\n\ny_test_numeric = y_test.cat.codes if hasattr(y_test, 'cat') else y_test\n\nfor model_name, proba in pred_probas.items():\n    if proba.shape[1] > 1:\n        y_score = proba[:, 1]\n    else:\n        y_score = proba.ravel()\n    fpr, tpr, _ = roc_curve(y_test_numeric, y_score, sample_weight=weights_test)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})', color=colors[model_name])\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC 커브', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.gcf().set_size_inches(7, 7)\nplt.tight_layout()\nplt.savefig('roc_curve_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15_files/figure-html/cell-8-output-1.png){width=661 height=661}\n:::\n:::\n\n\n",
    "supporting": [
      "15_files"
    ],
    "filters": [],
    "includes": {}
  }
}