[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Inboxes\n\n\n\n\n\n\n분류되지 않은 노트 (2)\n    \n\n\n\nTitle\nDate\nCategories\n\n\n\n\n다중 공산성\n2025-08-04\n머신러닝\n\n\n인간 관계론 - 데일 카네기\n2025-02-02\n독서, 인간 관계\n\n\n\n\n    \n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n현재 진행중인 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    ADP 실기 준비 - try 1\n                    on-going\n                \n                \n                    Started: 2025-06-16\n                    \n                     Ends: 2025-10-18\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석 python\n                \n                ADP 실기를 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    Toeic 준비\n                    on-going\n                \n                \n                    Started: 2025-08-22\n                    \n                     Ends: 2025-10-22\n                    \n                    Calculating...\n                \n                \n                    영어 자격증\n                \n                Toeic을 준비해 봅시다\n            \n        \n        \n        \n            \n                \n                    학부 3학년 2학기\n                    on-going\n                \n                \n                    Started: 2025-09-01\n                    \n                     Ends: 2025-12-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                3학년 2학기 학부 할 일 총 정리\n            \n        \n        \n        \n            \n                \n                    진로 준비\n                    on-going\n                \n                \n                    Started: 2025-08-27\n                    \n                     Ends: 2027-02-22\n                    \n                    Calculating...\n                \n                \n                    진로\n                \n                진로 준비\n            \n        \n        \n\n\n\n\n\n\n\n\nAreas\n\n\n\n\n\n관리 / 책임 영역\n\n\n\n\n    \n    \n        \n            Deep Learning\n        \n        \n        \n            42 Seoul\n        \n        \n        \n            Kaggle\n        \n        \n        \n            Machine Learning\n        \n        \n        \n            ROS\n        \n        \n\n\n\n\n\n\n\n\nResources\n\n\n\n\n\n진행 전인 프로젝트\n\n\n\n\n    \n    \n    \n\n\n\n관심 분야\n\n\n\n\n    \n    \n        \n            Blog\n        \n        \n        \n            Hadoop\n        \n        \n        \n            선형대수\n        \n        \n        \n            인생\n        \n        \n        \n            Helm\n        \n        \n        \n            Terraform\n        \n        \n        \n            Problem Solving\n        \n        \n        \n            AirFlow\n        \n        \n        \n            Smart Contract\n        \n        \n        \n            금융\n        \n        \n        \n            Quantum Programming\n        \n        \n\n\n\n\n\n\n\n\nArchives\n\n\n\n\n\n완료된 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    토익 스피킹 준비\n                    completed\n                \n                \n                    Started: 2025-07-19\n                    \n                     Ended: 2025-07-26\n                    \n                    Calculating...\n                \n                \n                    영어 자격증\n                \n                토익 스피킹을 준비해 봅시다\n            \n        \n        \n        \n            \n                \n                    학부 3학년 1학기\n                    completed\n                \n                \n                    Started: 2024-12-21\n                    \n                     Ended: 2025-06-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                3학년 1학기 학부 할 일 총 정리\n            \n        \n        \n        \n            \n                \n                    ADP 필기 준비\n                    completed\n                \n                \n                    Started: 2025-02-02\n                    \n                     Ended: 2025-02-22\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석\n                \n                과연 2번째 도전은 성공할 것인가\n            \n        \n        \n        \n            \n                \n                    2학년 2학기 학부 정리\n                    completed\n                \n                \n                    Started: 2024-09-02\n                    \n                     Ended: 2024-12-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                2학년 2학기 학부 개념 정리\n            \n        \n        \n        \n            \n                \n                    AWS SAA 준비\n                    completed\n                \n                \n                    Started: 2024-04-15\n                    \n                     Ended: 2024-05-22\n                    \n                    Calculating...\n                \n                \n                    자격증 cloud\n                \n                AWS SAA를 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    TOFEL 준비\n                    failed\n                \n                \n                    Started: None\n                    \n                     Ended: None\n                    \n                    Calculating...\n                \n                \n                    English\n                \n                준비해 봅시다\n            \n        \n        \n        \n            \n                \n                    OPIc 준비\n                    failed\n                \n                \n                    Started: 2025-06-16\n                    \n                     Ended: 2025-07-07\n                    \n                    Calculating...\n                \n                \n                    영어 자격증\n                \n                OPIc을 준비해 봅시다\n            \n        \n        \n\n\n\n보관중인 자료\n\n\n\n\n    \n    \n        \n            vault\n        \n        \n        \n            k8s"
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/1.html",
    "href": "posts/01_projects/bs_3_1/notes/others/1.html",
    "title": "자기 소개서",
    "section": "",
    "text": "자기소개 및 가치관 (500자 이내)\n\n저는 데이터 분석과 IT 인프라 설계 분야에 깊은 관심을 가지고 있는 산업공학과 학생입니다. 산업공학을 전공하며 시스템 최적화와 데이터 기반 의사결정에 대한 이론을 배우며 데이터 분석 및 IT 인프라 설계 분야에 관심을 가지게 되었고, 이를 실무에 적용할 수 있는 지식을 학습하고자 42서울 교육기관에서 2년 동안 IT 관련 학습을 진행했습니다. 또한 이 기간 동안 AWS와 ADsP(Advanced Data Analytics Semi-Professional) 자격증을 취득하며 클라우드 컴퓨팅과 데이터 분석에 대한 기초 역량을 쌓았습니다.\n저는 효율적이고 신뢰할 수 있는 시스템을 구축하는 것을 가장 중요한 가치로 삼고 있습니다. 이러한 시스템은 데이터 손실과 보안 위협을 방지할 뿐만 아니라, 장기적인 성장의 토대가 되기 때문입니다. 현재는 데이터와 블록체인 기술을 활용하여 복잡한 문제를 단순화하고, 효율적인 해결책을 찾는 데 큰 관심을 가지고 있습니다. 앞으로도 지속적인 학습과 경험을 통해 해당 분야에서 전문성을 키워가고자 합니다.\n\n졸업 후 IT 및 블록체인 분야에 관련해서 이루고자 하는 꿈과 선정 사유 (500자 이내)\n\n저는 데이터 분석, IT 인프라, 블록체인 기술을 융합하여 현실의 복잡한 문제들을 해결하고 혁신적인 가치를 창출하는 데 기여하고 싶습니다. 전공 수업과 프로젝트를 통해 데이터가 지닌 잠재력을 배워가면서, 동시에 데이터의 신뢰성과 보안이라는 중요한 과제에 대해서도 깊이 고민하게 되었습니다. 특히 42서울에서의 학습 경험을 통해, 안전하고 효율적인 데이터 활용을 위해서는 IT 인프라와 블록체인 기술의 역할이 매우 중요하다는 것을 깨달았습니다. 이러한 경험들을 바탕으로 IT 인프라와 블록체인 기술에 더욱 관심을 가지게 되었고, 관련 기술 서적과 온라인 자료를 통해 꾸준히 학습하며 이해의 폭을 넓혀가고 있습니다. 앞으로도 끊임없이 배우고 성장하여 데이터의 가치를 안전하게 실현할 수 있는 시스템을 만드는 데 기여하고 싶습니다.\n\n목표 달성을 위한 그간의 성과 및 계획 (500자 이내)\n\n저의 주요 성과로는 42서울에서의 프로젝트 경험과 AWS, ADsP 자격증 취득을 들 수 있습니다. 42서울에서 진행한 Solidity 기반 이더리움 스마트 컨트랙트 설계 및 배포 프로젝트를 통해 블록체인의 핵심 원리와 실제 활용 방안을 학습했습니다. 또한 Vagrant, Kubernetes(K8s), ArgoCD, GitLab helm 배포 프로젝트를 수행하며 온프레미스 환경에서의 인프라 설계와 개발 환경 관리 역량을 키웠고, 이를 통해 클라우드와 온프레미스 환경의 IT 인프라 운영에 대한 실질적인 이해도를 높일 수 있었습니다. 향후 계획으로는 학부 과정에 충실히 임하면서 데이터사이언스 대학원 진학을 위한 준비를 체계적으로 진행하고자 합니다. 대학원에서는 빅데이터 처리, 머신러닝, 딥러닝 등 데이터 분석의 핵심 기술을 심도 있게 학습하고자 합니다. 이와 병행하여 온라인 강좌 수강과 실전 프로젝트 수행을 통해 IT 인프라 및 블록체인 분야의 역량을 지속적으로 강화하고, 각종 공모전 참여를 통해 실력을 검증받고자 합니다. 궁극적으로는 이러한 기술들을 융합하여 데이터의 신뢰성과 보안을 보장하고, 효율적인 시스템을 설계하는 전문가로 성장하고 싶습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "자기 소개서"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "title": "봉사",
    "section": "봉사 계획서 2",
    "text": "봉사 계획서 2",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-실습일지",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-실습일지",
    "title": "봉사",
    "section": "봉사 실습일지",
    "text": "봉사 실습일지",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "href": "posts/01_projects/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "title": "봉사",
    "section": "봉사 결과보고서",
    "text": "봉사 결과보고서",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#setup과-생산주기",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#setup과-생산주기",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "setup과 생산주기",
    "text": "setup과 생산주기\n\nsetup: 기계를 준비하는데 필요한 것\n\n정확히 하나의 제품을 만드는 경우에도 setup이 필요함\n생산하는 양에 관계없이 setup 시간이 일정함\nsequence dependent setup: 순서에 따라 setup 시간이 달라짐\n\n생산주기(production cycle): setup + 생산의 과정을 반복\n\nsetup은 아무것도 못하고 시간을 버림",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#배치-생산과정",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#배치-생산과정",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "배치 생산과정",
    "text": "배치 생산과정\n\nbatch1: 부품 집합을 흐름 단위로 사용\n생산 주기: batch size만큼 생산하는 주기\n처리능력: \\(\\frac{batch size}{setup time + (batch size * processing time per unit)}\\)\n\nbatch size가 무한히 커질수록 \\(\\frac{1}{p}\\)로 수렴\nsetuptime이 0이여도 \\(\\frac{1}{p}\\)\n\n\n\n\nbatch는 클 수록 좋은가?\n\nbatch size가 커질수록 처리능력이 증가하지만 재고가 많아짐\n→ 처리능력 제약적 상황에서 bottleneck의 batch size를 늘리고, 수요 제약적 상황에서 non-bottleneck의 batch size를 줄이는게 좋음\n→ \\(\\frac{B}{S + Bp} = R → B = \\frac{SR}{1 - Rp}\\)\nR보다 크면 쓸데없이 제고가 쌓이고, 작으면 capacity가 낮아짐\n\nS가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨\np가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "경제적 주문량 모형",
    "text": "경제적 주문량 모형\n\n외부 공급자에게 부품을 주문하여 생산 및 배송이 이루어지는 경우\n단위시간당 발생하는 비용이 적을수록 좋다\n\n\n\n\n재고량 패턴(재고가 0이 되었을 때 정확히 도착하도록 주문할 수 있다고 가정)\n\n\n\nQ: 한 번에 주문하는 양\nR: 수요(기울기)\n주문 주기: \\(\\frac{Q}{R}\\)\n평균 재고량: \\(\\frac{Q}{2}\\)\n\n\n구매비용(purchase cost / variable cost): 단위 시간 당 구매비용은 Q에 영향을 받지 않음\n단위 재고 비용(h)\n\n단위 시간 당 발생하는 재고 비용: \\(h\\frac{Q}{2}\\)\n\n셋업(주문) 비용 (Fixed cost) (k): 주문량과 무관\n\n단위 시간 당 발생하는 셋업 비용: \\(\\frac{k}{\\frac{Q}{R}}\\)\n\n\n\n목적 함수: \\(C(Q) = \\frac{KR}{Q} + \\frac{hQ}{2}\\)\n경제적 주문량(EOQ): \\(Q^* = \\sqrt{\\frac{2KR}{h}}\\)\n\nK: 주문비용\nR: 수요량\nh: 단위 재고비용\n\nEOQ만큼 주문할 때 단위 시간당 비용\n\n\\(C(Q^*) = \\sqrt{2KhR}\\)\n\n단위당 비용 = \\(\\frac{C(Q^*)}{R} = \\sqrt{\\frac{2Kh}{R}}\\)\n수요가 증가함에 따라 EOQ는 늘어나는데 단위 당 비용은 감소\n\\(\\frac{C(Q)}{C(Q^*)} = \\frac{1}{2}(\\frac{Q^*}{Q} + \\frac{Q}{Q^*})\\)\n\\(\\frac{1}{주문 주기} ≠ 재고 회전율\\)\n\n주문 주기는 Q개가 다 없어지는 시간\n회전율은 Q/2개의 재고가 다 없어지는 시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "buffer or suffer",
    "text": "buffer or suffer\n\nbuffer 제고가 없으면 처리능력이 떨어질 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/06.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/06.html#footnotes",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "각주",
    "text": "각주\n\n\nbatch 1개는 부품 집합 1 단위 의미↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "프로세스 흐름 분석",
    "text": "프로세스 흐름 분석\n\n간트 차트\n\n\n\n방사선조영실 작업 칸트 차트\n\n\n\n작업시간을 표현\n프로세스 상의 작업 순서와 소요시간, 상호관계를 볼 수 있음\n대기: 수요와 공급의 불일치, 작업들에 존재하는 불확실성으로 생기는 것\n\n\n\n프로세스 평가를 위한 3가지 요소\n\n흐름률(flow rate / throughput): 실제 흐름 단위가 프로세스에 진입, 떠나는 비율\n\n\\(\\frac{흐름단위 수}{단위 시간}\\)\n흐름률이 오르면 생산 능력이 오른다.\n유량: 특정한 시간동안 관찰하는 양\n매출 원가(들어온 가격 기준)를 흐름률로 바라볼 수 있다.\n\n흐름시간(flow time): 하나의 흐름단위가 프로세스상에 머무는 시간\n\n흐름시간이 줄어들면 수요-공급 사이의 시간도 줄어든다.\n\n재고(inventory): 프로세스 상에 존재하는 흐름단위 수\n\n매출 원가 기준\n가공중인 제품 WIP(work-in-process)도 재고에 포함\n저량(stock): 특정 시점에서 관찰하는 양\n\n\n\n\n\n\n\n\n\n\n\n프로세스\n\n\n\n흐름(flow): 작업이 진행되는 것을 tracking\n\n단위: 일반적으로 산출물의 단위로 정의\n\n위의 그림에서 흐름 단위는 상품 1개, 서비스 받은 고객 1명",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#리틀의-법칙",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#리틀의-법칙",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "리틀의 법칙",
    "text": "리틀의 법칙\n위의 세개와 수요 공급간의 관계가 있다.\n\n\n\n기울기는 흐름률\n\n\n\nI = R * T (항상 성립)1\n\nI: 평균 재고 (flow time 동안 들어온 input)\nR: 평균 흐름률\nT: 평균 흐름시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고 관리 메커니즘",
    "text": "재고 관리 메커니즘\n\n재고를 카운트 하는 방법\n\ninput이 여러개일 경우 단순히 흐름단위만으로 재고를 표현하기 어려울 수 있다.\n\n\nIn terms of $s: I. 원가 기준\nIn terms of days-of-supply(DOS, 공급일수): \\(\\frac{I}{R} = T\\)\nIn terms of inventory turns(재고 회전율): \\(\\frac{R}{I} = \\frac{1}{T}\\)\n\n\n\nTurns and DOS at Kohl’s and Walmart\n\n\n\n두 회사의 재무재표\n\n\n\n위의 사례에서 두 기업의 전략을 볼 수 있음.\nKohl’s: 회전률이 낮은 대신 마진을 높임\nWalmart: 마진이 낮은 대신 회전률을 높임\n공급 일수와 회전율은 반비례 관계에 있다.\n\n\n\n재고가 부담이 되는 이유\n\n이자비용\n유지비용\n\n재고가 구식으로 변함\n물리적으로 부식됨\n사라질 수 있음\n저장공간과 추가적인 간접비 유발\n품질의 저하에 따르는 추가적인 비용 존재\n\n제품 당 재고 비용: \\(\\frac{단위 시간 당 재고 유지 비용}{단위 시간 당 재고 회전율}\\)\n\n\n\n재고 유지의 다섯 가지 이유2\n\n재고 유지는 기업 입장에서 부담이 되지만 그럼에도 불구하고 유지하는 이유가 있다.\n\n\n수송중재고(pipeline): 프로세스에 존재하는 재고\n계절재고(seasonal): 공급 능력은 고정되어 있는데 수요는 변동하는 경우(예측 가능한 수요), 미리 만들어둠\n\n계절 재고 vs 주기 재고\n계절 재고 vs 안전 재고\n\n계절 재고: 수요가 예측 가능할 때\n안전 재고: 수요가 예측 불가능할 때\n\n\n주기재고(cycle): 한 번에 많이 사는게 싸다. 규모의 경제 이용한 비용 절감\n완충재고(buffer, decoupling): 프로세스상의 작업 사이의 지속적 공급을 가능하게 해줌. 단 제고가 계속 쌓이지 않게 line balancing(각 프로세스에서 진행하는 일의 양의 밸런스)을 해줘야 함\n안전재고(safety): 불확실성에 대비해 예측된 수요보다 더 많이 재고를 유지함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/02.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/02.html#footnotes",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "각주",
    "text": "각주\n\n\n시험문제에 더 복잡하게 낸다고 하시긴 함↩︎\n이것들의 차이와 의미하는 바, 사례를 보고 어떤걸 의미하는지 알아야 한다.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html",
    "title": "예측",
    "section": "",
    "text": "4페이지 집중 못함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#판단적-기법정성적-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#판단적-기법정성적-기법",
    "title": "예측",
    "section": "판단적 기법(정성적 기법)",
    "text": "판단적 기법(정성적 기법)\n\n전문가의 경험과 직관에 의존하여 예측하는 기법\n비정량적 / 주관적 데이터로 정량적인 예측치를 구함\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치\n추세분석:치전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법\n시장조사법\n전문가 의견 종합법\n사례유추법: 비슷한 제품이랑 비교\n델파이 기법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#시계열-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#시계열-기법",
    "title": "예측",
    "section": "시계열 기법",
    "text": "시계열 기법\n\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가까울 수록 가중치를 크게 부여\n지수평활법: 과거의 모든 데이터를 가중 평균\n\n지수평활계수(α): 최근의 값을 더 높은 가중치가 부여되도록 추정\n\\(\\hat{y_{t+1}} = αy_t+ (1-α)\\hat{y_t} = \\hat{y_t} + α(y_t - \\hat{y_t}\\)\n예측치와 관측치 중 어디에 중점을 둘 지에 따라서 α 결정\n== 오차를 어느정도 반영할지에 따라서 α 결정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#상관관계-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#상관관계-기법",
    "title": "예측",
    "section": "상관관계 기법",
    "text": "상관관계 기법\n\n선행 지수법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#계절성-수요예측",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#계절성-수요예측",
    "title": "예측",
    "section": "계절성 수요예측",
    "text": "계절성 수요예측\n\n추세: 시간의 흐름에 따라 일정한 방향성을 가지고 수요가 변화\n계절성\n\n\n\n\n\n단순 변동, 추세, 계절성이 모두 있는 경우\n\n\n\n승법적 모델: (일정수준 + 추세) * 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 나눈다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 곱해서 예측치를 구한다.\n\n합산적 모델: (일정수준 + 추세) + 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 뺀다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 더해서 예측치를 구한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "title": "예측",
    "section": "예측의 품질 평가",
    "text": "예측의 품질 평가\n\n비편향 예측: 평균 예측오차가 0\n\n\nMSE: 편차가 클 수록 불이익\nMAE: 각 편차가 동일하게 나쁜 것으로 간주\nMAPE\n\n\n질문: 예측치가 음수일 수 있나?",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/13.html#일정계획",
    "href": "posts/01_projects/bs_3_1/notes/product/13.html#일정계획",
    "title": "일정 계획",
    "section": "일정계획",
    "text": "일정계획",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#intro",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#intro",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Intro",
    "text": "Intro\n\n지금까지는 변동성을 고려하지 않았지만 프로세스 성과 평가에 중요한 영향을 미친다.\n변동성이 대기시간에 미치는 영향을 살펴본다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#example",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#example",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Example",
    "text": "Example\n\n\n변동성\n\n불규칙한 도착 간격\n서비스 시간의 변동성\n영향: 재고, 대기시간, 산출 손실\n\nIU가 100 이하여도 대기가 발생할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-원인",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#변동성의-원인",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "변동성의 원인",
    "text": "변동성의 원인\n\n흐름단위의 input (\\(CV_a\\))\n\nrandom arrival\nincoming quality\nproduct mix\n\nprocessing time의 변동성 (\\(CV_p\\))\n\n그냥 내재적인 변동성\n숙련도 (일을 못해서 오래걸림)\n품질 (재작업)\n\n자원의 무작위적 가용성\n\n자원 고장\n작업자 출근 안함\nsetup time\n\n복수의 흐름단위가 무작위적 경로결정\n\n경로의 변동성\n\n\n\n변동성의 측정: \\(\\frac{표준편차}{평균}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (단일 자원)",
    "text": "대기시간 예측 (단일 자원)\n\n가정\n\n내재활용률은 100% 미만\n\nif D &gt; C, 대기 - 처리능력 부족 (+ 변동성)\nif D &lt; C, 대기 - 변동성\n\n안정적 도착: 평균 고객 수가 시점에 의존하지 않고, 길이에만 의존함\n\n만약 프로세스가 안정적이지 않다면 더 짧은 시간간격으로 나누어 접근\n\n지수분포를 따르는 도착간격\n\n\\(CV_a = 1\\)\n비기억 특성\n\n\n\n\n변수\n\na: 평균 도착 간격 (줄 기준)\np: 평균 서비스 시간\n\\(CV_a\\): 도착간격의 변동계수\n\\(CV_p\\): 서비스 시간의 변동계수\n\\(T_q\\): 대기 시간\n\\(I_q\\): 대기열의 재고\n\\(I_p\\): 서비스 중 재고\n\n\n\n\n공식\n\ncapacity: \\(\\frac{1}{p}\\)\nflow rate = demand(수요 제약적 상황을 가정하니까): \\(\\frac{1}{a}\\)\nutilization: \\(\\frac{p}{a}\\)\nT: \\(T_q\\) + p\n\\(I_p\\): (1 - u) * 0 + u * 1 = u\nI = \\(I_q\\) + \\(I_p\\) = \\(I_q\\) + utilization\n\\(T_q = p * \\frac{u}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n도착 간격이 지수분포를 따르지 않는 경우 근사치만을 제공\n\n\\(I_q = \\frac{1}{a} * T_q = \\frac{T_q}{a}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (복수 자원)",
    "text": "대기시간 예측 (복수 자원)\n\ncapacity: \\(\\frac{m}{p}\\)\nflow rate: \\(\\frac{p}{am}\\)\n\\(I = I_q + I_p = I_q + mu\\)\n\\(T_q = \\frac{p}{m} * \\frac{u^{\\sqrt{2(m+1)} - 1}}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n근사치만을 제공\n\nservice level: \\(P(T_q ≤ TWT)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/07.html#풀링",
    "href": "posts/01_projects/bs_3_1/notes/product/07.html#풀링",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "풀링",
    "text": "풀링\n\n대기할 수 있는 방법은 여러가지가 있다\n대기시간을 줄일 수 있는 방법에 사람을 많이 뽑는것 외에 다른 고려 요소\n\n\n\n풀링의 효과\n\n풀링되는 시스템이 서로 완전히 독립\n다양한 input을 처리할 수 있어야 함.\n→ 대기시간, 대기 인원 감소",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "title": "프로젝트 관리",
    "section": "프로젝트 vs 프로세스",
    "text": "프로젝트 vs 프로세스\n\n프로젝트: 일회성, 하나의 흐름단위, 제한된 시간 내에 완료해야 됨\n\n프로젝트 관리: 프로젝트를 구성하는 여러 활동들의 배치와 계획\n\n프로세스: 지속적, 여러 흐름 단위",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "title": "프로젝트 관리",
    "section": "주경로 기법(Critical Path Method, CPM)",
    "text": "주경로 기법(Critical Path Method, CPM)\n\n프로젝트 완료 시간 계산\n종속성 path 중 가장 긴 path가 주경로. 즉 프로젝트를 완료할 수 있는 가장 짧은 시간.\n\n이 path의 활동이 지연되면 프로젝트 전체가 지연됨\n프로세스 흐름도에서 개별작업의 처리능력이 중요했던것과 달리 프로젝트 종료 시간이 중요.\n\n\n\n주경로 찾기\n\nEST: 각 활동이 가장 빨리 시작할 수 있는 시간. max(선행 activity의 ECT)\nECT: 각 활동이 가장 빨리 끝날 수 있는 시간. EST + cost\nLCT: 각 활동이 가장 늦게 끝날 수 있는 시간. min(후행 activity의 LST)\nLST: 각 활동이 가장 늦게 시작할 수 있는 시간. LCT - cost\nSlack: LST - EST. 0이면 주 활동",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "href": "posts/01_projects/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "title": "프로젝트 관리",
    "section": "불확실성 하에서의 프로젝트 관리",
    "text": "불확실성 하에서의 프로젝트 관리\n\nCPM(Critical Path Method): 소요 시간이 정확하게 알려져 있다고 가정\nPERT(Program Evaluation and Review Technique): 소요 시간이 불확실하다고 가정\n\n\n가정\n\n주 경로는 각 활동의 평균 소요시간으로 구함\n주 경로는 바뀌지 않음\n각 활동의 소요시간은 독립적임\n각 활동의 소요시간이 정규분포를 따름\n\n\n\n특징\n\n\\(d_1, d_2, ..., d_n\\)이 확률변수이면 프로젝트 완성시간 (\\(X\\))도 확률변수\n\\(d_1, d_2, ..., d_n\\)이 정규분포를 따른다면 \\(X\\)도 정규분포를 따름",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#example-food-truck",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#example-food-truck",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: Food Truck",
    "text": "example: Food Truck\n\n변동(동일한 확률 가정)\n\n수요\n공급할 수 있는 양\n\n수요와 공급이 동시에 발생하지 않는 경우로 인해 평균 흐름률이 실제랑 다름.\n\n변동성이 흐름률에 영향을 미침\nbuffer가 있으면 흐름률 높일 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#example-병원-외상-센터",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#example-병원-외상-센터",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: 병원 외상 센터",
    "text": "example: 병원 외상 센터\n\n\n대기해야 할 상황이 있으면 다른 병원으로 이동\n\ndiversion 상태, loss(service를 못 받음)\n\n\n\nDiversion 상태 확률\n\nD &lt; C 가정하지 않음\n도착 간격은 지수분포 가정 (processing time 분포는 가정 안함)\n대기하지 않고 바로 이탈한다고 가정\n\\(P_m\\): 내재활용률과 자원의 수에 의해 결정됨\n\\(r = um = \\frac{p}{a}\\), 해야하는 일의 양을 의미\n\n단위: Erlang\n\n\n\n\n\nErlang Loss Table\n\n\n\n들어온 인원: \\(\\frac{1}{a}(1 - P_m(r))\\)\n안 들어온 인원: \\(\\frac{1}{a}P_m(r)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#erlang-loss-table",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "Erlang Loss Table",
    "text": "Erlang Loss Table\n\n\n\n얼랑 솔실 공식1",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#buffer의-역할",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#buffer의-역할",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer의 역할",
    "text": "buffer의 역할\n\n\n\n변동성으로 인해 capacity가 낮아지는 이유\n\n\n\n\n변동성이 없다면 cycle time은 1/capacity\n변동성이 있다면 cycle time은 늘어남. (시뮬레이션으로 계산)\n버퍼가 있으면 용량이 커질수록 1/capacity로 점점 줄어듦.\ncell layout을 사용하면 cycle time을 제일 많이 줄일 수 있음.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#footnotes",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "각주",
    "text": "각주\n\n\ndiversion 확률, 꽉 차있을 확률, 도착한 환자가 서비스 받을 확률, 다른 병원으로 갈 확률 시험에 나온다.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/00.html",
    "href": "posts/01_projects/bs_3_1/notes/computer/00.html",
    "title": "intro",
    "section": "",
    "text": "개별 과제는 없음\n8주차, 9주차 안나옴\n11주차 시험. LMS로 봄\n14주차 발표는 유튜브로 찍어서\n15주차 학교 안나옴\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "쌍대이론의 본질",
    "text": "쌍대이론의 본질\n\n모든 선형계획 문제는 쌍대문제를 가진다:\n\n원문제(Primal): 예를 들어 이익 최대화.\n쌍대문제(Dual): 자원비용 최소화.\n\n\n\n\n\n원 문제와 쌍대 문제의 관계\n\n\n\n원-쌍대 관계의 성질\n\n원문제의 최적해가 존재하면 쌍대문제의 최적해도 존재하며, 두 목적함수값은 같다.\n원문제의 해로부터 쌍대해를 읽을 수 있고, 그 역도 성립한다.\n쌍대해는 자원의 경제적 가치(잠재가격, shadow price)를 의미한다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "원-쌍대 관계와 상보기저해",
    "text": "원-쌍대 관계와 상보기저해\n\n상보해(Complementary Solutions)\n\n원문제의 기저해와 쌍대문제의 기저해는 서로 직접적으로 대응한다.\n최적해에서는 원문제와 쌍대문제의 목적함수값이 같다.\n\n\n\n상보여유성\n\n원문제의 기저변수가 0이 아니면, 대응 쌍대변수는 0이고, 그 반대도 성립한다.\n이 속성은 심플렉스 방법의 반복과정에서 두 문제의 해가 어떻게 연동되는지 설명한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "다른 원문제 형태의 쌍대문제",
    "text": "다른 원문제 형태의 쌍대문제\n\n비표준형(등식제약식, 변수의 음수 허용 등)에서도 쌍대문제는 항상 존재\n\n등식제약식은 쌍대에서 해당 쌍대변수의 부호제약을 제거(음수 허용)한다.\n변수의 음수 허용은 쌍대에서 등식제약식으로 나타난다.\n\n\n\nSOB(Sensible-Odd-Bizarre) 법칙\n\n원문제의 제약식 및 변수의 형태(≤, =, ≥, 비음, 무제약 등)에 따라 쌍대문제의 대응 형태를 쉽게 결정하는 규칙.\n대칭성: 쌍대문제의 쌍대는 원문제이므로, 두 문제의 관계는 완전히 대칭적이다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "민감도 분석",
    "text": "민감도 분석",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "href": "posts/01_projects/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "6.7 민감도 분석 적용 – 요약 및 주요 내용",
    "text": "6.7 민감도 분석 적용 – 요약 및 주요 내용\n6.7절 “민감도 분석 적용”은 선형계획(Linear Programming) 문제에서 민감도 분석(Sensitivity Analysis)을 실제로 어떻게 적용하는지, 그리고 다양한 매개변수 변화가 최적해에 어떤 영향을 미치는지 구체적으로 설명하는 부분입니다.\n\n주요 내용 요약\n\n민감도 분석의 출발점\n민감도 분석은 보통 자원(b₁, b₂, …, bₘ)의 공급량 변화가 해에 미치는 영향을 분석하는 것으로 시작합니다. 이는 실제 모델에서 자원의 양을 조정할 수 있는 융통성이 크기 때문입니다.\n우변(b) 변화의 영향\n자원(b)의 값이 변하면, 최종 심플렉스 표의 우변만 바뀌고 나머지(행 0의 비기저변수 계수 등)는 변하지 않을 수 있습니다. 이때는 우변만 수정해서 해가 여전히 가능(feasible)한지(기저변수 값이 모두 음이 아닌지) 확인하면 됩니다. 만약 불가능해지면 쌍대심플렉스법 등으로 재최적화가 필요합니다.\n증분 분석\n자원의 값이 변화할 때, 변화분만큼의 영향(증분)을 계산해서 새로운 해와 목적함수 값을 빠르게 구할 수 있습니다.\n허용범위(Allowable Range)\n각 자원(b)의 변화가 해의 가능성과 최적성을 유지할 수 있는 범위를 계산합니다. 이 범위 내에서는 잠재가격(dual price, shadow price)이 유효하게 적용됩니다.\n동시 변화와 100% 규칙\n여러 자원의 값이 동시에 변할 때, 각 변화가 허용범위 내에서 차지하는 비율의 합이 100%를 넘지 않으면 잠재가격을 이용한 해석이 유효합니다.\n목적함수 계수 변화\n비기저변수나 기저변수의 목적함수 계수(c)가 변할 때 해가 어떻게 변하는지, 허용범위를 어떻게 계산하는지 설명합니다.\n새로운 제약식 추가\n모델에 새로운 제약식이 추가되면, 기존 최적해가 여전히 가능해인지 확인하고, 아니라면 심플렉스 표에 새로운 행을 추가해 재최적화를 진행합니다.\n파라메트릭 분석\n하나 또는 여러 매개변수를 연속적으로 변화시키면서 최적해가 어떻게 달라지는지 체계적으로 분석합니다.\n\n\n\n예시: Wyndor Glass Co. 모델\n\nb₂(자원 2의 공급량)가 12에서 24로 증가하면, 기저해가 더 이상 가능하지 않게 되고, 쌍대심플렉스법을 통해 새로운 최적해를 구해야 함을 보여줍니다.\n허용범위 내에서만 자원의 변화에 대해 잠재가격이 유효하며, 이를 벗어나면 해가 바뀌고 잠재가격도 달라집니다.\n여러 자원이 동시에 변할 때 100% 규칙을 적용해, 변화의 합이 100%를 넘지 않으면 기존 해석이 유효함을 설명합니다.\n\n\n\n실무적 의의\n\n실제 기업(예: Pacific Lumber Company)의 대규모 산림관리 최적화 문제에 민감도 분석이 어떻게 적용되어, 불확실성 하에서 더 나은 의사결정과 수익 증대에 기여했는지 사례로 제시합니다.\n\n\n요약:\n6.7절은 선형계획의 해가 자원, 목적함수 계수, 제약식 등 모델의 매개변수 변화에 얼마나 민감한지, 그리고 이런 변화가 있을 때 해를 신속하게 갱신하거나 재최적화하는 절차를 구체적으로 다룹니다. 이를 통해 실제 의사결정에서 불확실성을 관리하고, 최적화 모델의 실용성을 높일 수 있음을 보여줍니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/11.html",
    "href": "posts/01_projects/bs_3_1/notes/OR/11.html",
    "title": "수송문제와 할당 문제들",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/03.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/03.html",
    "title": "random forest",
    "section": "",
    "text": "bootstrap sampling\nk개의 feature 중 랜덤하게 선택해서 노드를 추가\n각 tree 학습\n각 tree의 예측값을 투표하여 최종 예측값 결정\n\n분류: 다수결 투표\n회귀: 평균값\n\n\n\nvs bagging\n\nbagging: 데이터의 행에 무작위성을 부여\nrandom forest: 데이터의 행과 열에 무작위성을 부여\n\noob(out-of-bag) error: bootstrap sampling으로 학습에 사용되지 않은 데이터로 평가\n\n사실상 별도의 validation set을 필요로 하지 않음\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html",
    "title": "의사결정 트리",
    "section": "",
    "text": "불순도가 가장 낮은 leaves를 root에 두고, 그 다음 불순도가 낮은 leaf를 그 아래에 두는 방식으로 트리를 구성한다. leaf 노드의 과반수가 같은 클래스를 가지면 그 클래스를 리턴한다. overfit을 방비하기 위해 pruning을 하거나 max depth를 설정한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#과정",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#과정",
    "title": "의사결정 트리",
    "section": "과정",
    "text": "과정\n\n루트 노드에서 시작전체 데이터셋을 기준으로 시작하여 가장 좋은 분할속성(feature)을 선택\n분할 기준 평가각 속성에 대해 데이터를 분할했을 때의 분할 평가함수 적용\n\ngini index\nentropy\ninformation gain\ngain ratio\n\n최적의 분할 선택 평가된 기준 중 가장 순도가 높은 점수를 갖는 속성을 선택 (greedy)\n재귀적으로 하위 노드 분할분할된 하위 데이터에 대해 위의 과정을 반복\n종료 조건 만족 시 정지",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#algorithm",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#algorithm",
    "title": "의사결정 트리",
    "section": "Algorithm",
    "text": "Algorithm\n\nCART\n\n분할 기준\n\n분류: gini index\n회귀: MSE\n\n모든 분할에서 이진 트리로 분할\n사후 가지치기\n\n비용 복잡도 가지치기: \\(Total SSR + α(leaf size)\\)이 제일 작은 트리 선택\nα는 cross validation으로 결정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "의사결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#pattern-minig",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#pattern-minig",
    "title": "association rule mining",
    "section": "Pattern minig",
    "text": "Pattern minig\n\nBasic Concepts\n\npattern: dataset 안에서 함께 자주 발생하는 subsequences, substructures, set of items\n\n이 pattern은 인과관계를 의미하진 않는다.\n\nAssociation rule minig: 최소 지지도나 신뢰도를 넘는 모든 항목에 대해 pattern을 찾는다.\n\n\n\nApplications\n\nassociation rule, correlation, classification, clustering data mining의 기반이 될 수 있다.\n장바구니 분석\n연속 구매 분석\n\n\n\nTerminologies\n\n지지도(Support): 전체 거래 중 특정 항목 집합이 포함된 거래의 비율.\n신뢰도(Confidence): 항목 X를 포함하는 거래 중에서 항목 Y도 함께 포함하는 거래의 비율.\n빈발 패턴(frequent): 최소 지지도를 넘는 pattern\n\n빈발 항목 집합(frequent itemset): 단순한 묶음\n빈발 시퀀스\n\n\n\n\nclosed pattern\n\nx가 빈발이고, 지지도가 상위 집합들과 다른 집합 (지지도는 상위로 갈 수록 떨어짐)\n지지도 정보를 유지할 수 있다.\n\n신뢰도 계산할 때 사용할 수 있음\n\n\n\n\nmax patterns\n\nx가 빈발이고, 상위 집합들 모두가 빈발 집합이 아닌 집합\n지지도 정보는 유지되지 않음.\n\n신뢰도 계산할 때 사용할 수 없어서 사실 상 결과 요약 외의 용도는 없음\n\nDownward closure property: 어떤 itemset이 빈발하지 않으면, 그 모든 superset은 무조건 빈발하지 않는다. 대우도 성립\n\n교수님은 anti-monotone property로 설명하셨지만 이게 더 자주 사용되는 용어",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#association-rule",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#association-rule",
    "title": "association rule mining",
    "section": "Association Rule",
    "text": "Association Rule\n\nfind frequent itemsets\n\nApriori (breadth-first search)\nFP-Growth\nEclat (depth-first search)\n\ngenerate association rules\n\n모든 빈발 itemset I에 대해 모든 I의 subset s로 ‘s -&gt; (I - s)’ 규칙을 생성\n최소 신뢰도 조건을 만족하는 규칙만 남김",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html#algorithm",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html#algorithm",
    "title": "association rule mining",
    "section": "Algorithm",
    "text": "Algorithm\n\nApirori\n\nMonotone 성질을 이용하여 빈발하지 않는 집합은 후보에서 제거\n\n\nscan DB once to get 1-itemsets\n반복\n\nk개의 itemset에 대해 k+1-itemset의 후보를 생성\n\nself-join: k-itemset을 두 개 합쳐서 k+1-itemset을 생성\nprune: k+1-itemset을 생성할 때, k-itemset의 subset이 모두 빈발해야 k+1-itemset이 빈발할 수 있다.\n\nk+1-itemset 후보에 대해 DB를 scan하여 빈발한 itemset을 찾는다.\nk += 1\n빈발 itemset이 없으면 종료\n\n\n\nApriori의 단점: DB scan을 여러 번 해야함, 후보가 많아질 수 있음\n\n후보 수를 줄이는 방법: Hashing\n\n\n\n\nDHP (Direct Hashing and Pruning)\n\nHash값이 같은 itemset의 count를 합하고, minimum support를 넘는 itemset만 남김\nHash table을 매번 만드는 번거로움이 있지만, apriori보다 빠름\n반복\n\n빈발 항목 찾기, 후보 해시 테이블 생성\n\n데이터베이스를 scan하여 최소 지지도를 넘는 1-itemset 후보를 찾음\n동시에 조합 가능한 2-itemset을 만들어 mapping된 해시 테이블 bucket에 count += 1\n\n가지치기\n\n1-itemset 후보를 이용해 self-join, prune으로 2-itemset 후보 생성\n완성된 후보를 1단계에서 만든 hash table에 매핑해서 최소 지지도를 넘는 2-itemset bucket이 아닐 경우 배제",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/12.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/12.html",
    "title": "Dataminig 1차 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Dataminig 1차 발표 ppt"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/08.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/08.html",
    "title": "XGBoost",
    "section": "",
    "text": "결측치 자체 처리\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "XGBoost"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html",
    "title": "모르겠는 문제",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "모르겠는 문제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section",
    "title": "Homework",
    "section": "1",
    "text": "1\n\ntarget = ct.transform([['Mercedez Benz C class', 45000, 4]])\ntarget[:, 3:] = sc.transform(target[:, 3:])\ny_pred = regressor.predict(target)\nprint(y_pred)\n\n[36216.12684278]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-1",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-1",
    "title": "Homework",
    "section": "2",
    "text": "2\n\ntarget = ct.transform([['BMW X5', 86000, 7]])\ntarget[:, 3:] = sc.transform(target[:, 3:])\ny_pred = regressor.predict(target)\nprint(y_pred)\n\n[11152.07829935]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-2",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/09.html#section-2",
    "title": "Homework",
    "section": "3",
    "text": "3\n\nprint(regressor.score(X_test, y_test))\n\n0.8409834346492212",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/05.html#one-way-anova",
    "href": "posts/01_projects/bs_3_1/notes/statistics/05.html#one-way-anova",
    "title": "확률과 통계 R 실습 과제",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\na\n\nval1 &lt;- c(1.84, 2.67, 2.61, 2.95, 2.25, 2.35, 2.85, 2.58, 3.08, 2.25)\nval2 &lt;- c(1.79, 1.98, 2.01, 1.93, 2.24, 2.03, 2.22, 1.44, 2.24, 2.05)\nval3 &lt;- c(3.31, 2.24, 2.74, 2.36, 2.79, 2.20, 2.32, 2.82, 2.73, 2.78)\ndf &lt;- data.frame(\n  group = c(rep(\"일반 마우스\", 10), rep(\"트랙 패트\", 10), rep(\"버티컬 마우스\", 10)),\n  finger = c('왼중지', '왼검지', '오른약지', '왼약지', '왼엄지', '오른약지', '오른새끼', '오른엄지', '왼중지', '왼새끼', '오른중지', '오른엄지', '오른중지', '오른검지', '왼엄지', '왼검지', '오른새끼', '오른검지', '왼엄지', '오른중지', '왼검지', '왼새끼', '오른검지', '왼약지', '오른새끼', '오른엄지', '왼약지', '왼중지', '왼새끼', '오른약지'),\n  value = c(val1, val2, val3)\n)\ndf\n\n           group   finger value\n1    일반 마우스   왼중지  1.84\n2    일반 마우스   왼검지  2.67\n3    일반 마우스 오른약지  2.61\n4    일반 마우스   왼약지  2.95\n5    일반 마우스   왼엄지  2.25\n6    일반 마우스 오른약지  2.35\n7    일반 마우스 오른새끼  2.85\n8    일반 마우스 오른엄지  2.58\n9    일반 마우스   왼중지  3.08\n10   일반 마우스   왼새끼  2.25\n11     트랙 패트 오른중지  1.79\n12     트랙 패트 오른엄지  1.98\n13     트랙 패트 오른중지  2.01\n14     트랙 패트 오른검지  1.93\n15     트랙 패트   왼엄지  2.24\n16     트랙 패트   왼검지  2.03\n17     트랙 패트 오른새끼  2.22\n18     트랙 패트 오른검지  1.44\n19     트랙 패트   왼엄지  2.24\n20     트랙 패트 오른중지  2.05\n21 버티컬 마우스   왼검지  3.31\n22 버티컬 마우스   왼새끼  2.24\n23 버티컬 마우스 오른검지  2.74\n24 버티컬 마우스   왼약지  2.36\n25 버티컬 마우스 오른새끼  2.79\n26 버티컬 마우스 오른엄지  2.20\n27 버티컬 마우스   왼약지  2.32\n28 버티컬 마우스   왼중지  2.82\n29 버티컬 마우스   왼새끼  2.73\n30 버티컬 마우스 오른약지  2.78\n\n\n\n요인: 클릭하는 장치 종류\n수준: 일반 마우스, 트랙 패드, 버티컬 마우스\n반응치: 클릭 반응속도\n\n10개의 손가락에 대해 3개의 장치를 랜덤으로 사용하여 클릭 반응속도를 측정한다.\n\n\nb\n\ndf.means &lt;- tapply(df$value, INDEX=df$group, FUN=mean)\nboxplot(df$value ~ df$group, col=c(\"lightblue\", \"mistyrose\", \"lightcyan\"))\npoints(1:3, df.means, col=\"red\", pch=4, cex=1.5)\n\n\n\n\n\n\n\n\n\n\nc\n\ndf.sd &lt;- tapply(df$value, INDEX=df$group, FUN=sd)\ndf.sd.diff &lt;- max(df.sd) / min(df.sd)\nif (df.sd.diff &gt; 2) \n{\n  print(\"모집단의 분산이 동일하지 않음\")\n} else \n{\n  print(\"모집단의 분산이 동일하다고 가정\")\n}\n\n[1] \"모집단의 분산이 동일하다고 가정\"\n\n\n\n\nd\n\nlibrary(ggpubr)\n\nLoading required package: ggplot2\n\nggqqplot(val1)\n\n\n\n\n\n\n\nggqqplot(val2)\n\n\n\n\n\n\n\nggqqplot(val3)\n\n\n\n\n\n\n\n\n그림을 보니 반응치 값들이 정규분포를 따르는 것 같다!\n\n\ne\n\ndf.anova &lt;- aov(value ~ group, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  2.381  1.1907   11.17 0.000292 ***\nResiduals   27  2.878  0.1066                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nis_significant &lt;- summary(df.anova)[[1]][[\"Pr(&gt;F)\"]][1] &lt; 0.05\nif (is_significant)\n{\n  print(\"95%에서 처리별 모평균의 차이가 있음\")\n} else\n{\n  print(\"95%에서 처리별 모평균의 차이가 없음\")\n}\n\n[1] \"95%에서 처리별 모평균의 차이가 있음\"\n\n\n\n\nf\n\nif (is_significant)\n{\n  df.bon &lt;- pairwise.t.test(df$value, df$group, p.adjust.method = \"bonferroni\")\n  diff_pair &lt;- which(df.bon$p.value &lt; 0.05, arr.ind = TRUE)\n  if (length(diff_pair) &gt; 0)\n  {\n    cat(\"모평균의 차이가 있는 쌍\", \"\\n\")\n    for (i in 1:nrow(diff_pair)) \n    {\n      cat(colnames(df.bon$p.value)[diff_pair[i, \"col\"]], \"-\", rownames(df.bon$p.value)[diff_pair[i, \"row\"]], \"\\n\")\n    }\n  } else \n  {\n    print(\"모평균의 차이가 있는 처리 쌍 없음\")\n  }\n}\n\n모평균의 차이가 있는 쌍 \n버티컬 마우스 - 트랙 패트 \n일반 마우스 - 트랙 패트",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/05.html#anova-for-randomized-block-design",
    "href": "posts/01_projects/bs_3_1/notes/statistics/05.html#anova-for-randomized-block-design",
    "title": "확률과 통계 R 실습 과제",
    "section": "ANOVA for Randomized Block Design",
    "text": "ANOVA for Randomized Block Design\n\na\n\ndf &lt;- data.frame(\n  block = rep(c(\"아스팔트\", \"트랙\", \"모래밭\", \"잔디밭\"), each=3),\n  treatment = rep(c(\"런닝화\", \"축구화\", \"크록스\"), times=4),\n  value = c(6.1, 6.4, 6.5, 5.5, 5.9, 6.0, 6.9, 7.1, 7.5, 6.3, 6.9, 7.0)\n)\ndf\n\n      block treatment value\n1  아스팔트    런닝화   6.1\n2  아스팔트    축구화   6.4\n3  아스팔트    크록스   6.5\n4      트랙    런닝화   5.5\n5      트랙    축구화   5.9\n6      트랙    크록스   6.0\n7    모래밭    런닝화   6.9\n8    모래밭    축구화   7.1\n9    모래밭    크록스   7.5\n10   잔디밭    런닝화   6.3\n11   잔디밭    축구화   6.9\n12   잔디밭    크록스   7.0\n\n\n\n요인: 운동화 종류\n수준: 런닝화, 축구화, 크록스\nblock: 달리는 땅\n반응치: 50m 달리기 시간(초)\n\n\n\nb\n\ndf.anova.rbd &lt;- aov(value ~ treatment + block, data=df)\nsummary(df.anova.rbd)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    2 0.6317  0.3158   27.73  0.00093 ***\nblock        3 3.0492  1.0164   89.24 2.28e-05 ***\nResiduals    6 0.0683  0.0114                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n처리에 의한 변동이 통계적으로 유의미하다.\nblock에 의한 변동 역시 통계적으로 유의미하다.\n\n\nc\n\ndf.anova &lt;- aov(value~treatment, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ntreatment    2 0.6317  0.3158   0.912  0.436\nResiduals    9 3.1175  0.3464               \n\n\n처리에 의한 변동이 통계적으로 유의미하지 않다고 나온다.\nblock에 의한 변동이 통계적으로 유의미하기 때문에, block에 의한 변동을 제거하고 처리에 의한 변동을 분석해야 하기 때문이다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/05.html#simple-linear-regression",
    "href": "posts/01_projects/bs_3_1/notes/statistics/05.html#simple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\na\n\nx &lt;- runif(100, 0, 10)\nerror &lt;- rnorm(100, 0, 4)\ny &lt;- 3 * x - 2 + error\nmodel &lt;- lm(y ~ x)\nmodel\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -1.824        2.893  \n\n\n\n\nb\n\ncat(\"B1의 추정 기울기: \", coef(model)[2])\n\nB1의 추정 기울기:  2.892536\n\ntrust &lt;- confint(model, level=0.95)[2, ]\ncat(\"신뢰구간: \", trust, \"\\n\")\n\n신뢰구간:  2.624822 3.160251 \n\nif (3 &gt; trust[1] && 3 &lt; trust[2]) \n{\n  print(\"신뢰구간에 3 포함\")\n} else \n{\n  print(\"신뢰구간에 3 안 포함\")\n}\n\n[1] \"신뢰구간에 3 포함\"\n\n\n\n\nc\n\nplot(x, y, cex=0.7)\nabline(model, col = \"red\", lwd = 2)\nabline(-2, 3, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"실제 회귀선\", \"추정 회귀선\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\nd\n\ncnt &lt;- 0\nfor (i in 1:100) \n{\n  x &lt;- runif(100, 0, 10)\n  error &lt;- rnorm(100, 0, 4)\n  y &lt;- 3 * x - 2 + error\n  model &lt;- lm(y ~ x)\n  trust &lt;- confint(model, level=0.95)[2, ]\n  cnt &lt;- cnt + (3 &gt; trust[1] && 3 &lt; trust[2])\n}\ncnt\n\n[1] 96",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/05.html#multiple-linear-regression",
    "href": "posts/01_projects/bs_3_1/notes/statistics/05.html#multiple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\na\n\ndata(iris)\nmodel &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, data = iris)\nsummary(model)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88045 -0.20945  0.01426  0.17942  0.78125 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.04309    0.27058   3.855 0.000173 ***\nSepal.Length  0.60707    0.06217   9.765  &lt; 2e-16 ***\nPetal.Length -0.58603    0.06214  -9.431  &lt; 2e-16 ***\nPetal.Width   0.55803    0.12256   4.553  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3038 on 146 degrees of freedom\nMultiple R-squared:  0.524, Adjusted R-squared:  0.5142 \nF-statistic: 53.58 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nF 값의 p-value가 매우 작으므로, 모델이 통계적으로 유의미하다고 할 수 있다.\n각각의 독립변수에 대한 p-value 역시 모두 매우 작으므로, 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\n하지만 r-squared 값이 크지 않아서 모델이 종속변수의 변동을 잘 설명하지 못한다고 할 수 있다.\n\n\nb\n\nplot(model, 1)\n\n\n\n\n\n\n\n\n잔차가 U자 패턴을 보이므로, 선형성이 만족되지 않는 것 같다.\n\n\nc\n\nmodel2 &lt;- lm(Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), data = iris)\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + \n    Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82277 -0.16843 -0.00315  0.15300  0.77761 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.10783    1.37460  -2.261 0.025275 *  \nSepal.Length       2.32243    0.50141   4.632 8.08e-06 ***\nI(Sepal.Length^2) -0.15699    0.04264  -3.682 0.000327 ***\nPetal.Length      -0.89155    0.20300  -4.392 2.17e-05 ***\nI(Petal.Length^2)  0.06925    0.02336   2.965 0.003549 ** \nPetal.Width       -0.03666    0.38214  -0.096 0.923708    \nI(Petal.Width^2)   0.13095    0.10529   1.244 0.215648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2755 on 143 degrees of freedom\nMultiple R-squared:  0.6167,    Adjusted R-squared:  0.6006 \nF-statistic: 38.35 on 6 and 143 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nd\nSepal.Length, \\(\\text{Sepal.Length}^2\\), Petal.Length의 p-value가 모두 0.05보다 작으므로, 이 변수들이 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\nr-squared 값도 더 작아져서, c 모델이 b 모델보다 종속변수의 변동을 잘 설명한다고 할 수 있다.\n\n\ne\n\nplot(model2, 1)\n\n\n\n\n\n\n\n\nU자 패턴이 완만해진걸로 보인다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#단순선형회귀모형의-확장",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#단순선형회귀모형의-확장",
    "title": "Regression Analysis",
    "section": "단순선형회귀모형의 확장",
    "text": "단순선형회귀모형의 확장\n\n독립변수의 비선형 변환\n선형: 회귀계수에 대해 선형 → \\(x^2\\)도 선형",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#질문",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#질문",
    "title": "확률과 통계 1 정리",
    "section": "질문",
    "text": "질문\n\n\n\n이 공식은 무조건 t분포에서만 쓰이는건가?\n\n\n\n\n\n이거 어떻게 푸는거야\n\n\n\n모분산을 모르고 표본분산을 쓰면 무조건 t분포?\n쌍체표본은 무조건 t분포?\n적당한 β값은 존재하지 않은건가? 아니면 10%가 너무 큰건가?\n분모에서 분산 어떻게 추정함?\n모집단의 분포와 관계없이 표본분산 \\(S^2\\)은 \\(σ^2\\)의 불편추정량이다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#통계학",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#통계학",
    "title": "확률과 통계 1 정리",
    "section": "통계학",
    "text": "통계학\n\n불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률",
    "title": "확률과 통계 1 정리",
    "section": "확률",
    "text": "확률\n\n고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n통계적 의미: 특정 사건이 발생하는 상대도수의 극한\n\n각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포 정의 단계",
    "text": "확률 분포 정의 단계\n\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n확률 변수: 확률실험의 결과를 수치로 나타낸 변수",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#확률-분포",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포",
    "text": "확률 분포\n\n이산 확률 분포: 이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n베르누이 분포: 각 시행은 서로 독립적이고, 실패와 성공 두 가지 결과만 존재.\n\n단 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면 비복원 추출에서도 유효\n\n이항 분포: n번의 독립적인 베르누이 시행을 수행하여 성공 횟수를 측정\n기하 분포: 성공 확률이 p인 베르누이 시행에서 첫 성공까지의 시행 횟수\n초기하 분포: 베르누이 시행이 아닌 시행에서 성공하는 횟수\n포아송 분포: 임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수\n\nn이 매우 크고, p가 매우 작을 때, 이항 분포를 포아송 분포로 근사할 수 있다.\n\n\n연속 확률 분포: 연속 표본 공간에서 정의 가능\n\n균일 분포\n정규 분포\n\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)\n\nt 분포\n\n자유도가 커질수록 표준 정규분포에 근사함.\n t(n)\n\nf 분포\n\n\\(F = \\frac{X_1/ν_1}{X_2/ν_2}\\), \\(X_1 \\sim χ^2(ν_1)\\), \\(X_2 \\sim χ^2(ν_2)\\)\n\n감마 분포\n\n카이제곱 분포: α = v/2, θ = 2 인 감마분포\n\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)\n\n지수 분포: 포아송 분포에서 사건 발생 간격의 분포\n\n\\(\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)\\), \\(θ = 1/λ\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/00.html#표본의-분포",
    "href": "posts/01_projects/bs_3_1/notes/statistics/00.html#표본의-분포",
    "title": "확률과 통계 1 정리",
    "section": "표본의 분포",
    "text": "표본의 분포\n\n샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n통계량: 표본의 특성을 나타내는 값\n추정량: 아래의 조건을 만족하는 통계량\n\n불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n\n\n표본 평균의 분포\n\n모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 \\(σ^2\\)이면, \\(\\bar{X}\\)의 평균은 μ이고, 분산은 \\(σ^2/n\\)인 정규분포를 따른다.\n\n단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리)\n\n만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n\n\n\n표본 분산의 분포\n\n정규 모집단으로 부터 나온 표본의 분산 S에 대하여, \\(\\frac{(n-1)S^2}{σ^2}\\)은 자유도가 n-1인 카이제곱 분포를 따른다.\n\n모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n\n두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html",
    "href": "posts/04_archives/bs_2_2/index.html",
    "title": "2학년 2학기 학부 정리",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-09-02\n        종료일: 2024-12-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#details",
    "href": "posts/04_archives/bs_2_2/index.html#details",
    "title": "2학년 2학기 학부 정리",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 2학년 2학기 수강 과목들에 대한 개념 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#tasks",
    "href": "posts/04_archives/bs_2_2/index.html#tasks",
    "title": "2학년 2학기 학부 정리",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/index.html#related-posts",
    "href": "posts/04_archives/bs_2_2/index.html#related-posts",
    "title": "2학년 2학기 학부 정리",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "title": "Database Administration",
    "section": "Concurrency control",
    "text": "Concurrency control\nEnsuring that one user’s work does not inappropriately influence another user’s work\n\nStrict concurrency control requires locking the database, 다른 사용자의 동시 사용 허가 x\nLower concurrency control allows more throughput\n\n\nTransactions\nUsers submit Transactions(LUWs)\n\nAtmomic Transaction: 데이터베이스에서 일련의 작업들이 모두 성공적으로 수행되거나, 그렇지 않을 경우 작업이 전혀 수행되지 않아 데이터베이스가 변경되지 않는 상태를 유지하는 트랜잭션\n→ Before committed, all LUWs must be successfully completed, or rollback\nConcurrent Transactions: 여러 트랜잭션이 동시에 실행되는 것\n\nLost update problem: 두 트랜잭션이 동시에 같은 데이터를 수정할 때, 하나의 트렌잭션이 다른 트랜잭션의 변경을 덮어쓰는 문제\nInconsistent read problem: 한 트랜잭션이 데이터를 읽는 도중 다른 트랜잭션이 데이터를 수정하는 문제\n\nDirty read: commit 되기 이전에 수정된 데이터를 읽는 것. 만약 rollback이 될 경우 문제가 발생.\nNonrepeatable read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 값이 다른 경우\nPhantom read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 새로운 row가 추가된 경우\n\nResource locking\n\nImplicit locks: DBMS가 자동으로 수행하는 lock\nExplicit locks\nLOCK TABLES table_name READ -- or WRITE\nUNLOCK TABLES\nExclusive locks: 다른 트랜잭션에서 읽기/쓰기 불가\nShared locks: 다른 트랜잭션에서 읽기 가능, 쓰기 불가\nrock granularity: row-level vs table-level vs database-level\n\n\nSerializable Transactions: 가장 강력한 격리 수준 보장\n\nTwo-pase locking(2PL): growing phase와 shrinking phase로 나뉨\n\nACID Transaction\n\nAtomic: 성공한 transaction만 저장되어야 한다\nConsistent: 현재의 transaction이 마무리 되기 전 까지 record를 저장할 수 없다\n→ 트랜잭션의 살향 결과로 데이터베이스 상태가 모순되지 않음\nIsolated\n\nread uncommitted: 다른 트랜잭션에서 commit되지 않은 데이터도 읽을 수 있음\nread committed: 다른 트랜잭션이 commit된 데이터만 읽을 수 있음\nrepeatable read: 한 트랜잭션에서 하나의 스냅션만 사용\nserializable: 가장 강력한 격리 수준 보장\n\nDurable: 트랜잭션이 성공적으로 완료되면, 그 결과는 영구적으로 저장되어야 한다\n\n\n\n\n\nDeadlock / deadly embrace\n두 개 이상의 트랜잭션이 서로 unlock을 무한히 기다리는 상태\n\n\nlock\n\noptimistic locking\n\nassumption: No conflict will occur\nif no conflict occurs, the transaction is committed else it is rolled back and repeated\n\npessimistic locking\n\nassumption: Conflict will occur\nlock the data before the transaction starts\n\n\n\n\nCursor\nA cursor is a pointer into a set of rows that are the result set from an SQL SELECT statement\nDECLARE cursor_name CURSOR FOR SELECT column_name FROM table_name",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "title": "Database Administration",
    "section": "Backup and recovery",
    "text": "Backup and recovery\n\nRecovery\n\nvia Reprocessing\nvia Rollback and Rollforward\n\nlog file transaction을 undo할 때, before-images가 존재해함. (rollback) transaction을 redo할 때, after-images가 존재해함.(rollforward)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#security",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#security",
    "title": "Database Administration",
    "section": "Security",
    "text": "Security\nonly authenticated users perform authorized activities\n\nAuthentication: User ID와 password를 사용하여 사용자를 인증\nAuthorization: user groups(roles): dbcreator, public, … sql  GRANT SELECT, INSERT, UPDATE, DELETE ON table_name TO user_name",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/11.html#database-performance",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/11.html#database-performance",
    "title": "Database Administration",
    "section": "Database Performance",
    "text": "Database Performance\n\nindex\ndisk mirroring: 데이터 복제 말씀하신 듯\nRAID\nSANs\nDistributed database: service cluster partitioned replicated\n\n\nDBA Responsibilities\n\nuser reported errors를 모아서 system이 잘 돌아갈 수 있게 해야함\ndatabase 설정을 잘 관리해야함\n문서화 잘 해야함\ncloud로 db 관리(service level agreement)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization",
    "title": "Database Normalization",
    "section": "Normalization",
    "text": "Normalization\n\nprocess of organizing a database to reduce redundancy problem and improve data integrity",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "title": "Database Normalization",
    "section": "Functional Dependency",
    "text": "Functional Dependency\n\n하나의 atrribute가 다른 attribute의 value를 결정하는지 여부를 판단\nwell formed인지 판별할 수 있는 기준\nA(Determinant) -&gt; B(dependent): A가 결정되면 B도 결정된다면 B는 A에 함수적 종속\nEvery determinant must be a Candidate Key",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "title": "Database Normalization",
    "section": "Normalization Process",
    "text": "Normalization Process\n\nBCFNF: Boyce-Codd Normal Form =&gt; Each relation has only one theme\n\n\nIdentify all the Candidate Keys.\nIdentify all the Functional Dependencies.\nExamine the determinants of the functional dependencies\n\nplace the columns of the functional dependency in a new relation of their own\nmake the determinant of the functianl dependency the primary key of the new relation\nLeabe a copy of the determinant as a foreign key in the original relation\ncreate a referential integrity constraint between the original and new relation\n\nRepeat the process until every determinant of every relation is a candidate key",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#entity",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#entity",
    "title": "The Relational Model",
    "section": "entity",
    "text": "entity\na formal name for a thing that is being tracked one theme or topic (just single table)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#relation",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#relation",
    "title": "The Relational Model",
    "section": "Relation",
    "text": "Relation\n\na two-dimensional table that has specific charateristics\nCell of the table hold single value\nAll entries in a column are of the same kind\nNo two rows in a table are identical",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "title": "The Relational Model",
    "section": "domain & cartesian product",
    "text": "domain & cartesian product\n\ndomain: set of possible values for a column\ncartesian product: set of all possible combinations of rows from two tables",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "title": "The Relational Model",
    "section": "Presenting Relation Structures",
    "text": "Presenting Relation Structures\nRELATION_NAME(PrimaryKey, ForeignKey, ColumnName, …)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#key",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#key",
    "title": "The Relational Model",
    "section": "key",
    "text": "key\n\nidentify a row\nUnique Key(Primary Key)\nNonUnique Key(Foreign Key)\nComposite Key: Primary key가 두개 이상. Surrogate Key로 대체되곤 함.\nCandidate Key: unique한 columns\nSurrogate Key: 자동으로 할당되는 일련번호\nIDENTITY (start, increment)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "title": "The Relational Model",
    "section": "Referential Integrity Constraint",
    "text": "Referential Integrity Constraint\n\n모든 foriegn key는 존재하는 primary key와 매칭되야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/03.html#null-values",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/03.html#null-values",
    "title": "The Relational Model",
    "section": "Null values",
    "text": "Null values\n\nrequired, allow nulls 설정으로 null값을 허용할지 결정",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "title": "SQL",
    "section": "DDL (Data Definition Language)",
    "text": "DDL (Data Definition Language)\n\nCREATE (database, tables, views, indexes)\nALTER: modify columns / constraints\nDROP (database, tables, views, indexes)\nTRUNCATE: delete table data while keeping structure.\nMS Access에서는 지원하지 않음 =&gt; DELETE FROM table\n\nCREATE TABLE student (\n    id INT NOT NULL,\n    CourseID INT NOT NULL,\n    Name VARCHAR(100) UNIQUE, # unique는 자동으로 index 생성\n    Age INT,\n    CONSTRAINT STUDENT_PK PRIMARY KEY (id),\n    CONSTRAINT \n    COURSE_FK FOREIGN KEY (CourseID) \n    REFERENCES Course(CourseID) \n    ON UPDATE CASACADE \n    ON DELETE NO ACTION\n);\nALTER TABLE student ADD COLUMN major VARCHAR(100);\nALTER TABLE student ADD CONSTRAINT STUDENT_FK FOREIGN KEY (CourseID) REFERENCES Course(CourseID) ON DELETE CASCADE;\nALTER TABLE student ADD CONSTRAINT AGE_CHECK CHECK (Age &gt; 0);\nALTER TABLE student DROP CONSTRAINT AGE_CHECK;\nDROP TABLE student;\nTRUNCATE TABLE student;\n\nCREATE VIEW [view name] AS SELECT * FROM student;\n\nDML (Data Manipulation Language)\nINSERT INTO student VALUES (1, 'Alice', 20);\nUPDATE student SET age = 21, Name = 'babo' WHERE id = 1;\nDELETE FROM student WHERE id = 1;\n\n\nDQL (Data Query Language)\nA query create temporarily a new table.\nthis allows a query to create a new relation and feed information to another query as a subquery\nSELECT * FROM student;\nSELECT name \nFROM student \nWHERE age &gt; 20\nORDER BY name DESC, age ASC;\nSELECT DISTINCT name FROM student;\nSELECT name, age FROM student WHERE Age &gt; (SELECT AVG(Age) FROM student);\n\n\nJOIN\n\ninner join(equijoin)\n\nexplicit join: FROM table1 INNER JOIN table2 ON table1.id = table2.id\n(MS Access에서는 INNER를 명시해야됨)\nimplicit join: FROM table1, table2 WHERE table1.id = table2.id\n\nouter join\n\nleft outer join: FROM table1 LEFT JOIN table2 ON table1.id = table2.id\nright outer join: FROM table1 RIGHT JOIN table2 ON table1.id = table2.id",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "SQL"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "title": "표본의 분포",
    "section": "좋은 추정량이 되기 위한 조건",
    "text": "좋은 추정량이 되기 위한 조건\n\n불편성 (Unbiasedness) - 기본 조건\n추정량의 기대값이 추정하려는 모수와 같아야 함\n\\(E(\\hat{X}) = μ\\)\n\\(E(X_1) = μ\\)\n최소분산 (Minimum Variance)\n추정량의 분산이 가능한 작아야 함.\n표본의 갯수를 늘릴수록 분산이 줄어들어서 더 좋은 추정량이 됨\n\\(Var(\\hat{X}) = \\frac{σ^2}{n}\\)\n\\(Var(X_1) = \\sigma^2\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "title": "표본의 분포",
    "section": "표본평균의 분포",
    "text": "표본평균의 분포",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "title": "표본의 분포",
    "section": "표본분산의 분포",
    "text": "표본분산의 분포\n정규분포로 부터 추출된 표본의 \\(\\sum_{i=1}^{n} Z^2\\)은 자유도가 n인 카이제곱분포를 따름\n정규분포로 부터 추출된 표본의 \\(\\frac{(n-1)s^2}{\\sigma^2}\\)은 자유도가 n-1인 카이제곱분포를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "title": "표본의 분포",
    "section": "평균",
    "text": "평균\n\\(\\frac{\\hat{X} - μ}{s/\\sqrt{n}}\\) t-분포를 따름\nT분포: 표준 정규분포 Z, 자유도가 n인 카이제곱분포가 서로 독립일 때 \\(T=\\frac{Z}{\\sqrt{Y/n}}\\)\nT분포는 정규분포와 비슷하지만, 표본의 크기가 작을 때 정규분포보다 두꺼운 꼬리를 가짐\nT분포가 값이 더 작고, 신뢰도가 감소함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "title": "표본의 분포",
    "section": "분산",
    "text": "분산\n확률변수 U와 V가 자유도가 n1, n2인 카이제곱분포를 따르고 서로 독립이면, \\(F=\\frac{U/n1}{V/n2}\\)는 F분포를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "title": "확률변수의 기댓값",
    "section": "확률변수의 기댓값",
    "text": "확률변수의 기댓값\n\n\\(μ = E(x)\\)로 가정. (모집단)\ncovariance는 선형관계를 보여준다.\nx와 y는 독립이다 -&gt; cov(x, y) = 0",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "title": "확률변수의 기댓값",
    "section": "Moment Generationg Functions",
    "text": "Moment Generationg Functions\n\n평균과 분산만으로 확률분포를 설명하기에는 부족하다.\n\nmoment: \\(μ_k′ = E(X^k), k ∈ ℤ+\\)\nVar(x) = \\(μ_2′ - μ_1′^2\\)\nE(x) = \\(μ_1′\\)\n모든 k에 대해 검증 불가 =&gt; mgf(moment generating function)\nmgf: \\(M_X(t) = E(e^{tx})\\)\n\n\n\\(M_X(t) = 1 + tμ_1′ + \\frac{t^2}{2!}μ_2′ + \\frac{t^3}{3!}μ_3′ + ...\\)\n\\(M′(t) = μ_1′ + \\frac{2t}{2!}μ_2′ + \\frac{3t^2}{3!}μ_3′ + ...\\)\n\\(M′(0) = μ_1′\\)\n\\(M′′(0) = μ_2′\\)\n\\(M^{(k)}(0) = μ_k′\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "title": "확률과 통계의 정의",
    "section": "통계학의 정의",
    "text": "통계학의 정의\n\n불확실한 상황에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법 체계\n모집단으로부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 함\n\n\n\n모집단: 통계분석의 대상이 되는 모든 개체들의 집합\n표본: 모집단으로부터 일정한 규칙에 의해 추출한 부분집합",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "title": "확률과 통계의 정의",
    "section": "확률의 개념",
    "text": "확률의 개념\n\n모집단에서 특정 사건(event)의 상대도수의 극한\n\n\nLaw of Large Numbers\n무수히 많은 시행이 반복되면 상대도수에 의해 계산되는 확률(통계적 확률)이 이론적 확률로 수렴한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "title": "확률과 통계의 정의",
    "section": "Sample Space and Events",
    "text": "Sample Space and Events\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n\n\n\n\nevent는 여러 원소를 가질 수 있다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "title": "확률과 통계의 정의",
    "section": "확률의 정의",
    "text": "확률의 정의\n\n고전적 확률: 모든 simple event가 동일한 확률을 가질 때 P(A)는 sample space가 n개의 원소로 이루어져 있을 때 k개의 원소를 가지는 event A의 확률\n통계적 확률: simple event가 동일한 확률을 가지지 않아도 된다. 표본의 수가 무한대로 갈 때, 표본의 확률이 수렴하는 값\n\n\n확률의 성질\n\n모든x에 대하여 P(x) &gt;= 0\nP(sample space) = 1\nA와 B가 배반사건이면 P(A or B) = P(A) + P(B)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "title": "확률과 통계의 정의",
    "section": "조건부 확률",
    "text": "조건부 확률\n\nEvent B가 발생했을 때 Event A의 확률 \\[P(A|B) = \\frac{P(A∩B)}{P(B)}\\]\n결합확률 (joint probability): P(A∩B)\n주변확률 (marginal probability): P(A), P(B), …\n\n\nMultiplication Law\n\\[P(A∩B) = P(A|B)P(B)\\]",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "title": "확률과 통계의 정의",
    "section": "Independent Events",
    "text": "Independent Events\n\n두 사건 A와 B가 독립일 때, P(A|B) = P(A), P(B|A) = P(B)\nsample space는 임의의 event와 독립이다.\n공집합은 임의의 event와 독립이다. (P(∅∩A) = P(∅) * P(A) = 0 * P(A) = 0 = P(∅))",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "title": "확률과 통계의 정의",
    "section": "베이즈 정리",
    "text": "베이즈 정리\n\n\nsample space를 상호 배반인 {B1, B2, …, Bn}으로 분할 (partition)\n\\(P(A) = P(A∩B_1) + P(A∩B_2) + ... + P(A∩B_n)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "title": "정규 분포",
    "section": "정규 분포의 합",
    "text": "정규 분포의 합\n두 분포의 합이 같은 분포가 되는 경우는 흔치 않다 (uniform distribution도 같지 않다)\n두 정규분포의 합은 정규분포가 된다\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "title": "정규 분포",
    "section": "Chi-square 분포",
    "text": "Chi-square 분포\nα = ν/2, θ = 2인 감마분포\n\\(Z \\sim N(0,1)\\)일 때, \\(Z^2 \\sim χ^2(1)\\)\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "title": "Auditory Haptic",
    "section": "귀의 구조",
    "text": "귀의 구조",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "title": "Auditory Haptic",
    "section": "Sound Waves",
    "text": "Sound Waves\n\n소리는 압력이 변하는 것\n소리는 vibrating object에 의해 발생한다\n연못에 돌을 던지면 생기는 wave와 비슷함\n어떤 분자든 움직이고 압력을 만들 수 있는건 전달 가능함\n물속에서도 소리가 전달됨. (밀도가 높아서 더 빨리 전달됨)\n고체(층간소음, 철로), gas\n진공에서는 매질이 없어서 소리가 안들림(우주 공간) &lt;-&gt; 빛은 매질이 없어도 이동됨\n파동이 전기신호로 바뀌어서 들림\n\n\n물리적 특성\n\n\namplitude: 진폭의 크기 -&gt; volume\nwavelength: 진폭의 넓이 -&gt; pitch, 1초 안에 몇 번 진동하는지(주파수 10Hz = 10번 진동)\n\n\n\n사람이 느끼는 perception\n\nPitch(소리의 높낮이)\n사람이 들을 수 있는 주파수는 20Hz ~ 15kHz\n어릴 때는 고주파를 잘 들음 (고주파에 고막이 반응을 못해서)\n사람은 고주파에 반응을 잘 못함\n절대 음감이랑 관련\nTimbre(음색)\n음악에서는 악기마다 다른 음색이 있음\n음색은 여러 주파수의 하모니(complex set of resonance공명)로 결정됨\nAmplitude and loudness\n소리의 물리적 강도가 2배 증가할 때 우리가 느끼는 소리의 크기(loudness)가 배로 느껴짐(찾아보니까 2배는 아니긴 함) 160db까지 들을 수 있음 130db부터는 고통스러움\nSpatialisation\n소리는 어느 방향에서 소리가 나도 들을 수 있음 (omnidirectional).\n시각은 볼 수 있는 방향만 볼 수 있음\n\n\n\nSound intensity (dB)\n\n데시벨(dB)은 기준점에서 로그 스케일만큼 증가. (선형적 x)\nthreshold: 주변의 소음에 비해 소리가 들리는 정도. 주파수에 따라 다른 특성을 가짐.\n\n\n\n85 dB에 장시간 노출되면 청력 손상\n\n신경의 손상: 장시간 센 자극에 hair cell이 손상됨\nconduction damage: 소리의 세기가 너무 커서 고막이나 뼈에서 손상이 생김",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "title": "Auditory Haptic",
    "section": "Masking Effect",
    "text": "Masking Effect\n\n청각에서만 주로 나타남.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "title": "Auditory Haptic",
    "section": "Equal Loudness Contour",
    "text": "Equal Loudness Contour\n\n곡선은 사람들이 소리가 같은 크기라고 느끼는 지점을 나타냄\n인간 청력의 threshold가 주파수마다 다르다.\n저주파수와 고주파수는 중간 주파수에 비해 같은 강도에서 상대적으로 작게 들립니다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "title": "Auditory Haptic",
    "section": "Locating Sounds",
    "text": "Locating Sounds\n왼쪽 귀와 오른쪽 귀에서 들리는 소리의 차이\n\ndifference in phase(위상): 소리의 파장이 오목한 phase, 볼록한 phase 차이\ndifference in loudness: 가까운게 더 크게 들림\ndifference in onset: 가까운게 더 빨리 도달함\n\n\n여기까지가 소리의 mechanical한 특성이고, 이후는 이 소리를 인간이 어떻게 perception하는지에 관한 내용",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "title": "Auditory Haptic",
    "section": "Hearing Without Awareness",
    "text": "Hearing Without Awareness\n\nCocktail Party Effect: 주변 소음에서도 특정 소리를 들을 수 있음\nEx) 친구 이름을 듣고 반응하는 것, 한국인들이 한국말을 잘 듣는것\nDichotic Listening: 두 귀에 다른 소리를 들려주고 정보를 인식했는지 확인\nignored 귀에서 여전히 정보를 인식할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "title": "Auditory Haptic",
    "section": "Alarms",
    "text": "Alarms\n\nOverview\nomnidirectional한 특성때문에 visual alarm에 비해 자주 사용된다\nEx) 소방차 사이렌 소리\n\n주변 소음에 비해 충분히 db이 커야함 주변 소음과의 차이가 15dB minimal, 30dB required\n하지만 소리가 너무 크면 청각에 손상을 일으킬 수 있음\n안전상의 이유로 소리는 85 ~ 90dB 이하로 유지되어야 함\nmasking 위협때문에 여러 주파수를 혼합해서 냄\n다른 signal과 헷갈리지 않아야함\nEx) 병원의 환자실에 여러 장비가 있는데 장비마다 알람이 구분이 안되면 안됨.\nInformative and distinctive\n각각의 physical dimension(pitch(4), duration(4), amplitude(4))은 4개를 넘게 쓰지 마라\nEx) 컴퓨터 메인보드, 장비 고장 시 비프음 기준이 있음\nEx) 자동차\n\nstereotypic: 어디서 소리가 오는지\n\npitch\n\n\n\n\nNon-speech Alarm\n\nlanguage independent\n글로벌하게 가고 싶다면\nNSA가 유용하다는 증거\n클릭을할 때 딸깍 소리가 나면 실수가 덜 함\n비디오 게이머들은 소리가 없으면 게임을 못함\n일시적이고 부수적인 상태 정보 전달에 효과적\n예시) 게임에서:\n\nHP가 부족할 때 주기적인 경고음\n\n아이템 획득 시 짧은 효과음\n\n배경에서 지속적으로 재생되는 상태 알림음\n\nstereo sound로 방향을 알려줄 수 있음\n비쥬얼로는 3d 표현하기 어려움\n\n\n\nVoice Alarm\n\n자연스러운 방법으로 기기와 통신할 수 있음\nSymbolic alarm에 비해 더 많은 정보를 전달할 수 있음\nSymbolic alarm은 학습을 해야한다는 단점이 있음\nNon-Speech에 비한 한계\n\n소리가 섞이면 헷갈림\n\nmore susceptible to frequency-specific masking\n사람의 voice는 정해진 주파수가 있다.\n그 주파수에 소리가 섞이면 소리를 못들을 수 있음\n다국어 환경을 고려해야함\n\nSound Transmission Problem\n말하고자 하는 바가 전달이 잘 안될 수 있음\nEx) 파일럿 안내. 라디오에서 사용할 수 있는 대역폭이 제한되있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "title": "Auditory Haptic",
    "section": "Voice Recognition",
    "text": "Voice Recognition\nSound Transmission Problem을 해결하기 위해 사용됨\n\nArticulation index: pure bottom-up(signal의 특성에 의존) approach\nsignal이 얼마나 명확하게 잘 들리는지 평가\n1.0: 주변소음에 상관없이 잘 들리는 상태\n0.0: 주변소음에 묻혀서 소리가 들리지 않는 상태\nSpeech intelligibility measure\npoor signal quality is compensated by top-down processing =&gt; 어떻게 top-down processing을 잘 할까?를 테스트 해 보았다.\n전달하는 정보의 양을 제한, 문장의 형태로 전달하는게 좋음\n긍정이나 부정이 잘 나타나는 단어를 사용(Ok는 애매함)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "title": "Auditory Haptic",
    "section": "Tactile Perception",
    "text": "Tactile Perception\n\nTouch is complex\nOnly bi-directional communication channel: 접촉하거나 움직이거나 한 다음에 반응을 얻는 등, input과 output이 동시에 일어남\n환경에 대한 정보를 포괄해서 전달함\n온도, 표면의 거칠기, 등등\nfeedback을 제공함\n수용체가 피부 변형을 감지함\n민감도는 단위 면적당 촉점이 얼마나 분포되어 있는지에 따라 결정됨",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "title": "Auditory Haptic",
    "section": "Tactile information",
    "text": "Tactile information\n\n운동 감각을 통해 받아들이는 것 (force feedback)\n\n\n\n큰 물체는 움직이기 어렵게 함 =&gt; 더 세밀하게 조종 가능\n\n게임에서 많이 사용됨\n\n\n촉감을 통해 받아들이는 것 (vibration feedback)\n\n\n\nusing vibration for information transfer\nsimilar physical characteristics to auditory signal\n\namplitude, frequency, duration, wave pattern\n\nused for navigation aid\n중요한 정보는 시각, 나머지 feedback은 촉각, 청각으로 받아들임.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "title": "Human Information Processing Model",
    "section": "인간 정보 처리 과정",
    "text": "인간 정보 처리 과정\n\n\n1. 감각 처리 (Sensory Processing)\n\n주요 감각: 시각, 청각, 운동 감각(proprioception)\n운동 감각: 생리적 신호, 중력과 가속도에 따른 몸의 위치 감각\nSTSS (Short Term Sensory Store)\n\n각 감각기관별 정보 저장 공간\n주의 집중 불필요, 정보 그대로 저장\n빠르게 소멸 (예: 시각 - Iconic Memory 200-300ms, 청각 - Echoic Memory 2-8s)\n\n\n\n\n2. 지각 (Perception)\n\n정보 해석 과정\nTop-Down Processing: 과거 기억으로 정보 해석 (Long Term Memory 활용)\nBottom-Up Processing: 새로운/익숙하지 않은 정보 해석\nPreattentive Processing: 주의 집중 없이 자동적 정보 해석 (예: Stroop Effect)\n\n\n\n3. 주의 자원 (Attention Resources)\n\n전반적인 정보 처리 과정에 영향\nSearch Light Metaphor: 주의 집중의 비유적 설명\n주의 실패 유형:\n\nSelective Attention: 잘못된 곳에 집중\nFocused Attention: 주의 집중 부족\nDivided Attention: 다중 작업 시 주의 분산\nSustained Attention: 장시간 주의 유지 실패\n\n\n\n\n4. 장기 기억 (Long Term Memory)\n\n학습을 통한 정보 저장\n유형:\n\nDeclarative Memory (What): 사실적 지식 (Episodic, Semantic)\nProcedural Memory (How): 절차적 지식\n\n기억 실패: Encoding, Storage, Retrieval 단계에서 발생 가능\n\n\n\n5. 인지 (Cognition)\n\nWorking Memory:\n\n단기적 정보 처리 및 조작\n제한된 용량 (7±2 items)\nLong Term Memory와 Perception으로부터 정보 획득\n\n\n\n\n6. 반응 선택 (Response Selection)\n\n의사결정 이론:\n\nSignal Detection Theory\nExpected Value\nBayesian Decision Theory\nMulti-attribute Theory\n\nInformation Processing\n\nAttention and working memory\nHeuristics and biases: 인간은 합리적이지 않음\n\nNaturalistic Decision Making: Recognition-Primed Decision Model (직감)\n\n\n\n7. 반응 실행 (Response Execution)\n\n\n8. 시스템 환경 (System Environment)\n\nclosed-loop 형태의 피드백 제공\ndelay가 발생시 성능 저하",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Human Information Processing Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "title": "Display",
    "section": "Display Purpose",
    "text": "Display Purpose\n\n사람의 인지와 시스템의 실제 정보 사이의 커뮤니케이션을 위한 중간 다리 역할.\n시스템이 무엇을 하는 중이고, 무엇을 해야 하고, 어떻게 작동하는지 오퍼레이터에게 전달하기 위한 목적(mental model을 만들기 위함)\n설계된 sensory input을 통해서 파악하게 해야함\n다른 sensory input과 구별이 되야함.\n사용자가 이해할 수 있어야함(Compatible)\n\nConceptual Compatibility\nex) 플로피 디스크 심볼은 저장 용도로 사용됨\nmovement compatibility(pictorial realism): 실제와 유사한 모양을 사용하면 이해하기 쉬움\nex) 엘리베이터가 위 아래로 움직이니까 스케일을 linear로 맞춤",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "title": "Display",
    "section": "Display Rules",
    "text": "Display Rules\n\nFour Cardinal Rules\n\n꼭 필요한 정보만 제공해라\n필요한 수준의 정확도만 제공하라 (ex. 소숫점 3자리까지만. 굳이 다 보여주지 않아도 됨)\n가장 direct, simple, understandable, and usable하게 정보를 제공하라\nex) 지하철 디스플레이에서 열차가 언제 도착하는지 알려줘야 하는데 이상한걸 보여줘서 멘탈 워크로드가 높아진다.\n고장이나 작업 실패의 경우 명확히 어디서 문제가 발생했는지 바로 알아차리게 제공하라\n첫번째 원칙을 위반할 수도 있다(alarm flooding)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "title": "Display",
    "section": "Types of Displays",
    "text": "Types of Displays\n\nAuditory\n\nDetectable\nDiscrmination\nMeaningful\nMain problem: hearing ability depends on environments / background noise (auditory spatial coding 인지하기 힘듦)\n\nTactual(Haptic)\n\nDetectable: 손처럼 민감한 부분은 가능하지만, 둔감한 부분은 어렵다\nDiscrimination: nomal job이랑 구분되어야 한다\nMeaningful: tactual display에서는 어려운 부분. convention이 없음\nMain problem: 잘 안쓰이고, 손 이외에는 사용하기 어렵다\n\nOlfactory Displays - smell\n\nDetectable\nDiscrimination\nMeaningful\nMain problem: 냄새에 대한 민감도가 사람마다 다르다. regenerate 하기가 어렵다. 후각이 금방 마비된다. 전쟁에서 후각을 이용한 의사전달을 시도하기도 함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "title": "Display",
    "section": "Visual Displays",
    "text": "Visual Displays\n\nAppropriate if:\n\nNoisy environment\n한 자리에 머무는 경우(traditional. 옛날엔 들고 다니기 어려웠음)\nmessage가 길거나 복잡한 경우, spatial coding이 필요한 경우\n\nGuiding principles for design:\n\n눈에 잘 띄어야함(배경과 전경의 차이가 구분되어야함)\nLegible, 쉽게 보고 읽을 수 있어야함(ex. 엠뷸런스의 글씨)\nUnderstandable\nmain problem: 시각이 overload됨. 정보가 너무 많음\n\n\n\nDynamic Information\n변화하는 정보를 보여주는 것에는 4가지 원칙이 있다.\n\nSituation awareness\n가까운 미래에 무슨 일이 일어날 지 예측할 수 있어야함\n상황 인식 3단계:\n\nPerception: 무엇이 일어나고 있는지 인지(check readings)\nComprehension: 그것이 무엇을 의미하는지 이해\nProjection: 미래에 무슨 일이 일어날지 예측\n\n\n\nQuantitative readings\n정확한 값을 보여주는 용도로 사용됨\n\n고정 스케일의 움직이는 초점 (generally best)\n움직이는 스케일의 고정 초점\ndigital display (변동성이 큰 경우 그냥 숫자만 보여주는것보다 스케일, 초점을 사용하는게 더 효과적임)\nDesign of Analog Scales\n\n일반적으로 fixed scale, moving pointer가 좋다\n숫자의 증가는, linear 스케일에 움직이는 포인터가 자연스럽다.\nex) 온도계, 엘리베이터\n같은 작업을 하는 여러개의 pointer, scale indicator를 섞어 쓰지 말아라.\ncontrol, display가 혼합된경우 control로 pointer로 움직여라\n작은 변화 감지가 중요한 경우는 moving pointer가 더 좋다\n범위가 너무 큰 경우는 moving scale이 더 좋다\n\n\n\n\nQualitative readings\n대략적인 값, 트렌드, 변화의 비율, 변화의 방향을 보여주는 용도로 주로 사용됨.\n\ncontinuous data converted to range\n의미를 강조하고 싶을 때 color를 보조 도구로써 보여주기도 함\nShape coding\nex) 교통 표지판 8각형은 stop을 의미\nZone coding\nex) 신호등의 위치가 고정되어 있음\n\nRedundancy gain: 시각 청각 촉각, 혹은 칼라코드, 위치코드 같이 여러가지 정보(multi-modal)를 제공하면 정확히 해석할 수 있다.\n\n\nCheck readings\n시스템의 상태를 확인할 수 있어야함\n\nqualitaative reading의 특별한 case\n정상인 상태는 명확히 보여줘야함\n정상적인 것은 align해서, 비정상적인 것은 삐뚤어지게 설계해서 pre-attentive processing을 유도하라\n시각 정보를 보완하기 위해 청각 시그널을 제공하라",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "title": "Display",
    "section": "Signal and Warning Lights",
    "text": "Signal and Warning Lights\n실질적인, 잠재적인 위험 상황을 알리는 용도\n일반적으로 하나의 라이트만 사용함\n\nsteady-state light: 지속적인 싱태를 나타냄\nflashing light: 위급 상황 (flash 비율은 3-10 per seconds)\n배경에 비해 최소 두 배 이상 밝아야함\n유효 시야 30도 안쪽에 배치해야함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "title": "Sensor System (Visual)",
    "section": "눈의 구조",
    "text": "눈의 구조\n\n결막 (conjunctiva): 눈을 보호\n각막 (cornea): 빛을 굴절\n홍채(iris): 빛의 양을 조절\n동공 (pupil): 빛이 들어오는 곳\n수정체(lens): 빛을 집중하는 역할\n공막 (sclera): 눈을 보호\n유리체 (vitreous humor): 눈을 유지\n망막(retina): 빛을 감지\n망막의 세포\n\nNerve cell: 빛을 감지\nPhotoreceptor: 빛을 감지\n\n간상세포(cone): 세부적인 정보, 색상 인식, photopic conditions. fovea에 몰려있음. 짧은 파장의 색에 더 민감함\n막대세포(rod): 어두운 곳에서 활동, 주변 시야 빛을 받으면 rhodopsin이 분해됨, scotopic conditions. 긴 파장의 색에 더 민감함.\n\nChoroid: 영양 공급\n\n시신경(optic nerve): 망막에서 뇌로 정보 전달\n맹점(optic disk)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "title": "Sensor System (Visual)",
    "section": "light adaption",
    "text": "light adaption\n\n눈이 어두운 곳에서 밝은 곳으로 이동할 때, 시간이 걸림\n명순응동안 rod sensitivity가 감소하고 cone sensitivity가 증가\n어두운 곳에서 밝은 곳으로 이동할 때, 눈이 눈부실 수 있음\n암순응동안 cone sensitivity가 감소하고 rod sensitivity가 증가",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "title": "Sensor System (Visual)",
    "section": "color vision",
    "text": "color vision\n\ncone cell의 photo-pigment(RGB 64:32:2)로 색상을 인식\n망막 중앙에는 파란색이 없음\nsharpness는 brightness와 color difference에 영향을 받음\n사람은 7백만가지 색상을 인식할 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "title": "Sensor System (Visual)",
    "section": "design with color",
    "text": "design with color\n\nMono-chromatic: 단색\nAnalogous: 비슷한 색\nComplementary: 반대 색\n\n\nbefore design\n\n굳이 흑백을 안쓰고 color를 사용해야하는 이유가 있는지\ncolor가 텍스트나 object에 적합한지\ncolor가 이해나 관습에 도움이 되는지\n노안 / 색맹 고려",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "title": "Sensor System (Visual)",
    "section": "depth perception",
    "text": "depth perception\n\ndepth judgment\n\nobject-centered cues\n\n\nlinear perspective: 두 평행선이 좁을 수록 더 멀리 있는 것으로 인식\ninterposition(occlusion): 물체가 다른 물체를 가리면 가려진 물체가 더 멀리 있는 것으로 인식\nheight in the plane: 물체가 높이 있을수록 더 멀리 있는 것으로 인식\nlight and shadow: 빛과 그림자로 물체의 거리를 인식\nrelative size: 물체가 작을수록 더 멀리 있는 것으로 인식\ntexture gradient: 물체가 멀어질수록 세부적인 텍스처가 사라짐\nbrightness: 물체가 밝을수록 더 가까이 있는 것으로 인식\naerial perspective: 물체가 먼발에서 가까워질수록 색이 흐려짐\nmotion parallax: 물체가 빠르게 움직일수록 더 가까이 있는 것으로 인식 fixation point\n\n\nobserver-centered cues\n\n\nbinocular disparity: 두 눈의 시각적 차이\nconvergence: 눈이 물체를 바라볼 때 발생하는 각도\naccommodation: 눈의 렌즈가 물체를 바라볼 때 발생하는 조절. 가까운 물체일수록 렌즈가 더 둥글어짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html",
    "href": "posts/04_archives/aws_saa/index.html",
    "title": "AWS SAA 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-04-15\n        종료일: 2024-05-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증cloud",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#details",
    "href": "posts/04_archives/aws_saa/index.html#details",
    "title": "AWS SAA 준비",
    "section": "Details",
    "text": "Details\nAWS Solution Architect Associate 자격증을 취득하였습니다.\n자격증 링크",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#tasks",
    "href": "posts/04_archives/aws_saa/index.html#tasks",
    "title": "AWS SAA 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#참고-자료",
    "href": "posts/04_archives/aws_saa/index.html#참고-자료",
    "title": "AWS SAA 준비",
    "section": "참고 자료",
    "text": "참고 자료\n\nAWS Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/index.html#related-posts",
    "href": "posts/04_archives/aws_saa/index.html#related-posts",
    "title": "AWS SAA 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "title": "김형훈의 학습 블로그",
    "section": "AWS FSx",
    "text": "AWS FSx\n\nAmazon FSx for Windows File Server: fully managed Windows file system\nAmazon FSx for Lustre: fully managed Lustre file system, seamlessly integrated with S3 (can read and write data directly to S3)\nAmazon FSx for NetApp ONTAP: fully managed NetApp ONTAP file system, point-in-time snapshots, data deduplication, and data compression\nAmazon FSx for OpenZFS: fully managed OpenZFS file system, point-in-time snapshots, data deduplication, and data compression\n\n\nFile System Deployment Options\n\nScratch File System: temporary storage for data processing. no replication.\nPersistent File System: long-term storage for data processing. replicate data across multiple Availability Zones.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Storage Gateway",
    "text": "AWS Storage Gateway\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.\nS3 File Gateway: store and retrieve objects in Amazon S3 using file protocols (NFS, SMB), cache data locally, not glacier.\nFSx File Gateway: store and retrieve objects in Amazon FSx using file protocols (NFS, SMB) for window file server.\nVolume Gateway: store and retrieve objects in Amazon S3, EBS Snapshot using iSCSI protocol.\n\ncached volume: cache frequently accessed data locally.\nstored volume: entire dataset stored locally, asynchronously backed up to S3.\n\nTape Gateway: store and retrieve objects in Amazon S3 using virtual tape library (VTL) interface",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "href": "posts/04_archives/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "title": "김형훈의 학습 블로그",
    "section": "AWS DataSync",
    "text": "AWS DataSync\n\nAWS DataSync is a data transfer service that makes it easy for you to automate moving data between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server.\nFile permissions and metadata are preserved during transfer.\nif not aws to aws, need agent to transfer data.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/19_DR.html",
    "href": "posts/04_archives/aws_saa/notes/19_DR.html",
    "title": "Disaster Recovery(DR)",
    "section": "",
    "text": "Disaster Recovery(DR)\n\nRPO: Recovery Point Objective\n\nthe maximum acceptable amount of data loss measured in time\n\nRTO: Recovery Time Objective\n\nthe maximum acceptable amount of time to recover the system  ## DR Strategies\n\nBackup and Restore: Simple but RPO and RTO are high\nPilot Light: Minimal version of the environment is always running\nWarm Standby: A scaled-down version of a fully functional environment is always running\nHot-site / Multi-Site Approach: Fully functional environment is always running\n\n\n\nDatabase Migration Service(DMS)\n\nmigrate data from one database to another\nsource is available during migration\nHomogeneous Migration: same database engine\nHeterogeneous Migration: different database engine. must use Schema Conversion Tool(SCT)\nContinuous Data Replication using CDC\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Disaster Recovery(DR)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/06_route53.html",
    "href": "posts/04_archives/aws_saa/notes/06_route53.html",
    "title": "Route53",
    "section": "",
    "text": "Domain/subdomain\nRecord type (A, AAAA, CNAME, NS)\n\nA: IPv4\nAAAA: IPv6\nCNAME: Canonical name. cant be used for root domain\nNS: Name server (another DNS server)\nalias: Route53 specific. can be used for root domain. free. health check. no TTL, can’t be used for ec2 instance\n\nValue\nRouting policy\n\nSimple: one record with multiple values, choose randomly by client, no health check, if alias then specify only one\nWeighted: split traffic based on weight\nLatency based: split traffic based on latency\nFailover: primary and secondary\nGeolocation\nMultivalue answer: multiple values, health check, choose randomly by client\nGeo-proximity\n\nTTL\n\n\n\n\nEndpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/06_route53.html#health-check",
    "href": "posts/04_archives/aws_saa/notes/06_route53.html#health-check",
    "title": "Route53",
    "section": "",
    "text": "Endpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/03_ebs.html",
    "href": "posts/04_archives/aws_saa/notes/03_ebs.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/03_ebs.html#what-is-ebs",
    "href": "posts/04_archives/aws_saa/notes/03_ebs.html#what-is-ebs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#aws-lambda",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#aws-lambda",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#dynamodb",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#dynamodb",
    "title": "김형훈의 학습 블로그",
    "section": "DynamoDB",
    "text": "DynamoDB\n\nAmazon DynamoDB is a fully managed, serverless, key-value and document database that delivers single-digit millisecond performance at any scale.\nstandard table: high availability, durability, and performance\nIA table: infrequently accessed data\nmax item size: 400KB\nprovisioned mode, on-demand mode\n\n\nDynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement - from milliseconds to microseconds - even at millions of requests per second.\nmicroseconds latency\nno need to modify application \n\n\n\nDynamoDB Streams\n\nDynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables.\ncan trigger lambda function, SQS, Kinesis\n24 hours retention period\nlimit: 5 active streams per table\n\n\n\nGlobal Tables\n\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution.\nautomatic replication\nmust enable DynamoDB Streams\n\n\n\nbackup and restore\n\nPoint-in-Time Recovery\n\nPoint-in-time recovery helps protect your DynamoDB tables from accidental write or delete operations.\nrestore to any point in time within 35 days\nthe recovery process creates a new table\n\non-demand backup\n\nrestore to any point\nthe recovery process creates a new table",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#api-gateway",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#api-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "API Gateway",
    "text": "API Gateway\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nEdge-optimized API: global, use CloudFront\nRegional API: regional, use API Gateway\nPrivate API: VPC endpoint",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#step-functions",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#step-functions",
    "title": "김형훈의 학습 블로그",
    "section": "Step Functions",
    "text": "Step Functions\n\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/11_serverless.html#amazon-cognito",
    "href": "posts/04_archives/aws_saa/notes/11_serverless.html#amazon-cognito",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Cognito",
    "text": "Amazon Cognito\n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.\nUser Pools: user directory\nIdentity Pools: federated identity, temporary access AWS resources",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sqs",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sqs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sns",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#aws-sns",
    "title": "김형훈의 학습 블로그",
    "section": "AWS SNS",
    "text": "AWS SNS\n\npublish/subscribe messaging service\nSNS FIFO (only SQS can subscribe)\nmessage filtering\n\n\nFanout\nSNS + multiple SQS",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "href": "posts/04_archives/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Kinesis",
    "text": "Amazon Kinesis\n\nKinesis Data Streams\n\nreal-time data streaming service\ndata retention: default 24 hours, maximum 365 days\nonce data inserted, cannot be deleted\nprovisioned mode, on-demand mode\nVPC endpoint available \n\n\n\nKinensis Data Firehose\n\ndata transformation, compression, encryption\nbatch data delivery\nserverless  \n\n\n\nKinesis Data Analytics\n\n\nKinesis Video Streams",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html",
    "title": "Amazon Redshift",
    "section": "",
    "text": "SQLqueries -S3as data source - supportsCSV,JSON,Parquet,ORCdata formats - 5$ per TB scanned ## Performance Improvement - usecolumnardata formats (less scan) =&gt;Parquet,ORCby usingAWS Glue- compress data =&gt;GZIP,Snappy,LZO-partition datasetsin S3 for easy querying on virtual columns (path) -Use larger files` (&gt; 128MB) to minimize overhead",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#federated-query",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#federated-query",
    "title": "Amazon Redshift",
    "section": "Federated Query",
    "text": "Federated Query\nallows you to query data in relational, non-relational, object, … in a single query on AWS or on-premises",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#cluster",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#cluster",
    "title": "Amazon Redshift",
    "section": "cluster",
    "text": "cluster\n\nleader node\ncompute node",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "title": "Amazon Redshift",
    "section": "snapshots and DR",
    "text": "snapshots and DR\n\nMulti-AZ for some cluster\nsnapshots are point-in-time backups in S3\nchange is saved\nautomate snapshot, manual snapshote",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "title": "Amazon Redshift",
    "section": "loading data into redshift",
    "text": "loading data into redshift\n\nAmazon Kinesis Data Firehose\nAmazon S3 copy\nEC2 instance JDBC driver",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "href": "posts/04_archives/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "title": "Amazon Redshift",
    "section": "Redshift Spectrum",
    "text": "Redshift Spectrum\n: query data directly in S3 without loading it into Redshift",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/07_S3.html",
    "href": "posts/04_archives/aws_saa/notes/07_S3.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/07_S3.html#aws-s3",
    "href": "posts/04_archives/aws_saa/notes/07_S3.html#aws-s3",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/02_ec2.html",
    "href": "posts/04_archives/aws_saa/notes/02_ec2.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/02_ec2.html#what-is-ec2",
    "href": "posts/04_archives/aws_saa/notes/02_ec2.html#what-is-ec2",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/14_machine_learning.html",
    "href": "posts/04_archives/aws_saa/notes/14_machine_learning.html",
    "title": "Amazon Rekognition",
    "section": "",
    "text": "Amazon Rekognition\n\nfind objects, people, text, scenes in image and video analysis ## content moderation\ndetect explicit and suggestive content\na2i for human review\n\n\n\nAmazon Transcribe\n: speech-to-text service\n\n\nAmazon Polly\n: text-to-speech service\n\n\nAmazon Translate\n: language translation service\n\n\nAmazon Comprehend\n: natural language processing service\n\n\nAmazon Lex\n: chatbot service\n\n\nAmazon SageMaker\n: machine learning service\n\n\nAmazon Forecast\n: time series forecasting service\n\n\nkendra\n: document search service\n\n\nAmazon Personalize\n: personalized recommendation service\n\n\nTextract\n: OCR service\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon Rekognition"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html",
    "href": "posts/04_archives/adp_실기/notes/01.html",
    "title": "pandas data 구조",
    "section": "",
    "text": "pandas: numpy를 라벨링한거",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#before",
    "href": "posts/04_archives/adp_실기/notes/01.html#before",
    "title": "pandas data 구조",
    "section": "Before",
    "text": "Before\n데이터를 호출하고, 데이터 내용과 요약 / 통계 정보를 확인해야함\n칼럼명이 칼럼 타입을 변경해야할 때도 있음\n\nPandas 사용 준비\n\n라이브러리 설치\n라이브러리 호출\n\n\nimport pandas as pd\n\npd.set_option('display.max_rows', 10)\n\n\n\nDataFrame 선언\n\nimport numpy as np\ndataset = np.array([['kor', 70], ['math', 80]])\n# declare df 1\ndf = pd.DataFrame(dataset, columns=['class', 'score'])\n# declare df 2\ndf = pd.DataFrame([['kor', 70], ['math', 80]], columns=['class', 'score'])\n# declare df 3\ndf = pd.DataFrame({'class': ['kor', 'math'], 'score': [70, 80]})\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\n\nDataFrame 읽고 저장\n\n# filepath = '../book/data/data.csv'\n# data = pd.read_csv(filepath, na_values='NA', encoding='utf8')\n# data.to_csv('result.csv', header=True, index=True, encoding='utf8')\n\n\n\nDataFrame 출력\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.8 KB\n\n\n\niris.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\nsepal length와 petal width의 값의 차이가 크다.\n전처리 과정에서 변수 정규화 수행의 근거가 된다.\n\n\nindex / column 명 변경\n\ndf.index\n\nRangeIndex(start=0, stop=2, step=1)\n\n\n\nlist(df.index)\n\n[0, 1]\n\n\n\ndf.index = ['A', 'B']\ndf.index\n\nIndex(['A', 'B'], dtype='object')\n\n\n\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\nA\nkor\n70\n\n\nB\nmath\n80\n\n\n\n\n\n\n\n\ndf.set_index('class', drop=True, append=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nscore\n\n\nclass\n\n\n\n\n\nkor\n70\n\n\nmath\n80\n\n\n\n\n\n\n\n\ndf.reset_index(drop=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\niris.columns\n\nIndex(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)'],\n      dtype='object')\n\n\n\niris.columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\niris\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.columns = iris.columns.str.replace(' ', '_')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\n데이터 타입 변경\n사용 가능한 타입\n\nint\nfloat\nbool\ndatetime\ncategory\nobject\n\n\niris.dtypes\n\nsepal_length    float64\nsepal_width     float64\npetal_length    float64\npetal_width     float64\ndtype: object\n\n\n\niris['sepal_length'] = iris['sepal_length'].astype('int')\niris[['sepal_width', 'petal_length']] = \\\niris[['sepal_width', 'petal_length']].astype('int')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n4\n5\n3\n1\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6\n3\n5\n2.3\n\n\n146\n6\n2\n5\n1.9\n\n\n147\n6\n3\n5\n2.0\n\n\n148\n6\n3\n5\n2.3\n\n\n149\n5\n3\n5\n1.8\n\n\n\n\n150 rows × 4 columns",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "href": "posts/04_archives/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "title": "pandas data 구조",
    "section": "row / coumn 선택 추가 삭제",
    "text": "row / coumn 선택 추가 삭제\n\nrow 선택\n\niris[0:4]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n\n\n\n\n\n\n\ncolumn 선택\nSeries 형식으로 출력\n\niris['sepal_length']\n\n0      5\n1      4\n2      4\n3      4\n4      5\n      ..\n145    6\n146    6\n147    6\n148    6\n149    5\nName: sepal_length, Length: 150, dtype: int64\n\n\nDataFrame 형식으로 출력\n\niris[['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n...\n...\n...\n\n\n145\n6\n3\n\n\n146\n6\n2\n\n\n147\n6\n3\n\n\n148\n6\n3\n\n\n149\n5\n3\n\n\n\n\n150 rows × 2 columns\n\n\n\n\n\ncolumn, row 선택\n\niris.loc[0:4, ['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n\n\n\n\n\n\niris.iloc[0:4, [1, 2]]\n\n\n\n\n\n\n\n\nsepal_width\npetal_length\n\n\n\n\n0\n3\n1\n\n\n1\n3\n1\n\n\n2\n3\n1\n\n\n3\n3\n1\n\n\n\n\n\n\n\n\n\nrow 추가\n\n# 방법 1: concat 사용\n# df = pd.concat([df, pd.DataFrame([{'class': 'eng', 'score': 90}])], ignore_index=True)\n\n# 방법 2: loc 사용 \ndf.loc[len(df)] = {'class': 'eng', 'score': 90}\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n2\neng\n90\n\n\n\n\n\n\n\n\n\ncolumn 추가\n\ndf['yo'] = df['score'] + 10\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n2\neng\n90\n100\n\n\n\n\n\n\n\n\n\nrow 삭제\n\ndf.drop(2, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n\n\n\n\n\n\n\ncolumn 삭제\n\ndf.drop(columns=['yo'], inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/01.html#조건-선택",
    "href": "posts/04_archives/adp_실기/notes/01.html#조건-선택",
    "title": "pandas data 구조",
    "section": "조건 선택",
    "text": "조건 선택\n\niris[(iris['sepal_length'] &gt; 5) & (iris['sepal_width'] &lt; 3)]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n54\n6\n2\n4\n1.5\n\n\n58\n6\n2\n4\n1.3\n\n\n62\n6\n2\n4\n1.0\n\n\n63\n6\n2\n4\n1.4\n\n\n68\n6\n2\n4\n1.5\n\n\n...\n...\n...\n...\n...\n\n\n130\n7\n2\n6\n1.9\n\n\n132\n6\n2\n5\n2.2\n\n\n133\n6\n2\n5\n1.5\n\n\n134\n6\n2\n5\n1.4\n\n\n146\n6\n2\n5\n1.9\n\n\n\n\n29 rows × 4 columns\n\n\n\n\ndf.loc[df['score'] &gt; 70, '합격'] = 'Pass'\ndf.loc[df['합격'] != 'Pass', '합격'] = 'Fail'\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\n\n\n\n\n0\nkor\n70\nFail\n\n\n1\nmath\n80\nPass\n\n\n\n\n\n\n\n\nimport numpy as np\n\ncondition_list = [(df['score'] &gt;= 70), \n                  (df['score'] &lt; 70) & (df['score'] &gt;= 60),\n                  (df['score'] &lt; 60)]\ngrade_list = ['A', 'B', 'C']\ndf['grade'] = np.select(condition_list, grade_list, default='F')\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n결측치 탐색\n\ndf.isna().sum()\n\nclass    0\nscore    0\n합격       0\ngrade    0\ndtype: int64\n\n\n\ndf.notna().sum(1) # 행 기준\n\n0    4\n1    4\ndtype: int64\n\n\n\n\n결측치 제거\n\n# dropna(axis=0, how='any' or 'all', thresh=None, subset=None, inplace=False)\ndf.dropna()\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n\n결측치 대체\n\n# fillna(value=None, method=None ('pad', 'ffill', 'backfill', 'bfill'), axis=None, inplace=False, limit=None)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "pandas data 구조"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-전처리의-의미",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-전처리의-의미",
    "title": "데이터 전처리",
    "section": "데이터 전처리의 의미",
    "text": "데이터 전처리의 의미\n\n데이터 클리닝\n데이터 통합\n데이터 변환\n데이터 축소\n불균형 데이터 처리\n데이터 분할",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#이상치-확인-및-정제",
    "href": "posts/04_archives/adp_실기/notes/03.html#이상치-확인-및-정제",
    "title": "데이터 전처리",
    "section": "이상치 확인 및 정제",
    "text": "이상치 확인 및 정제\n\n이상치 확인\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.core.common import random_state\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine['class'] = wine_load.target\nwine['class'] = wine['class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nplt.boxplot(wine['color_intensity'], whis=1.5)\nplt.title('Boxplot of color_intensity')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef outliers_iqr(dt, col):\n    q1, q3 = np.percentile(dt[col], [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return dt[(dt[col] &lt; lower_bound) | (dt[col] &gt; upper_bound)]\n\noutliers = outliers_iqr(wine, 'color_intensity')\noutliers\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nclass\n\n\n\n\n151\n12.79\n2.67\n2.48\n22.0\n112.0\n1.48\n1.36\n0.24\n1.26\n10.80\n0.48\n1.47\n480.0\nclass_2\n\n\n158\n14.34\n1.68\n2.70\n25.0\n98.0\n2.80\n1.31\n0.53\n2.70\n13.00\n0.57\n1.96\n660.0\nclass_2\n\n\n159\n13.48\n1.67\n2.64\n22.5\n89.0\n2.60\n1.10\n0.52\n2.29\n11.75\n0.57\n1.78\n620.0\nclass_2\n\n\n166\n13.45\n3.70\n2.60\n23.0\n111.0\n1.70\n0.92\n0.43\n1.46\n10.68\n0.85\n1.56\n695.0\nclass_2\n\n\n\n\n\n\n\n\n\n이상치 정제\n\n이상치 제거\n\n\ndrop_outliers = wine.drop(index=outliers.index)\n\nprint(\"Original:\", wine.shape)\nprint(\"Drop outliers:\", drop_outliers.shape)\n\nOriginal: (178, 14)\nDrop outliers: (174, 14)\n\n\n\n이상치 대체\n\n이상치를 NULL로 만든 후, 결측치와 함께 대체\n\nwine.loc[outliers.index, 'color_intensity'] = np.NaN\n\nwine['color_intensity'].fillna(wine['color_intensity'].mean(), inplace=True)\nwine.loc[outliers.index, 'color_intensity']\n\n/tmp/ipykernel_13054/3568685677.py:3: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n\n\n151    4.908678\n158    4.908678\n159    4.908678\n166    4.908678\nName: color_intensity, dtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#범주형-데이터-처리",
    "href": "posts/04_archives/adp_실기/notes/03.html#범주형-데이터-처리",
    "title": "데이터 전처리",
    "section": "범주형 데이터 처리",
    "text": "범주형 데이터 처리\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris['Class'] = load_iris().target\niris['Class'] = iris['Class'].map({0: 'Setosa', \n                                   1:'Versicolour', \n                                   2: 'Virginica'})",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-분할",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-분할",
    "title": "데이터 전처리",
    "section": "데이터 분할",
    "text": "데이터 분할\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(iris.drop(\n  columns='Class'), iris['Class'], test_size=0.2, random_state=1004)\nprint('X_train: ', X_train.shape, 'X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape, 'y_test: ', y_test.shape)\n\nX_train:  (120, 4) X_test:  (30, 4)\ny_train:  (120,) y_test:  (30,)\n\n\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n87\n6.3\n2.3\n4.4\n1.3\n\n\n67\n5.8\n2.7\n4.1\n1.0\n\n\n131\n7.9\n3.8\n6.4\n2.0\n\n\n\n\n\n\n\n\ny_train.head(3)\n\n87     Versicolour\n67     Versicolour\n131      Virginica\nName: Class, dtype: object\n\n\n\niris['Class'].value_counts()\n\nClass\nSetosa         50\nVersicolour    50\nVirginica      50\nName: count, dtype: int64\n\n\n\ny_train.value_counts()\n\nClass\nVersicolour    41\nSetosa         40\nVirginica      39\nName: count, dtype: int64",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-스케일링",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-스케일링",
    "title": "데이터 전처리",
    "section": "데이터 스케일링",
    "text": "데이터 스케일링\n\nStandard Scaler\n\n평균이 0, 분산이 1이 되도록 변환\n이상치에 민감하다.\n회귀분석보다는 분류분석에 적합\n\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\nStdScaler = StandardScaler()\n\nStdScaler.fit(X_train)\nX_train_sc = StdScaler.transform(X_train)\nX_test_sc = StdScaler.transform(X_test)\n\n\n\nMin-Max Scaler\n\n0 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nMinMaxScaler = MinMaxScaler()\n\nMinMaxScaler.fit(X_train)\nX_train_sc = MinMaxScaler.transform(X_train)\n\nX_test_sc = MinMaxScaler.transform(X_test)\n\n\n\nMax Abs Scaler\n\n-1 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nMaxAbsScaler = MaxAbsScaler()\n\nMaxAbsScaler.fit(X_train)\nX_train_sc = MaxAbsScaler.transform(X_train)\n\nX_test_sc = MaxAbsScaler.transform(X_test)\n\n\n\nRobust Scaler\n\n중앙값을 0으로 설정하고, IQR을 사용하여 잉상치 영향을 최소화함\n\n\nfrom sklearn.preprocessing import RobustScaler\n\nRobustScaler = RobustScaler()\n\nRobustScaler.fit(X_train)\nX_train_sc = RobustScaler.transform(X_train)\n\nX_test_sc = RobustScaler.transform(X_test)\n\n\n\n다시 완본으로 변경\n\nscaler.inverse_transform()\n\n\npd.DataFrame(X_train_sc).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.384615\n-1.4\n0.028369\n0.000000\n\n\n1\n0.000000\n-0.6\n-0.056738\n-0.200000\n\n\n2\n1.615385\n1.6\n0.595745\n0.466667\n\n\n\n\n\n\n\n\nX_original = RobustScaler.inverse_transform(X_train_sc)\n\npd.DataFrame(X_original).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n6.3\n2.3\n4.4\n1.3\n\n\n1\n5.8\n2.7\n4.1\n1.0\n\n\n2\n7.9\n3.8\n6.4\n2.0",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#차원-축소",
    "href": "posts/04_archives/adp_실기/notes/03.html#차원-축소",
    "title": "데이터 전처리",
    "section": "차원 축소",
    "text": "차원 축소\n\nfeatures = []\nx = iris.drop(columns='Class')\n\nx = StandardScaler().fit_transform(x)\n\npd.DataFrame(x).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\npca_fit = pca.fit(x)\n\nprint(pca.singular_values_)\nprint(pca.explained_variance_ratio_.cumsum())\n\n[20.92306556 11.7091661   4.69185798  1.76273239]\n[0.72962445 0.95813207 0.99482129 1.        ]\n\n\n\nplt.title('Scree Plot')\nplt.plot(pca.explained_variance_ratio_, 'o-')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "href": "posts/04_archives/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "title": "데이터 전처리",
    "section": "데이터 불균형 문제 처리",
    "text": "데이터 불균형 문제 처리",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html",
    "href": "posts/04_archives/k8s/index.html",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#details",
    "href": "posts/04_archives/k8s/index.html#details",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#tasks",
    "href": "posts/04_archives/k8s/index.html#tasks",
    "title": "k8s",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#참고-자료",
    "href": "posts/04_archives/k8s/index.html#참고-자료",
    "title": "k8s",
    "section": "참고 자료",
    "text": "참고 자료\n\nCKA Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/index.html#related-posts",
    "href": "posts/04_archives/k8s/index.html#related-posts",
    "title": "k8s",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html",
    "href": "posts/04_archives/k8s/notes/4_security.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#authentication",
    "href": "posts/04_archives/k8s/notes/4_security.html#authentication",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#authorization",
    "href": "posts/04_archives/k8s/notes/4_security.html#authorization",
    "title": "김형훈의 학습 블로그",
    "section": "Authorization",
    "text": "Authorization\n\nAPI groups\n\nk8s API is divided into groups\ncore group is the default group\ngroup has its own set of resources and verbs \n\n\n\nRBAC\n\ncreate Role object (namespace scoped resources)\ncreate RoleBinding object\n\nor\n\ncreate ClusterRole object (cluster scoped resources)\ncreate ClusterRoleBinding object\n\n\n\nservice account\n\ncreate ServiceAccount object\nthen it create token\nthen create secret object with the token\nthen secret object is linked to the service account\nand the token is automatically mounted to the pod\n\n=&gt; but this is not secure, and scalable =&gt; TokenRequest API is used",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/4_security.html#image-security",
    "href": "posts/04_archives/k8s/notes/4_security.html#image-security",
    "title": "김형훈의 학습 블로그",
    "section": "image security",
    "text": "image security\nif you use private image registry, you need to create secret object 1. create docker-registry type secret 2. add imagePullSecrets field in the pod spec",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html",
    "href": "posts/04_archives/k8s/notes/6_network.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html#core-dns",
    "href": "posts/04_archives/k8s/notes/6_network.html#core-dns",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/6_network.html#network-plugin",
    "href": "posts/04_archives/k8s/notes/6_network.html#network-plugin",
    "title": "김형훈의 학습 블로그",
    "section": "network plugin",
    "text": "network plugin\n\nbridge type network\n - all container runtime solutions use same bridge script - and you can use third party plugins like flannel, calico, weave, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html#metrics-server",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html#metrics-server",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/2_logging_monitoring.html#cadvisor",
    "href": "posts/04_archives/k8s/notes/2_logging_monitoring.html#cadvisor",
    "title": "김형훈의 학습 블로그",
    "section": "cAdvisor",
    "text": "cAdvisor\n\ncontainer advisor\nsub-component of kubelet\ncollects, aggregates, processes, and exports information about running containers",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- master nodes: manage the worker nodes and the pods in the cluster - etcd: key-value store for all cluster data - kube-scheduler: schedules pods to worker nodes - kube-controller-manager: runs controller processes - replication controller: ensures that the correct number of pods are running - node controller: monitors the nodes - worker nodes: host the pods that are the components of the application - kubelet: communicates with the master node - kube-proxy: forwards requests to the correct pod\n\n\n  - initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.\n\n\n\n\nkey-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.\n\n\n\n\n\n\n\nkube-api-server\n\n\n\n\n\n\n\n\n\n\n\n\nmust be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#etcd",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#etcd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "key-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#kube-api-server",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#kube-api-server",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "kube-api-server",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/0_core_concept.html#kubelet",
    "href": "posts/04_archives/k8s/notes/0_core_concept.html#kubelet",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "must be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/notes/0_overview.html#how-vault-encrypt-data",
    "href": "posts/04_archives/vault/notes/0_overview.html#how-vault-encrypt-data",
    "title": "Overview",
    "section": "how vault encrypt data",
    "text": "how vault encrypt data",
    "crumbs": [
      "PARA",
      "Archives",
      "vault",
      "Notes",
      "Overview"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#etl",
    "href": "posts/04_archives/adp_필기/notes/03.html#etl",
    "title": "2 - 데이터 처리 프로세스",
    "section": "ETL",
    "text": "ETL\n\n1. ETL 개요\n\nETL(Extract, Transformation, Load): 데이터 이동 및 변환 절차\nbatch ETL, real-time ETL으로 나뉨\n\n\n\n\nETL 작업 단계\n\n\n\nInterface: 다양한 소스로부터 데이터 휙득을 위한 인터페이스(OLEDB, ODBC, FTP)\nStaging: 정기적으로 데이터 원천으로 부터 저장. 아직은 정규화 x\nProfiling: staging table의 데이터 특성을 식별하고, 품질 측정\nCleansing: profiling된 데이터를 보정\nIntegration: 데이터 충돌을 해소하고, 데이터를 통합. 아마 여기서 정규화가 이루어질듯(왜 책에 설명 똑바로 안해놓지)\nExport: 운영보고서 생성, 데이터웨어하우스 / 데이터마트에 적재하기 위한 최적화(denormalization) 진행\n\n\n\n2. ODS 구성\n\n통합된 데이터를 저정하는 중간 저장소\n실시간, 거의 실시간으로 데이터 적재\n\n\n\n3. 데이터 웨어하우스\n\nODS를 통해 정제 / 통합된 데이터를 분석 및 보고서 생성을 위해 저장\n\n특징\n\n주제중심성\n영속성/비휘발성\n통합성\n시계열성\n\n모델링 기법\n\n스타 스키마(조인 스키마)\n\n제 3정규형의 fact 테이블과 제 2정규형의 차원 테이블로 구성\n복잡성이 낮지만, 데이터 무결성이 떨어짐\n\n\n\n\n스노우플레이크 스키마\n\n스타 스키마의 차원 테이블을 제 3정규형으로 정규화한 상태\n데이터 무결성이 높지만, 복잡성이 높음\n\n\n\n\n\n\n\n\n\n제 1 정규형: 반복되는 record나 다치 attribute를 포함하지 않음 제 2 정규형: 부분 종속성(primary key의 일부가 다른 일부를 종속함)이 없음 제 3 정규형: 이행적 종속성(primary key가 아닌 attribute의 종속성)이 없음\n\n\n\n\n\n4. ODS vs DW",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#cdcchange-data-capture",
    "href": "posts/04_archives/adp_필기/notes/03.html#cdcchange-data-capture",
    "title": "2 - 데이터 처리 프로세스",
    "section": "CDC(change data capture)",
    "text": "CDC(change data capture)\n\n1. CDC 개념 및 특징\n\n데이터 변경을 감지하고, 변경된 데이터를 추출하는 기술\n하드웨어 계층부터 어플리케이션 계층까지 다양한 수준에서 적용 가능\n\n\n\n2. CDC 구현 기법\n\nTime Stamp on Rows\nVersion Numbers on Rows: 참조테이블을 같이 사용하는게 일반적이라고 한다.\nStatus on Rows: time stamp, version number 보완 용도로, 사람이 레코드 반영 여부를 직접 판단할 수 있게 적용할 수 있음\nTime/Version/Status on Rows\nTriggers on Tables: message queue로 변경 발생시 subscribe 된 대상에 publish하는 방식. 시스템 관리 복잡도가 높아짐\nEvent Programming: 어플리케이션에 데이터 변경 식별 기능을 추가\nLog Scanner on Database: 데이터 스키마 변경 불필요, 어플리케이션 영향 최소화, 지연시간 최소화\n\n\n\n3. CDC 구현 방식\n\nPush: 데이터 원천에서 변경 식별(agent)\nPull: 대상 시스템에서 원천을 주기적으로 모니터링",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#eai",
    "href": "posts/04_archives/adp_필기/notes/03.html#eai",
    "title": "2 - 데이터 처리 프로세스",
    "section": "EAI",
    "text": "EAI\n\n1. EAI의 개념 및 특징\n\n기업 내 혹은 기업 간 정보시스템을 연계하여 동기화.\nETL은 batch 처리 중심, EAI는 실시간 혹은 근접 실시간 처리 중심\n\n\n\n2. 데이터 연계 방식\n\n\nETL/CDC는 운영 데이터와 분석을 위한 데이터베이스가 구분되지만, EAI는 그냥 통합\n\n\n\n3. EAI 구성요소\n\nAdapter: 시스템 간 데이터 변환\nBroker: 데이터 전송\nBus: 데이터 전송 경로 설정\nTransformer: 데이터 형식 변환\n\n\n\n4. EAI 구현 유형\n\nMediation: Publish/Subscribe 방식\nFederaion: Request/Reply 방식\n\n\n\n5. EAI 활용 효과\n\n협력사, 파트너, 고객과의 상호 협력 프로세스 연계\n그룹 및 지주 회사 계열사들 간 상호 관련 데이터 동기화 등을 위한 데이터 표준화 기반 제공\n\n\n\n6. EAI vs ESB\n\n추가적인 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "href": "posts/04_archives/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "데이터 통합 및 연계 기법",
    "text": "데이터 통합 및 연계 기법\n\n\n빅데이터는 시각화도 하고, NoSQL 같은 환경에서도 사용한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "href": "posts/04_archives/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "대용량의 비정형 데이터 처리 방법",
    "text": "대용량의 비정형 데이터 처리 방법\n\n2. 대규모 분산 병렬 처리\n\n하둡:\n\nMapReduce와 HDFS를 기반으로 한 분산 병렬 처리 프레임워크\n비공유 분산 아키텍쳐\n선형적인 성능과 용량 확장\nMapReduce failover\n\n\n\nHadoop ecosystem\n\n\n\n3. 데이터 연동\n대규모 연산을 데이터베이스에서 처리하기 어렵기 때문에, 하둡으로 복사해와서 MapReduce 연산 후, 결과를 다시 데이터베이스에 기록하기 위해 스쿱 사용\n\nSqoop\n\nJDBC를 지원하는 RDBMS, Hbase와 Hadoop 간 데이터 전송(Import, Export)\nSQL 질의로 데이터 추출\nMapReduce 사용\n\n\n\n\n4. 데이터 질의 기술\n\nHive: SQL과 유사한 HiveQL 질의, batch 처리\nSQL on Hadoop: SQL 질의, 실시간 처리\n\napache Drill, Stinger, Shark, Tajo, Impala, HAWQ, Presto",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-과제-발굴",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-과제-발굴",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 과제 발굴",
    "text": "분석 과제 발굴\n\n풀어야 할 다양한 문제를 데이터 분석 문제로 변환 후, 프로젝트를 수행할 수 있는 과제 정의서 형태로 도출\n\n\n\n\n도출을 위한 접근 방법\n\n\n최적의 의사결정은 두 접근 방식이 상호 보완 관계에 있을 때 가능하다.\n\n\n1. 하향식 접근법\n\n사물을 why 관점에서 보는 방식\n\n\n\n문제 탐색: 문제를 해결함으로써 발생하는 가치에 중점\n\n비즈니스 모델기반\n분석 기회 발굴의 범위 확장\n외부참조 모델 기반\n분석 유즈 케이스\n\n문제 정의: 식별된 비즈니스 문제를 데이터의 문제로 변환\n해결방안 탐색: 분석 역량과, 분석 기법 및 시스템 존재 여부를 고려한다.\n타당성 검토\n\n경제적 타당성: 비용대비 편익 분석 관점의 접근\n데이터 및 기술적 타당성\n\n\n\n\n2. 상향식 접근법\n\n사물을 what 관점에서 보는 방식\n\n\n비지도 학습\n지도 학습\n\n\n프로토타이핑 접근법",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 기획 방향성 도출",
    "text": "분석 기획 방향성 도출\n\n1. 분석기획의 특징\n\n과제를 발굴, 정의하고 의도했던 결과를 도출할 수 있도록 적절하게 관리할 수 있는 방안을 사전에 계획하는 일련의 작업 (말 그대로 기획)\n\n\n\n3. 목표 시점 별 분석 기획 방안\n\n\n\n4. 분석 기획시 고려사항\n\n가용 데이터\n적절한 활용방안과 유즈케이스\n장애요소들에 대한 사전계획 수립",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-방법론",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-방법론",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 방법론",
    "text": "분석 방법론\n\n방법론은 절차, 방법, 도구와 기법, 템플릿과 산출물로 구성된다.\n\n\n\n\n방법론 절차의 구성 요소\n\n\n\n폭포수 모델\n프로토타입 모델\n나선형 모델\n\n\n1. KDD 분석 방법론\n\n비즈니스 도메인에 대한 이해, 프로젝트 목표 설정\n데이터셋 선택\n데이터 전처리: 잡음, 이상치, 결측치 처리. 추가로 요구되는 데이터 셋이 필요한 경우, 데이터 선택 프로세스로 돌아감\n데이터 변환: 데이터 차원 축소, 학습용 데이터, 시험용 데이터 분리\n데이터 마이닝\n데이터 마이닝 결과 평가\n\n\n\n2. CRISP-DM 분석 방법론\n\n\n\nCRISP-DM 4레벨 구조\n\n\nGeneric Tasks 예시: 데이터 정제\nSpecialized Tass 예시: 범주형 데이터 정제, 연속형 데이터 정제\n\n\n\nCRISP-DM 6Phase\n\n\n\n업무 이해\n데이터 이해: 데이터셋 선택, 데이터 전처리\n데이터 준비: 데이터 변환\n모델링: 모델 평가\n평가: 모델 적용성 평가\n전개\n\n\n\n3. 빅데이터 분석 방법론\n\n\n\n빅데이터 분석 방법론의 5단계\n\n\n\n분석 기획\n\n비즈니스 이해 및 범위 설정\n프로젝트 정의 및 계획 수립 → SOW\n프로젝트 위험 계획 수립 → 회피, 전이, 완화, 수용\n\n데이터 준비\n\n필요 데이터 정의\n데이터 스토어 설계\n데이터 수집 및 정합성 점검\n\n데이터 분석\n\n분석 데이터 준비\n텍스트 분석\n탐색적 분석\n모델링 → 훈련용, 테스트용 데이터 분리\n모델 평가\n\n시스템 구현\n평가 및 전개",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "href": "posts/04_archives/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 프로젝트 관리 방안",
    "text": "분석 프로젝트 관리 방안\n\n1. 분석과제 관리를 위한 5가지 주요 영역\n\nData Size\nData Complexity\nSpeed: 분석 모델의 성능 및 속도를 고려해야한다.\nAnalytic Complexity: 분석 모델의 정확도를 높이면서 해석이 가능하도록 최적 모델을 찾아야 한다.\nAccurancy & Precision: 정확도, 정밀도\n\n\n\n3. 분석 프로젝트 관리방안\n\n범위\n시간\n원가\n품질\n통합\n조달\n자원\n리스크\n의사소통\n이해관계자",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 데이터 저장 기술",
    "text": "분산 데이터 저장 기술\n\n1. 분산 파일 시스템\n\nGFS(Google File System): 구글의 분산 파일 시스템\n\nchunk: 64MB\n트리 구조가 아닌, 해시 테이블 구조로 관리\nPOSIX 인터페이스 지원하지 않음\n단일 마스터 노드가 메모리상에서 메타데이터 관리\n마스터 노드에 대한 로그를 기록하고, 마스터의 상태를 섀도우 마스터 노드에 복제\n하나의 파일에 대한 primary node를 정하고, 다른 노드에 복제본 분산 저장\n낮은 응답 지연시간보다 높은 처리율 중시\nMaster Node, Chunk Node, Client 구성\n\nHDFS(Hadoop Distributed File System): 아파치 하둡의 분산 파일 시스템\n\nGFS의 clone project\nPOSIX 인터페이스 지원하지 않음\nblock: 128MB\nNameNode가 메타데이터 관리\n낮은 응답 지연시간보다 높은 처리율 중시\nNameNode, DataNode, 보조 네임 노드, job tracker, task tracker 구성\n\nLustre: 고성능 컴퓨팅을 위한 분산 파일 시스템\n\nPOSIX 인터페이스 지원\nchunk가 아닌 striping 방식 데이터 저장\nClient Filesystem, Metadata Server, 객체 저장 서버로 구성\n\n\n\n\n2. 데이터베이스 클러스터\n\n\n\n\n\n\n\n무공유 디스크\n\n각 노드가 완전히 분리된 데이터를 가짐\nOracle RAC를 제외한 대부분의 클러스터가 채택\n노드 확장에 제한이 없음\n\n공유 디스크\n\nSAN과 같은 네트워크로 모든 노드가 디스크 공유\n노드 확장시 디스크 병목현상 고려 필요\n\n\n\n\n\n\nOrace RAC 데이터베이스 서버: 확장성보다는 고가용성이 중요한 서비스에 적합\nIBM DB2 ICE(integrated cluster environment)\n마이크로소프트 SQL Server: 전역 스키마가 없어서 모든 노드에 질의를 해야함. active-stanby 구성\nMySQL:\n\n클러스터에 참여하는 노드는 최대 255, 그 중 데이터 노드는 최대 48개까지 가능\n운영중에 노드를 추가 삭제 불가\n\n\n\n\n3. NoSQL\n\nGoogle BigTable:\n\n공유 디스크 방식\nRow Key 순으로 정렬 되어 있고, Row 내부적으로는 Column Key 순으로 정렬\nColumn Key, Value, Timestamp로 구성\nChubby를 이용해 마스터 노드 관리\n\nHBase\nAmazon SimpleDB\n\nschema가 없고, Domain(table), Item(record), Attribute(column), Value으로 구성\n\n마이크로소프트 SSDS: Container(table), Entity(record), Property(column)로 구성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 컴퓨팅 기술",
    "text": "분산 컴퓨팅 기술\n\n1. MapReduce\n\n대용량 데이터를 분산 처리할 수 있는 모델\n보통 64MB를 기준으로 데이터 분할\n하나의 블록당 하나의 Map Task, 사용자가 지정한 갯수만큼의 Reduce Task 생성\nCount 작업에 적합하고, Sort 작업에는 적합하지 않음\n\n\nGoogle MapReduce\nHadoop MapReduce\n\n절차: 1. Split 1. Map 1. Combine 1. Partition 1. Shuffle 1. Sort 1. Reduce\n\n\n2. 병렬 쿼리 시스템\n\nGoogle Sawzall: MapReduce에 대한 이해가 없어도 쉽게 사용 가능\nApache Pig\nApache Hive\n\n\n\n3. SQL on Hadoop",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/04.html#클라우드-인프라-기술",
    "href": "posts/04_archives/adp_필기/notes/04.html#클라우드-인프라-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "클라우드 인프라 기술",
    "text": "클라우드 인프라 기술\n\n2. CPU 가상화\n\n하이퍼바이저: 하드웨어 리소스를 가상화하여 여러 개의 가상 머신을 생성하는 소프트웨어\n\nbare-metal hypervisor: 하드웨어와 host 운영체제 사이에 hypervisor가 존재  \nhosted hypervisor: host 운영체제와 guest 운영체제 사이에 hypervisor가 존재\n\nContainer\n\n\n\n3. 메모리 가상화\n\nVMKernnel: hypervisor 내에 Show Page Table을 두고, 각 VM의 Guest OS의 Page Table을 관리\nMemory Ballooning: Guest OS의 메모리를 빼앗아서 다른 VM에 할당\nTransparent Page Sharing: 같은 내용의 메모리 페이지는 VM들이 공유\nMemory Overcommitment: VM에 할당된 메모리보다 더 많은 메모리를 할당할 수 있음\n\n\n\n4. I/O 가상화\n\n가상 이더넷: 가상 머신 간의 네트워크 통신을 위한 가상 네트워크. LAN 세그먼트를 가상화\n공유 이더넷 어댑터: 하나의 물리적 네트워크 어댑터를 여러 VM이 공유. 병목현상 발생 가능\n가상 디스크 어댑터",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-이해",
    "href": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-이해",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 이해",
    "text": "빅데이터의 이해\n\n1. 빅데이터의 정의\n\n\n좁은 범위\n\n데이터 자체의 특성에 초점을 맞춘 정의\n3V(다양성, 속도, 규모)를 강조\n\n중간 범위\n\n데이터 자체뿐 아니라 처리, 분석 방법도 포함하는 정의\n\n넓은 관점\n\n인재, 조직 변화까지 포함한 정의\n\n\n\n∴ 기존 방식으로는 얻을 수 없는 통찰 및 가치 창출\n\n\n2. 출현 배경과 변화\n\n산업계: 고객 데이터가 축적되며 새로운 가치 활용\n학계: 거대 데이터 활용 분야가 늘어나며 통계 도구들이 발전\n기술발전: 관련기술의 발전\n\n\n\n3. 빅데이터의 기능\n\n산업혁명의 석탄, 철: 산업 전반에 혁명적 변화를 가져옴\n21세기의 원유: 생산성을 향상시키고, 기존에 없던 새로운 범주의 산업을 만들어낼 것으로 전망\n렌즈: 데이터가 산업에 영향을 미침\n플랫폼\n\n\n\n4. 빅데이터가 만들어 내는 본질적인 변화\n\n사전처리 → 사후처리\n표본조사 → 전수조사\n질 → 양\n인과관계 → 상관관계",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "href": "posts/04_archives/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 가치와 영향",
    "text": "빅데이터의 가치와 영향\n\n1. 빅데이터의 가치\n빅데이터는 아래와 같은 이유로 가치 선정이 어렵다.\n\n데이터 활용방식: 데이터를 언제 어디서 누가 사용할지 미리 예측하기 어려움\n새로운 가치 창출: 기존에 없던 가치를 창출하기 때문에 가치를 예측하기 어려움\n분석 기술 발전: 현재 가치가 없더라도, 추후 기술이 발전하면 가치가 생길 수 있음\n\n\n\n2. 빅데이터의 영향\n빅데이터는 다양한 주체(기업, 정부, 개인)에 영향을 미친다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#비즈니스-모델",
    "href": "posts/04_archives/adp_필기/notes/01.html#비즈니스-모델",
    "title": "1 - 데이터의 가치와 미래",
    "section": "비즈니스 모델",
    "text": "비즈니스 모델\n\n1. 빅데이터 활용 사례\n여러가지 활용 사례가 있다.\n\n\n2. 빅데이터 활용 기본 테크닉\n\n연관규칙학습: 범주형 데이터의 변인들간의 규칙을 발견. 비지도 학습 (ex. 장바구니 분석)\n유형(군집)분석: 데이터를 분류하거나 군집화. 비지도 학습 (not 분류분석)\n유전자 알고리즘: 최적해를 찾는 알고리즘\n기계학습: 훈련한 데이터로 예측\n회귀분석: 연속형 데이터의 독립변수와 종속변수의 관계를 수학적으로 모델링해서 예측\n감정분석: 비정형 데이터 분석\n소셜네트워크분석(사회관계망분석): 비정형 데이터 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "href": "posts/04_archives/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "title": "1 - 데이터의 가치와 미래",
    "section": "위기 요인과 통제 방안",
    "text": "위기 요인과 통제 방안\n\n1. 빅데이터 시대의 위기 요인과 통제 방안\n\n사생활 침해: 동의에서 책임으로\n책임 원칙 훼손: 결과 기반 책임 원칙 고수\n데이터 오용: 알고리즘 접근 허용, 알고리즈미스트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/01.html#미래의-빅데이터",
    "href": "posts/04_archives/adp_필기/notes/01.html#미래의-빅데이터",
    "title": "1 - 데이터의 가치와 미래",
    "section": "미래의 빅데이터",
    "text": "미래의 빅데이터\n\n1. 빅데이터 활용의 3요소\n\n데이터: 모든것의 데이터화\n기술: 인공지능\n인력: 데이터 사이언티스트, 알고리즈미스트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화의-정의",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화의-정의",
    "title": "5 - 시각화 디자인",
    "section": "시각화의 정의",
    "text": "시각화의 정의\n\n1. 데이터 시각화의 중요성\n데이터 시각화의 목적은 데이터 분석과 의사소통\n\n\n2. 시각 이해와 시각화\n\n\n데이터: 디자인의 대상이 될 수 없음\n정보\n\n데이터가 의미를 전달하기위한 형태를 가짐\n자기조직화 되지 않은 일반적인 의미를 가지고 있고, 생산자와 사용자의 관점에 따라 다르게 전달될 수 있다.\n\n지식: 다른 영역의 정보가 자기조직화된 형태\n지혜: 지식이 내면화되어 개인적 맥락에 포함된 형태. 명시적으로 상대에게 전달하기 어려움\n\n\n\n\n정보 인터랙션 디자인(사진좀 보이게 올려둬라 좀..)\n\n\n\n\n3. 시각화 분류와 구분\n\n\n데이터 시각화\n정보 시각화\n정보 디자인 \n\n데이터 시각화, 정보 시각화, 인포그래픽도 정보 디자인의 범위에 속한다고 볼 수 있다.\n대표적인 예시로 나폴레옹 행군 다이어그램, 나이팅게일 폴라 지역 다이어그램이 있다.\n\n인포그래픽(뉴스 그래픽): 중요한 정보를 한 장의 그래픽으로 표현한 것. 원 데이터는 취급 안함.\n\n정보형 메세지: 객관적인 정보를 전달하는데 목적을 둠. 대표적인 예시: 워싱턴 지하철 지도\n설득형 메세지: 대충 포스터 생각하면 됨",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화-프로세스",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화-프로세스",
    "title": "5 - 시각화 디자인",
    "section": "시각화 프로세스",
    "text": "시각화 프로세스\n\n1. 정보 디자인 프로세스\n\n데이터 수집\n모든 것을 읽기\n내리티브 찾기\n문제의 정의\n계층 구조 만들기\n와이어프레임 그리기\n포맷 선택하기\n시각 접근 방법 결정하기\n정제와 테스트\n세상에 선보이기\n\n\n\n2. 빅데이터 시각화 프로세스\n\n\n\n시각화 프로세스\n\n\n\n\n\n방법론\n\n\n\n\n\n에드워드 터프티 시각 정보 디자인 7원칙",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/14.html#시각화-방법",
    "href": "posts/04_archives/adp_필기/notes/14.html#시각화-방법",
    "title": "5 - 시각화 디자인",
    "section": "시각화 방법",
    "text": "시각화 방법\n\n2. 정보 구조화\n\n데이터 수집 및 탐색\n데이터 분류: 확장자 맞게 분류\n데이터 배열: LATCH\n\nLocation\nAlphabet\nTime\nCategory\nHierarchy: 정보의 변화에 따라 데이터의 값이나 중요도 순서로 정렬\n\n데이터 재배열(관계 맺기):\n\n\n\n3. 정보 시각화\n\n\n\n\n좋은 그래프 디자인\n\n\n\n범례 만들지 말고, 직접 그려 넣은거\n테두리, 보조선 없는거\n굵은 글씨 대신 글자를 흐리게\n색깔은 최대한 적게 사용\n\n\n\n4. 정보 시각 표현\n\n자크 베르탱의 그래픽 7요소\n\n위치: 가장 중요한거는 좌측 상단에 배치\n크기\n모양\n색\n명도\n기울기\n질감\n\n타이포그래피\n\n산세리프: 돌기가 없음. 제목에 적합\n세리프: 돌기가 있음. 본문에 적합\n\n아이소타이프",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터와-정보",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터와-정보",
    "title": "1 - 데이터 이해",
    "section": "데이터와 정보",
    "text": "데이터와 정보\n\n1. 데이터\n\n객관적 사실을 나타내는 존재적 특성과, 추론 예측 전망 추정을 위한 근거가 되는 당위적 특성을 모두 포함하는 개념\n단위: 바이트(byte), 킬로바이트(KB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB), 페타바이트(PB), 엑사바이트(EB), 제타바이트(ZB), 요타바이트(YB)\n유형:\n\n정성적 데이터: 비정형 데이터, 주관적 내용, 통계분석이 어려움\n정량적 데이터: 정형 데이터, 객관적 내용, 통계분석이 용이함\n\n지식 경영의 핵심 이슈인 암묵지와 형식지를 연결하는 역할을 함\n\n\n\n\n\n\n\n\n정형 데이터: 표 형태로 정리된 데이터\n반정형 데이터: HTML, XML, JSON 등의 형태(스키마, 메타데이터)가 있고, 연산이 불가능한 데이터\n비정형 데이터: 형태가 없고, 연산이 불가능한 데이터\n\n\n\n\n\n\n\n\n\n\n\n암묵지:\n\n학습과 경험을 통해 개인에게 체화되어 잇지만 겉으로 드러나지 않는 지식\n개인에게 축적된 내면화된 지식 → 조직의 지식으로 공통화\n\n형식지:\n\n문서나 메뉴얼처럼 형상화된 지식\n언어, 기호, 숫자로 표출화된 지식 → 개인의 지식으로 연결화\n\n\n∴ 내면화 → 공통화 → 표출화 → 연결화 → 내면화\n\n\n\n\n\n2. 데이터와 정보의 관계\n\n데이터(data): 그 자체로는 의미가 중요하지 않은 객관적인 사실\nex) A마트는 100원, B마트는 200원에 휴지를 판다.\n정보(information): 데이터를 가공하여 의미를 부여한 결과물\nex) A마트가 100원에 판 휴지는 B마트보다 100원 싸다.\n지식(knowledge): 정보를 구조화하여 유의미한 정보를 분류하고 개인적인 경험을 결합시켜 고유의 지식으로 내재화된 것\nex) 가격이 더 저렴한 A마트에 가서 휴지를 사야겠다.\n지혜(wisdom): 지식의 축적과 아이디어가 결합된 창의적인 결과물\nex) A마트의 다른 물건도 B마트보다 저렴할 것이다.\n\n\n\n\nDIKW 피라미드",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스 정의와 특징",
    "text": "데이터베이스 정의와 특징\n\n1. 데이터베이스의 정의\n기존에는 정형 데이터 관리의 의미로 사용되다가, 빅데이터의 출현으로 비정형 데이터까지 포함하는 개념으로 확장됨\n\n\n2. 데이터베이스의 일반적인 특징\n\n통합된 데이터: 동일한 내용의 데이터가 중복되어 있지 않다.\n저장된 데이터\n공용 데이터\n변화되는 데이터: 데이터베이스에는 항상 현재의 정확한 데이터를 유지한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/00.html#데이터베이스의-활용",
    "href": "posts/04_archives/adp_필기/notes/00.html#데이터베이스의-활용",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스의 활용",
    "text": "데이터베이스의 활용\n\n1. 1980년대 기업 내부 데이터베이스\n\nOLTP(On-Line Transaction Processing)\n\n데이터베이스의 데이터를 실시간으로 갱신하는 프로세싱.\n구조가 복잡하고, 현재의 단기간 데이터.\n갱신이 동적이고, 엑세스 빈도가 높다.\n질의가 단순하고, 주기적이다.\n\nOLAP(On-Line Analytical Processing)\n\n데이터 조회, 분석 위주.\n구조가 단순하고, 과거의 장기간 요약 데이터.\n갱신이 정적이고, 엑세스 빈도가 보통이다.\n질의가 복잡하다.\n\n\n\n\n2. 2000년대 기업 내부 데이터베이스\n\nCRM(Customer Relationship Management): 고객 관리 시스템\nSCM(Supply Chain Management): 공급망 관리 시스템\n\n\n\n3. 각 분야별 내부 데이터베이스\n\n제조부문\n\nERP(Enterprise Resource Planning): 기업 내부 자료를 하나의 통합 시스템으로 재구축\nBI(Business Intelligence): 기업의 수많은 데이터를 정리, 분석해 의사결정에 활용하는 프로세스\nCRM\nRTE(Real-Time Enterprise): ERP, SCM, CRM 등의 부문별 전산화 시스템을 하나로 통합\n\n금융부문\n\nEAI(Enterprise Application Integration)\nEDW(Enterprise Data Warehouse): BPR, CRM, BSC 등의다양한 분석 시스템을 위한 원천\n\n유통부문\n\nKMS(Knowledge Management System)\nRFID(Radio Frequency Identification): 주파수를 이용해 ID를 식별\n\n\n\n\n4. 사회기반구조로서의 데이터베이스\n\nEDI(Electronic Data Interchange): 전자상거래를 위한 표준화된 데이터 포맷\nVAN(Value Added Network): EDI를 위한 통신망 (카드 결제 시, 가맹점과 카드사 사이에서 승인 요청 및 결과 전달을 중계함.)\nCALS(Commerce At Light Speed): 제품의 설계, 생산, 유통, 판매 등의 모든 과정을 통합한 경영정보시스템",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/09.html#데이터-변경-및-요약",
    "href": "posts/04_archives/adp_필기/notes/09.html#데이터-변경-및-요약",
    "title": "4 - 데이터 마트",
    "section": "데이터 변경 및 요약",
    "text": "데이터 변경 및 요약\n\n요약 변수: 전체적 특성을 대표하여 aggregate한 변수. 제활용성이 높다.\n파생 변수: 기존 데이터를 변환, 조합, 계산하여 새롭게 만든 변수. 주관이 개입될 수 있다.\n\n\n\n\n요약 변수 예시\n\n\n\n1. reshape 패키지 활용",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 데이터 마트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#데이터-마이닝-개요",
    "href": "posts/04_archives/adp_필기/notes/11.html#데이터-마이닝-개요",
    "title": "4 - 정형 데이터 마이닝",
    "section": "데이터 마이닝 개요",
    "text": "데이터 마이닝 개요\n\n데이터 마이닝 분석 방법\n\n지도 학습\n\n의사결정 나무\n인공신경망\n회귀 분석\n사례기반 추론\nk-최근접 이웃\n\n비지도 학습\n\nOLAP\n연관성 규칙\n군집 분석\nSOM\n\n\n\n\n데이터 마이닝 추진 단계\n\n목표 설정\n데이터 준비\n가공\n기법 적용\n검증\n\n\n\n데이터 분할\n\n구축용(추정용, 훈련용): 50%\n검정용: 30%\n시험용: 20%\nfold-out\nk-fold\nleave-one-out\n\n\n\n성과 분석\n\n정분류율\n오분류율\n민감도(재현율): 실제 True인데 True라고 예측한 비율\n특이도: 실제 False인데 False라고 예측한 비율\n정밀도: True라고 예측했는데 True인 비율\nF1-score: \\(\\frac{정밀도 * 재현율}{정밀도 + 재현율}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#앙상블-기법",
    "href": "posts/04_archives/adp_필기/notes/11.html#앙상블-기법",
    "title": "4 - 정형 데이터 마이닝",
    "section": "앙상블 기법",
    "text": "앙상블 기법\n\n여러 개의 분석 모델을 결합하여 하나의 모델을 구축하는 기법\n\n\n배깅\n\n여러 부트스트랩(복원 추출된 샘플)에 대해 동일한 모델을 독립적으로 학습시키고, 결과를 투표하여 최종 결과를 결정\n\n\n\n부스팅\n\n부트스트랩을 순차적으로 학습시키며, 이전 모델의 오차를 보완하는 방식\nGradient Boosting\nXGBoost\nLightGBM",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#분류-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#분류-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "분류 분석",
    "text": "분류 분석\n\n지도학습. 데이터의 범주형 속성 값을 예측\n\n\n의사결정 나무\n\n이상치에 민감하지 않다.\n대용량 데이터에 대해 적합하다.\n과적합 문제 발생 가능성\n\n\n성장\n\n분리\n\n이산형 변수\n\n카이제곱량\n지니지수: \\(1 - \\sum_{i=1}^{n} p_i^2\\). 낮춰주는 변수 선택\n엔트로피: \\(-\\sum_{i=1}^{n} p_i \\log_2 p_i\\). 낮춰주는 변수 선택\n\n범주형 변수\n\n분산\nF 통계량\n\n\n정지 기준: 의사경정 나무의 높이, 리프 노드의 최소 갯수\n\n가지치기\n타당성 평가\n예측\n\n\n\n인공신경망 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#som",
    "href": "posts/04_archives/adp_필기/notes/11.html#som",
    "title": "4 - 정형 데이터 마이닝",
    "section": "SOM",
    "text": "SOM\n\n고차원의 데이터를 이해하기 쉬운 저차원의 데이터로 변환\n구성\n\n입력층\n경쟁층",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#군집-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#군집-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "군집 분석",
    "text": "군집 분석\n\n사전정보가 없는 상태에서 관측값들의 거리 또는 유사성을 이용하여 군집을 형성하는 분석 방법\n군집분석은 알고리즘에 따라 결과가 매번 다르고, 명확한 정답이나 정답을 찾기 위한 p-value가 없다.\noutlier에 민감하다.\n\n\n계층적 군집 분석\n\n군집의 갯수를 모를 때, 우선적으로 갯수를 정하기 위해 사용\n가까운 개체끼리 차례로 묶거나 멀리 떨어진 개체를 분리해 가는 방식\n한 번 분류된 개체는 재분류되지 않음\n계층적 군집분석 단계\n\nDistance Measure 결정\n\n연속형 변수\n\n유클리디안 거리\n맨하탄 거리\n민코우스키 거리: 유클리디안 거리(L2)와 맨하탄 거리(L1)의 일반화된 공식.\n표준화 거리: 표준편차로 표준화된 길이의 유클리디안 거리\n마할라노비스 거리: 공분산으로 표준화된 길이의 유클리디안 거리\n체비셰프 거리: x 좌표 차이와 y 좌표 차이 중 최댓 값\n캔버라 거리: 두 벡터의 각 차이의 비율\n\n범주형 변수\n\n자카드 거리: \\(1 - \\frac{A \\cap B}{A \\cup B}\\)\n코사인 거리: \\(1 - \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)\n\n\nClustering Algorithm 결정\n\n합병에 의한 방법: 가장 가까운 거리를 가진 두 군집을 합침\n\n단일 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최솟값 사용\n완전 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최댓값 사용\n평균 연결법: 군집의 개체들 사이의 모든 거리 조합의 평균 사용\n와드 연결법: ESS(군집 내 제곱합)의 증가량이 최소가 되는 두 군집을 합침\n\n분할에 의한 방법\n\n다이아나 연결법\n\n\n군집의 갯수 결정: 1, 2번 단계에서 나온 dendrogram을 보고 알아서 결정\n분석의 타당성 검토\n\n\n\n\n비계층적(분할적) 군집 분석\n\n군집의 갯수를 알고 있을 때 사용\n판정기준을 최적화 시키는 방법으로 군집을 나눔\n한 번 분류된 개체도 재분류될 수 있음\nk-means\n\nk개의 군집을 사전에 설정\n군집의 초기 시작 포인트를 설정\n각 군집의 중심을 계산하여, 개체들을 다시 가장 가까운 군집에 재할당\n3 반복\n\n혼합분포군집\n\nk-means와 비슷하지만, 군집의 형태가 원형이 아닐 때도 사용 가능\n\nPAM\n\nk-means와 비슷하지만, 중심을 평균이 아닌 중앙값으로 설정\n연속형이 아닌 여러 종류의 변수가 혼합된 경우에도 사용할 수 있음\n\n\n\n\n타당성 지표\n\nsilhouette\nDunn index",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/11.html#연관-분석",
    "href": "posts/04_archives/adp_필기/notes/11.html#연관-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "연관 분석",
    "text": "연관 분석\n\n지지도: \\(\\frac{A \\cap B}{전체}\\)\n신뢰도: \\(\\frac{지지도}{A}\\)\n향상도: \\(\\frac{신뢰도}{B}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#what-is-airflow",
    "href": "posts/02_areas/air_flow/notes/00.html#what-is-airflow",
    "title": "Getting Started",
    "section": "What is Airflow",
    "text": "What is Airflow\n\nopen source platform to pragramatically author, schedule and monitor workflows\nNot a data processing framework\nNot a Real time streaming solution (only for batch processing)\nNot a data storage system\nand simple linear workflow might overkill",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#why-airflow",
    "href": "posts/02_areas/air_flow/notes/00.html#why-airflow",
    "title": "Getting Started",
    "section": "Why Airflow",
    "text": "Why Airflow\n\nautomation\nvisibility\nflexibility and scalability\nextensibility",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#core-components",
    "href": "posts/02_areas/air_flow/notes/00.html#core-components",
    "title": "Getting Started",
    "section": "Core Components",
    "text": "Core Components\n\nWebserver: provides UI\nScheduler: triggers tasks. ensure that task runs in correct time and order\nmeta database: memmory, communication between components\ntrigger: daemon that listens to external events and triggers tasks\nexecuter: traffic controller that decide how tasks are executed (sequential or parallel, local or remote)\nqueue\nworker",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#core-concepts",
    "href": "posts/02_areas/air_flow/notes/00.html#core-concepts",
    "title": "Getting Started",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nDAG\n\nDirected Acyclic Graph\ncollection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies\nno cycles in dependencies graph\n\n\n\nOperator\n\ndefines a single task in a workflow\ne.g. BashOperator, PythonOperator, EmailOperator, etc.\n\n\n\nTask / Task Instance\n\nspecific instance of an operator\nwhen operator assigned to a DAG, it becomes a task\n\n\n\nWorkflow\n\nentire process defined by DAG\nDAG = workflow",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/00.html#arcitecture",
    "href": "posts/02_areas/air_flow/notes/00.html#arcitecture",
    "title": "Getting Started",
    "section": "Arcitecture",
    "text": "Arcitecture",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html",
    "href": "posts/02_areas/machine_learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#details",
    "href": "posts/02_areas/machine_learning/index.html#details",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#tasks",
    "href": "posts/02_areas/machine_learning/index.html#tasks",
    "title": "Machine Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#참고-자료",
    "href": "posts/02_areas/machine_learning/index.html#참고-자료",
    "title": "Machine Learning",
    "section": "참고 자료",
    "text": "참고 자료\n\n이 책\nudemy machine learning 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/index.html#related-posts",
    "href": "posts/02_areas/machine_learning/index.html#related-posts",
    "title": "Machine Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/17.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/17.html#preprocessing",
    "title": "Eclat",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/17.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/17.html#modeling",
    "title": "Eclat",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'escalope', 'mushroom cream sauce'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'herb & pepper', 'ground beef'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'tomato sauce', 'ground beef'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'olive oil', 'light cream'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'olive oil', 'whole wheat pasta'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'pasta', 'shrimp'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    return list(zip(lhs, rhs, supports))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n2\npasta\nescalope\n0.005866\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n6\nlight cream\nolive oil\n0.003200\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n8\npasta\nshrimp\n0.005066\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Support')\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n2\npasta\nescalope\n0.005866\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n8\npasta\nshrimp\n0.005066\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n6\nlight cream\nolive oil\n0.003200",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#load-library-and-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#load-library-and-data",
    "title": "data preprocessing",
    "section": "Load Library and data",
    "text": "Load Library and data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/00-data.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nx\n\narray([['France', 44.0, 72000.0],\n       ['Spain', 27.0, 48000.0],\n       ['Germany', 30.0, 54000.0],\n       ['Spain', 38.0, 61000.0],\n       ['Germany', 40.0, nan],\n       ['France', 35.0, 58000.0],\n       ['Spain', nan, 52000.0],\n       ['France', 48.0, 79000.0],\n       ['Germany', 50.0, 83000.0],\n       ['France', 37.0, 67000.0]], dtype=object)\n\n\n\ny\n\narray(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n      dtype=object)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#taking-care-of-missing-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#taking-care-of-missing-data",
    "title": "data preprocessing",
    "section": "Taking care of Missing data",
    "text": "Taking care of Missing data\n\ndelete\nreplace\n\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x[:, 1:3])\nx[:, 1:3] = imputer.transform(x[:, 1:3])\nprint(x)\n\n[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#encoding-cagegorical-data",
    "href": "posts/02_areas/machine_learning/notes/01.html#encoding-cagegorical-data",
    "title": "data preprocessing",
    "section": "Encoding Cagegorical data",
    "text": "Encoding Cagegorical data\n\n단순히 categorical 변수를 1, 2, 3으로 변형하면 순서가 고려된 것으로 간주될 수 있다.\n그래서 [0, 0, 1], [1, 0, 1] 이런 식으로 one hot encoding을 진행한다.\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x)\n\n[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)\n\n[0 1 0 0 1 1 0 1 0 1]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "href": "posts/02_areas/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "title": "data preprocessing",
    "section": "Split dataset into training set and test set",
    "text": "Split dataset into training set and test set\n\nfeature scaling 이전에 진행되어야함. (test set은 모델이 모르는 정보가 되야하기 때문)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/01.html#feature-scaling",
    "href": "posts/02_areas/machine_learning/notes/01.html#feature-scaling",
    "title": "data preprocessing",
    "section": "feature scaling",
    "text": "feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n[[0.0 1.0 0.0 -1.2231822690784795 -1.074632541818236]\n [1.0 0.0 0.0 0.628120624661922 0.5410562913562817]\n [0.0 1.0 0.0 1.4215361505506654 1.5284216894073759]\n [0.0 1.0 0.0 0.09917694073609294 -0.19697441021726317]\n [1.0 0.0 0.0 -0.2975308222082788 0.09225383769669344]\n [0.0 0.0 1.0 -0.16529490122682156 -0.4463091066948125]\n [0.0 0.0 1.0 -1.6198900320228513 -1.6131954862097422]\n [1.0 0.0 0.0 1.157064308587751 1.1693797264797055]]\n\n\n\nprint(X_test)\n\n[[0.0 0.0 1.0 -0.06244474046346582 -1.2541535232820715]\n [1.0 0.0 0.0 -0.5620026641711934 -0.7155905788905655]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "href": "posts/02_areas/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "title": "Naive Bayes",
    "section": "독립성 가정이 필요한 수학적 이유",
    "text": "독립성 가정이 필요한 수학적 이유\nNaive Bayes는 베이즈 정리를 기반으로 합니다. 클래스 \\(C\\)와 특성 벡터 \\(X = (x_1, x_2, ..., x_n)\\)이 있을 때, 베이즈 정리는 다음과 같습니다:\n\\[P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\\]\n여기서 \\(P(C|X)\\)는 특성 \\(X\\)가 주어졌을 때 클래스 \\(C\\)일 확률입니다. 문제는 \\(P(X|C)\\)를 계산하기가 어렵다는 점입니다. 특성이 많을수록 가능한 \\(X\\) 조합의 수가 기하급수적으로 증가하기 때문입니다.\n이 문제를 해결하기 위해 Naive Bayes는 모든 특성이 서로 조건부 독립이라고 가정합니다. 즉:\n\\[P(x_i|C, x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i|C)\\]\n이 독립성 가정을 통해 \\(P(X|C)\\)를 다음과 같이 단순화할 수 있습니다:\n\\[P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C) = \\prod_{i=1}^{n} P(x_i|C)\\]\n이렇게 특성 간 독립성을 가정함으로써 복잡한 결합 확률을 개별 특성의 확률들의 곱으로 계산할 수 있게 되어 계산이 매우 단순해집니다. 이것이 바로 Naive Bayes에서 “naive(순진한)” 독립성 가정이 반드시 필요한 이유입니다.\n이제 Naive Bayes에서 독립성 가정이 필요한 이유가 수학적으로 명확하게 설명되었습니다. 이 설명을 통해 알 수 있듯이:\n\nNaive Bayes는 베이즈 정리를 사용하여 P(C|X)를 계산합니다.\n문제는 P(X|C)를 계산하는 것이 복잡하다는 점입니다.\n독립성 가정을 통해 P(X|C)를 개별 특성들의 조건부 확률 곱으로 단순화할 수 있습니다.\n이 단순화가 없다면, 특성의 조합이 많아질수록 계산이 기하급수적으로 복잡해집니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/11.html#preprocessing",
    "title": "Naive Bayes",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/11.html#modeling---linear",
    "title": "Naive Bayes",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GaussianNB?Documentation for GaussianNBiFittedGaussianNB()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#predict",
    "href": "posts/02_areas/machine_learning/notes/11.html#predict",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[61  4]\n [10 25]]\n\n\n0.86",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/11.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/11.html#predict-1",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[61  4]\n [10 25]]\n\n\n0.86",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/18.html",
    "href": "posts/02_areas/machine_learning/notes/18.html",
    "title": "Upper Confidence Bound",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Upper Confidence Bound"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/04.html#preprocessing",
    "title": "Polynorminal Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#linear-regression-model",
    "href": "posts/02_areas/machine_learning/notes/04.html#linear-regression-model",
    "title": "Polynorminal Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(x, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#polynorminal-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#polynorminal-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Polynorminal Linear Regression",
    "text": "Polynorminal Linear Regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=4)\nx_poly = poly.fit_transform(x)\nregressor2 = LinearRegression()\nregressor2.fit(x_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#visualize-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#visualize-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Linear Regression",
    "text": "Visualize Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor.predict(x), color='blue')\nplt.title('Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "href": "posts/02_areas/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Poly Linear Regression",
    "text": "Visualize Poly Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor2.predict(x_poly), color='blue')\nplt.title('Poly Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/13.html#preprocessing",
    "title": "Random Forest",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/13.html#modeling---linear",
    "title": "Random Forest",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(criterion='entropy', n_estimators=10)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#predict",
    "href": "posts/02_areas/machine_learning/notes/13.html#predict",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[54  4]\n [ 6 36]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/13.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/13.html#predict-1",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[54  4]\n [ 6 36]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/08.html#preprocessing",
    "title": "Logistic Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/08.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(x_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/08.html#predict",
    "href": "posts/02_areas/machine_learning/notes/08.html#predict",
    "title": "Logistic Regression",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[52  9]\n [12 27]]\n\n\n0.79",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/02.html#preprocessing",
    "title": "Simple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/02-data.csv')\n\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#train",
    "href": "posts/02_areas/machine_learning/notes/02.html#train",
    "title": "Simple Linear Regression",
    "section": "train",
    "text": "train\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#predict",
    "href": "posts/02_areas/machine_learning/notes/02.html#predict",
    "title": "Simple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/02.html#visualize",
    "title": "Simple Linear Regression",
    "section": "visualize",
    "text": "visualize\n\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(X_test, y_test, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/02.html#evaluate",
    "href": "posts/02_areas/machine_learning/notes/02.html#evaluate",
    "title": "Simple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9261621443754907",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#k-means-algorithm",
    "href": "posts/02_areas/machine_learning/notes/14.html#k-means-algorithm",
    "title": "k-means clustering",
    "section": "K-means++ algorithm",
    "text": "K-means++ algorithm\n\n시작점을 잘 선택하여 수렴 속도를 높이는 알고리즘\n초기 중심점을 선택할 때, 멀리 떨어진 중심점을 선택하도록 함\n\n첫 번째 중심점을 랜덤하게 선택\n나머지 중심점을 선택할 때, 각 데이터 포인트와 가장 먼 중심점을 선택\nk개의 중심점을 선택할 때까지 반복",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/14.html#preprocessing",
    "title": "k-means clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/14.html#modeling",
    "title": "k-means clustering",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n  cluster = KMeans(n_clusters=i, init='k-means++')\n  cluster.fit(x)\n  wcss.append(cluster.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Cluster')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster = KMeans(n_clusters=5, init='k-means++')\ny_kmeans = cluster.fit_predict(x)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/14.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/14.html#visualize",
    "title": "k-means clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], c='cyan', label='Cluster 5')\nplt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], s=100, c='black', label='Centroids')\nplt.legend()\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html",
    "href": "posts/02_areas/선형대수/index.html",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#details",
    "href": "posts/02_areas/선형대수/index.html#details",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#tasks",
    "href": "posts/02_areas/선형대수/index.html#tasks",
    "title": "선형대수",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#참고-자료",
    "href": "posts/02_areas/선형대수/index.html#참고-자료",
    "title": "선형대수",
    "section": "참고 자료",
    "text": "참고 자료\n\nKhan Academy 강의\n3Blue1Brown 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/index.html#related-posts",
    "href": "posts/02_areas/선형대수/index.html#related-posts",
    "title": "선형대수",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/06.html#linear-independence",
    "href": "posts/02_areas/선형대수/notes/06.html#linear-independence",
    "title": "linear independence",
    "section": "Linear independence",
    "text": "Linear independence\n\nDefinition\n\nDependence: one of the vectors in the set can be written as a linear combination of the others.\nIndependence: ⫬ dependence\n\n\n\nTheorem\nS = \\({v_1, v_2, ..., v_n}\\)\n\\(S\\) is linearly dependent ⟺ ∃(\\(c_i\\) is not 0) \\(c_1v_1 + c_2v_2 + ... + c_nv_n = 0\\) is \\(c_1 = c_2 = ... = c_n = 0\\).\n\nif \\(c_1 = c_2 = ... = c_n = 0\\), then \\(S\\) is linearly independent.\n벡터의 원소 수가 \\(n\\)인 경우, span(\\(S\\))의 차원은 \\(n\\) 이하이다. (나머지 벡터는 선형 결합으로 표현 가능)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear independence"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/05.html#선형결합",
    "href": "posts/02_areas/선형대수/notes/05.html#선형결합",
    "title": "선형결합과 생성",
    "section": "선형결합",
    "text": "선형결합\n벡터들의 상수배 합으로 만들 수 있는 벡터의 집합",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "선형결합과 생성"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#what-is-vector",
    "href": "posts/02_areas/선형대수/notes/04.html#what-is-vector",
    "title": "벡터와 공간",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있고, 2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\n\nvector의 수학적 표현\nvector는 ordered list인 tuple 형태로 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\ndomain과 dimension에 따라 vector는 다음과 같이 표현할 수 있다.\n\\[\n\\vec{v} ∈ R^2\n\\]\n\n1차원: \\(R^1\\)\n2차원: \\(R^2\\)\n3차원: \\(R^3\\)\nn차원: \\(R^n\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-합",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-합",
    "title": "벡터와 공간",
    "section": "vector의 합",
    "text": "vector의 합\nvector의 합은 각 성분별로 더한 결과를 반환한다.\n\n기하학적 의미\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-scalar-곱",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-scalar-곱",
    "title": "벡터와 공간",
    "section": "vector의 scalar 곱",
    "text": "vector의 scalar 곱\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#vector의-차",
    "href": "posts/02_areas/선형대수/notes/04.html#vector의-차",
    "title": "벡터와 공간",
    "section": "vector의 차",
    "text": "vector의 차\nvector의 차는 각 성분별로 뺀 결과를 반환한다.\n기하학적으로는 두 벡터의 끝점을 연결한 벡터가 된다.\n\\(\\vec{x} - \\vec{y}\\)는 y에서 x를 연결한 벡터가 된다.\n\\(\\vec{y} - \\vec{x}\\)는 x에서 y를 연결한 벡터가 된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/04.html#단위-벡터",
    "href": "posts/02_areas/선형대수/notes/04.html#단위-벡터",
    "title": "벡터와 공간",
    "section": "단위 벡터",
    "text": "단위 벡터\n\\[\n\\vec{v} = \\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\n위의 벡터를 단위 벡터의 합으로 만들면 다음과 같다.\n\\[\n\\hat{i} = \\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix},\n\\hat{j} = \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\\[\n\\vec{v} = 3\\hat{i} + 4\\hat{j}\n\\]\n\n\n\n\n\n\nScalar 배를 한 기저 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#what-is-linear-algebra",
    "href": "posts/02_areas/선형대수/notes/00.html#what-is-linear-algebra",
    "title": "what is linear algebra",
    "section": "what is linear algebra",
    "text": "what is linear algebra\n선형 방정식을 matrix와 vector로 표현해서 다루는 수학\n\\(ax^2 + bx + c = 0\\) (x)\n\\(ax_1 + bx_2 + c = 0\\) (0)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#what-is-vector",
    "href": "posts/02_areas/선형대수/notes/00.html#what-is-vector",
    "title": "what is linear algebra",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있다.\n2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\nvector는 수학적으로, 아래와 같이 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/00.html#example",
    "href": "posts/02_areas/선형대수/notes/00.html#example",
    "title": "what is linear algebra",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 연립 1차 방정식을 matrix와 vector로 표현해보자\n\\[\n\\underset{A}{\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}}\n\\underset{x}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} =\n\\begin{bmatrix}\n1x + 2y \\\\\n2x + 5y\n\\end{bmatrix} =\n\\underset{b}{\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "href": "posts/02_areas/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "title": "2-기초(2)",
    "section": "행렬의 곱을 바라보는 관점",
    "text": "행렬의 곱을 바라보는 관점\n\n내적으로 바라보기\n\n\\[\nA = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\quad (a_x = \\text{column vector})\n\\]\n\\[\nAB = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & b_3\n\\end{bmatrix} =\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3 \\\\\na_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\\na_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3\n\\end{bmatrix}\n\\]\n\nrank-1 matrix의 합\n\n\\[\nAB = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\nb_3^T\n\\end{bmatrix} =\na_1^Tb_1 + a_2^Tb_2 + a_3^Tb_3\n\\]\n\nColumn space로 바라보기\n\n\\[\nAx = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} = a_1x_1 + a_2x_2 + a_3x_3\n\\]\n\nRow space로 바라보기\n\n\\[\nx^TA = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix} = x_1a_1^T + x_2a_2^T + x_3a_3^T\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#span과-column-space",
    "href": "posts/02_areas/선형대수/notes/02.html#span과-column-space",
    "title": "2-기초(2)",
    "section": "span과 column space",
    "text": "span과 column space\n\ncolumn space: column vector들이 span하는 영역\nspan: linear combination으로 만들어지는 모든 벡터들의 집합\nlinear combination: vector들을 scalar 배 하고 더한 것\nlinear independent: span하는 vector들이 서로 독립적인 경우\n수학적 정의: \\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\) 일 때 \\(a_1 = a_2 = \\cdots = a_n = 0\\) 인 경우\nbasis: 어떤 공간을 이루는 필수적인 구성요소 (linear independent, span)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#항등행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#항등행렬",
    "title": "2-기초(2)",
    "section": "항등행렬",
    "text": "항등행렬\n\\(AI = IA = A\\)를 만족하는 행렬 \\(I\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#역행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#역행렬",
    "title": "2-기초(2)",
    "section": "역행렬",
    "text": "역행렬\n\\(Ax = b\\)를 만족하는 \\(x\\)를 찾는 것은 \\(A^{-1}Ax = A^{-1}b\\)를 만족하는 \\(x\\)를 찾는 것과 같다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#대각-행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#대각-행렬",
    "title": "2-기초(2)",
    "section": "대각 행렬",
    "text": "대각 행렬\ndiagonal을 제외한 모든 요소가 0인 행렬 (square, rectangular 모두 가능)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#orthogonal-행렬",
    "href": "posts/02_areas/선형대수/notes/02.html#orthogonal-행렬",
    "title": "2-기초(2)",
    "section": "Orthogonal 행렬",
    "text": "Orthogonal 행렬\n행렬의 모든 column들이 orthonormal vector인 경우\n\\(Q^{-1} = Q^T\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#행렬의-rank",
    "href": "posts/02_areas/선형대수/notes/02.html#행렬의-rank",
    "title": "2-기초(2)",
    "section": "행렬의 rank",
    "text": "행렬의 rank\nrank: 행렬이 가지는 independent한 column의 개수 → column space의 차원\nrank(A) = rank(A^T)\n\nfull-column rank: 해가 없거나 한 개 존재\nfull-row rank: 해가 무한하다\nfull rank: 해가 한 개 있다.\nrank-deficient: b가 column space에 속하지 않는 경우 해가 없고, 그렇지 않으면 해가 무한하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/02.html#null-space",
    "href": "posts/02_areas/선형대수/notes/02.html#null-space",
    "title": "2-기초(2)",
    "section": "Null space",
    "text": "Null space\n\\(Ax = 0\\)을 만족하는 모든 \\(x\\)의 집합\nA가 m x n 행렬이라면, dim(N(A)) = n - rank(A)\nnull space와 row space는 orthogonal하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html",
    "href": "posts/02_areas/kaggle/index.html",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#details",
    "href": "posts/02_areas/kaggle/index.html#details",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#tasks",
    "href": "posts/02_areas/kaggle/index.html#tasks",
    "title": "Kaggle",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#참고-자료",
    "href": "posts/02_areas/kaggle/index.html#참고-자료",
    "title": "Kaggle",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/index.html#related-posts",
    "href": "posts/02_areas/kaggle/index.html#related-posts",
    "title": "Kaggle",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html",
    "href": "posts/02_areas/42_seoul/index.html",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#details",
    "href": "posts/02_areas/42_seoul/index.html#details",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#tasks",
    "href": "posts/02_areas/42_seoul/index.html#tasks",
    "title": "42 Seoul",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/index.html#related-posts",
    "href": "posts/02_areas/42_seoul/index.html#related-posts",
    "title": "42 Seoul",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#intro",
    "href": "posts/02_areas/42_seoul/notes/01.html#intro",
    "title": "ft_transcendence - github action",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul 공통과정 6서클 과제\n\n\n42 Seoul 공통과정의 마지막 과제입니다. 이 프로젝트는 개발자가 선호하는 라이브러리와 프레임워크를 자유롭게 선택하여 구현할 수 있다는 점이 특징입니다.\n대형 협업 과제인 만큼, 과제에 명시되어있지 않지만 협업을 위한 툴도 공부해서 다양하게 적용해볼 수 있는 좋은 과제인것 같습니다. 저같은 경우에는 coursera, udemy 강의를 통해 agile 협업 방식과 github에서의 적용 방법에 대해 공부를 했고, 프로젝트 진행에 있어서 꽤 도움이 됐던걸로 기억합니다. 사실 프로젝트를 진행하다보니, agile 방식을 온전히 다 적용하기엔 적합하지 않다고 판단했지만, Kanban Board로 프로젝트를 관리하는 것 같은 부분은 꽤 유용하게 활용할 수 있었습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "title": "ft_transcendence - github action",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n해당 과제는 실시간 Pong 게임 매칭 웹사이트를 만드는게 목표입니다. 저는 이번 프로젝트에서 github action 설정, User Management Backend 설계와 42 API를 이용한 OAuth 인증, JWT 구현, Game History를 Block Chain으로 저장하는 파트를 담당했습니다.\n참고한 자료는 다음과 같습니다:\n\nGoogle Agile Project 관리\nGithub Action Docs\nGithub CLI Docs\nDjango udemy 강좌\nDjango Rest Framework Docs\nDjango Simple JWT\nJWT Token 탈취 대응 시나리오\nmicro service에서 JWT 활용 방법\nRefresh Token을 사용해야 하는 이유\nCookie에서의 same site 옵션\nBitcoin 백서\nSolidity Udemy 강의\nSolidity Docs\nnomad coder 블록체인 시리즈\n블록체인 강의\n\n\n\n\n\n\n\n이 포스팅에서는 github action setting, jwt, block chain 부분만 다루겠습니다.\n전체 코드는 비공개 되어있는 상태입니다.\n\n\n\n\n\nGithub Action Setting\ngithub를 이용해서 agile 방법론을 적용할 수 있도록 의도했고, 자동화와 template을 이용해 통일성 있는 구조를 유지하려고 했습니다.\n1. 회의를 통해 진행해야 하는 작업을 Kanban board에 정리한다.\n\n\n\nGithub Kanban Board\n\n\n각각의 column에는 다음과 같은 내용이 들어갑니다.\n\nDiscussion: 논의가 필요한 작업. 개개인이 자유롭게 올릴 수 있습니다\nBacklog: Discussion에 있는 내용 중 구현하기로 회의에서 정한 작업\nReady: Back log에 있는 작업 중 이번 Sprint에서 구현할 작업들\nIn Progress: Ready에 있는 작업 중 누군가가 작업중인 것\nDone: master branch에 merge가 완료된 작업\n\n자세한 내용은 meeting 부분을 참고해 주세요.\n참고로 Disccusion에 작업을 올리는 방법은 template에 맞게 issue를 올리면 됩니다.\n\n\n\nDiscussion template\n\n\n아래와 같이 설정 파일을 만들어서 ‘.github/ISSUE_TEMPLATE/’ 폴더 안에 저장하면 issue create 시 자동으로 template이 뜨게 할 수 있습니다.\nname: New discussion\ndescription: new discussion\ntitle: \"[DISCUSSION]\"\nlabels: [\"enhancement\"]\nprojects: [\"org_name/5\"]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        해당 기능과 관련된 request가 이미 존재하는지 확인해주세요.\n  - type: textarea\n    id: story\n    attributes:\n      label: Story\n      description: 해당 기능에 대한 설명이나 필요한 배경을 작성해주세요.\n      placeholder: 자유로운 양식으로 작성해주세요.\n    validations:\n      required: true\n2. Kanban board를 보고 개인이 능동적으로 고유 브랜치에 작업을 진행한다.\n\n\n\n빨간 밑줄 부분을 설정해줍니다.\n\n\nKanban board의 Ready section에 있는 작업을 클릭해서 들어간 후, assignees를 본인으로 선택해서 작업하면 됩니다. task completion criteria라는 내용이 보이는데, 이는 회의를 통해 결정하는 것으로, 나중에 작업이 완료되고 pull request 시, 평가자가 작업에 완성도에 대해 판단할 수 있는 기준으로 제공됩니다.\n자동화 코드는 아래와 같이 구현했습니다.\nname: Create branch\non:\n  issues:\n    types: [ assigned ]\n  pull_request:\n    types: [ opened, closed ]\njobs:\n  create_issue_branch_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n\n      # gh 명령어를 이용해 project의 상태를 In progress로 수정해줍니다.\n      - name: Project in-progress\n        if: github.event.action == 'assigned'\n        run: |\n          PROJECT_ID=$(gh project view 5 --owner organization-for-practice --format=json --jq '.id')\n          ITEM_ID=$(gh project item-list 5 --owner organization-for-practice --format=json --jq \".items[] | select(.content.number == ${NUMBER}) | .id\")\n          FIELD_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].id')\n          SINGLE_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].options[] | select(.name == \"In progress\") | .id')\n          gh project item-edit --id ${ITEM_ID} --field-id ${FIELD_ID} --single-select-option-id ${SINGLE_ID} --project-id ${PROJECT_ID}\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUMBER: ${{ github.event.issue.number }}\n\n      # assign한 작업에 대한 branch를 새로 만들어줍니다.\n      - name: Create Issue Branch\n        uses: robvanderleek/create-issue-branch@main\n        env:\n          GITHUB_TOKEN: ${{ steps.generate_token.outputs.token }}\n위의 코드는 assign한 작업을 Ready column에서 In progress column으로 옮겨주고, 자동으로 작업할 branch를 만들어줍니다.\nbranch 자동 생성은 이 workflow를 사용하였고, 적용 시 아래와 같이 브랜치가 생성됩니다.\nautoLinkIssue: true\nautoCloseIssue: true\nbranchName: tiny\ncommentMessage: |\n  \\\"${branchName}\\\" branch 생성 완료.\n  해당 branch를 통해서 main에 pull request 올려주세요.\nbranches:\n  - label: 'task list'\n    prefix: feature/${issue.title[12,27],}/\n    copyIssueAssigneeToPR: true\n  - label: 'bug'\n    prefix: hot_fix/${issue.title[6,21],}/\n    copyIssueAssigneeToPR: true\n  - label: '*'\n    skip: true\n위의 config 파일을 작성해주면 아래와 같이 브랜치가 생성됩니다.\n\n\n\n자동 생성된 branch\n\n\n이름도 자동으로 생성되게 해서 convention을 지켜야 한다는 부담을 줄여줬습니다.\n3. 작업이 완료되면, 모든 조건을 충족하는지 확인한 후, master에 merge 한다.\n\n\n\npull request 화면\n\n\n작업이 완료됬다고 판단되면 위 화면과 같이 pull request를 생성하고, Reviewer를 설정해주면 됩니다.\n\n\n\ntask completion criteria\n\n\n그러면 이전에 설정했던 기준들이 자동으로 불러와지고, 모든 항목에 체크가 완료되어야 merge를 할 수 있게 설정했습니다. 구현 코드는 아래와 같습니다.\nname: Master merge rutine\non:\n  pull_request_target:\n    types: [ opened, synchronize ]\n    branches:\n      - master\nenv:\n  PR_NUM: ${{ github.event.pull_request.number }}\n  GH_REPO: ${{ github.repository }}\njobs:\n  get_checklist:\n    runs-on: ubuntu-latest\n    if: github.event.action == 'opened'\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n      - name: Get issue\n        id: issue_num\n        env:\n          BRANCH: ${{ github.event.pull_request.head.ref }}\n        run: |\n          echo $BRANCH | grep -o 'feature\\/.*\\/i[0-9]\\+' || echo $BRANCH | grep -o 'hot_fix\\/.*\\/i[0-9]\\+'\n          TMP=$(echo $BRANCH | grep -o 'i[0-9]\\+')\n          echo \"NUMBER=${TMP#i}\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Get issue body\n        id: issue_body\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUM: ${{ steps.issue_num.outputs.number }}\n        run: |\n          echo \"CONTENTS&lt;&lt;EOF\" &gt;&gt; $GITHUB_OUTPUT\n          gh issue view ${NUM} --json body --jq '.body' &gt;&gt; $GITHUB_OUTPUT\n          echo \"EOF\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Update checklist\n        run: |\n          gh pr comment $PR_NUM --body \"${BODY}\"\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          BODY: \"${{ steps.issue_body.outputs.contents }}\"\nmerge가 완료된 branch는 자동으로 삭제가 되도록 설정을 해주었습니다.\n이제 아래는 실제 프로젝트를 진행할 때 만들었던 rule들입니다.\n\n1. work flow\ngithub flow로 진행됩니다.\n\n\n\ngithub flow\n\n\n\n매 작업은 master branch의 HEAD를 기반으로 이루어집니다.\npr을 올리지 않는 개인 작업용 local branch는 자유롭게 생성해주세요.\nmaster에 직접적인 push는 관리자를 제외하고는 불가능합니다.\nmaster에 대한 merge는 squash merge로 진행됩니다.\n그 외의 merge는 rebase로 진행해주세요.\n\n\n\n2. work\n\nkanban board의 'Ready' 섹션에서 하나를 정해서 새로운 기능에 대한 작업을 진행해주세요.\n선택한 작업은 assignees에 자신의 팀원을 등록 후, Start Date를 해당 날짜로 설정해주세요.\nassignees 등록이 완료되면 자동으로 target branch가 생성됩니다.\n해당 branch에 팀원들이 필요한 기능들을 자유로운 방식으로 구현한 후, master branch에 merge 해주세요.\n단, 해당 branch에 대한 merge는 rebase로 진행해주세요.\nhot_fix issue나, new feature request issue는 discussion의 필요성이 있을 경우에 등록해주세요.\n작업 중, 현재 작업하는 범위 외에서 추가적인 기능이 필요할 경우 관련 issue에 comment를 남기거나, reopen 해주세요.\n\n\n\n3. commit message convention\n아래의 명령어를 입력해주세요\ngit config commit.template .github/COMMIT_MESSAGE_TEMPLATE\n이후, -m 옵션 없이 ’git commit’으로 message를 입력해주세요.\n\n\nCOMMIT_MESSAGE_TEMPLATE\n\n# commit message template\n# ▼ &lt;Title&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;body&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;footer&gt; 작성\n\n\n# About Convention\n#   &lt;Title&gt;\n#       - 필수로 입력해주세요\n#       - 형식: &lt;type&gt;: &lt;short summary&gt;\n#\n#       &lt;type&gt;\n#           - config: 설정 관련 파일 작성 또는 변경\n#           - docs: 문서 변경사항\n#           - feat: 새로운 기능\n#           - fix: 버그 수정\n#           - refactor: 기능 추가나 버그 수정이 아닌 변경 사항\n#           - remove: 코드나 파일 제거\n#           - style: 스타일 작성 또는 수정\n#           - test: 누락된 테스트 추가 또는 기존 테스트 수정\n#           - core: 기능 구현 외 시스템 관련 작업\n#\n#       &lt;short summary&gt;\n#           - 변경 사항에 대한 간단한 설명\n#           - 첫글자 소문자, 현재 시제, 명령문으로 마지막에 .(마침표) 없이 작성\n#\n#   &lt;body&gt;\n#       - 선택적으로 입력 해주세요\n#       - 현재 시제, 명령문으로 작성\n#       - 변경 사항의 동기(왜)를 설명\n#       - 변경 효과를 설명하기 위해 이전 동작과 현재 동작의 비교를 포함할 수 있음\n#\n#   &lt;footer&gt;\n#       - 선택적으로 입력 해주세요\n#       - 해당 commit과 관련된 task의 issue 번호들을 적어주세요\n#       - 'bug'나 'task list' label이 붙은 issue는 제외해주세요\n#       - ex) closes #&lt;issue 번호&gt; closes #&lt;issue 번호&gt; ...\n\n\n\n\n\n\n\ncommit message template은 이 사이트를 참고해서 만들었습니다.\n\n\n\n\n\n4. pull request\n\npull request는 500줄의 코드를 넘어가지 않게 작성 바랍니다.\n모든 check list를 통과한 request만 master에 merge 가능합니다.\nreviewers에는 해당 작업과 관련된 domain의 팀원을 선택해주세요. 최소 1명 이상의 동료에게 평가를 받은 request만 merge 가능합니다.\n\n\n\n5. meeting\n\ndaily meeting\n\n매일 정해진 시간에 팀원들은 각각 다음과 같은 사안에 대해 논의합니다.\n\n개인이 어제 작업한 내용\n개인이 오늘 작업할 내용\n개인이 현재 도움이 필요한 내용\n\n이후, 새로운 내용이 추가된 ('Disccusion' 섹션에 있는) issue 중 다음과 같은 내용에 대해 논의합니다.\n\n해당 issue가 유효한가\n추가적으로 필요하거나 필요 없는 내용\n해당 issue의 priority (매우 급함 / 급함 / 안 급함)\n해당 issue의 estimate (작업하는데 필요한 노력의 정량적인 수치)\n\n추가적으로, project의 'Back log' 항목에서 'Ready' 항목으로 추가해야 할 작업에 대해 논의하거나 'Ready' 항목에서 'Back log' 항목으로 제외할 작업에 대해 논의할 수 있습니다.\n\nsprint planning / retrospective\n\n2주에 한번 진행.\n이전 sprint에 대한 평가와 이후 sprint를 위한 계획을 세웁니다.\n\nplanning\n\nProject의 'Back log' 항목 중 본격적으로 작업을 진행할 항목을 정합니다.\ndaily meeting 시간을 조정할 수 있습니다.\n\nretrospective\n\n이전 sprint의 문제점에 대해 서로 의논해봅니다.\n\n\n\n\n\n\n\n\n\n프로젝트를 하다보니, 생각보다 진행 속도가 빨라서 2주에 한번 진행하는 sprint는 유명무실해져버렸습니다. 실제로는 daily meeting만 진행을 했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/01.html#outro",
    "href": "posts/02_areas/42_seoul/notes/01.html#outro",
    "title": "ft_transcendence - github action",
    "section": "outro",
    "text": "outro\n내용이 너무 길어져서 2편에 계속 포스팅 하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#intro",
    "href": "posts/02_areas/42_seoul/notes/09.html#intro",
    "title": "cloud-1 코드 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n개념 설명에 이어서 진행하도록 하겠습니다.\n\n\n\n\n\n\n전체 코드는 github repo에서 확인하실 수 있습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "title": "cloud-1 코드 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\npre requirements\n이 프로젝트를 진행하기 위해 필요한 것들은 다음과 같습니다.\n\nAWS IAM 계정\nPacker\nTerraform\nAnsible\njq\nboto3\n\n\n\nbuild\n최종 build는 (42 seoul 사람에게 익숙한) makefile을 사용했습니다.\n\n\n\n\n\n\n제가 아직 로컬에서 돌려볼만한 다른 build 툴을 배우지 않아서 makefile을 사용하긴 했지만, 사실 c언어도 아니고..이 과제 구현에서 이 tool이 그렇게 어울리진 않은거 같긴 합니다.\n\n\n\n\n\n.env\n\n# only 1 line variable is allowed\n\nAWS_REGION=\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nSERVER_INSTANCE_COUNT=\n\n# public subnet에 접근할 수 있는 ip address를 지정해줍니다.\nSSH_IP=\n\n# public subnet에 접근할 때 사용할 ssh key path를 지정해줍니다.\nSSH_PUBLIC_KEY_PATH=\nSSH_PRIVATE_KEY_PATH=\n\n# docker compose setting\nMYSQL_USER=\nMYSQL_PASSWORD=\nMYSQL_ROOT_PASSWORD=\nDATABASE_NAME=\nSITE_TITLE=\nADMIN_NAME=\nADMIN_PASSWORD=\nADMIN_EMAIL=\nUSER_NAME=\nUSER_PASSWORD=\nUSER_EMAIL=\n\n\n\nMakefile\n\n# .env의 내용들을 makefile의 변수로 load 해줍니다.\n\ninclude .env\nexport\n\n먼저 필요한 변수들을 모두 .env에 저장해 한번에 관리할 수 있게 구현했습니다. 저장된 .env 내용은 makefile에서 위의 명령어로 불러와 build 명령어 실행시 사용할 수 있게 했습니다.\nmakefile이 .env 파일을 읽을 때 한 줄씩 읽기 때문에, 위의 방식으로 구현하면 여러 줄에 걸친 환경변수는 사용하기 어려울 수 있습니다. (그럴땐 그냥 makefile 말고 다른 tool을 쓰면 됩니다)\n\n\nMakefile\n\n.PHONY: provision deploy all destroy re build_ami\n\nall: build_ami provision deploy\n\nbuild_ami: packer\n    packer init $(PACKER_PATH)/database.pkr.hcl\n    @PKR_VAR_AWS_REGION=$(AWS_REGION) \\\n    PKR_VAR_MYSQL_USER=$(MYSQL_USER) \\\n    PKR_VAR_MYSQL_PASSWORD=$(MYSQL_PASSWORD) \\\n    PKR_VAR_DATABASE_NAME=$(DATABASE_NAME) \\\n    PKR_VAR_MYSQL_ROOT_PASSWORD=$(MYSQL_ROOT_PASSWORD) \\\n    packer build $(PACKER_PATH)/database.pkr.hcl\n\nprovision: build_ami terraform\n    terraform -chdir=$(PROVISION_PATH) init\n    @TF_VAR_AWS_REGION=$(AWS_REGION) \\\n    TF_VAR_SERVER_INSTANCE_COUNT=$(SERVER_INSTANCE_COUNT) \\\n    TF_VAR_SSH_IP=$(SSH_IP) \\\n    TF_VAR_SSH_PUBLIC_KEY_PATH=$(SSH_PUBLIC_KEY_PATH) \\\n    terraform -chdir=$(PROVISION_PATH) apply -auto-approve\n\ndeploy: ansible\n    @DB_PRIVATE_IP=\"$(shell terraform -chdir=$(PROVISION_PATH) output -json db_private_ip | jq -r '.[]' | tr '\\n' ' ')\" \\\n    ANSIBLE_HOST_KEY_CHECKING=False \\\n    ANSIBLE_REMOTE_USER=ubuntu \\\n    AWS_DEFAULT_REGION=$(AWS_REGION) \\\n    ANSIBLE_PYTHON_INTERPRETER=auto_silent \\\n    ansible-playbook \\\n    -i $(DEPLOY_PATH)/inventories \\\n    --private-key=$(SSH_PRIVATE_KEY_PATH) \\\n    $(DEPLOY_PATH)/server.yml \n\nbuild 과정은 ami 생성, provision, ansible deploy 순서로 진행됩니다.\n각 과정에 필요한 변수들은 명령어 수행 시 환경변수로 제공해줍니다. 대표적으로 ansible의 경우, provision 이후 생성된 database ec2의 private ip를 전달하고 있습니다.\n\n\nPacker 코드\n이 프로젝트에서는 데이터베이스 서버를 Private subnet에 위치시키고, Public subnet의 EC2만 이 데이터베이스에 접근할 수 있도록 설계했습니다. Private subnet에 있는 서버는 SSH 접근이 제한되기 때문에 Ansible로 직접 설정하기는 어렵습니다. 이런 경우 Packer로 미리 설정된 AMI를 생성하는 방법을 생각해볼 수 있습니다.\n구현한 Packer 파일 구조는 아래와 같습니다.\npacker/\n├── database.pkr.hcl\n└── ansible/\n    ├── _requirements/                      # docker compose setting files\n    ├── roles/setting_docker/tasks\n    │   └── main.yml\n    └── database.yml                        # playbook\n먼저 기본 이미지로 Ubuntu 20.04를 사용하도록 작성했습니다.\n\n\ndatabase.pkr.hcl\n\nsource \"amazon-ebs\" \"database\" {\n  region  = var.AWS_REGION\n  profile = \"default\"\n\n  ami_name      = \"hyunghki-database-${formatdate(\"YYYYMMDDhhmmss\", timestamp())}\"\n  instance_type = \"t2.micro\"\n  source_ami_filter {\n    filters = {\n      name                = \"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"\n      root-device-type    = \"ebs\"\n      virtualization-type = \"hvm\"\n    }\n    most_recent = true\n    owners      = [\"099720109477\"]\n  }\n  ssh_username = \"ubuntu\"\n}\n\nPacker는 기본적으로 이미지 생성을 위한 최소한의 기능만 제공하지만, 다양한 플러그인을 지원합니다. 여기서는 Ansible 플러그인을 사용하여 데이터베이스 서버 설정을 자동화했습니다.\n\n\ndatabase.pkr.hcl\n\nbuild {\n  sources = [\"source.amazon-ebs.database\"]\n\n  provisioner \"ansible\" {\n    playbook_file = \"${path.root}/ansible/database.yml\"\n    user = \"ubuntu\"\n    ansible_env_vars = [\n      \"ANSIBLE_HOST_KEY_CHECKING=False\",\n      \"MYSQL_USER=${var.MYSQL_USER}\",\n      \"MYSQL_PASSWORD=${var.MYSQL_PASSWORD}\",\n      \"DATABASE_NAME=${var.DATABASE_NAME}\",\n      \"MYSQL_ROOT_PASSWORD=${var.MYSQL_ROOT_PASSWORD}\",\n      \"ANSIBLE_PYTHON_INTERPRETER=auto_silent\"\n    ]\n  }\n}\n\n\n\nansible/database.yml\n\n- hosts: all\n  gather_facts: false\n  become: true\n  roles:\n    # docker compose를 machine에 설치해줍니다.\n    - role: setting_docker\n\n  tasks:\n    # docker compose에 필요한 파일들을 옮겨줍니다.\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    # 적절한 환경변수와 함께 docker compose 명령어를 실행합니다.\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        MYSQL_ROOT_PASSWORD: \"{{ lookup('env', 'MYSQL_ROOT_PASSWORD') }}\"\n\n이렇게 Ansible과 Packer를 조합하면 멱등성이 보장되는 안정적인 서버 이미지를 생성할 수 있습니다.\n참고로 packer에서 ansible plugin을 사용할 때 taget host를 ami가 build되는 임시 EC2로 간주하기 때문에, inventory는 사용하지 않습니다. 자세한 내용은 ansible part를 참고해주세요.\n\n\nTerraform 코드\n이제 본격적으로 provision을 해보겠습니다. 잠시 전체적인 구조를 다시 한번 보겠습니다.\n\n\n\n구현 aws 구조\n\n\n필요한 리소스는 VPC, subnet, security group, ec2 입니다.\nserver ec2와 database ec2는 환경변수 SERVER_INSTANCE_COUNT에 지정된 갯수 만큼 생성됩니다. database ec2는 이전 단계에서 생성한 ami를 사용해줍니다.\npublic, private subnet의 갯수는 임의로 생성했습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── main/\n│   ├── main.tf\n│   ├── data.tf\n│   ├── output.tf\n│   └── variables.tf\n└── modules/network/\n    ├── main.tf\n    ├── output.tf\n    └── variables.tf\nmain.tf에서는 aws_instance를 생성하고, 그 외 VPC, subnet과 같은 리소스는 network module로 분리해서 생성했습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_vpc\" \"main_vpc\" {\n  cidr_block           = \"10.0.0.0/16\"\n  instance_tenancy     = \"default\"\n  enable_dns_hostnames = \"true\"\n}\n\nresource \"aws_subnet\" \"public-1\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.1.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\nresource \"aws_subnet\" \"public-2\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.2.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}c\"\n}\n\nresource \"aws_subnet\" \"private\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.3.0/24\"\n  map_public_ip_on_launch = \"false\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\n먼저 VPC와 subnet을 생성합니다.\ncidr_block은 private ip 중에서 겹치지 않는 범위로 지정해줍니다.\n\n\n\n\n\n\nPrivate IP ranges\n\n\n\n\nClass A: 10.0.0.0–10.255.255.255\nClass B: 172.16.0.0–172.31.255.255\nClass C: 192.168.0.0–192.168.255.255\n\n\n\nPublic subnet이 인터넷과 통신하기 위해서는 Internet Gateway와 Route Table이 필요합니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_internet_gateway\" \"gate_way\" {\n  vpc_id = aws_vpc.main_vpc.id\n}\n\nresource \"aws_route_table\" \"public_route_table\" {\n  vpc_id = aws_vpc.main_vpc.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.gate_way.id\n  }\n}\n\nresource \"aws_route_table_association\" \"public-1\" {\n  subnet_id      = aws_subnet.public-1.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\nresource \"aws_route_table_association\" \"public-2\" {\n  subnet_id      = aws_subnet.public-2.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\n모든 외부 트래픽을 Internet Gateway로 보내도록 Route Table을 설정하고, 이를 두 개의 Public subnet에 연결했습니다.\n참고로 VPC 내부 통신은 자동으로 라우팅됩니다. 같은 VPC 안에 있는 리소스들은 VPC의 기본 라우팅 테이블을 통해 서로 통신할 수 있기 때문에 내부 통신을 위한 route table은 따로 생성하지 않았습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_security_group\" \"server_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"server_sg\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = var.SSH_CIDR_BLOCKS\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_security_group\" \"database_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"efs_sg\"\n\n  ingress {\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.server_sg.id]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n마지막으로 security group입니다.\nserver ec2의 ssh 접근은 환경변수를 통해 ansible을 실행하는 머신의 ip에서만 접근 가능하도록 설정해줬습니다.\ndatabase ec2는 server ec2만 접근할 수 있도록 설정했습니다.\n\n\nmain/main.tf\n\n# 사용자가 지정한 경로의 ssh key를 사용해 ec2에 접근 가능하도록 설정했습니다.\nresource \"aws_key_pair\" \"my_labtop\" {\n  key_name   = \"my_labtop\"\n  public_key = file(var.SSH_PUBLIC_KEY_PATH)\n}\n\nmodule \"network\" {\n  source = \"../modules/network\"\n\n  AWS_REGION           = var.AWS_REGION\n  SSH_CIDR_BLOCKS      = [\"${var.SSH_IP}/32\"]\n}\n\nresource \"aws_instance\" \"server\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.latest_ubuntu.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.server_sg_id]\n  # subnet은 2개를 번걸아가면서 사용하도록 설정했습니다.\n  subnet_id              = module.network.public_subnets[count.index % 2]\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"serverNode\"\n  }\n}\n\nresource \"aws_instance\" \"database\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.database_ami.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.database_sg_id]\n  subnet_id              = module.network.private_subnets\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"dbNode\"\n  }\n}\n\n최종적으로 main.tf에서 network module을 불러와서 필요한 리소스를 생성한 후, server와 database ec2를 생성했습니다.\n\n\nmain/data.tf\n\ndata \"aws_ami\" \"latest_ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"]\n}\n\ndata \"aws_ami\" \"database_ami\" {\n  most_recent = true\n  owners = [\"self\"]\n  filter {\n    name = \"name\"\n    values = [\"hyunghki-database-*\"]\n  }\n  filter {\n    name = \"root-device-type\"\n    values = [\"ebs\"]\n  }\n  filter {\n    name = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nserver ec2는 기본 ubuntu 20.04 이미지를 사용하고, database ec2는 이전에 생성한 ami를 사용했습니다.\n\n\nansible 코드\n이제 필요한 설정을 진행하겠습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── _requirements/                      # docker compose setting files\n├── inventories/\n│   └── aws_ec2.yml\n├── roles/setting_docker/tasks\n│   └── main.yml\n└── server.yml\n먼저 용어를 알아야 합니다.\n\nInventory (인벤토리)\n인벤토리는 Ansible이 관리할 호스트(서버)의 목록입니다. 호스트를 그룹으로 묶어 관리할 수 있습니다.\nPlaybook (플레이북)\n플레이북은 Ansible에서 작업을 정의하는 YAML 파일입니다. 플레이북은 하나 이상의 플레이로 구성되며, 각 플레이는 특정 호스트 그룹에 대해 수행할 작업(task)을 정의합니다.\nRole (롤)\n롤은 Ansible에서 재사용 가능한 구성 단위입니다. 플레이북을 모듈화하고 구조화하여 재사용성을 높이는 데 사용됩니다.\n\nInventory에서 server 그룹을 정의한 후, playbook으로 docker compose 환경을 설정하겠습니다.\n\n\naws_ec2.yml\n\nplugin: aws_ec2\nkeyed_groups:\n  - key: tags\ncompose:\n  ansible_host: public_ip_address\nleading_separator: False\nfilters:\n  instance-state-name: running\n\nAWS EC2 동적 인벤토리 설정입니다. Terraform으로 생성한 EC2 인스턴스들을 자동으로 관리할 수 있습니다.\n\n\nserver.yml\n\n- hosts: \"Name_serverNode\"\n  gather_facts: false\n  become: true\n  roles:\n    - role: setting_docker\n  tasks:\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    - name: Split array values from DB_PRIVATE_IP\n      set_fact:\n        target: \"{{ lookup('env', 'DB_PRIVATE_IP') | split(' ') }}\"\n\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        DOMAIN_NAME: \"{{ ansible_host }}\"\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        SITE_TITLE: \"{{ lookup('env', 'SITE_TITLE') }}\"\n        ADMIN_NAME: \"{{ lookup('env', 'ADMIN_NAME') }}\"\n        ADMIN_PASSWORD: \"{{ lookup('env', 'ADMIN_PASSWORD') }}\"\n        ADMIN_EMAIL: \"{{ lookup('env', 'ADMIN_EMAIL') }}\"\n        USER_NAME: \"{{ lookup('env', 'USER_NAME') }}\"\n        USER_PASSWORD: \"{{ lookup('env', 'USER_PASSWORD') }}\"\n        USER_EMAIL: \"{{ lookup('env', 'USER_EMAIL') }}\"\n        DB_PRIVATE_IP: \"{{ target[ansible_play_hosts.index(inventory_hostname)] }}\"\n\n    - name: all done message\n      debug:\n        msg: \"https://{{ ansible_host }}\"\n\n’Name’이 ’serverNode’인 인스턴스들만 선택하여 설정을 진행하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#실행",
    "href": "posts/02_areas/42_seoul/notes/09.html#실행",
    "title": "cloud-1 코드 설명",
    "section": "실행",
    "text": "실행\n먼저 .env 파일에 환경변수를 설정해줍니다.\nip 정보도 알아낸 후, SSH_IP에 설정해줍니다.\n\n\n\nnaver에 내 ip 검색\n\n\n\n\n.env\n\n# only 1 line variable is allowed\nAWS_REGION=ap-northeast-2\nAWS_ACCESS_KEY_ID=********************\nAWS_SECRET_ACCESS_KEY=********************\nSERVER_INSTANCE_COUNT=2\nSSH_IP=121.135.181.56\nSSH_PUBLIC_KEY_PATH=~/.ssh/id_rsa.pub\nSSH_PRIVATE_KEY_PATH=~/.ssh/id_rsa\nMYSQL_USER=dudu\nMYSQL_PASSWORD=secret\nMYSQL_ROOT_PASSWORD=secret\nDATABASE_NAME=cloud\nSITE_TITLE='hyunghki blog'\nADMIN_NAME=admin\nADMIN_PASSWORD=secret\nADMIN_EMAIL=admin@example.com\nUSER_NAME=user\nUSER_PASSWORD=secret\nUSER_EMAIL=user@example.com\n\n그후 make 명령어를 입력하면 자동으로 build가 진행됩니다.\n\n\n\n명령어 실행 결과\n\n\nbuild가 완료되면 완료 메세지의 ip로 접속해줍니다.\n\n\n\n접속 페이지\n\n\nwordpress 접속 페이지가 잘 뜨는 것을 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#결과",
    "href": "posts/02_areas/42_seoul/notes/09.html#결과",
    "title": "cloud-1 코드 설명",
    "section": "결과",
    "text": "결과\n\n\n\n최종 점수\n\n\n\n\n\n최종 평가",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/09.html#outro",
    "href": "posts/02_areas/42_seoul/notes/09.html#outro",
    "title": "cloud-1 코드 설명",
    "section": "outro",
    "text": "outro\n솔직히 일반적으로 사용되는 cloud 구조를 적용한건 아니긴 하지만, 과제에 맞춰서 진행하기 위해 고민하는 과정에서 다양한 구조를 적용해봤는데, 그 과정이 나름 학습에 도움이 된거 같습니다. 이 분야에 공부를 꽤 했고, 그 내용들을 다양하게 고민하며 적용해보고 싶다면 이 프로젝트가 괜찮은 선택지가 될 수도 있어 보입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html",
    "href": "posts/02_areas/deep_learning/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#details",
    "href": "posts/02_areas/deep_learning/index.html#details",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#tasks",
    "href": "posts/02_areas/deep_learning/index.html#tasks",
    "title": "Deep Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#참고-자료",
    "href": "posts/02_areas/deep_learning/index.html#참고-자료",
    "title": "Deep Learning",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/index.html#related-posts",
    "href": "posts/02_areas/deep_learning/index.html#related-posts",
    "title": "Deep Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#신경망이란",
    "href": "posts/02_areas/deep_learning/notes/01.html#신경망이란",
    "title": "신경망",
    "section": "신경망이란",
    "text": "신경망이란\n\n퍼셉트론에서 가중치를 자동으로 학습하는 방법이다.\n입력층, 은닉층, 출력층으로 구성된다.\n\n앞에서 살펴본 퍼셉트론 함수를 다시 살펴보자.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때, \\(y = h(b + w_1x_1 + w_2x_2)\\)로 표현하면 다음과 같이 표현할 수 있다.\n\\[\nh(x) = \\begin{cases} 0 & (x ≤ 0) \\\\ 1 & (x &gt; 0) \\end{cases}\n\\]\n이때 \\(h(x)\\)는 활성화 함수(activation function)라고 한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#활성화-함수",
    "href": "posts/02_areas/deep_learning/notes/01.html#활성화-함수",
    "title": "신경망",
    "section": "활성화 함수",
    "text": "활성화 함수\n\n계단 함수(step function): 앞서 살펴본 함수\n\n\nimport numpy as np\n\ndef step_function(x):\n    return np.array(x &gt; 0, dtype=int)\n\nprint(step_function(np.array([-1.0, 1.0, 2.0])))\n\n[0 1 1]\n\n\n\nimport matplotlib.pyplot as plt\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = step_function(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n시그모이드 함수: \\(h(x) = \\frac{1}{1 + \\exp(-x)}\\)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = sigmoid(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n계단 함수와의 차이점\n\n시그모이드가 더 부드러움\n\n계단 함수와의 공통점\n\n입력이 작을 때는 0에 가깝고, 입력이 커지면 1에 가까워짐\n입력이 아무리 작거나 커도 출력은 0에서 1 사이\n비선형 함수 (선형 함수는 은닉층 업싱도 똑같이 구현할 수 있기 때문에 신경망에서 활성화 함수는 반드시 비선형 함수여야 함)\n\nReLU 함수: \\(h(x) = \\begin{cases} x & (x &gt; 0) \\\\ 0 & (x ≤ 0) \\end{cases}\\)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\nSoftMax 함수: \\(y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\\)\n\n\ndef softmax(a):\n    exp_a = np.exp(a)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsoftmax 함수는 값이 기하급수적으로 증가하기 때문에 쉽게 overflow가 발생할 수 있음.\n따라서 다음과 같이 개선이 필요함\n\ndef softmax(a):\n    c = np.max(a)\n    exp_a = np.exp(a - c)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsofrmax 함수 출력의 총합은 1이고, 개별 출력은 0에서 1 사이이다.\n따라서 softmax 함수의 출력을 확률로 해석할 수 있다.\n여기서 softmax 함수는 입력 값의 대소관계가 유지된다는 성질이 있기 때문에 학습이 아닌, 추론 단계에서는 보통 생략한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/01.html#층-신경망-구성",
    "href": "posts/02_areas/deep_learning/notes/01.html#층-신경망-구성",
    "title": "신경망",
    "section": "3층 신경망 구성",
    "text": "3층 신경망 구성\n입력층에서 1층으로 신호 전달\n\nX = np.array([1.0, 0.5])\nW1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\nB1 = np.array([0.1, 0.2, 0.3])\n\nA1 = np.dot(X, W1) + B1\nZ1 = sigmoid(A1)\n\nprint(A1)\nprint(Z1)\n\n[0.3 0.7 1.1]\n[0.57444252 0.66818777 0.75026011]\n\n\n1층에서 2층으로 신호 전달\n\nW2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\nB2 = np.array([0.1, 0.2])\n\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n\n\n2층에서 출력층으로 신호 전달\n\ndef identity_function(x):\n    return x\n\nW3 = np.array([[0.1, 0.3], [0.2, 0.4]])\nB3 = np.array([0.1, 0.2])\n\nA3 = np.dot(Z2, W3) + B3\nY = identity_function(A3)\n\n출력층의 활성화 함수는 보통 풀고자 하는 문제의 성질에 맞게 정함\n\n회귀: 항등 함수\n2클래스 분류: 시그모이드 함수\n다중 클래스 분류: 소프트맥스 함수",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-sdk",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-sdk",
    "title": "Qiskit",
    "section": "Qiskit SDK",
    "text": "Qiskit SDK\nQiskit SDK(패키지 이름: qiskit)는 확장된(정적, 동적, 스케줄된) 양자 회로, 연산자, 프리미티브 수준에서 양자 컴퓨터를 다루기 위한 오픈소스 소프트웨어 개발 키트입니다. 이 라이브러리는 Qiskit의 핵심 구성 요소로, 양자 계산을 위한 가장 광범위한 도구 모음을 제공하며, 다른 많은 구성 요소가 여기에 연결됩니다.\nQiskit SDK의 가장 유용한 기능은 다음과 같습니다:\n\n회로 구축 도구(qiskit.circuit): 레지스터, 회로, 명령, 게이트, 매개변수, 제어 흐름 객체를 초기화하고 조작하기 위한 도구.\n회로 라이브러리(qiskit.circuit.library): 회로, 명령, 게이트의 방대한 범위 - 회로 기반 양자 계산의 핵심 구성 요소.\n양자 정보 라이브러리(qiskit.quantum_info): 샘플링 노이즈 없이 정확한 계산을 통해 양자 상태, 연산자, 채널을 다루는 툴킷. 입력 관측 가능 항목을 지정하고 프리미티브 쿼리의 출력 충실도를 분석하는 데 사용.\n트랜스파일러(qiskit.transpiler): 특정 장치 토폴로지에 맞게 양자 회로를 변환 및 적응시키고, 실제 양자 처리 장치(QPU)에서 실행을 최적화.\n프리미티브(qiskit.primitives): Sampler와 Estimator 프리미티브의 기본 정의와 참조 구현을 포함하는 모듈로, 다양한 양자 하드웨어 제공자가 이를 기반으로 자체 구현을 파생할 수 있음. Qiskit Runtime 프리미티브에 대한 자세한 내용은 문서에서 확인 가능.\n\n\n설치\nQiskit SDK 설치에 대한 자세한 소개는 설치 페이지를 확인하세요. 지금 설치할 준비가 되었다면 다음 명령어를 실행하세요:\npip install qiskit",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "href": "posts/03_resources/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "title": "Qiskit",
    "section": "벤치마킹과 Benchpress 패키지",
    "text": "벤치마킹과 Benchpress 패키지\n벤치마킹은 개발 워크플로우의 여러 단계에서 양자 소프트웨어의 상대적 성능을 비교하는 데 중요합니다. 예를 들어, 양자 소프트웨어 벤치마킹 테스트는 회로 구축, 조작, 트랜스파일링의 속도와 품질을 평가할 수 있습니다. IBM Quantum은 가능한 한 성능이 뛰어난 SDK를 제공하기 위해 노력하며, 이를 위해 Qiskit SDK는 주요 대학, 국립 연구소, IBM 연구원들이 개발한 1,000개 이상의 테스트를 통해 벤치마킹됩니다. 이러한 테스트에 사용되는 벤치마킹 스위트는 Benchpress라는 이름으로 오픈소스 패키지로 제공됩니다. 이제 Benchpress 패키지를 사용해 양자 SDK 성능을 직접 분석할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-runtime",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-runtime",
    "title": "Qiskit",
    "section": "Qiskit Runtime",
    "text": "Qiskit Runtime\nQiskit Runtime은 IBM Quantum® 하드웨어에서 양자 계산을 실행하기 위한 클라우드 기반 서비스입니다. qiskit-ibm-runtime 패키지는 이 서비스의 클라이언트로, Qiskit IBM Provider의 후속 버전입니다. Qiskit Runtime 서비스는 양자 계산을 간소화하고 IBM Quantum 하드웨어에 최적화된 Qiskit 프리미티브 구현을 제공합니다. Qiskit Runtime 프리미티브를 시작하려면 문서를 방문하세요.\nQiskit Runtime은 추가적인 고전 및 양자 컴퓨팅 자원을 활용하도록 설계되었으며, 오류 억제(error suppression)와 오류 완화(error mitigation) 같은 기술을 사용해 양자 회로 실행에서 더 높은 품질의 결과를 반환합니다. 예로는 오류 억제를 위한 동적 디커플링(dynamical decoupling), 오류 완화를 위한 판독 완화(readout mitigation)와 제로 노이즈 외삽(ZNE)이 있습니다. 이러한 옵션 설정 방법은 오류 완화 설정 페이지에서 확인할 수 있습니다.\nQiskit Runtime은 IBM 하드웨어에서 양자 프로그램을 실행하기 위해 세 가지 실행 모드(Job, Session, Batch)를 제공하며, 각각은 서로 다른 사용 사례와 양자 작업 큐에 대한 영향을 가집니다: - Job: 지정된 샷 수로 실행되는 단일 프리미티브 쿼리. - Session: 양자 컴퓨터에서 반복 작업 부하를 여러 작업으로 효율적으로 실행. - Batch: 모든 작업을 한 번에 제출해 병렬 처리. 참고: Open Plan 사용자는 세션 작업을 제출할 수 없습니다.\nQiskit Runtime을 빠르게 설치하려면 다음 명령어를 실행하세요:\npip install qiskit-ibm-runtime\n개발 환경 설정에 대한 자세한 내용은 설치 페이지에서 확인할 수 있습니다.\n\nQiskit Runtime은 오픈소스인가요?\n간단히 답하면, 전부는 아닙니다. IBM Quantum 장치에서 양자 프로그램을 실행하는 기술적 세부 사항(오류 완화 및 억제 포함)을 처리하는 Qiskit Runtime 서비스 소프트웨어는 오픈소스가 아닙니다. 그러나 Qiskit Runtime 클라이언트(사용자가 Qiskit Runtime 서비스에 접근하는 인터페이스), 서버 측에서 실행되는 Qiskit SDK, 오류 완화에 사용되는 일부 소프트웨어는 오픈소스입니다. Qiskit 오픈소스 활동에 참여하려면 GitHub 조직(github.com/Qiskit 및 github.com/Qiskit-Extensions)을 방문하세요.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-serverless",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-serverless",
    "title": "Qiskit",
    "section": "Qiskit Serverless",
    "text": "Qiskit Serverless\n유틸리티 규모의 양자 응용 프로그램을 만들려면 일반적으로 다양한 컴퓨팅 자원 요구 사항이 필요합니다. Qiskit Serverless(qiskit-ibm-catalog.QiskitServerless)는 양자-고전 자원 전반에 걸쳐 작업 부하를 실행하기 위한 간단한 인터페이스를 제공합니다. 여기에는 IBM Quantum Platform에 프로그램 배포, 원격 작업 부하 실행, 멀티 클라우드 및 양자 중심 슈퍼컴퓨팅 사용 사례를 위한 쉬운 자원 관리가 포함됩니다. 자세한 내용은 Qiskit Serverless 문서에서 확인할 수 있습니다: - 고전 작업(전처리, 후처리 등) 병렬화. - 노트북이 꺼져 있어도 클라우드에서 장기 작업 유지. - 클라우드에 재사용 가능한 프로그램 배포.\nQiskit Serverless를 바로 사용하려면 다음 명령어로 설치하세요:\npip install qiskit_serverless",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-functions",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-functions",
    "title": "Qiskit",
    "section": "Qiskit Functions",
    "text": "Qiskit Functions\nQiskit Functions(qiskit-ibm-catalog.QiskitFunctionsCatalog)는 알고리즘 발견과 응용 프로토타이핑을 가속화하도록 설계된 추상화된 서비스입니다. Qiskit Functions Catalog를 탐색해보세요: - Circuit Functions: 트랜스파일링, 오류 억제, 오류 완화, 후처리 기술을 포함하며, 추상 회로와 원하는 측정 관측 가능 항목을 입력으로 받는 서비스. 사용자는 하드웨어 성능 관리를 신경 쓰지 않고 새로운 알고리즘과 응용을 탐구 가능. - Application Functions: 고전에서 양자로의 매핑, 하드웨어 최적화, 하드웨어 실행, 후처리를 포함한 전체 양자 워크플로우를 제공. 사용자는 산업별 친숙한 입력과 출력으로 응용 프로토타이핑 가능.\nPremium Plan 회원은 IBM 제공 함수에 즉시 접근하거나 파트너가 제공하는 함수를 파트너로부터 직접 라이선스 구매 가능합니다. 카탈로그는 다음 명령어로 설치 가능:\npip install qiskit-ibm-catalog",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "title": "Qiskit",
    "section": "Qiskit Transpiler as a Service",
    "text": "Qiskit Transpiler as a Service\nQiskit Transpiler Service(패키지 이름: qiskit-ibm-transpiler)는 IBM Quantum Premium Plan 사용자에게 클라우드에서 원격 트랜스파일링 기능을 제공하는 새로운 실험적 서비스입니다. 로컬 Qiskit SDK 트랜스파일러 기능 외에도, 이 서비스를 통해 트랜스파일링 작업은 IBM Quantum 클라우드 자원과 AI 기반 트랜스파일러 패스를 활용할 수 있습니다. 클라우드 기반 트랜스파일링을 Qiskit 워크플로우에 통합하는 방법은 문서에서 확인하세요.\n트랜스파일러 서비스는 다음 명령어로 설치 가능:\npip install qiskit-ibm-transpiler",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-addons",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-addons",
    "title": "Qiskit",
    "section": "Qiskit Addons",
    "text": "Qiskit Addons\nQiskit Addons는 유틸리티 규모 알고리즘 발견을 위한 연구 기능 모음입니다. 이러한 기능은 Qiskit의 고성능 기반 위에 양자 알고리즘 생성 및 실행 도구를 구축합니다. Addons는 워크플로우에 연결되어 새로운 양자 알고리즘을 확장하거나 설계하는 모듈형 소프트웨어 구성 요소입니다. 사용 가능한 Qiskit Addons 세트와 시작 방법은 문서에서 확인하세요.\n관심 있는 연구 기능에 따라 여러 Addons가 있으며, 각기 pip로 설치 가능: - Sample-based Quantum Diagonalization (SQD): pip install qiskit-addon-sqd - Approximate Quantum Compilation (AQC): pip install qiskit-addon-aqc-tensor[quimb-jax] - Operator Backpropagation (OBP): pip install qiskit-addon-obp - Multi-product Formulas (MPF): pip install qiskit-addon-mpf",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/01.html#qiskit-생태계",
    "href": "posts/03_resources/quantum_programming/notes/01.html#qiskit-생태계",
    "title": "Qiskit",
    "section": "Qiskit 생태계",
    "text": "Qiskit 생태계\nQiskit 외에도 “Qiskit” 이름을 사용하는 많은 오픈소스 프로젝트가 있으며, 이는 Qiskit 자체의 일부는 아니지만 Qiskit과 인터페이스를 이루며 핵심 Qiskit 워크플로우를 보완하는 유용한 추가 기능을 제공합니다. 일부 프로젝트는 IBM Quantum 팀이 유지 관리하며, 다른 일부는 더 넓은 오픈소스 커뮤니티에서 지원됩니다. Qiskit SDK는 모듈화되고 확장 가능한 방식으로 설계되어 개발자들이 이를 확장하는 프로젝트를 쉽게 만들 수 있습니다.\nQiskit 생태계의 인기 있는 프로젝트: - Qiskit Aer(qiskit-aer): 현실적인 노이즈 모델을 포함한 양자 컴퓨팅 시뮬레이터 패키지. 여러 시뮬레이션 방법으로 노이즈 유무에 따라 양자 회로 실행 가능. IBM Quantum 유지 관리. - qBraid SDK(qbraid): 양자 소프트웨어 및 하드웨어 제공자를 위한 플랫폼 독립적 양자 런타임 프레임워크로, 프로그램 사양 정의부터 작업 제출, 후처리 및 결과 시각화까지 양자 작업의 전체 수명 주기 관리를 간소화. qBraid 유지 관리. - mthree: M3(Matrix-free Measurement Mitigation) 측정 완화 기술을 구현하는 패키지로, 차원 축소 후 직접 LU 분해 또는 O(1) 단계로 수렴하는 전처리 반복 방법을 사용해 수정된 측정 확률을 계산하며 병렬 처리 가능. IBM Quantum 유지 관리.\nQiskit 생태계 페이지에서 프로젝트 카탈로그와 자신의 프로젝트를 추천하는 방법에 대한 정보를 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html",
    "href": "posts/03_resources/금융/index.html",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#details",
    "href": "posts/03_resources/금융/index.html#details",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#tasks",
    "href": "posts/03_resources/금융/index.html#tasks",
    "title": "금융",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/index.html#related-posts",
    "href": "posts/03_resources/금융/index.html#related-posts",
    "title": "금융",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html",
    "href": "posts/03_resources/smart_contract/index.html",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#details",
    "href": "posts/03_resources/smart_contract/index.html#details",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#tasks",
    "href": "posts/03_resources/smart_contract/index.html#tasks",
    "title": "Smart Contract",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#참고-자료",
    "href": "posts/03_resources/smart_contract/index.html#참고-자료",
    "title": "Smart Contract",
    "section": "참고 자료",
    "text": "참고 자료\n\nblock chain 강의 사이트",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/index.html#related-posts",
    "href": "posts/03_resources/smart_contract/index.html#related-posts",
    "title": "Smart Contract",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html",
    "href": "posts/03_resources/problem_solve/index.html",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#details",
    "href": "posts/03_resources/problem_solve/index.html#details",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#tasks",
    "href": "posts/03_resources/problem_solve/index.html#tasks",
    "title": "Problem Solving",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/problem_solve/index.html#related-posts",
    "href": "posts/03_resources/problem_solve/index.html#related-posts",
    "title": "Problem Solving",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "title": "Terraform Cloud",
    "section": "What is Terraform Cloud?",
    "text": "What is Terraform Cloud?\n\nTerraform Open-Source를 확장해주는 서비스\n\n\n\n\nTerraform Open-Source의 한계\n\n\n\n기존의 terraform을 대규모 팀 단위에서 사용하기엔 무리가 있음 → TFC\non-premise 환경을 위한 Terraform Enterpise 서비스도 존재함.\nTACOS: Terraform Automation & Collaboration Software",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#what-is-organization",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#what-is-organization",
    "title": "Terraform Cloud",
    "section": "What is Organization?",
    "text": "What is Organization?\n\nworkspaces, policies, terraform modules를 공유하는 공간\n\n\n\n\nOrganization level에서 모든 setting이 이루어짐\n\n\n\n하나의 조직을 운용하는 것이 일반적이나, 조직 구조에 따라 여러 조직을 생성해서 운용할 수 있다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "href": "posts/03_resources/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "title": "Terraform Cloud",
    "section": "Authenticating to TFC",
    "text": "Authenticating to TFC\n\nweb interface\nCLI\n\n\nToken\n\nUser Tokens\nTeam Tokens: CI/CD pipeline에 주로 사용됨\nOrganization Tokens",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html",
    "href": "posts/03_resources/인생/index.html",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#details",
    "href": "posts/03_resources/인생/index.html#details",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#tasks",
    "href": "posts/03_resources/인생/index.html#tasks",
    "title": "인생",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/index.html#related-posts",
    "href": "posts/03_resources/인생/index.html#related-posts",
    "title": "인생",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html",
    "href": "posts/03_resources/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#details",
    "href": "posts/03_resources/blog/index.html#details",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#tasks",
    "href": "posts/03_resources/blog/index.html#tasks",
    "title": "Blog",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    PARA 구조에 맞게 블로그 구조 변경\n                \n                \n            \n\n            \n            \n                \n                    \n                    게시글에 관련 게시글, 관련 directory 추가\n                \n                별로 마음에 들진 않지만 일단 완성\n            \n\n            \n            \n                \n                    \n                    Hugo 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    google analytics 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    about me 페이지 작성\n                \n                \n            \n\n            \n            \n                \n                    \n                    link 미리보기 기능 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    task 리스트 캘린더 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    종합 task 캘린더 추가",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#참고-자료",
    "href": "posts/03_resources/blog/index.html#참고-자료",
    "title": "Blog",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/index.html#related-posts",
    "href": "posts/03_resources/blog/index.html#related-posts",
    "title": "Blog",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/0.html#overview",
    "href": "posts/03_resources/blog/notes/0.html#overview",
    "title": "PARA Blog 제작",
    "section": "Overview",
    "text": "Overview\n유튜브에서 ‘제2의 두뇌’ 관련 영상을 보고 이 구조로 제 학습 블로그에 적용하면 좋겠다는 생각이 들었습니다. Quarto로 만들어진 제 블로그에 이 구조를 적용하는 것이 생각보다 쉽지 않긴 했지만, 나름 해볼만 했습니다.\n사실 이 글을 작성하는 시점에는 이미 블로그 리뉴얼이 어느 정도 완료된 상태입니다. 코드가 최적화되지 않아 따로 제작 과정을 상세히 공유하지는 않으려 합니다만, 제 GitHub 레포에서 전체 코드를 확인하실 수 있습니다. 이전 블로그는 여기에서 확인할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/0.html#이후-목표",
    "href": "posts/03_resources/blog/notes/0.html#이후-목표",
    "title": "PARA Blog 제작",
    "section": "이후 목표",
    "text": "이후 목표\n블로그를 완성하고 보니 Quarto의 필요성에 대해 다시 한번 생각해보게 되었습니다. 데이터 분석을 공부하는 입장에서 Quarto는 분명 대체 불가능한 장점들이 있지만, 웹사이트 구조를 구축하는 데에는 일정 부분 한계가 있어 보입니다.\nDocument를 읽어보던 중 Quarto와 Hugo를 통합하는 방법이 있다는 것을 알게 되었습니다. 이를 통해 Hugo로 블로그의 기본 구조를 만들고, R과 Python 코드 실행 환경으로 Quarto를 활용하는 방안을 고려하고 있습니다.\n이번이 jekyll, framer 블로그에 이어서 세번째로 만드는 블로그입니다. 저는 웹 개발보다는 다른 분야에 집중하고 싶기 때문에, 다음 리뉴얼을 마지막으로 블로그 구조 개선을 마무리해보려 합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "all.html",
    "href": "all.html",
    "title": "전체 게시글",
    "section": "",
    "text": "정렬\n       디폴트\n         \n          날짜 - 날짜(오름차순)\n        \n         \n          날짜 - 날짜(내림차순)\n        \n         \n          제목\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n제목\n\n\n날짜\n\n\n분류\n\n\n\n\n\n\nQuestion\n\n\n \n\n\n \n\n\n\n\nADP 정리 노트\n\n\n2025-09-27\n\n\n자격증\n\n\n\n\nBMP 표준 개요\n\n\n2025-09-25\n\n\n프로세스 경영\n\n\n\n\nAfter\n\n\n2025-09-24\n\n\nEnglish\n\n\n\n\n재고 관리\n\n\n2025-09-23\n\n\n공급사슬관리\n\n\n\n\n프로세스 경영 구축 방법론\n\n\n2025-09-22\n\n\n프로세스 경영\n\n\n\n\n무한 급수\n\n\n2025-09-22\n\n\n공급사슬관리\n\n\n\n\n코드 snippet\n\n\n2025-09-21\n\n\n데이터 분석\n\n\n\n\nBefore\n\n\n2025-09-18\n\n\nEnglish\n\n\n\n\nTFC 게임 개요\n\n\n2025-09-17\n\n\n공급사슬관리\n\n\n\n\nS&OP: Sales and Operations Planning\n\n\n2025-09-16\n\n\n공급사슬관리\n\n\n\n\n수요 관리\n\n\n2025-09-10\n\n\n공급사슬관리\n\n\n\n\n개인 발표 - 탄소배출 측정 기술\n\n\n2025-09-09\n\n\n신재생에너지\n\n\n\n\n품질변동과 공정능력\n\n\n2025-09-08\n\n\n품질경영\n\n\n\n\nBPM 개요\n\n\n2025-09-08\n\n\n프로세스 경영\n\n\n\n\n프로세스 경영 개요\n\n\n2025-09-04\n\n\n프로세스 경영\n\n\n\n\n품질관리의 기본개념\n\n\n2025-09-03\n\n\n품질경영\n\n\n\n\nSCM 의사결정\n\n\n2025-09-03\n\n\n공급사슬관리\n\n\n\n\nIntro\n\n\n2025-09-02\n\n\n시뮬레이션\n\n\n\n\nIntro\n\n\n2025-09-02\n\n\n공급사슬관리\n\n\n\n\n관심 분야 JD\n\n\n2025-08-27\n\n\n금융\n\n\n\n\nToeic 문법\n\n\n2025-08-22\n\n\n영어\n\n\n\n\nBasic\n\n\n2025-08-20\n\n\nmachine learning\n\n\n\n\n자기소개서\n\n\n2025-08-19\n\n\n대학원, 자기소개서\n\n\n\n\n데이터 모델의 이해\n\n\n2025-08-19\n\n\nsql\n\n\n\n\nSQL 기본 및 활용\n\n\n2025-08-19\n\n\nsql\n\n\n\n\nword2vec\n\n\n2025-08-17\n\n\ndeep learning\n\n\n\n\n전처리\n\n\n2025-08-16\n\n\n확률 통계\n\n\n\n\n연구실\n\n\n2025-08-14\n\n\n대학원\n\n\n\n\n관심 분야 JD\n\n\n2025-08-14\n\n\n대학원\n\n\n\n\n전치행렬\n\n\n2025-08-13\n\n\n선형 대수\n\n\n\n\n역함수와 역변환\n\n\n2025-08-13\n\n\n선형 대수\n\n\n\n\nlinear transformations\n\n\n2025-08-12\n\n\n선형 대수\n\n\n\n\n자연어와 단어의 분산 표현\n\n\n2025-08-10\n\n\ndeep learning\n\n\n\n\n학습 관련 기술들\n\n\n2025-08-06\n\n\ndeep learning\n\n\n\n\nEDA\n\n\n2025-08-05\n\n\n확률 통계\n\n\n\n\n다중 공산성\n\n\n2025-08-04\n\n\n머신러닝\n\n\n\n\n분류 - 신용 카드 사기 검출\n\n\n2025-08-02\n\n\n머신 러닝\n\n\n\n\n합성곱 신경망\n\n\n2025-08-01\n\n\ndeep learning\n\n\n\n\n토익 스피킹 후기\n\n\n2025-08-01\n\n\n후기\n\n\n\n\n오차역전법\n\n\n2025-08-01\n\n\ndeep learning\n\n\n\n\n3학년 1학기 후기\n\n\n2025-08-01\n\n\n후기\n\n\n\n\n텍스트 분석 - 감성 분석\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n텍스트 분석 - 20 뉴스그룹 분류\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n텍스트 분석\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n신경망 학습\n\n\n2025-07-30\n\n\ndeep learning\n\n\n\n\n차원 축소\n\n\n2025-07-29\n\n\n머신 러닝\n\n\n\n\n회귀\n\n\n2025-07-28\n\n\n머신 러닝\n\n\n\n\n분류 - 앙상블\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n분류 - 산탄데르 고객 만족 예측\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n분류 - 결정 트리\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n대학원 준비\n\n\n2025-07-26\n\n\n대학원\n\n\n\n\n일정 정리\n\n\n2025-07-16\n\n\n영어\n\n\n\n\n다차원 척도법 (Multidimensional Scaling)\n\n\n2025-07-13\n\n\n확률 통계\n\n\n\n\nTips\n\n\n2025-07-13\n\n\n영어\n\n\n\n\n외생 변수 추가하기\n\n\n2025-07-12\n\n\n확률 통계, 시계열 분석\n\n\n\n\n계절성 고려\n\n\n2025-07-12\n\n\n확률 통계, 시계열 분석\n\n\n\n\n자기귀모형\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n이동평균과정 모델링\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n복잡한 시계열 모델\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n확률보행\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\n단순 미래 예측\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\nOverview\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\n두 변수의 연관성과 독립성\n\n\n2025-07-08\n\n\n확률 통계\n\n\n\n\nMCMC\n\n\n2025-07-08\n\n\n확률 통계\n\n\n\n\nHadoop Ecosystem\n\n\n2025-07-07\n\n\nHadoop, Big Data\n\n\n\n\n의사결정분석\n\n\n2025-07-06\n\n\n확률 통계\n\n\n\n\n검정\n\n\n2025-07-06\n\n\n확률 통계\n\n\n\n\n포아송 과정\n\n\n2025-07-04\n\n\n확률 통계\n\n\n\n\nTitanic\n\n\n2025-07-03\n\n\n확률 통계\n\n\n\n\n순열검정과 전통적인 비모수통계\n\n\n2025-07-02\n\n\n확률 통계\n\n\n\n\nintro\n\n\n2025-07-02\n\n\n확률 통계\n\n\n\n\n회귀\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n켤레사전분포\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n로지스틱 회귀\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n비교\n\n\n2025-06-30\n\n\n확률 통계\n\n\n\n\n최솟값, 최댓값 그리고 혼합 분포\n\n\n2025-06-24\n\n\n확률 통계\n\n\n\n\n분류\n\n\n2025-06-24\n\n\n확률 통계\n\n\n\n\n설문 script 정리\n\n\n2025-06-21\n\n\n영어\n\n\n\n\n돌발 script 정리\n\n\n2025-06-21\n\n\n영어\n\n\n\n\n오픽 구조 파악\n\n\n2025-06-20\n\n\n영어\n\n\n\n\n공산과 가산\n\n\n2025-06-20\n\n\n확률 통계\n\n\n\n\n수량 추정\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n비율 추정\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n분포\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n베이즈 정리\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n개요\n\n\n2025-06-18\n\n\nHelm, infra\n\n\n\n\n확률\n\n\n2025-06-17\n\n\n확률 통계\n\n\n\n\nEDA\n\n\n2025-06-17\n\n\n데이터 분석\n\n\n\n\n청소년기의 심리·정서적 요인을 통한 성인 진입기 진로 안정형·탐색형 성향 분류 예측\n\n\n2025-06-16\n\n\n보고서, data mining\n\n\n\n\nclustering\n\n\n2025-06-14\n\n\ndata mining\n\n\n\n\nXGBoost\n\n\n2025-06-14\n\n\ndata mining, 학부 정리\n\n\n\n\n데이터 전처리\n\n\n2025-06-13\n\n\ndata mining\n\n\n\n\nrandom forest\n\n\n2025-06-13\n\n\ndata mining\n\n\n\n\n확률과 통계 R 실습 과제\n\n\n2025-06-05\n\n\n확률과 통계, 보고서\n\n\n\n\n수송문제와 할당 문제들\n\n\n2025-06-05\n\n\nOR, 학부 정리\n\n\n\n\n네트워크 최적화 모형\n\n\n2025-06-05\n\n\nOR, 학부 정리\n\n\n\n\nAnalysis of categorical data\n\n\n2025-06-05\n\n\n확률과 통계\n\n\n\n\n시험 범위\n\n\n2025-05-31\n\n\nData Structure, 학부 정리\n\n\n\n\n시험 범위\n\n\n2025-05-31\n\n\nOR, 학부 정리\n\n\n\n\nensemble\n\n\n2025-05-31\n\n\ndata mining, 학부 정리\n\n\n\n\n일정 계획\n\n\n2025-05-28\n\n\n생산시스템관리\n\n\n\n\n기준생산계획 및 자재소요계획\n\n\n2025-05-26\n\n\n생산시스템관리\n\n\n\n\npreprocessing\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\nanalysis\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\n총괄생산계획\n\n\n2025-05-19\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 6\n\n\n2025-05-18\n\n\n보고서, OR\n\n\n\n\nRegression Analysis\n\n\n2025-05-13\n\n\n확률과 통계\n\n\n\n\n예측\n\n\n2025-05-12\n\n\n생산시스템관리\n\n\n\n\n데이터마이닝 1차 팀과제 script\n\n\n2025-05-10\n\n\n보고서, data mining\n\n\n\n\n분류\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\nSupport vector machine\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\n수송문제와 할당 문제들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n선형계획을 위한 다른 알고리즘들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n프로젝트 관리\n\n\n2025-04-30\n\n\n생산시스템관리\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 산술 손실\n\n\n2025-04-28\n\n\n생산시스템관리\n\n\n\n\nassociation rule mining\n\n\n2025-04-28\n\n\ndata mining\n\n\n\n\n컴퓨팅적 사고 1차 발표 구현 raw script\n\n\n2025-04-26\n\n\n보고서\n\n\n\n\nOR 과제 - 2\n\n\n2025-04-19\n\n\n보고서, OR\n\n\n\n\n쌍대이론과 민감도 분석 (part 6)\n\n\n2025-04-19\n\n\nOR, 학부 정리\n\n\n\n\nSimplex Method (part 5)\n\n\n2025-04-18\n\n\nOR, 학부 정리\n\n\n\n\nANOVA\n\n\n2025-04-15\n\n\n확률과 통계\n\n\n\n\nclassification with trees\n\n\n2025-04-14\n\n\ndata mining\n\n\n\n\n컴퓨팅적사고 발표 ppt\n\n\n2025-04-13\n\n\n보고서\n\n\n\n\nDataminig 1차 발표 ppt\n\n\n2025-04-13\n\n\ndata mining, 보고서\n\n\n\n\nOR 과제 - 4\n\n\n2025-04-10\n\n\n보고서, OR\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 대기시간 문제\n\n\n2025-04-09\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 3\n\n\n2025-04-06\n\n\n보고서, OR\n\n\n\n\nCoding pipeline\n\n\n2025-04-05\n\n\nAir flow\n\n\n\n\n시험 범위\n\n\n2025-04-03\n\n\nData Structure, 학부 정리\n\n\n\n\nSimplex 표 계산\n\n\n2025-04-03\n\n\nOR, 학부 정리\n\n\n\n\n배치 생산 및 경제적 주문량 모형\n\n\n2025-04-02\n\n\n생산시스템관리\n\n\n\n\n인건비 추정과 감축\n\n\n2025-03-31\n\n\n생산시스템관리\n\n\n\n\n통계적 가설검정\n\n\n2025-03-27\n\n\n확률과 통계\n\n\n\n\n제품 설계 기법 및 기업 프로세스 유형\n\n\n2025-03-26\n\n\n생산시스템관리\n\n\n\n\nGetting Started\n\n\n2025-03-26\n\n\nAir flow\n\n\n\n\n공급 프로세스의 이해: 프로세스 처리능력 평가\n\n\n2025-03-19\n\n\n생산시스템관리\n\n\n\n\n신경망\n\n\n2025-03-18\n\n\ndeep learning\n\n\n\n\n퍼셉트론\n\n\n2025-03-17\n\n\ndeep learning\n\n\n\n\ntitanic\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nUpper Confidence Bound\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nNull space and Column space\n\n\n2025-03-16\n\n\n선형 대수\n\n\n\n\nEclat\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nApriori\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\n안녕하세요. 데이터 분석 전문가(진)입니다.\n\n\n2025-03-14\n\n\nadp, 후기\n\n\n\n\nOR 과제 - 1\n\n\n2025-03-13\n\n\n보고서, OR\n\n\n\n\n통계적 추정\n\n\n2025-03-13\n\n\n확률과 통계\n\n\n\n\n봉사\n\n\n2025-03-13\n\n\n봉사\n\n\n\n\n조직을 프로세스 관점에서 바라보기\n\n\n2025-03-12\n\n\n생산시스템관리\n\n\n\n\n가감법으로 연립방정식을 풀기 위한 행렬\n\n\n2025-03-11\n\n\n선형 대수\n\n\n\n\nMatching Supply with Demand\n\n\n2025-03-10\n\n\n생산시스템관리\n\n\n\n\nvector dot product, cross product\n\n\n2025-03-09\n\n\n선형 대수\n\n\n\n\nk-means clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nhierarchical clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nLinear Programming Algorithm\n\n\n2025-03-08\n\n\nOperational Research\n\n\n\n\nIntro\n\n\n2025-03-07\n\n\nOR, 학부 정리\n\n\n\n\n확률과 통계 1 정리\n\n\n2025-03-06\n\n\n확률과 통계\n\n\n\n\n나의 단점에 관한 고찰\n\n\n2025-03-06\n\n\n인생\n\n\n\n\nQuantum Programming\n\n\n2025-03-06\n\n\nQuantum Programming\n\n\n\n\nQiskit\n\n\n2025-03-06\n\n\nQuantum Programming\n\n\n\n\nRandom Forest\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nNaive Bayes\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nIntro\n\n\n2025-03-05\n\n\n생산시스템관리\n\n\n\n\nDecision Tree Classification\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nSubspaces and the basis\n\n\n2025-03-03\n\n\n선형 대수\n\n\n\n\nlinear independence\n\n\n2025-03-02\n\n\n선형 대수\n\n\n\n\nSupport Vector Machine\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nK Nearest Neighbors\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nrandom forest\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nSupport Vector Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nLogistic Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nDecision Tree Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nPolynorminal Linear Regression\n\n\n2025-02-27\n\n\nmachine learning\n\n\n\n\noverview\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\ndata preprocessing\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nSimple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nMultiple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\n머신 러닝\n\n\n2025-02-25\n\n\n데이터 분석\n\n\n\n\n시험을 보고 왔습니다.\n\n\n2025-02-22\n\n\nadp, 후기\n\n\n\n\nwhat is a blockchain?\n\n\n2025-02-22\n\n\n블록 체인\n\n\n\n\n4 - 정형 데이터 마이닝\n\n\n2025-02-20\n\n\nadp\n\n\n\n\n4 - 비정형 데이터 마이닝\n\n\n2025-02-18\n\n\nadp\n\n\n\n\ninception-of-things part 1\n\n\n2025-02-17\n\n\nvagrant, k8s, argoCD, gitlab, 42 seoul\n\n\n\n\n4 - 통계분석\n\n\n2025-02-16\n\n\nadp\n\n\n\n\n4 - 데이터 마트\n\n\n2025-02-15\n\n\nadp\n\n\n\n\nTerraform Cloud\n\n\n2025-02-11\n\n\nterraform, terraform cloud, devops, IaC\n\n\n\n\n5 - 시각화 인사이트 프로세스\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n5 - 시각화 디자인\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n3 - 분석 마스터 플랜\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n3 - 데이터 분석 기획의 이해\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n2 - 데이터 처리 프로세스\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n2 - 데이터 처리 기술\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n성적 장학금\n\n\n2025-02-05\n\n\n장학금\n\n\n\n\n1 - 데이터의 가치와 미래\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 데이터 이해\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n인간 관계론 - 데일 카네기\n\n\n2025-02-02\n\n\n독서, 인간 관계\n\n\n\n\n선형결합과 생성\n\n\n2025-01-31\n\n\n선형 대수\n\n\n\n\n벡터와 공간\n\n\n2025-01-30\n\n\n선형 대수\n\n\n\n\ncloud-1 코드 설명\n\n\n2025-01-30\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\ncloud-1 개념 설명\n\n\n2025-01-28\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\n3-몰라\n\n\n2025-01-22\n\n\n선형 대수\n\n\n\n\n자기 소개서\n\n\n2025-01-17\n\n\n자기 소개서\n\n\n\n\nft_transcendence - github action\n\n\n2025-01-17\n\n\nagile, github action, 42 seoul\n\n\n\n\n2-기초(2)\n\n\n2025-01-11\n\n\n선형 대수\n\n\n\n\n2-기초(1)\n\n\n2025-01-10\n\n\n선형 대수\n\n\n\n\nSecond Brain - 티아고 포르테\n\n\n2025-01-09\n\n\n학습, 독서\n\n\n\n\nwhat is linear algebra\n\n\n2025-01-07\n\n\n선형 대수\n\n\n\n\n돈의 심리학 - 모건 하우절\n\n\n2025-01-02\n\n\n금융, 독서\n\n\n\n\n데이터 전처리\n\n\n2025-01-02\n\n\n데이터 분석\n\n\n\n\nEDA와 시각화\n\n\n2024-12-30\n\n\n데이터 분석\n\n\n\n\nPARA Blog 제작\n\n\n2024-12-26\n\n\n블로그\n\n\n\n\n맥도날드 키오스크 UI 개선 보고서\n\n\n2024-11-27\n\n\n보고서, 인간 공학\n\n\n\n\n숭실대학교 학생식당 식자제 SCM 설계\n\n\n2024-11-26\n\n\n보고서, database\n\n\n\n\nControl\n\n\n2024-11-21\n\n\n인간 공학\n\n\n\n\n표본의 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n중심 극한 정리\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n정규 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\nDisplay\n\n\n2024-11-14\n\n\n인간 공학\n\n\n\n\n연속형 확률분포\n\n\n2024-11-05\n\n\n확률과 통계\n\n\n\n\nAttention\n\n\n2024-11-05\n\n\n인간 공학\n\n\n\n\nDatabase Design\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nDatabase Administration\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nASP.NET\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\n4조 기말과제 제안서\n\n\n2024-10-30\n\n\n보고서, database\n\n\n\n\n이산형 확률분포\n\n\n2024-10-28\n\n\n확률과 통계\n\n\n\n\n데이터베이스설계및활용 개인과제 #2\n\n\n2024-10-27\n\n\n보고서, database\n\n\n\n\n확률변수의 기댓값\n\n\n2024-10-16\n\n\n확률과 통계\n\n\n\n\nSignal Detection Theory\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nAuditory Haptic\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nData Modeling and the Entity-Relationship Model\n\n\n2024-10-14\n\n\ndatabase\n\n\n\n\nSQL\n\n\n2024-09-27\n\n\ndatabase\n\n\n\n\nSensor System (Visual)\n\n\n2024-09-24\n\n\n인간 공학\n\n\n\n\nDatabase Normalization\n\n\n2024-09-24\n\n\ndatabase\n\n\n\n\nThe Relational Model\n\n\n2024-09-17\n\n\ndatabase\n\n\n\n\nHuman Information Processing Model\n\n\n2024-09-17\n\n\n인간 공학\n\n\n\n\nResearch Method in Human Factors\n\n\n2024-09-10\n\n\n인간 공학\n\n\n\n\n확률변수와 확률분포\n\n\n2024-09-03\n\n\n확률과 통계\n\n\n\n\nIntroduction to Human Factors\n\n\n2024-09-03\n\n\n인간 공학\n\n\n\n\nAn Overview of Database\n\n\n2024-09-03\n\n\ndatabase\n\n\n\n\n확률과 통계의 정의\n\n\n2024-09-02\n\n\n확률과 통계\n\n\n\n\nmetrics server\n\n\n2024-05-15\n\n\n \n\n\n\n\nmanual scheduling\n\n\n2024-05-15\n\n\n \n\n\n\n\nk8s cluster architecture\n\n\n2024-05-15\n\n\n \n\n\n\n\nfail tolerance\n\n\n2024-05-15\n\n\n \n\n\n\n\ncore DNS\n\n\n2024-05-15\n\n\n \n\n\n\n\nPersistant volume\n\n\n2024-05-15\n\n\n \n\n\n\n\nHA in master node\n\n\n2024-05-15\n\n\n \n\n\n\n\nAuthentication\n\n\n2024-05-15\n\n\n \n\n\n\n\nwhat is ebs\n\n\n2024-04-30\n\n\n \n\n\n\n\nwhat is EC2\n\n\n2024-04-30\n\n\n \n\n\n\n\ndatabase choice in aws\n\n\n2024-04-30\n\n\n \n\n\n\n\naws global infrastructure\n\n\n2024-04-30\n\n\n \n\n\n\n\nVPC\n\n\n2024-04-30\n\n\n \n\n\n\n\nRoute53\n\n\n2024-04-30\n\n\n \n\n\n\n\nPerformance Improvement\n\n\n2024-04-30\n\n\n \n\n\n\n\nOverview\n\n\n2024-04-30\n\n\nvault, devops\n\n\n\n\nKMS(Key Management Service)\n\n\n2024-04-30\n\n\n \n\n\n\n\nELB\n\n\n2024-04-30\n\n\n \n\n\n\n\nDisaster Recovery(DR)\n\n\n2024-04-30\n\n\n \n\n\n\n\nDefine IAM\n\n\n2024-04-30\n\n\n \n\n\n\n\nCloudFront\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon Rekognition\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon RDS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon CloudWatch\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Snow Family\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS SQS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS S3\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Organization\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Lambda\n\n\n2024-04-30\n\n\n \n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람을-대하는-기본-기술",
    "href": "posts/00_inboxes/notes/01.html#사람을-대하는-기본-기술",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람을 대하는 기본 기술",
    "text": "사람을 대하는 기본 기술\n\n꿀을 얻으려면 벌집을 걷어차지 마라\n남을 비난하고 원망하며 불평하는 것은 어떤 바보라도 할 수 있다. 실제로 바보들은 그렇게 한다. 하지만 남을 이해하고 용서하려면 인격과 자제력이 필요하다.\n사람을 비난하는 대신 그들을 이해하려고 노력해 보자. 그들이 왜 그런 행동을 하는지 곰곰이 생각해 보자. 그편이 비난하는 것보다 훨씬 이롭고 흥미롭다.\n\n\n사람을 대하는 핵심 비결\n누군가에게 어떤 일을 하게 만드는 방법은 상대방이 그 일을 하고 싶게 만드는 것 뿐이다. 강제적인 방법들은 반드시 역효과를 일으킨다.\n타인에게 어떤 일을 하게 하려면 그 사람이 원하는 것을 주는 방법밖에 없다. 인간의 본성이 지닌 가장 깊은 충동이 바로 중요한 사람이 되고자 하는 욕망이다.\n이러한 갈망을 제대로 충족시켜 주는 사람은 다른 사람의 마음을 사로잡을 수 있다. 타인을 진실된 마음으로 칭찬하자. 이는 이기적이고 거짓인 아첨과는 다르다.\n\n\n이 일을 해내는 사람은 세상을 얻을 것이고, 그렇지 못한 사람은 외로운 길을 걸을 것이다.\n상대방에게 영향을 미치는  방법은 그사람이 원하는 것을 이야기하고, 이를 얻는 방법을 보여 주는 것이다.\n\n\n\n\n\n\n…이 부분 예시로 드는 것들이 조금 오버스럽다고 느껴진다.\n문화가 달라서 그런가? 아니면 번역 이슈인가?\n무슨 말을 하는진 알겠는데, 몇몇 부분은 별로 공감이 안 된다.",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "href": "posts/00_inboxes/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들에게 호감을 얻는 6가지 방법",
    "text": "사람들에게 호감을 얻는 6가지 방법\n\n이렇게 하면 어디서든 환영받을 것이다.\n다른 사람에게 관심이 없는 사람은 인생에서 가장 큰 어려움을 겪고, 다른 사람에게 가장 큰 상처를 준다. 상대방에게 진심으로 관심을 가져라\n\n\n좋은 첫인상을 남기는 간단한 방법\n미소를 지어라\n\n\n이렇게 하지 않으면 문제가 생길 것이다.\n사람의 이름을 기억하라\n\n\n좋은 대화 상대가 되는 쉬운 방법\n상대의 이야기를 경청하고, 상대가 자신에 관해 이야기하도록 격려하라\n\n\n사람들의 관심을 얻는 방법\n상대방의 관심사에 대해 이야기하라\n\n\n사람들에게 즉시 호감을 얻는 방법\n모든 사람은 자신이 상대보다 우월하다고 생각한다. 항상 상대방이 자신을 중요하다고 느끼게 하라",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "href": "posts/00_inboxes/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들의 마음을 사로잡는 12가지 방법",
    "text": "사람들의 마음을 사로잡는 12가지 방법\n\n논쟁으로는 이길 수 없다\n자기 의사에 반하여 설득당한 사람은 여전히 자기 생각을 바꾸지 않는 법이다. 논쟁에서 최선의 결과를 얻는 유일한 방법은 논쟁을 피하는 것뿐이다.\n\n\n적을 만드는 확실한 방법과 이를 피하는 방법\n되도록 남들보다 지혜로운 사람이 되거라. 하지만 남들에게 그렇다고 말하지 않도록 해라.\n상대가 틀린 말을 해도 굳이 지적하지 마라\n\n\n틀렸다면, 인정하라\n조금 잘못했는데, 상대가 비난할거 같으면 오바해서 자기 잘못을 시인하라\n\n\n이성에 호소하는 확실한 방법\n우호적인 방식으로 시작하라\n\n\n소크라테스의 비결\n상대방이 동의할 수 밖에 없는 질문을 유도하라.\n\n\n불만을 잠재우는 안전밸브\n여기부터 읽어야함",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#second-brain의-핵심-기능",
    "href": "posts/03_resources/blog/notes/1.html#second-brain의-핵심-기능",
    "title": "Second Brain - 티아고 포르테",
    "section": "Second Brain의 핵심 기능",
    "text": "Second Brain의 핵심 기능\n\n아이디어를 구체화한다\n머릿속에서 아이디어를 분리하여 구체적인 형태로 만들어야 한다.\n아이디어 사이의 연관성을 새롭게 밝혀낸다.\n다양한 자료를 한곳에 보관하면 자료간 연결 작업이 촉진되며, 생각지 못한 연관성을 찾아낼 가능성을 높일 수 있다.\n시간을 두고 아이디어를 발전시킨다.\n사람들은 줄곧 아이디어를 떠올릴 때 최신 정보에 중요성을 더 부여하는 경향이 있다.\n몇년 간 축적된 아이디어를 마음껏 이용할 수 있다면 더 좋을 것이다.\n나만의 독특한 관점을 정교하게 다듬는다.\n작가의 벽에 부딪히는 것은 적절한 단어를 떠올릴 수 없다는 것이 아니라, 글을 쓸 탄약이 부족하다는 것이다.\n자신의 견해를 지지할 수 있는 자료를 지속적으로 모아야한다.\n\n\n머리는 아이디어를 생각하는 곳이지 보관하는 곳이어선 안된다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "href": "posts/03_resources/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "title": "Second Brain - 티아고 포르테",
    "section": "중요한 것을 기억하는 4 단계 (CODE)",
    "text": "중요한 것을 기억하는 4 단계 (CODE)\n\nCapture: 공명하는 내용을 수집하라\n당신과 마음에 닿는 내용을 분별하여 보관하고 나머지는 버려라\nOrganazie: 실행을 목표로 정리하라\n실행을 염두에 두고 정리하라.\nDistill: 핵심을 찾아 추출하라\n메모의 요점을 정리하라.\n메모를 저장한 이유, 생각하던 내용, 무엇이 당신의 관심을 끌었는지에 대한 설명\nExpress: 작업한 결과물을 표현하라\n개인적이고 구체적이며 검증된 정보는 실제로 사용할 때에 비로소 지식이 된다.\n당신이 아는 내용을 다른 사람과 공유하기 전까지는 그저 이론에 불과하다.\n\n\nCapture\n미래에 어떻게 될지 전혀 모르는데 무엇을 저장할지 어떻게 결정할 수 있을까? 어떤 정보가 보관할 가치가 있는지 정확히 알아내도록 통찰력을 키우기 위해 리처드 파인만의 좋아하는 12가지 문제 방법을 제시한다. 자신에게 흥미를 불러일으키는 열린 질문들을 자유롭게 적어보자. 그후 해당 질문들을 학습의 방향을 제시하는 북극성으로 삼아 활용한다.\n\n\n\n\n\n\n직접 적어본 질문들\n\n\n\n\n쇠퇴하지 않는 사람이 되기 위해 꾸준히 해야하는 활동에는 어떤게 있을까?\n운에 좌절하지 않기 위해 어떤걸 준비해야 할까?\n학점을 잘 받으려면 어떻게 공부해야 할까?\n소중한 인연은 무엇인가?\n무엇을 위해 발전해야 하는가?\n시간이 지나도 가치있는건 무엇일까?\n공명하는 지식이 매번 진실일까?\n돈을 잘 벌려면 어떻게 해야할까?\n\n\n\n그런 다음, 해당 주제와 관련된 자료에서 아래의 기준에 해당하는 내용들을 선별한다. 수집하는 자료는 외부에 존재하는 자료뿐만 아니라 자료를 수집하면서 얻은 내면 세계의 아이디어 역시 그 대상이될 수 있다.\n\n영감을 불러일으키는가\n나와 내 일에 유용한가\n개인적인 정보인가\n가족이나 친구들과 나눈 문자 메세지들도 수집의 대상이 될 수 있다.\n놀랄 만한 사실인가\n기존의 알고있는 자료만 수집하면 확증편향의 위험이 있다.\nsecond brain은 이미 알고 있는 내용을 또 확인하는 방법이 되어서는 안 된다.\n\n다음과 같은 유형들은 보관하기에 적합하지 않다.\n\n민감한 정보\n포토샵 파일이나 비디오 영상처럼 전용 앱이 필요한 경우\n대용량 파일\n공동 편집이 필요한 경우\n\n\n\nOrganize\n수집한 자료를 정리할 때, 종류별로 나누지 않고, 얼마나 실행 가능한지에 따라 정리할 수 있다. 주제와 하위 항목으로 연달아 이루어진 복잡한 계층 체계에 따라 메모를 정리하는 대신, 이것은 어떤 프로젝트에 가장 도움이 될까?라는 간단한 질문 하나에만 답하면 된다.\n\nPARA\n\nProject: 일이나 생활에서 현재 진행 중이며 단기간 노력이 필요한 일\n시작과 끝이 존재. 완성, 승인, 착수, 발표처럼 구체적이고 확실한 결과가 있어야 한다.\nArea: 오랫동안 관리하고 싶고 장기적으로 책임지는 일\n정해진 종료 날짜와 최종 목표가 없음.\nResource: 향후 도움이 될 수 있는 주제 혹은 관심사\n현재 진행하는 프로젝트 혹은 영역과 관련 없는 자료, 당분간 실행할 수 없는 메모나 파일 등을 보관할 수 있다.\nArchive: 전에는 위의 세 가지 유형에 속했지만, 지금은 비활성화된 항목\n완료하거나 취소된 프로젝트, 이제는 관리하지 않는 책임 영역, 흥미를 잃은 자원 등을 보관할 수 있다.\n\nPARA 정리 방식은 부엌 정리 방식과 유사하다. 부엌에 있는 물건들은 전부 식사를 준비하도록 설계되고 정리된다. 각각의 상위 폴더들을 비유하면, archive는 냉동고, resource는 식료품 저장고, 영역은 냉장고, 프로젝트는 불 위에서 끓고 있는 냄비나 팬과 같다.\n부엌을 음식 종류에 따라 정리하면 얼마나 터무니없을지 상상해보라. 신선한 과일과 말린 과일, 과일 주스와 냉동 과일은 모두 과일로 만들었다는 이유로 같은 장소에 보관될 것이다. 그런데 이것이 바로 대부분의 사람들이 파일과 메모를 정리하는 방식이다. 책을 읽으며 메모했다는 이유만으로 책 메모는 책 메모끼리, 다른 사람의 말을 인용했다는 이유만으로 인용문은 인용문끼리 보관한다.\n\n\n\nDistill\n메모는 단순한 수집을 넘어 실제 활용이 가능한 형태로 정제되어야 한다. 이를 위해 다음과 같은 단계별 요약 과정을 거친다.\n\n메모 수집: 먼저 빠르게 수집, 정리 이후 정제는 나중에 진행\n굵게 처리: 중요한 문장이나 구절을 표시\n하이라이트 처리: 굵게 처리된 내용 중 핵심을 강조\n핵심 요약: 최종적으로 메모의 핵심을 추출\n\n이 과정에서 주의할 점들\n\n과다 하이라이트 처리: 이전 단계 내용의 10-20% 정도만 선별\n목적 없는 하이라이트 처리: 무작정 시작하지 않고, 메모를 어떻게 사용할지 알게 될 때까지 기다린 후, 필요에 따라 하이라이트 한다.\n어려운 방식의 하이라이트 처리: 본인의 직관에 맞게 흥미로운 구절들을 하이라이트 한다.\n\n\n\n\n\n\n\n메모의 생존 여부는 ’얼마나 쉽게 찾을 수 있는가’에 달려있다.\n\n\n\n\n\nExpress\n맡은일을 중간 단계로 나누어서 최대한 빠르게 결과물을 도출하라 도출한 작업물들을 중간단계로써 다른 프로젝트에 사용할 때, 도움을 얻을 수 있다. 도출한 결과물들을 다른사람들과 공유를 해서 피드백을 받아라",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/blog/notes/1.html#창조력을-완성하는-과정",
    "href": "posts/03_resources/blog/notes/1.html#창조력을-완성하는-과정",
    "title": "Second Brain - 티아고 포르테",
    "section": "창조력을 완성하는 과정",
    "text": "창조력을 완성하는 과정\n언제든 참신한 아이디어를 떠올릴 수 있다고 기대해서는 안된다. 혁신과 문제 해결은 흥미로운 아이디어를 체계적으로 불러일으켜 우리가 인식하게 하는 일상에 달려 있다.\n세컨드브레인은 창의적인 과정들을 아이디어 수집, 정리, 핵심 추출, 조립의 단계로 표준화하여 우리의 뇌 활동을 돕는다.\n\n창의적인 프로젝트를 완료할 때 도움이 되는 전략\n\n아이디어 군도: 프로젝트 수행에 필요한 모든 문서를 모은다. 그리고 해당 문서들을 연결하라.\n헤밍웨이 다리: 현재 진행중인 프로젝트에서 다음과 같은 사항들을 메모에 기록하라.\n\n다음 단계에는 어떤 이야기를 쓸 지\n현재 상황\n잊어버리기 쉬운 세부 사항\n다음 작업 시간의 목표\n\n범위 조금씩 축소하기: 프로젝트의 복잡한 문제가 드러나면 과감하게 범위를 축소하라\n\n\n\n효율적인 실행을 위한 세 가지 습관\n\n\n\n\n\n\n정리정돈은 타고난 특성이 아닌 습관이다.\n\n\n\n\n체크리스트 습관\n\n수집: 프로젝트에 대한 내 생각을 수집하라\n\n이 프로젝트에 대해 이미 알고 있는 것은 무엇인가?\n알아내야 하지만 아직 모르는 것은 무엇인가?\n목표나 목적은 무엇인가?\n통찰력을 얻으려면 누구와 대화해야 하는가?\n아이디어를 얻으려면 어떤 것을 읽거나 들어야 하는가?\n\n검토: 관련 메모가 있을 만한 폴더나 태그를 검토하라\n검색: 모든 폴더에서 관련 용어를 검색하라\n이동: 관련 메모를 프로젝트 폴더로 이동하거나 태그를 설정하라\n작성: 수집한 메모로 개요를 작성하고 프로젝트를 계획하라\n\n\n\n리뷰 습관\n\n주간 리뷰\n일주일 동안 작업한 모든 메체의 메모를 검토하라 그리고 이번 주 할 과제를 정하라\n월간 리뷰\n솔직히 이런건 잘 안할거 같다.\n\n\n\n알아차리는 습관",
    "crumbs": [
      "PARA",
      "Resources",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_resources/인생/notes/02.html",
    "href": "posts/03_resources/인생/notes/02.html",
    "title": "나의 단점에 관한 고찰",
    "section": "",
    "text": "나는 단점이 없다고 생각했다. 그래서 자기소개서 같은 곳에 장점, 단점을 적는 칸이 있으면 아래같이 유머 아닌 유머같은 답을 적곤 했다.\n\n\n\n장점\n단점이 없다.\n\n\n단점\n\n\n\n\n최근 드는 생각인데, 내 단점은 사람을 대할 때 꽤나 간사한 면이 있다는 것이다.\n사회적으로나, 능력적으로나, 누구든 나보다 잘나다고 생각되는 사람 앞에서 나는 수줍고 소심한 사람이 된다. 뭐.. 사실 이정도는 누구나 그런 면이 있을 수 있다 생각한다. 하지만 나보다 열등하다고 생각되는 사람 앞에서 나는 꽤 강압적이고 무례한 사람이 된다. 흔히 말하는 ’강약약강’이라는 말이 아마 나를 잘 설명해주는 것 같다.\n사실 어쩌면 이런 강압적인 모습이 나의 본성이 아닐까 하는 생각이 든다. 수줍고 소심한 모습은 아마 사회화된 또 다른 나의 모습이 아닐까. 왜냐하면 나는 나의 부모님에게서 수줍고 소심한 모습을 본 적이 없기 때문이다.\n그렇다면 나는 이 단점들을 극복해야 하나? 극복한다면 어떤 모습이 바람직한 나의 모습일까? 쓸모없는 사회화된 모습을 덜어야 할까? 아니면 추한 본성을 덜어야 할까? 사실 지금의 나도 살아가는데 그렇게 큰 불편함은 없긴 하다. 조금씩 밸런스를 맞춰가며 살아가야지.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "인생",
      "Notes",
      "나의 단점에 관한 고찰"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html",
    "href": "posts/03_resources/tofel_준비/index.html",
    "title": "TOFEL 준비",
    "section": "",
    "text": "BEFORE-START\n    \n    \n        시작일: None\n        종료일: None\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        English",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#details",
    "href": "posts/03_resources/tofel_준비/index.html#details",
    "title": "TOFEL 준비",
    "section": "Details",
    "text": "Details\nTOFEL을 준비해 봅시다.",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#tasks",
    "href": "posts/03_resources/tofel_준비/index.html#tasks",
    "title": "TOFEL 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/tofel_준비/index.html#related-posts",
    "href": "posts/03_resources/tofel_준비/index.html#related-posts",
    "title": "TOFEL 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html",
    "href": "posts/03_resources/terraform/index.html",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#details",
    "href": "posts/03_resources/terraform/index.html#details",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#tasks",
    "href": "posts/03_resources/terraform/index.html#tasks",
    "title": "Terraform",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#참고-자료",
    "href": "posts/03_resources/terraform/index.html#참고-자료",
    "title": "Terraform",
    "section": "참고 자료",
    "text": "참고 자료\n\nKodeKloud - Terraform cloud",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/terraform/index.html#related-posts",
    "href": "posts/03_resources/terraform/index.html#related-posts",
    "title": "Terraform",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_resources/smart_contract/notes/block_chain_basic/00.html",
    "href": "posts/03_resources/smart_contract/notes/block_chain_basic/00.html",
    "title": "what is a blockchain?",
    "section": "",
    "text": "oracle(회사 아님): a trusted third party that provides data to the blockchain\nchain link: a decentralized oracle network that connects smart contracts to external data sourcesa\nsmart contract: trust minimized agreements, unbrakable promises\nmetamask는 nimonics를 이용해 private key를 생성. 다계정을 만들 때는 nimonics + &lt;index&gt;를 이용해 계정 생성\nprivate key는 transaction을 sign할 때 사용. public key는 transaction을 verify할 때 사용\nverify된 transaction은 miner에 의해 블록에 추가됨\ngas price: Base Fee + Priority Fee\ntransaction fee: 실제로 지불하는 금액. Gas Prics * used gas (&lt; gas limit). transaction fee - burnt fee만큼 즉, priority fee * used gas만큼 miner에게 지급됨\ngas fee: transaction을 처리하는데 필요한 비용. gas fee가 높을수록 빨리 처리됨\n\nBase fee: network congestion에 따라 변동. Base fee * used gas 만큼 소각됨\nMax fee: 사용자가 지불할 수 있는 최대 gas price\nMax Priority: 사용자가 지불할 수 있는 최대 fee + tip\n\nconsensus algorithm: 블록체인 네트워크의 모든 노드가 동의하는 방식 (nakamoto consensus: proof of work + longest chain)\n\nChain selection:\n\nlongest chain: 가장 긴 체인을 선택\n\nsybil resistance: 한 사람이 여러 개의 가짜 계정을 만들고 시스템을 조작하는 Sybil 공격을 방어하는 능력\n\nproof of works: hash를 0으로 만드는 nonce를 찾음. 제일 먼저 찾은 사람이 블록을 추가할 수 있음 (transaction fee + block reward(네트워크에서 새로 발행하는 코인. 갈수록 줄어듦))\nproof of stake\n\n\nL1: base layer of blockchain ecosystem\nL2: application built outside of the L1 and hooks back into the L1\nroll up: L2에서 발생한 transaction을 L1에 기록하는 방식\n\noptimistic roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되기 전까지는 롤백 가능\nzk roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되면 롤백 불가능\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "Smart Contract",
      "Notes",
      "Block Chain Basic",
      "what is a blockchain?"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#금융-행동의-개인차",
    "href": "posts/03_resources/금융/notes/00.html#금융-행동의-개인차",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "1. 금융 행동의 개인차",
    "text": "1. 금융 행동의 개인차\n금융 시장에서 개인의 행동 차이는 단순히 정보의 우위나 지적 능력의 차이가 아닌, 개인의 경험과 가치관에서 비롯된다. 우리는 각자의 경험을 바탕으로 나름의 합리적인 의사결정을 내린다. 따라서 겉보기에 비합리적으로 보이는 행동도 개인의 맥락에서는 충분히 이해될 수 있다. 돈 문제에 있어서 누구나 미친짓을 한다. 거의 모두가 이 게임이 처음이기 때문이다. 하지만 실제로 미친사람은 없다. 누구나 자신만의 경험에 근거해서 합리적으로 보이는 의사결정을 내릴 뿐이다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#운과-리스크의-역할",
    "href": "posts/03_resources/금융/notes/00.html#운과-리스크의-역할",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "2. 운과 리스크의 역할",
    "text": "2. 운과 리스크의 역할\n금융 시장에서의 결과는 우리의 행동만으로 결정되지 않는다. 운의 영향력을 인정하고, 리스크를 적절히 관리하는 것이 중요하다. 이를 위해 우리는 다음과 같은 질문들을 스스로에게 던져야 한다\n\n추가적인 수익이 정말 필요한가?\n타인과의 비교가 판단을 흐리고 있지는 않은가?\n’충분함’의 기준은 무엇인가?\n돈보다 우선시해야 할 가치는 무엇인가?\n\n어느 정도가 충분한지 깨닫고 리스크를 멈출줄 알아야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#지속가능한-투자의-원칙",
    "href": "posts/03_resources/금융/notes/00.html#지속가능한-투자의-원칙",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "3. 지속가능한 투자의 원칙",
    "text": "3. 지속가능한 투자의 원칙\n일회성 수익보다는 지속가능한 수익이 더 가치있다. 투자에는 두 가지 다른 기술이 필요하다\n\n수익 창출: 리스크 감수, 낙관적 사고, 적극적 태도\n자산 보존: 신중함, 위험 관리, 절제\n\n최고의 수익률은 일회성이어서 반복할 수 없는 경향이 있다. 꽤 괜찮은 수익률을 오랫동안 반복할 수 있는게 훌륭한 투자다. 성공적인 투자자는 대중이 비이성적일 때도 침착함을 유지할 수 있는 사람이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#돈과-시간의-관계",
    "href": "posts/03_resources/금융/notes/00.html#돈과-시간의-관계",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "4. 돈과 시간의 관계",
    "text": "4. 돈과 시간의 관계\n돈의 진정한 가치는 그것이 우리에게 주는 시간의 자유에 있다. 돈이 주는 가장 큰 배당금은 시간이다 단순히 부자(rich)가 되는 것과 진정한 부(wealthy)를 이루는 것은 다르다. 진정한 부자들은 겉으로 보이는 치장(rich)에 돈을 쓰기 보다는 부를 축적(wealthy)하여 자유를 얻는다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "href": "posts/03_resources/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "5. 금융시장의 불변요소와 가변요소",
    "text": "5. 금융시장의 불변요소와 가변요소\n금융 시장에서 인간의 기본적인 행동 패턴은 크게 변하지 않는다. 탐욕, 공포, 스트레스 상황에서의 반응 등은 시대가 바뀌어도 유사하다. 반면, 시장 트렌드, 산업 구조, 투자 방식 등은 끊임없이 진화한다.",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#리스크-관리의-중요성",
    "href": "posts/03_resources/금융/notes/00.html#리스크-관리의-중요성",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "6. 리스크 관리의 중요성",
    "text": "6. 리스크 관리의 중요성\n\n파산 위험이 있는 리스크는 절대 감수하지 않는다\n계획이 실패했을 때를 대비한 백업 플랜이 필수적이다\n시장의 변동성은 피해야 할 벌금이 아닌, 수수료로 인식해야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#현실적인-목표-설정",
    "href": "posts/03_resources/금융/notes/00.html#현실적인-목표-설정",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "7. 현실적인 목표 설정",
    "text": "7. 현실적인 목표 설정\n\n이상적인 목표와 현실적인 스트레스 상황은 큰 차이가 있다\n과거의 비현실적 목표는 과감히 버려야 한다\n내가 지금과 다른 사람일 때 세웠던 목표는 생명 유지 장치를 달고 시간을 질질 끌 게 아니라 가차 없이 버리는 편이 낫다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/금융/notes/00.html#시장의-본질-이해",
    "href": "posts/03_resources/금융/notes/00.html#시장의-본질-이해",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "8. 시장의 본질 이해",
    "text": "8. 시장의 본질 이해\n\n극단적 상황은 오래 지속되지 않는다\n투자 성공의 대가를 이해하고 지불할 준비가 필요하다\n시장을 완벽히 통제할 수 있다는 환상을 버려야 한다",
    "crumbs": [
      "PARA",
      "Resources",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍이란?",
    "text": "양자 프로그래밍이란?\n양자 프로그래밍은 양자 컴퓨터의 힘을 활용해 알고리즘과 소프트웨어를 개발하는 것으로, 중첩(superposition), 얽힘(entanglement), 양자 병렬성(quantum parallelism)과 같은 양자 역학 원리를 사용합니다. 이는 양자 회로를 설계하고, 양자 게이트를 적용하며, 큰 수를 인수분해하는 쇼어(Shor) 알고리즘이나 데이터베이스 검색을 위한 그로버(Grover) 검색 알고리즘과 같은 양자 알고리즘을 구현하는 작업을 포함합니다.\n양자 프로그래밍은 아직 초기 단계에 있지만 암호학, AI, 최적화, 과학적 시뮬레이션 등에서 잠재적인 응용 가능성을 가지고 있습니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 언어",
    "text": "양자 프로그래밍 언어\n양자 프로그래밍 언어는 정의상 양자 컴퓨터용 프로그램을 작성하기 위해 설계된 언어입니다. 양자 프로그래밍 언어를 고전 프로그래밍 언어와 구분 짓는 요소는 양자 시스템의 원리(큐비트, 얽힘, 중첩 법칙 등)에 기반해 양자 알고리즘을 평가하는 방식입니다.\n양자 컴퓨팅에 널리 사용되는 프로그래밍 언어로는 Qiskit, Cirq, Q# 등이 있으며, 이들은 고전 컴퓨팅보다 훨씬 빠르게 복잡한 문제를 해결할 수 있는 양자 알고리즘 개발을 가능하게 합니다. 특히 암호학, 최적화, 머신러닝 분야에서 두각을 나타냅니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 vs 고전 프로그래밍",
    "text": "양자 프로그래밍 vs 고전 프로그래밍\n양자 프로그래밍과 고전 프로그래밍 사이에는 근본적인 차이가 있습니다. 각각의 논리, 언어, 응용 분야가 다르며, 이는 양자 컴퓨팅과 고전 컴퓨팅의 차이와 비슷합니다.\n\n고전 프로그래밍\n고전 프로그래밍은 이진 논리에 기반하며, 정보는 비트(0과 1)로 표현되고 계산은 결정론적 단계를 따릅니다. 프로그램은 CPU나 GPU와 같은 고전 하드웨어에서 실행되며, AND, OR, NOT 같은 부울 논리 게이트를 사용해 순차적이거나 병렬적으로 연산을 수행합니다. Python, C++, Java 같은 전통적인 프로그래밍 언어를 사용하며, 주어진 입력에 대해 출력은 항상 예측 가능합니다.\n고전 컴퓨터는 웹 개발부터 과학적 시뮬레이션까지 일상적인 대부분의 응용 프로그램을 처리합니다. 하지만 암호학이나 복잡한 최적화와 같이 대규모 계산이 필요한 문제에서는 한계를 보입니다.\n\n\n양자 프로그래밍\n양자 프로그래밍은 양자 역학 원리에 기반하며, 중첩 상태에 존재하고 얽힐 수 있는 큐비트를 사용해 훨씬 빠른 계산을 수행합니다. 고전 프로그램과 달리 양자 프로그램은 확률적(probabilistic)입니다. 즉, 출력은 큐비트를 반복적으로 측정해 얻어지며, 이 과정에서 큐비트는 확정된 상태로 붕괴합니다.\n양자 프로그래밍은 Qiskit(Python 기반), Quipper(Haskell 기반), Cirq 같은 특수 양자 언어를 필요로 하며, IBM Quantum이나 Google Sycamore 같은 양자 프로세서에서 작동합니다. 양자 회로는 Hadamard, CNOT, Pauli-X 등의 양자 게이트를 사용하며, 암호학, 최적화, 양자 시뮬레이션과 같은 분야에서 전례 없는 능력을 제공합니다. 다만 기술은 아직 개발 중입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "href": "posts/03_resources/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "title": "Quantum Programming",
    "section": "집에서 양자 프로그래밍: 가능할까?",
    "text": "집에서 양자 프로그래밍: 가능할까?\n과거에는 양자 프로그래밍이 복잡성과 양자 컴퓨팅 하드웨어의 접근성 문제로 인해 대부분의 개인에게 불가능해 보였을 수 있습니다. 하지만 BlueQubit의 등장으로 양자 개발은 열정가와 초보자 모두에게 현실이 되었습니다.\nBlueQubit은 누구나 언제 어디서나 양자 컴퓨팅의 힘을 경험할 수 있게 하는 고급스럽고 사용자 친화적인 플랫폼입니다. BlueQubit이 양자 컴퓨팅 입문자에게 최고의 선택인 이유 중 하나는 사용 편의성입니다. 더 나은 사용자 경험을 제공하는 데 초점을 맞춘 이 플랫폼은 기술적 세부 사항에 깊이 들어가지 않아도 양자 컴퓨터의 능력을 활용할 수 있게 합니다.\nCirq와 Qiskit 같은 오픈소스 라이브러리와 매끄럽게 통합되어 사용자는 집에서도 양자 프로그램을 실행할 수 있습니다. 이 기능은 인프라 투자 없이 양자 컴퓨팅의 잠재력을 탐구하고자 하는 개발자와 연구자에게 무한한 가능성을 열어줍니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "href": "posts/03_resources/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "title": "Quantum Programming",
    "section": "양자 컴퓨팅 언어의 유형",
    "text": "양자 컴퓨팅 언어의 유형\n양자 컴퓨팅 언어는 양자 알고리즘을 프로그래밍하고 실행하는 데 각기 다른 역할을 하며 다양한 형태로 존재합니다. 여기에는 고급 양자 프로그래밍 언어, 저수준 명령어 세트, 소프트웨어 개발 키트가 포함됩니다.\n\n양자 프로그래밍 언어\n양자 프로그래밍 언어는 양자 알고리즘을 표현하고 큐비트, 양자 게이트, 측정을 제어하기 위해 설계되었습니다. 양자 프로그램 작성을 위한 고수준 추상화를 제공합니다. 고전 언어와 달리 중첩, 얽힘, 양자 병렬성과 같은 양자 특유의 연산을 지원합니다.\n예로는 Qiskit(Python 기반), Quipper(Haskell 기반), Silq(고수준 양자 언어), Q#(Microsoft의 양자 언어)가 있습니다. 이 언어들은 연구자와 개발자가 양자 응용 프로그램을 구축하고 고전 코드와 통합해 하이브리드 양자-고전 계산을 가능하게 합니다.\n\n\n양자 명령어 세트\n양자 명령어 세트는 양자 하드웨어를 직접 제어하는 저수준 명령을 정의합니다. 이는 고전 컴퓨팅의 어셈블리 언어와 비슷합니다. Hadamard, CNOT, 위상 게이트 같은 양자 연산을 위한 게이트 수준 명령을 제공하며, 서로 다른 양자 하드웨어 아키텍처에서 효율적인 실행을 보장합니다.\n예로는 OpenQASM(IBM), Quil(Rigetti), Blackbird(Xanadu)가 있습니다. 이들은 양자 알고리즘과 물리적 큐비트 간의 인터페이스 역할을 합니다.\n\n\n양자 소프트웨어 개발 키트\n양자 SDK는 양자 프로그램을 개발하고, 테스트하고, 실행하기 위한 도구, 라이브러리, 시뮬레이터를 제공합니다. 고수준 프로그래밍 언어와 양자 하드웨어 간의 간극을 메웁니다. 대표적인 SDK로는 Qiskit(IBM), Cirq(Google), PennyLane(Xanadu), Braket(AWS)이 있습니다. 이 SDK들은 양자 회로 시뮬레이션, 실제 양자 장치에서 알고리즘 실행, 기존 응용 프로그램에 양자 컴퓨팅 통합을 가능하게 해 연구와 실용적 채택을 가속화합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "href": "posts/03_resources/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "title": "Quantum Programming",
    "section": "인기 있는 양자 프로그래밍 언어와 라이브러리",
    "text": "인기 있는 양자 프로그래밍 언어와 라이브러리\n양자 시스템의 힘을 활용하기 위해 다양한 프로그래밍 언어와 라이브러리가 개발되었습니다. 이들은 양자 회로를 생성, 조작, 실행하도록 특별히 설계되었으며 고전 프로그래밍 언어와는 다릅니다. 다음은 익숙해질 만한 최고의 양자 프로그래밍 언어 목록입니다:\n\nQiskit\nQiskit은 IBM에서 만든 오픈소스 양자 컴퓨팅 프레임워크입니다. 양자 회로 설계 및 실행을 위한 사용하기 쉬운 인터페이스와 양자 시스템 시뮬레이션 및 양자 알고리즘 최적화 도구를 제공합니다. 널리 채택된 도구로, 초보자와 숙련된 개발자 모두에게 최고의 양자 프로그래밍 언어 중 하나입니다.\n\n\nCirq\nCirq는 Google Quantum AI에서 개발한 인기 있는 양자 프로그래밍 라이브러리입니다. 개발자가 시뮬레이터와 실제 양자 하드웨어에서 양자 회로를 생성, 편집, 실행할 수 있게 합니다. 사용자 친화적인 인터페이스와 강력한 기능으로 양자 프로그래밍을 탐구하려는 이들에게 최고의 선택입니다.\n\n\nPyQuil\nPyQuil은 Rigetti Computing에서 만든 독창적인 양자 명령어 언어로, 양자 프로그래밍에 독특한 접근 방식을 제공합니다. 양자 알고리즘 생성 과정을 단순화하도록 설계된 PyQuil은 Rigetti의 양자 프로세서 및 시뮬레이터와의 호환성을 유지하며 양자 응용 프로그램 개발을 간소화합니다.\n\n\nQ\nMicrosoft에서 개발한 Q#은 양자 프로그래밍을 위해 특화된 도메인별 언어입니다. Quantum Development Kit(QDK)와 통합되어 개발자가 양자 알고리즘을 고전 및 양자 하드웨어에서 작성, 테스트, 디버깅하기 쉽게 합니다. 고수준 문법과 풍부한 라이브러리로 Q#은 양자 응용 프로그램 생성을 단순화합니다.\n\n\nQasm과 OpenQasm\nQasm(Quantum Assembly Language)과 그 오픈소스 버전인 OpenQasm은 양자 회로를 위한 중급 표현입니다. 이 언어들은 양자 명령을 위한 표준 형식을 제공하여 다양한 플랫폼에서 양자 회로를 설계하고 시뮬레이션하기 쉽게 합니다. 특히 OpenQasm은 모듈성과 확장성을 지원해 복잡한 양자 프로그램을 효율적으로 작성할 수 있게 합니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#마무리",
    "href": "posts/03_resources/quantum_programming/notes/00.html#마무리",
    "title": "Quantum Programming",
    "section": "마무리",
    "text": "마무리\n양자 프로그래밍은 산업을 변화시킬 엄청난 잠재력을 가진 흥미로운 분야입니다. 쇼어 알고리즘과 그로버 알고리즘 같은 핵심 알고리즘을 이해하고, Qiskit, Cirq, PyQuil, Q#, OpenQasm과 같은 인기 언어와 라이브러리를 사용하면 초보자도 자신 있게 양자 세계에 입문할 수 있습니다.\n양자 컴퓨팅 회사인 BlueQubit은 사용자 친화적인 인터페이스, 강력한 양자 시뮬레이터, 실제 양자 하드웨어 접근성을 제공하여 개발자가 양자 컴퓨팅의 힘을 활용하고 혁신을 이끌어내기에 이상적인 선택입니다. 지금 가입하고 프로그래밍을 시작하세요.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/notes/00.html#자주-묻는-질문",
    "href": "posts/03_resources/quantum_programming/notes/00.html#자주-묻는-질문",
    "title": "Quantum Programming",
    "section": "자주 묻는 질문",
    "text": "자주 묻는 질문\n\n양자 컴퓨팅을 위한 C 언어란 무엇인가요?\nC 자체는 양자 컴퓨팅에 일반적으로 사용되지 않지만, QCOR(Quantum Computing ORchestration)은 C++의 확장으로 양자 프로그래밍과 고전 컴퓨팅을 통합합니다. 이 언어는 양자 하드웨어와 시뮬레이터와 함께 작동하도록 설계되어 개발자가 하이브리드 양자-고전 알고리즘을 효율적으로 작성할 수 있게 합니다. 그러나 오늘날 대부분의 양자 프로그래밍은 Qiskit(Python), Cirq(Python), Q#(Microsoft의 양자 언어)와 같은 고수준 언어에 의존합니다. 이는 사용 편의성과 양자 특유의 기능을 제공하기 때문입니다.\n\n\nPython은 양자 컴퓨팅에 사용되나요?\n네, Python은 Qiskit, Cirq, PennyLane과 같은 강력한 양자 컴퓨터 프로그래밍 라이브러리 덕분에 양자 컴퓨팅에 널리 사용됩니다. 이 라이브러리들은 직관적인 API, 양자 회로 시뮬레이터, 실제 양자 하드웨어에서 프로그램을 실행할 수 있는 도구를 제공합니다. Python의 유연성과 단순함은 양자 연구에 이상적이며, 양자 알고리즘을 구축, 테스트, 배포하면서 고전 계산과 통합하기 쉽게 합니다. IBM Quantum Experience와 Amazon Braket 같은 많은 양자 컴퓨팅 플랫폼도 Python 기반 프레임워크를 지원합니다.\n\n\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 무엇인가요?\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 사용 사례와 하드웨어 호환성에 따라 다릅니다. Qiskit(Python 기반)은 사용자 친화적인 인터페이스와 IBM Quantum의 강력한 지원으로 초보자와 연구자에게 널리 사용됩니다. Cirq(역시 Python 기반)는 Google의 양자 하드웨어에 최적화되어 있으며, Q#(Microsoft)는 고전 통합과 함께 양자 알고리즘 개발에 설계되었습니다.\n기타 주목할 만한 양자 컴퓨팅 프로그래밍 언어로는 Silq(고수준 양자 프로그래밍), Quipper(Haskell 기반), OpenQASM(어셈블리 스타일 양자 언어)이 있습니다. Python 기반 프레임워크가 이 분야를 지배하고 있으므로 Qiskit과 Cirq가 가장 인기 있는 선택입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html",
    "href": "posts/03_resources/quantum_programming/index.html",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#details",
    "href": "posts/03_resources/quantum_programming/index.html#details",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#tasks",
    "href": "posts/03_resources/quantum_programming/index.html#tasks",
    "title": "Quantum Programming",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#참고-자료",
    "href": "posts/03_resources/quantum_programming/index.html#참고-자료",
    "title": "Quantum Programming",
    "section": "참고 자료",
    "text": "참고 자료\n\n인프런 양자 프로그래밍 강의",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_resources/quantum_programming/index.html#related-posts",
    "href": "posts/03_resources/quantum_programming/index.html#related-posts",
    "title": "Quantum Programming",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론이란",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론이란",
    "title": "퍼셉트론",
    "section": "퍼셉트론이란",
    "text": "퍼셉트론이란\n다수의 신호를 입력으로 받아 하나의 신호를 출력하는 것\n\\[\ny = \\begin{cases} 0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\ 1 & (w_1x_1 + w_2x_2 &gt; \\theta) \\end{cases}\n\\]",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#논리-회로",
    "href": "posts/02_areas/deep_learning/notes/00.html#논리-회로",
    "title": "퍼셉트론",
    "section": "논리 회로",
    "text": "논리 회로\n파라미터 \\((w_1, w_2, θ)\\)의 값을 조정하여 AND, OR, NAND 게이트를 구현할 수 있다.\n머신 러닝의 목적은, 기계가 알아서 파라미터의 값을 적절히 조정하는 것이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론-구현",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론-구현",
    "title": "퍼셉트론",
    "section": "퍼셉트론 구현",
    "text": "퍼셉트론 구현\n\nAND 게이트\n\ndef AND(x1, x2):\n    w1, w2, theta = 0.5, 0.5, 0.7 # parameter\n    return (x1*w1 + x2*w2 &gt; theta)\n\n\nprint(AND(0, 0))\nprint(AND(1, 0))\nprint(AND(0, 1))\nprint(AND(1, 1))\n\nFalse\nFalse\nFalse\nTrue\n\n\n여기서 θ를 \\(-b\\)로 치환하고 식을 다시 정리하면 다음과 같다.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때 \\(w_1\\)과 \\(w_2\\)(가중치)는 각각의 입력신호가 결과에 주는 영향력을 조절하고, \\(b\\)(편향)은 뉴런이 얼마나 쉽게 활성화되는지를 조정한다. (가중치 합이 -b를 초과할 때만 뉴런이 활성화된다.)\n이제 재구성한 식과, numpy를 이용하여 NAND와 OR 게이트를 구현해보자.\n\n\nNAND 게이트\n\nimport numpy as np\n\ndef NAND(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([-0.5, -0.5])\n  b = 0.7\n  return (b + np.sum(x * w) &gt; 0)\n\n\ndef OR(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([0.5, 0.5])\n  b = -0.2\n  return (b + np.sum(x * w) &gt; 0)\n\n세 게이트의 차이는 오직 파라미터의 값이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론의-한계",
    "href": "posts/02_areas/deep_learning/notes/00.html#퍼셉트론의-한계",
    "title": "퍼셉트론",
    "section": "퍼셉트론의 한계",
    "text": "퍼셉트론의 한계\nAND, NAND, OR 게이트는 만들 수 있지만, XOR 게이트는 만들 수 없다. 다른 게이트들과 다르게 선형적으로 구분이 안되기 때문이다.\n하지만 AND NAND OR 게이트를 다음과 같이 배치하면 XOR 게이트를 만들 수 있다.\n\n\n\n\n\nflowchart LR\n    x1((x1)) --&gt; OR\n    x2((x2)) --&gt; OR\n    x1 --&gt; NAND\n    x2 --&gt; NAND\n    OR[OR 게이트] --&gt; AND\n    NAND[NAND 게이트] --&gt; AND\n    AND[AND 게이트] --&gt; output((XOR 출력))\n\n\n\n\n\n\n(mermaid로는 이렇게 그리는게 최선이다.)\n이와 같이 여러 퍼셉트론을 연결한 형태를 다층 퍼센트론이라고 한다.\n\nXOR 게이트\n\ndef XOR(x1, x2):\n  s1 = OR(x1, x2)\n  s2 = NAND(x1, x2)\n  return AND(s1, s2)\n\n\nprint(XOR(0, 0))\nprint(XOR(0, 1))\nprint(XOR(1, 0))\nprint(XOR(1, 1))\n\nFalse\nTrue\nTrue\nFalse",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#intro",
    "href": "posts/02_areas/42_seoul/notes/08.html#intro",
    "title": "cloud-1 개념 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n다음 학기 시작 전까지 개념공부만 하면서 시간을 보내려고 하니까 프로젝트가 하고 싶어졌습니다. 원래는 python 과제를 하려고 했는데, 이전에 cloud 과제를 진행하다가 말았던게 기억나서 이어서 해보면 괜찮겠다 생각했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "href": "posts/02_areas/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "title": "cloud-1 개념 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nAWS SAA Udemy 강의\nansible terraform Udemy 강의\n\n이 강의들도 본 지 1년이 다되어가긴 하지만..과제할 때 사용한 제 배경지식이 여기서 나온거니까요. 과제를 진행하실 분들은 한번 수강해보시면 도움이 될 것 같습니다.\n\n\n\n\n\n\n이 포스팅에서 docker와 nginx, wordpress, mysql 구조에 대한 설명은 생략하겠습니다.\n전체 코드는 github repo에서 확인하실 수 있습니다.\n\n\n\n\n\nWhat is IaC?\n이 프로젝트의 목표는 IaC(Infrastructure as Code) tool을 이용하여 wordpress 사이트를 cloud에 자동으로 배포하는 것입니다.\nIaC는 인프라 구성을 코드로 관리하는 방식으로, 수동으로 리소스를 생성하고 설정하는 방식에 비해 버전 관리가 간편하고, 동일한 환경을 쉽게 재현하거나, 코드 리뷰 등의 방식으로 휴먼 에러를 줄이는 데 용이하게 사용할 수 있습니다.\n이번 프로젝트에서는 Packer, Terraform, Ansible 세 가지 IaC tool을 조합해 사용했습니다\n\n\nPacker: 인프라 생성 전, 상세 설정이 되어있는 image를 build할 수 있는 tool 입니다.\nTerraform: cloud 인프라를 생성하는 tool입니다. packer에서 생성한 ami를 사용할 수 있습니다.\nAnsible: 서버 내부의 상세 설정을 자동화합니다. 일반적인 bash script와는 다르게 멱등성 있는 설정이 가능하다는 점이 큰 장점입니다. 이때, 서버는 python이 설치되어 있어야 하고, ssh로 접근 가능해야 합니다.\n\n위의 이미지 처럼, packer로 필요한 설정이 완료된 image를 생성한 뒤, 그 이미지를 기반으로 cloud infra를 terraform으로 생성하고, 생성된 infra의 상세 설정을 ansible을 이용해서 구현해줄 것입니다.\nPacker와 Ansible은 서버 설정 자동화라는 동일한 기능을 수행하는 도구입니다. 두 도구는 각각 다양한 특징과 장단점이 있지만, 이 과제에서 알아야 하는 차이점은 아래와 같습니다.\nPacker는 임시 EC2 인스턴스를 생성하여 그 위에서 필요한 설정을 완료한 후, 해당 인스턴스를 AMI로 변환하는 방식으로 동작합니다. 이렇게 생성된 AMI는 이후 실제 인프라 구축 시 그대로 사용할 수 있습니다. 따라서 최종 목적지 서버가 SSH 접근이 제한되는 환경이더라도, 미리 필요한 모든 설정이 완료된 이미지를 사용할 수 있다는 장점이 있습니다.\n반면에 Ansible은 SSH 접근이 가능한 서버에서만 동작하지만, Packer와 달리 인프라 구축 후에 얻을 수 있는 정보(예: EC2의 IP)를 활용할 수 있습니다.\n이러한 특성을 고려하여 이 프로젝트에서는 두 도구를 상황에 맞게 조합하여 사용했습니다.\n\n\n전체적인 구조\n\n\n\n구현 aws 구조\n\n\nPublic subnet의 EC2들에 대한 ssh 접근은 관리용 컴퓨터에서만(terraform, ansible 코드가 실행되는 컴퓨터) 접근이 가능하도록 제한했고, MySQL의 데이터는 Private subnet의 EC2에 저장한 뒤 Public subnet의 EC2만 접근할 수 있도록 설정했습니다. Public subnet의 EC2는 사용자가 원하는 갯수를 설정할 수 있고, 그 갯수에 맞춰서 private subnet의 dbms EC2가 생성되도록 설계했습니다.\n실제 프로덕션 환경이라면 위와 같은 구조로는 설계하지 않습니다. 일단 EC2 머신들을 Auto Scaling Group으로 묶고, 그 앞에 Network Load Balancer를 두어 단일 엔드포인트로 관리하는 것이 좋습니다. 또한 Database는 AWS RDS를 이용하고, WordPress의 파일 시스템은 EFS나 S3를 활용해 Stateless하게 구현하는게 좋습니다.\n\n\n\n조금 더 일반적인 구조(물론 docker compose는 잘 안쓸것 같긴 합니다)\n\n\n제 구현에서는 각 서버가 독립적인 상태와 엔드포인트를 가지고 있습니다.\n그렇게 한 이유는 일단 aws free tier 서비스만으로 과제를 구현하려고 했던게 제일 크고요..(NLB는 사용할 수 없었습니다.) 나머지는 과제 제약사항 때문인데,\n\n\n\n과제 제약사항\n\n\n모든 프로세스는 컨테이너 안에서 동작해야 한다는 제약때문에, aws RDS는 사용할 수 없었습니다. 그리고 database는 public internet에서 접근할 수 없다고 해서, db는 private subnet의 ec2에서 돌아가게 설계했습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/08.html#outro",
    "href": "posts/02_areas/42_seoul/notes/08.html#outro",
    "title": "cloud-1 개념 설명",
    "section": "outro",
    "text": "outro\n여기서 구현된 infra 구조는 사실 별로 근본있는 구조는 아니니까, 이것보다는 IaC 툴을 얼마나 편리하게 사용할 수 있는지에 초점을 맞춰서 봐주시길 바라고 있습니다.\n이어서 코드에 대한 설명은 다음 게시글에 포스팅하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#intro",
    "href": "posts/02_areas/42_seoul/notes/04.html#intro",
    "title": "inception-of-things part 1",
    "section": "Intro",
    "text": "Intro\n\n\n\n42 seoul outer 과제\n\n\n42 Seoul의 공통 과정을 마무리하면, 원하는 분야를 선택하여 심화 과제를 수행할 수 있습니다. 그중에서도 ’Inception-of-Things’는 인프라 관련 심화 과제로, 가장 많은 경험치를 얻을 수 있는 과제입니다.\n얼핏 보면 매우 어려운 과제처럼 느껴질 수 있지만, 개념을 확실히 이해하고 공부한다면 누구나 빠르게 완료할 수 있다고 생각합니다. 저의 경우, CKA 자격증 취득을 목표로 k8s를 공부하던 중 우연히 팀원을 구하게 되어 이 과제를 수행하게 되었습니다. 배경지식이 어느 정도 있는 상태에서 진행하다 보니, 크게 어렵지 않게 잘 마무리할 수 있었던 것 같습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#개요",
    "href": "posts/02_areas/42_seoul/notes/04.html#개요",
    "title": "inception-of-things part 1",
    "section": "개요",
    "text": "개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nCKA Udemy 강의\nArgoCD Udemy 강의\ngitlab helm 베포 Docs\nVagrant Docs\n\n\n\n\n\n\n\n전체 코드는 비공개 되어있는 상태입니다",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#개념-설명",
    "href": "posts/02_areas/42_seoul/notes/04.html#개념-설명",
    "title": "inception-of-things part 1",
    "section": "개념 설명",
    "text": "개념 설명\ncluster는 노드(컴퓨터)들의 논리적인 집합을 의미합니다. 일반적으로, 하나의 컴퓨터로 처리하기 어려운 방대한 양의 작업을 처리하기 위해 도입을 합니다.\n클러스터는 특정한 목적을 가지고 있고, 그 안의 노드들을 각자 맡은 역할을 수행합니다. (보통 클러스터 내부의 노드들을 관리하는 master, 작업을 수행하는 worker로 구분할 수 있습니다.) 이때, k8s는 분산된 노드(컴퓨터)들을 하나의 클러스터로 묶어주고, 관리해주는 도구로써 사용할 수 있습니다.\n\n\n\n\n\n\n노트\n\n\n\n컴퓨팅 능력을 확장할 목적으로 수직적 확장과 수평적 확장을 고려할 수 있습니다.\n수직적 확장은 cpu나 memmory 성능을 높여서 단일 노드의 성능을 향상시키는 것을 의미하고, 수평적 확장은 작업을 분산시킬 수 있는 여러 노드를 추가하는 것을 의미합니다.\n클러스터링은 수평적으로 확장된 컴퓨팅 리소스들을 그룹화 해주는 것을 의미합니다.\n\n\n한 가지 주의해야 하는 것은, k8s 자체는 노드를 생성(provision)해주는 도구가 아니라는 것입니다. 즉, provision 단계는 k8s clustering 이전에 진행되어야 합니다.\n\n\n\nPart 1 구조\n\n\nPart 1에서는 vagrant tool을 이용해서 master, agent 역할을 하는 두 대의 가상 머신을 local에서 provision하고, k3s를 이용해서 clustering 하는 것을 요구합니다. 참고로 k3s는 k8s의 경량화 버전입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#코드-설명",
    "href": "posts/02_areas/42_seoul/notes/04.html#코드-설명",
    "title": "inception-of-things part 1",
    "section": "코드 설명",
    "text": "코드 설명\n파일 구조는 아래와 같습니다.\np1/\n├── scripts/\n│   ├── agent.yml\n│   └── server.yml\n└── Vagrantfile\nvagrant는 local에서 가상 머신을 생성하고, provision을 할 수 있는 도구입니다. 사용자가 원하는 스펙을 Vagrantfile 이름의 파일에 정의하면, vagrant up 명령어를 통해 간단하게 가상머신을 생성할 수 있습니다.\n과제 요구사항에 맞게 spec을 정의해줍니다.\n\n\nVagrantfile\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"bento/ubuntu-24.04\"\n  config.vm.box_version = \"202404.26.0\"\n\n  config.vm.define \"hyunghkiS\" do |control|\n    control.vm.hostname = \"hyunghkiS\"\n    control.vm.network \"private_network\", ip: \"192.168.56.110\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiS\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/server.sh\"\n  end\n  config.vm.define \"hyunghkiSW\" do |control|\n    control.vm.hostname = \"hyunghkiSW\"\n    control.vm.network \"private_network\", ip: \"192.168.56.111\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiSW\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/agent.sh\"\n  end\nend\n\n저 just for evaluation 부분은 아마 과제 명세서에 ifconfig 명령어를 입력해보는 부분 때문에 추가한 것 같습니다. (사실 이 글을 쓰는 시점은 과제를 수행하고 1년이 지난 시점이라 기억이 가물가물 합니다.)\n\n\nserver.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s - server --node-ip 192.168.56.110\nK3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)\necho $K3S_TOKEN &gt; /vagrant/k3s_token # vagrant 공유 폴더에 master token 정보를 저장해주었습니다.\n\n\n\nagent.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\nK3S_TOKEN=$(cat /vagrant/k3s_token) # vagrant 공유 폴더에 저장된 master token 정보를 읽어옵니다.\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.56.110:6443 K3S_TOKEN=$K3S_TOKEN sh -s - --node-ip 192.168.56.111\n\nk3s 공식 문서를 참고해서 master와 agent를 clustering 해주는 스크립트를 작성해주었습니다. 각각 노드 안에서 로직이 실행되어, 하나는 master로, 하나는 agent로 역할을 수행하게 됩니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/04.html#outro",
    "href": "posts/02_areas/42_seoul/notes/04.html#outro",
    "title": "inception-of-things part 1",
    "section": "Outro",
    "text": "Outro\n오랜만에 해당 과제의 로직을 다시 보니까 기억이 잘 안납니다.\n남은 부분은 천천히 포스팅하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_areas/kaggle/notes/titanic/00.html#data-이해",
    "href": "posts/02_areas/kaggle/notes/titanic/00.html#data-이해",
    "title": "titanic",
    "section": "Data 이해",
    "text": "Data 이해\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('_data/train.csv')\ntest = pd.read_csv('_data/test.csv')\n\n\ntrain.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 177개\n\n\ntest.describe()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n418.000000\n418.000000\n332.000000\n418.000000\n418.000000\n417.000000\n\n\nmean\n1100.500000\n2.265550\n30.272590\n0.447368\n0.392344\n35.627188\n\n\nstd\n120.810458\n0.841838\n14.181209\n0.896760\n0.981429\n55.907576\n\n\nmin\n892.000000\n1.000000\n0.170000\n0.000000\n0.000000\n0.000000\n\n\n25%\n996.250000\n1.000000\n21.000000\n0.000000\n0.000000\n7.895800\n\n\n50%\n1100.500000\n3.000000\n27.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1204.750000\n3.000000\n39.000000\n1.000000\n0.000000\n31.500000\n\n\nmax\n1309.000000\n3.000000\n76.000000\n8.000000\n9.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 86개\nFare 결측치 1개: 이 정도는 그냥 삭제해도 될듯\n\n\nplt.figure(figsize=(15, 10))\n\n# Age 분포 확인\nplt.subplot(2, 2, 1)\nsns.boxplot(x='Survived', y='Age', data=train)\nplt.title('Age 분포 (생존 여부별)')\n\n# Fare 분포 확인\nplt.subplot(2, 2, 2)\nsns.boxplot(x='Survived', y='Fare', data=train)\nplt.title('Fare 분포 (생존 여부별)')\n\n# Pclass에 따른 Age 분포\nplt.subplot(2, 2, 3)\nsns.boxplot(x='Pclass', y='Age', data=train)\nplt.title('Age 분포 (객실 등급별)')\n\n# Pclass에 따른 Fare 분포\nplt.subplot(2, 2, 4)\nsns.boxplot(x='Pclass', y='Fare', data=train)\nplt.title('Fare 분포 (객실 등급별)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain_x = train.drop('Survived', axis=1).values\ntrain_y = train['Survived'].values\ntest_x = test.values",
    "crumbs": [
      "PARA",
      "Areas",
      "Kaggle",
      "Notes",
      "Titanic",
      "titanic"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#dot-product",
    "href": "posts/02_areas/선형대수/notes/08.html#dot-product",
    "title": "vector dot product, cross product",
    "section": "Dot Product",
    "text": "Dot Product\n\n\\(v ⋅ w = \\sum_{i=1}^{n} v_i w_i\\)\n\n\nProperties\n\n\\(v ⋅ w = w ⋅ v\\)\n\\(v ⋅ (w + u) = v ⋅ w + v ⋅ u\\)\n\\(v ⋅ (c w) = c (v ⋅ w)\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#length-of-vector",
    "href": "posts/02_areas/선형대수/notes/08.html#length-of-vector",
    "title": "vector dot product, cross product",
    "section": "Length of vector",
    "text": "Length of vector\n\n\\(||v|| = \\sqrt{v ⋅ v}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#some-properties",
    "href": "posts/02_areas/선형대수/notes/08.html#some-properties",
    "title": "vector dot product, cross product",
    "section": "Some properties",
    "text": "Some properties\nfor non-zero vectors \\(v\\) and \\(w\\):\n\ncauchy-schwarz inequality\n\n\\(|v ⋅ w| ≤ ||v|| ||w||\\)\n\\(|v ⋅ w| = ||v|| ||w||\\) ⟺ \\(v = cw\\)\n\n\n\nTriangle inequality\n\n\\(||v + w|| ≤ ||v|| + ||w||\\)\n\n\n\nAngle between vectors\n\n\\(cosθ = \\frac{v ⋅ w}{||v|| ||w||}\\)\nif \\(v\\) and \\(w\\) are orthogonal, then \\(v ⋅ w = 0\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#cross-product",
    "href": "posts/02_areas/선형대수/notes/08.html#cross-product",
    "title": "vector dot product, cross product",
    "section": "Cross Product",
    "text": "Cross Product\n\nonly for 3D vectors\nget a vector that is orthogonal to both \\(v\\) and \\(w\\)\n\\(v × w = (v_2 w_3 - v_3 w_2, v_3 w_1 - v_1 w_3, v_1 w_2 - v_2 w_1)\\)\n\\(sinθ = \\frac{||v × w||}{||v|| ||w||}\\)\n\\(v × w = 0\\) ⟺ \\(v\\) and \\(w\\) are parallel\nv와 w로 이루어진 평행사변형의 넓이는 \\(||v × w||\\)이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#triple-productlagrange-identity",
    "href": "posts/02_areas/선형대수/notes/08.html#triple-productlagrange-identity",
    "title": "vector dot product, cross product",
    "section": "Triple product(lagrange identity)",
    "text": "Triple product(lagrange identity)\n\n\\(a x (b x c) = b(a ⋅ c) - c(a ⋅ b)\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "href": "posts/02_areas/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "title": "vector dot product, cross product",
    "section": "3차원의 평면에서 직선의 방정식",
    "text": "3차원의 평면에서 직선의 방정식\n\n평면에 대한 법선벡터 \\((a, b, c)\\)와 평면 위의 한 점 \\((x_p, y_p, z_p)\\)이 주어졌을 때, 평면의 방정식은 다음과 같다.\n\\(ax + by + cz = D\\), \\(D = ax_p + by_p + cz_p\\)\n평행한 평면은 a, b, c의 계수가 같은 평면이다.\n\\(\\frac{Ax_0 + By_0 + Cz_0 + D}{\\sqrt{A^2 + B^2 + C^2}}\\)은 평면과 점 \\((x_0, y_0, z_0)\\) 사이의 거리이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/09.html#reduced-row-echelon-form",
    "href": "posts/02_areas/선형대수/notes/09.html#reduced-row-echelon-form",
    "title": "가감법으로 연립방정식을 풀기 위한 행렬",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\n각 행의 선행항을 1로 만들고, 그 열의 다른 항을 0으로 만드는 방법을 행렬로 표현한 것이다.\n이때 선행항의 변수를 pivot variable이라고 하고, 다른 변수들은 free variable이라고 한다.\n관행적으로 pivot entry는 우하향으로 이동하고, zeroed out 행은 맨 아래쪽에 위치한다.\n기약행렬을 이용해 연립방정식으로 풀 수 없는 식을 행렬의 선형 결합으로 표현할 수 있다.\n\n\n혹은 식이 유효하지 않다면, 해가 없는 경우이며, 이것은 평행한 조건이 존재할 경우 발생한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "가감법으로 연립방정식을 풀기 위한 행렬"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/07.html#subspaces",
    "href": "posts/02_areas/선형대수/notes/07.html#subspaces",
    "title": "Subspaces and the basis",
    "section": "Subspaces",
    "text": "Subspaces\n\n\\(S\\) is a subset of \\(V\\).\n\nS ⊆ V\nS is a vector space\n\ninclude zero vector\nclosed under addition\nclosed under scalar multiplication",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/07.html#basis",
    "href": "posts/02_areas/선형대수/notes/07.html#basis",
    "title": "Subspaces and the basis",
    "section": "Basis",
    "text": "Basis\n\nminimum set of vectors that spans the subset\n\\(S\\) is a basis of \\(V\\) ⟺\n\nelements of \\(S\\) are linearly independent\n\\(S\\) spans \\(V\\)\n\n특정 부분집합의 basis의 linear combination으로 표현되는 모든 벡터는 유일하다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#metrix-vector-product",
    "href": "posts/02_areas/선형대수/notes/10.html#metrix-vector-product",
    "title": "Null space and Column space",
    "section": "Metrix vector product",
    "text": "Metrix vector product\n\nSee \\(A\\)’s column space as a set of vectors. → \\(A\\)’s column space is the set of all linear combinations of the columns of \\(A\\).\nSee \\(B\\)’s row space as a set of vectors.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#null-space",
    "href": "posts/02_areas/선형대수/notes/10.html#null-space",
    "title": "Null space and Column space",
    "section": "Null space",
    "text": "Null space\n\n\\(N = \\{x \\in \\mathbb{R}^n | Ax = 0\\}\\)\n\n\\(N\\) is Null space of \\(A\\).\n\n\n\nN(A) = N(rref(A))\nif N(A) = {0}, then column vector of \\(A\\) is linearly independent. → column vector of \\(A\\) is not a basis for C(A) → pivot variable의 합이 free variable을 만든다. (redundant column)\ndim(N(A)) = nullity of A = # of free variables in rref(A)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#column-space",
    "href": "posts/02_areas/선형대수/notes/10.html#column-space",
    "title": "Null space and Column space",
    "section": "Column Space",
    "text": "Column Space\n\nC(A) = span(columns of A)\nrref(A)의 pivot variable의 column vector가 C(A)의 basis이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/10.html#column-space의-평면의-방정식",
    "href": "posts/02_areas/선형대수/notes/10.html#column-space의-평면의-방정식",
    "title": "Null space and Column space",
    "section": "Column Space의 평면의 방정식",
    "text": "Column Space의 평면의 방정식\n\n\\({\\rightVectorBar{b} | A\\rightVectorBar{x} = \\rightVectorBar{b} ∧ \\rightVectorBar{x} ∈ R^n}\\)\n위를 만족하는 기약행렬을 만들어, 해가 존재하도록 방정식을 구성하면 평면의 방정식을 구할 수 있다.\n혹은 column space의 basis를 구하고, 이를 이용해 평면의 방정식을 구할 수 있다.\n\\(R^n\\)을 span하는 basis의 vector는 n개이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#vector",
    "href": "posts/02_areas/선형대수/notes/01.html#vector",
    "title": "2-기초(1)",
    "section": "Vector",
    "text": "Vector\nvector는 크기와 방향을 가지고 있다.\n\nExample\n\\[\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n크기: \\(\\sqrt{9 + 4} = \\sqrt{13}\\)\n방향: \\(tan^{-1}(\\frac{2}{3})\\)\n\n크기와 방향이 같으면 같은 벡터이다.\n\n\n덧셈\n벡터의 덧셈을 기하학적으로 알아보자\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.\n\n\nScalar 배\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]\n마찬가지로 좌표평면으로 나타내는건 귀찮아서 생략하겠다.\n\n\n\n\n\n\nScalar 배를 한 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#전치-transpose",
    "href": "posts/02_areas/선형대수/notes/01.html#전치-transpose",
    "title": "2-기초(1)",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n행렬 \\(A\\)의 요소 \\(a_{ij}\\)는 A의 Transpose인 \\(A^T\\)의 \\(a_{ji}\\)가 된다. 즉, 행렬 \\(A\\)를 전치하면 diagnal(대각선 요소)를 제외한 모든 요소가 대각선을 기준으로 서로 뒤바뀐다.\n\nSymmetrix matrix: \\(A = A^T\\)인 행렬, 즉 대각선을 기준으로 값이 전부 같은 행렬 Hermitian matrix: \\((A^*)^T = A^H(conjugate transpose) = A\\)를 만족하는 행렬\n\nVector의 경우에는 Column Vector의 경우, Transpose시 Row Vector로, Row Vector의 경우도 반대로 작용한다.\n\nProperties\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\(\\color{red}{(AB)^T = B^TA^T}\\)\n\\((A^TA)^T\\)와 \\((AA^T)^T\\)의 결과는 항상 자기 자신이 된다. → Symmetrix matrix\n\\(C(A)^T = CA^T\\)\n\\(det(A^T) = det(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#inner-product-projection",
    "href": "posts/02_areas/선형대수/notes/01.html#inner-product-projection",
    "title": "2-기초(1)",
    "section": "Inner Product & Projection",
    "text": "Inner Product & Projection\n\\[\n\\underset{a}{\\begin{bmatrix}\n1 \\\\\n3\n\\end{bmatrix}} *\n\\underset{b}{\\begin{bmatrix}\n5 \\\\\n1\n\\end{bmatrix}} = 1 * 5 + 3 * 1 = 8 = a^Tb = b^Ta\n\\]\n갑자기 등장한 \\(a^Tb\\)가 의미하는건 아래와 같다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n\n||a||는 a 벡터의 크기를 의미한다.\n\n위의 식을 그림으로 표현해보자\n\n\n\n\n\n\n\n\n\n내적은 초록색 화살표와 파란색 화살표의 곱으로 표현할 수 있다.\n이는 a 벡터가 b 벡터의 방향에 대해 얼마나 투영되었는지를 나타낸다.\n두 벡터의 방향이 일치할 때 내적의 값이 가장 크고, 수직일 때 0 (안 닮음을 의미), 반대 방향일 때 가장 작은 값이 된다.\n\n단위 벡터(크기가 1인 벡터) 계산\n위의 식으로 부터 다음의 추론 과정을 통해 단위 벡터를 계산할 수 있다.\n\\(a^Ta = ||a||^2\\)\n∴ \\(||a|| = \\sqrt{a^Ta}\\)\n∴ 단위 벡터는 \\(\\frac{a}{||a||}\\) = \\(\\frac{a}{\\sqrt{a^Ta}}\\)\n\n\n정사형 벡터의 좌표 계산\n벡터의 좌표는 방향과 크기의 곱으로 표현할 수 있다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n정사형 벡터의 크기는 \\(\\frac{a^Tb}{||b||} = \\frac{a^Tb}{\\sqrt{b^Tb}}\\)\n장사형 벡터의 방향은 b의 단위 벡터와 같다.\n즉, 정사형 벡터의 좌표는 \\(\\frac{a^Tb}{\\sqrt{b^Tb}} * \\frac{b}{\\sqrt{b^Tb}} = \\frac{a^Tb}{b^Tb}b\\)\n\\(a^T\\frac{b}{\\sqrt{b^Tb}}*\\frac{b}{\\sqrt{b^Tb}}\\)로도 구할 수 있다.\n\na와 수직으로 연결되는 정사형 벡터 \\(\\hat{x}\\)\n\\((a-b\\hat{x})^Tb\\hat{x} = 0\\)\n\\(a^Tb - b^Tbb\\hat{x} = 0\\)\n\\(\\hat{x} = \\frac{a^Tb}{b^Tb}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/01.html#norm",
    "href": "posts/02_areas/선형대수/notes/01.html#norm",
    "title": "2-기초(1)",
    "section": "Norm",
    "text": "Norm\n크기를 나타내는 것(0 포함, 양 음수 scalar)\n\n2-Norm (\\(l_2\\)-norm)\n벡터의 물리적인 길이.\n\\[\na = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\n\\(||a||_2 = \\sqrt{1^2+2^2+3^2} = (|1|^{\\color{red}{2}}+|2|^{\\color{red}{2}}+|3|^{\\color{red}{2}})^{\\color{red}{\\frac{1}{2}}}\\)\n2 제곱에, \\(\\frac{1}{2}\\)여서 2-norm이다.\n\n두 벡터 사이의 거리는 두 벡터의 차이의 2-norm이다.\n\n\n\n1-Norm (\\(l_1\\)-norm)\n1 제곱에 \\(\\frac{1}{1}\\)을 계산해주면 된다.\n\\(||a||_1 = (|1|^1+|2|^1+|3|^1)^{\\frac{1}{1}}\\)\n\n\np-Norm (\\(l_p\\)-norm)\n\\(||a||_p = (|x_1|^p+|x_2|^p+|x_3|^p+...)^{\\frac{1}{p}} = (\\underset{t}{\\Sigma} |x_t|^p)^{\\frac{1}{p}} \\quad (p ≥ 1)\\)\n\n\ninfinity-Norm\n\\(||a||_∞ = \\underset{t}{max}|x_t|\\)\n1-norm, 2-norm, infinity-norm의 값이 1이 되는 모든 벡터들을 좌표평면에 나타내면 다음과 같다.\n\n\n\n\n\n\n\n\n\n같은 벡터일 때, 1-norm ≥ 2-norm ≥ ∞-norm 순으로 크다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#가우스-조던-소거법",
    "href": "posts/02_areas/선형대수/notes/03.html#가우스-조던-소거법",
    "title": "3-몰라",
    "section": "가우스 조던 소거법",
    "text": "가우스 조던 소거법\n\n선형대수의 목표는 \\(Ax = b\\)에서 x를 찾는 것이다.\n\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n이 수식을 다시 살펴보자. 위의 수식은 아래와 같이 적용할 수 있다.\n\\[\\begin{aligned}\n2x + 4y \\quad  &= 8 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 열립방정식을 풀면 \\(y = 1\\)이라는 결과를 얻는다. 다시 \\(y=1\\)을 대입해서 \\(x=2\\)라는 값을 구할 수 있다.\n이제 이를 matrix와 vector로 풀어보자.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}\n\\]\n이를 확장행렬로 표현하면 다음과 같다\n\\[\n[A|b] = \\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n2 & 5 & | & 9\n\\end{bmatrix}\n\\]\n이제 가우스 조던 소거법을 적용해보자\n적용 순서는 다음과 같다.\n\n양 변에 0이 아닌 상수배를 해준다.\n상수배를 한 행을 다른행에 더하거나 뺀다.\n행끼리 자리 바꾼다.\n\n이에 맞춰서 위의 식을 풀이하면,\n\n두 번째 행에서 첫 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n\n첫 번째 행에서 두 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 0 & | & 2 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n따라서 \\(x = 2\\), \\(y = 1\\)이라는 해를 얻을 수 있다.\n즉 가우스조던 소거법은 왼쪽을 항등행렬로 만들고, 그 오른쪽에 있는 값이 답이되는 소거법이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#역행렬-구하기",
    "href": "posts/02_areas/선형대수/notes/03.html#역행렬-구하기",
    "title": "3-몰라",
    "section": "역행렬 구하기",
    "text": "역행렬 구하기\n역행렬을 구할 수 있다면 x의 값을 쉽게 구할 수 있다. (\\(x = A^{-1}b\\))\n가우스 조던 소거법을 이용해 역행렬을 구해보자.\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\nc & d & | & 0 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & \\frac{ad-bc}{a} & | & -\\frac{c}{a} & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & 0 & | & \\frac{ad}{ad-bc} & \\frac{-ab}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 0 & | & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n∴ A^{-1} = \\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\n\ninvertible\n역행렬이 존재할 경우 invertible하다고 한다.\n\nnon singular matrix\ndet(A) ≠ 0: ad - bc(determinant) = 0인 경우 역행렬이 존재하지 않는다.\nA가 full rank이다\nN(A) = 0",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#determinant",
    "href": "posts/02_areas/선형대수/notes/03.html#determinant",
    "title": "3-몰라",
    "section": "determinant",
    "text": "determinant\n정사각행렬의 element로 scalar 값을 만드는 함\n\n3 x 3 행렬의 det\n\\[\nA=\n\\begin{bmatrix}\na & b & c\\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n\\]\n\\(det(A) = a(ei - fh) - b(di-fg)+c(dh-eg)\\)\nLaplace expansion or cofactor expansion\n\n\nproperties\n\ndet(A) = 0 이면 A is singular\nA가 rank-deficient 이면 det(A) = 0\ndiagonal or triangular matrix, det(A) = 대각요소의 곱\n항등행렬의 det=1\ndet(cA) = \\(c^ndet(A)\\) (A = nxn)\n\\(det(A^T) = det(A)\\)\ndet(AB) = det(A)det(B)\n\\(\\color{red}{det(A^{-1}) = \\frac{1}{det(A)}}\\)\n\\(\\color{red}{det(A) = λ_1λ_2,...,λ_n}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#trace",
    "href": "posts/02_areas/선형대수/notes/03.html#trace",
    "title": "3-몰라",
    "section": "Trace",
    "text": "Trace\n정사각 행렬에 대해서만 정의되는 것, diagonal 전부 더함\n\\(tr(A) = \\sum_{i=1}^{n}a_{ii}\\)\n\ntr(A + B) = tr(A) + tr(B)\ntr(cA) = ctr(A)\n\\(tr(A^T) = tr(A)\\)\ntr(AB) = tr(BA)\n\\(tr(a^Tb) = tr(ba^T)\\)\ntr(ABCD) = tr(BCDA) = tr(CDAB) = tr(DABC) (cyclic property)\n\\(tr(A) = \\sum_{i=1}^{n}\\lambda_i\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/03.html#최소자승법",
    "href": "posts/02_areas/선형대수/notes/03.html#최소자승법",
    "title": "3-몰라",
    "section": "최소자승법",
    "text": "최소자승법",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/09.html#preprocessing",
    "title": "K Nearest Neighbors",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/09.html#modeling",
    "title": "K Nearest Neighbors",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/09.html#predict",
    "href": "posts/02_areas/machine_learning/notes/09.html#predict",
    "title": "K Nearest Neighbors",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[57  5]\n [ 5 33]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/16.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/16.html#preprocessing",
    "title": "Apriori",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])\ntransactions\n\n[['shrimp',\n  'almonds',\n  'avocado',\n  'vegetables mix',\n  'green grapes',\n  'whole weat flour',\n  'yams',\n  'cottage cheese',\n  'energy drink',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'honey',\n  'salad',\n  'mineral water',\n  'salmon',\n  'antioxydant juice',\n  'frozen smoothie',\n  'spinach',\n  'olive oil'],\n ['burgers',\n  'meatballs',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chutney',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'energy bar',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'light cream',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'chicken',\n  'honey',\n  'oil',\n  'cooking oil',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'black tea',\n  'salmon',\n  'eggs',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'honey',\n  'french fries',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'eggs',\n  'chocolate',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'ham',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'shrimp',\n  'pasta',\n  'avocado',\n  'honey',\n  'white wine',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soup',\n  'avocado',\n  'milk',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'energy bar',\n  'black tea',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomato sauce',\n  'light cream',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'avocado',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'strong cheese',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'hot dogs',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'avocado',\n  'french fries',\n  'hot dogs',\n  'brownies',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'cereals',\n  'clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'black tea',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'brownies',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'frozen vegetables',\n  'tomatoes',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'chicken',\n  'gums',\n  'soda',\n  'body spray',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'mineral water',\n  'cider',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'bug spray',\n  'shallot',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'avocado',\n  'cider',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'tomatoes',\n  'tomato sauce',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'milk',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'butter',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['asparagus',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'french fries',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'energy bar',\n  'french wine',\n  'eggs',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'almonds',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'soup',\n  'escalope',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ham',\n  'frozen vegetables',\n  'pepper',\n  'oil',\n  'extra dark chocolate',\n  'tea',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'barbecue sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'energy bar',\n  'almonds',\n  'eggs',\n  'corn',\n  'mayonnaise',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'chocolate',\n  'soup',\n  'almonds',\n  'eggs',\n  'hot dogs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'chocolate',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'energy bar',\n  'pet food',\n  'carrots',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'gums',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'soup',\n  'avocado',\n  'milk',\n  'olive oil',\n  'green grapes',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'soup',\n  'cake',\n  'cooking oil',\n  'chicken',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'escalope',\n  'mushroom cream sauce',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'oil',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'eggs',\n  'french fries',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'salmon',\n  'white wine',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'olive oil',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'antioxydant juice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['carrots',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'cooking oil',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'pancakes',\n  'chicken',\n  'chili',\n  'tea',\n  'french fries',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'chocolate',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cooking oil',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'light cream',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'cake',\n  'gums',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'olive oil',\n  'eggs',\n  'rice',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'nonfat milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'soup',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'barbecue sauce',\n  'carrots',\n  'chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'frozen vegetables',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'cake',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'chicken',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'spaghetti',\n  'eggs',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'strong cheese',\n  'light cream',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'yams',\n  'milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'ground beef',\n  'soup',\n  'avocado',\n  'milk',\n  'black tea',\n  'eggs',\n  'barbecue sauce',\n  'carrots',\n  'cookies',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'french fries',\n  'strawberries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'yams',\n  'chicken',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'honey',\n  'cake',\n  'rice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'light mayo',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mineral water',\n  'corn',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'french wine',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'whole wheat rice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'milk',\n  'green tea',\n  'french fries',\n  'cookies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'whole wheat rice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'meatballs',\n  'chicken',\n  'blueberries',\n  'cooking oil',\n  'champagne',\n  'yogurt cake',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'milk',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'olive oil',\n  'french wine',\n  'eggs',\n  'french fries',\n  'champagne',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'honey',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bacon',\n  'eggs',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'tomatoes',\n  'energy bar',\n  'french wine',\n  'antioxydant juice',\n  'french fries',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'spinach',\n  'soda',\n  'energy drink',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'energy bar',\n  'chicken',\n  'eggs',\n  'cake',\n  'french fries',\n  'body spray',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'fromage blanc',\n  'bacon',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'vegetables mix',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'cereals',\n  'brownies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pancakes',\n  'cooking oil',\n  'gluten free bar',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'soup',\n  'black tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'grated cheese',\n  'tomatoes',\n  'chocolate',\n  'fromage blanc',\n  'honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'milk',\n  'olive oil',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'soup',\n  'chutney',\n  'cereals',\n  'energy drink',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'soup',\n  'avocado',\n  'salmon',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'vegetables mix',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'shrimp',\n  'pasta',\n  'mineral water',\n  'olive oil',\n  'eggs',\n  'cake',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'green grapes',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'meatballs',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'chocolate',\n  'olive oil',\n  'french wine',\n  'salmon',\n  'rice',\n  'light mayo',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'french wine',\n  'vegetables mix',\n  'rice',\n  'clothes accessories',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'dessert wine',\n  'spaghetti',\n  'chicken',\n  'cake',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'flax seed',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'energy bar',\n  'green grapes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'cake',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'salt',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salt',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'red wine',\n  'mineral water',\n  'french fries',\n  'hand protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'energy drink',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'salmon',\n  'green tea',\n  'french fries',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'pasta',\n  'tomatoes',\n  'milk',\n  'frozen smoothie',\n  'sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'olive oil',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'antioxydant juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'almonds',\n  'eggs',\n  'strawberries',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'antioxydant juice',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'oil',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'butter',\n  'chicken',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'cake',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'oil',\n  'cereals',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'spinach',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'green tea',\n  'shampoo',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'mint green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'red wine',\n  'spaghetti',\n  'mineral water',\n  'salmon',\n  'eggs',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'shallot',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'spaghetti',\n  'muffins',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'tomato juice',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'yams',\n  'mineral water',\n  'muffins',\n  'frozen smoothie',\n  'hot dogs',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'soup',\n  'milk',\n  'olive oil',\n  'salmon',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'french wine',\n  'green grapes',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'dessert wine',\n  'ground beef',\n  'soup',\n  'salmon',\n  'eggs',\n  'gums',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'carrots',\n  'french fries',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cooking oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'eggs',\n  'cake',\n  'mashed potato',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'strong cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'soup',\n  'rice',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'avocado',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'herb & pepper',\n  'ground beef',\n  'pancakes',\n  'eggs',\n  'light cream',\n  'rice',\n  'champagne',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'french wine',\n  'vegetables mix',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'yams',\n  'butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'barbecue sauce',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'french fries',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'salmon',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'eggs',\n  'green tea',\n  'frozen smoothie',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'chicken',\n  'cider',\n  'green grapes',\n  'honey',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'milk',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'protein bar',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'bacon',\n  'nonfat milk',\n  'spinach',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'spinach',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'energy bar',\n  'honey',\n  'rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'chocolate',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'strawberries',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'salmon',\n  'honey',\n  'extra dark chocolate',\n  'green tea',\n  'mushroom cream sauce',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'tomatoes',\n  'soup',\n  'butter',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'tomato juice',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'whole wheat rice',\n  'cereals',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'vegetables mix',\n  'spinach',\n  'french fries',\n  'escalope',\n  'cauliflower',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'eggs',\n  'soda',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'shallot',\n  'strawberries',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'blueberries',\n  'oil',\n  'cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'frozen smoothie',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'butter',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'salmon',\n  'pancakes',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'salmon',\n  'eggs',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['magazines',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'extra dark chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'french wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'salt',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'escalope',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'fromage blanc',\n  'whole wheat rice',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'honey',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'shrimp',\n  'mineral water',\n  'barbecue sauce',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'french fries',\n  'escalope',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'soup',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'energy drink',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'grated cheese',\n  'herb & pepper',\n  'shrimp',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'red wine',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'mushroom cream sauce',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cake',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'almonds',\n  'cake',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'frozen vegetables',\n  'milk',\n  'butter',\n  'salmon',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'whole wheat pasta',\n  'whole wheat rice',\n  'chicken',\n  'green tea',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'olive oil',\n  'salmon',\n  'green beans',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'tomato sauce',\n  'mineral water',\n  'green grapes',\n  'eggs',\n  'gums',\n  'light cream',\n  'oil',\n  'green tea',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'honey',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'oil',\n  'ketchup',\n  'chili',\n  'pet food',\n  'eggplant',\n  'green tea',\n  'escalope',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'cake',\n  'blueberries',\n  'tea',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'salmon',\n  'carrots',\n  'frozen smoothie',\n  'pasta',\n  'mashed potato',\n  'shallot',\n  'light mayo',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'olive oil',\n  'whole wheat rice',\n  'eggplant',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'eggs',\n  'ham',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'brownies',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'pancakes',\n  'french fries',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pancakes',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'honey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'tomato sauce',\n  'olive oil',\n  'chili',\n  'french fries',\n  'cookies',\n  'salt',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'bramble',\n  'frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'meatballs',\n  'butter',\n  'eggs',\n  'salad',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'chocolate',\n  'olive oil',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pepper',\n  'soup',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'pancakes',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french wine',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'bramble',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'frozen vegetables',\n  'tomatoes',\n  'cake',\n  'chicken',\n  'green tea',\n  'toothpaste',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'olive oil',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'cooking oil',\n  'cereals',\n  'protein bar',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'burger sauce',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'whole wheat rice',\n  'yogurt cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'eggs',\n  'green tea',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'herb & pepper',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'salmon',\n  'nonfat milk',\n  'green grapes',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bug spray',\n  'oatmeal',\n  'sandwich',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'almonds',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'butter',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'pepper',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'rice',\n  'cooking oil',\n  'chocolate',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'butter',\n  'chocolate',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'chutney',\n  'cooking oil',\n  'asparagus',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'champagne',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'mineral water',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'almonds',\n  'cereals',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'champagne',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'oil',\n  'tomato juice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'french wine',\n  'french fries',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green grapes',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'oil',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'cake',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomatoes',\n  'ground beef',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cider',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'cookies',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'escalope',\n  'champagne',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'escalope',\n  'shallot',\n  'protein bar',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'parmesan cheese',\n  'spaghetti',\n  'meatballs',\n  'butter',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'tomato sauce',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'french fries',\n  'asparagus',\n  'low fat yogurt',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'light cream',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'chicken',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'energy bar',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'eggs',\n  'gums',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'pancakes',\n  'chocolate',\n  'escalope',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'yams',\n  'flax seed',\n  'salmon',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soda',\n  'pet food',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'olive oil',\n  'chicken',\n  'eggs',\n  'extra dark chocolate',\n  'melons',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'ground beef',\n  'yams',\n  'milk',\n  'strong cheese',\n  'salmon',\n  'muffins',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'tea',\n  'pancakes',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'turkey',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'shallot',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'butter',\n  'cooking oil',\n  'chocolate',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'butter',\n  'eggs',\n  'green tea',\n  'cottage cheese',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'whole wheat rice',\n  'cooking oil',\n  'chocolate',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'parmesan cheese',\n  'mineral water',\n  'olive oil',\n  'bacon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green grapes',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'milk',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'chocolate',\n  'french fries',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'gums',\n  'cooking oil',\n  'frozen smoothie',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'energy bar',\n  'vegetables mix',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'nonfat milk',\n  'cooking oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'low fat yogurt',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'parmesan cheese',\n  'butter',\n  'whole wheat rice',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'avocado',\n  'milk',\n  'chicken',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'black tea',\n  'french wine',\n  'pancakes',\n  'rice',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'mineral water',\n  'eggs',\n  'cake',\n  'oil',\n  'cooking oil',\n  'toothpaste',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'olive oil',\n  'black tea',\n  'vegetables mix',\n  'cooking oil',\n  'green tea',\n  'frozen smoothie',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cider',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'energy bar',\n  'eggs',\n  'cooking oil',\n  'pancakes',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'escalope',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'rice',\n  'barbecue sauce',\n  'clothes accessories',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'green tea',\n  'french fries',\n  'sandwich',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'salmon',\n  'green tea',\n  'cookies',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'soup',\n  'salmon',\n  'pancakes',\n  'chicken',\n  'mint green tea',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'almonds',\n  'french wine',\n  'yogurt cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'nonfat milk',\n  'eggs',\n  'gums',\n  'chocolate',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'ground beef',\n  'olive oil',\n  'energy bar',\n  'chicken',\n  'brownies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'cooking oil',\n  'frozen smoothie',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['vegetables mix',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'escalope',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'french wine',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'herb & pepper',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'almonds',\n  'muffins',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'salmon',\n  'antioxydant juice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'ground beef',\n  'soup',\n  'energy bar',\n  ' asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'butter',\n  'pancakes',\n  'french fries',\n  'frozen smoothie',\n  'white wine',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'frozen vegetables',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'shrimp',\n  'frozen vegetables',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'olive oil',\n  'almonds',\n  'cake',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'almonds',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'pet food',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'olive oil',\n  'energy bar',\n  'light cream',\n  'oil',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'mineral water',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bramble',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pepper',\n  'milk',\n  'eggs',\n  'green tea',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'butter',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'vegetables mix',\n  'eggs',\n  'whole wheat rice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'burger sauce',\n  'escalope',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'body spray',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'whole wheat pasta',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'eggs',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'black tea',\n  'french wine',\n  'cider',\n  'chutney',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'chicken',\n  'light cream',\n  'green tea',\n  'chocolate',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'blueberries',\n  'cooking oil',\n  'chocolate',\n  'cookies',\n  'champagne',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'salmon',\n  'muffins',\n  'chocolate',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'herb & pepper',\n  'parmesan cheese',\n  'milk',\n  'olive oil',\n  'pancakes',\n  'honey',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'butter',\n  'whole wheat rice',\n  'french fries',\n  'escalope',\n  'cookies',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'champagne',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'chicken',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'carrots',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'honey',\n  'green beans',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'light cream',\n  'rice',\n  'oil',\n  'chicken',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'mineral water',\n  'milk',\n  'eggs',\n  'low fat yogurt',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'tomato sauce',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'protein bar',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'honey',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'parmesan cheese',\n  'french wine',\n  'french fries',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'milk',\n  'olive oil',\n  'muffins',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'shrimp',\n  'spaghetti',\n  'olive oil',\n  'honey',\n  'strawberries',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'pancakes',\n  'light cream',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'avocado',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'burgers',\n  'almonds',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'clothes accessories',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'honey',\n  'whole wheat rice',\n  'green beans',\n  'french fries',\n  'frozen smoothie',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'almonds',\n  'strong cheese',\n  'cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'rice',\n  'eggplant',\n  'hand protein bar',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'nonfat milk',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'cookies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'bug spray',\n  'french fries',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'milk',\n  'cider',\n  'chicken',\n  'green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'chicken',\n  'salmon',\n  'pancakes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'chocolate',\n  'grated cheese',\n  'frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'butter',\n  'bacon',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'pepper',\n  'chocolate',\n  'fromage blanc',\n  'eggs',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'ground beef',\n  'avocado',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'green tea',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'avocado',\n  'bacon',\n  'oil',\n  'french fries',\n  'brownies',\n  'pancakes',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'chicken',\n  'cake',\n  'burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cooking oil',\n  'tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'soup',\n  'vegetables mix',\n  'pancakes',\n  'body spray',\n  'melons',\n  'protein bar',\n  'asparagus',\n  'mayonnaise',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'muffins',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'milk',\n  'gums',\n  'light cream',\n  'cooking oil',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'pancakes',\n  'french fries',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'tomato sauce',\n  'milk',\n  'extra dark chocolate',\n  'melons',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'chicken',\n  'chocolate',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'frozen smoothie',\n  'sandwich',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'frozen smoothie',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'rice',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'cream',\n  'black tea',\n  'almonds',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'parmesan cheese',\n  'ground beef',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'pancakes',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'soda',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'chicken',\n  'rice',\n  'oil',\n  'cooking oil',\n  'hot dogs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'chocolate',\n  'champagne',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'yogurt cake',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'olive oil',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'extra dark chocolate',\n  'cereals',\n  'french fries',\n  'frozen smoothie',\n  'pancakes',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'soup',\n  'avocado',\n  'milk',\n  'yogurt cake',\n  'energy drink',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'chocolate',\n  'ground beef',\n  'mineral water',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'olive oil',\n  'rice',\n  'antioxydant juice',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomato sauce',\n  'spaghetti',\n  'milk',\n  'pancakes',\n  'energy drink',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'strong cheese',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'cider',\n  'cooking oil',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'fromage blanc',\n  'vegetables mix',\n  'pancakes',\n  'honey',\n  'hot dogs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'milk',\n  'butter',\n  'black tea',\n  'eggs',\n  'frozen smoothie',\n  'light mayo',\n  'shampoo',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'cooking oil',\n  'frozen smoothie',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'blueberries',\n  'soda',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'ground beef',\n  'soup',\n  'milk',\n  'butter',\n  'honey',\n  'cake',\n  'mint green tea',\n  'brownies',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'napkins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'energy bar',\n  'butter',\n  'almonds',\n  'fromage blanc',\n  'eggs',\n  'cake',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'honey',\n  'pasta',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'milk',\n  'french wine',\n  'muffins',\n  'pancakes',\n  'champagne',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'cake',\n  'rice',\n  'cereals',\n  'french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'strong cheese',\n  'cereals',\n  'escalope',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'fresh tuna',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'cottage cheese',\n  'energy drink',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'honey',\n  'extra dark chocolate',\n  'carrots',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cottage cheese',\n  'strawberries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'avocado',\n  'cake',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'chocolate bread',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy bar',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'champagne',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'butter',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'green tea',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomato sauce',\n  'milk',\n  'butter',\n  'bacon',\n  'salmon',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'hot dogs',\n  'melons',\n  'protein bar',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'burger sauce',\n  'brownies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'pancakes',\n  'whole wheat rice',\n  'green tea',\n  'french fries',\n  'cookies',\n  'shallot',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'butter',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'honey',\n  'whole wheat rice',\n  'cooking oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'chili',\n  'pet food',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'butter',\n  'chicken',\n  'salt',\n  'mayonnaise',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'fromage blanc',\n  'honey',\n  'gums',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'sparkling water',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'butter',\n  'vegetables mix',\n  'green grapes',\n  'pancakes',\n  'eggs',\n  'cake',\n  'barbecue sauce',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'yams',\n  'eggs',\n  'burger sauce',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'eggs',\n  'chocolate',\n  'champagne',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cereals',\n  'salt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'chocolate',\n  'french fries',\n  'escalope',\n  'cookies',\n  'brownies',\n  'pancakes',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'yams',\n  'chicken',\n  'cooking oil',\n  'chocolate',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'bug spray',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'oil',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'chicken',\n  'fromage blanc',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'champagne',\n  'cookies',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'milk',\n  'butter',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'chicken',\n  'eggs',\n  'frozen smoothie',\n  'cauliflower',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'milk',\n  'energy bar',\n  'almonds',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'milk',\n  'cake',\n  'cooking oil',\n  'french fries',\n  'escalope',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'whole wheat rice',\n  'cake',\n  'frozen smoothie',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'red wine',\n  'pepper',\n  'spaghetti',\n  'fromage blanc',\n  'salmon',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'soup',\n  'chocolate',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'vegetables mix',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'parmesan cheese',\n  'cider',\n  'muffins',\n  'spinach',\n  'honey',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'mushroom cream sauce',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'almonds',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'butter',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'french fries',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'whole wheat rice',\n  'eggplant',\n  'chocolate',\n  'cookies',\n  'shallot',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'blueberries',\n  'light cream',\n  'rice',\n  'green tea',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'bacon',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'fromage blanc',\n  'ketchup',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'french fries',\n  'salt',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'yogurt cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'meatballs',\n  'milk',\n  'olive oil',\n  'chicken',\n  'honey',\n  'frozen smoothie',\n  'escalope',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'eggs',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'olive oil',\n  'blueberries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'oil',\n  'carrots',\n  'hand protein bar',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'chicken',\n  'fromage blanc',\n  'salmon',\n  'eggs',\n  'cake',\n  'frozen smoothie',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'butter',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'salmon',\n  'oil',\n  'cooking oil',\n  'frozen smoothie',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'escalope',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'carrots',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'soup',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cooking oil',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'olive oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'pancakes',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chicken',\n  'french fries',\n  'cottage cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'mineral water',\n  'muffins',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'spaghetti',\n  'milk',\n  'cider',\n  'eggs',\n  'honey',\n  'cake',\n  'green tea',\n  'french fries',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ...]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/16.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/16.html#modeling",
    "title": "Apriori",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'light cream', 'chicken'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'ground beef', 'herb & pepper'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'light cream', 'olive oil'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'shrimp', 'pasta'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts       = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Lift')\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/15.html#preprocessing",
    "title": "hierarchical clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/15.html#modeling",
    "title": "hierarchical clustering",
    "section": "Modeling",
    "text": "Modeling\n\nimport scipy.cluster.hierarchy as sch\n\ndendogram = sch.dendrogram(sch.linkage(x, method='ward'))\nplt.title('dendogram')\nplt.xlabel('Customers')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\nyh = hc.fit_predict(x)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/15.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/15.html#visualize",
    "title": "hierarchical clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[yh == 0, 0], x[yh == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[yh == 1, 0], x[yh == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[yh == 2, 0], x[yh == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[yh == 3, 0], x[yh == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[yh == 4, 0], x[yh == 4, 1], c='cyan', label='Cluster 5')\nplt.legend()\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/03.html#preprocessing",
    "title": "Multiple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/03.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# model automatically avoid dummy variable trap\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.2)\n\n# in multiple linear regression, we don't need to apply feature scaling",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/03.html#modeling",
    "title": "Multiple Linear Regression",
    "section": "modeling",
    "text": "modeling\n\nfrom sklearn.linear_model import LinearRegression\n\n# model automatically choose best model (dont need to apply 후진제거법)\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#predict",
    "href": "posts/02_areas/machine_learning/notes/03.html#predict",
    "title": "Multiple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), \n                      y_test.reshape(len(y_test), 1)), 1))\n\n[[ 93771.16 101004.64]\n [ 74466.6   78239.91]\n [ 50652.19  69758.98]\n [105956.49  99937.59]\n [145441.49 129917.04]\n [ 37211.29  64926.08]\n [ 71764.09  71498.49]\n [111249.19 108552.04]\n [ 59299.22  65200.33]\n [120840.79 118474.03]\n [162627.18 149759.96]\n [120230.67 126992.93]\n [166842.52 156991.12]\n [119593.28 108733.99]\n [123155.58 110352.25]\n [170183.11 155752.6 ]\n [198552.13 191050.39]\n [157465.39 132602.65]\n [ 98810.02  97427.84]\n [195490.18 192261.83]\n [142926.02 144259.4 ]\n [106210.25  96778.92]\n [ 89245.08  97483.56]\n [ 45068.42  14681.4 ]\n [ 41277.39  42559.73]\n [115735.66 105733.54]\n [176218.16 182901.99]\n [137986.12 141585.52]\n [ 88264.34  96479.51]\n [ 96578.58  89949.14]\n [ 82567.67  77798.83]\n [196724.5  191792.06]\n [111704.   111313.02]\n [102560.86 103282.38]\n [139310.55 124266.9 ]\n [ 87261.82  81005.76]\n [ 86587.2   96712.8 ]\n [107978.65 122776.86]\n [130333.68 134307.35]\n [188346.5  166187.94]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/03.html#evaluate",
    "href": "posts/02_areas/machine_learning/notes/03.html#evaluate",
    "title": "Multiple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.913139010635797",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/00.html#machine-learning-process",
    "href": "posts/02_areas/machine_learning/notes/00.html#machine-learning-process",
    "title": "overview",
    "section": "Machine Learning Process",
    "text": "Machine Learning Process\n\nData Pre-Processing\n\nimport data\nclean data\nsplit data trainig and testing\nfeature scailing\n\nnormalization: \\(\\frac{x - min(x)}{max(x) - min(x)}\\)\nstandardization: \\(\\frac{x - μ}{σ}\\)\n\n\nModeling\n\nbuild / train model\nmake predictions\n\nEvaluation\n\ncalculate performance metrix\nmake a verdict",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "overview"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/12.html#preprocessing",
    "title": "Decision Tree Classification",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/12.html#modeling---linear",
    "title": "Decision Tree Classification",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy')",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#predict",
    "href": "posts/02_areas/machine_learning/notes/12.html#predict",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[56  8]\n [ 8 28]]\n\n\n0.84",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/12.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/12.html#predict-1",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[56  8]\n [ 8 28]]\n\n\n0.84",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/10.html#preprocessing",
    "title": "Support Vector Machine",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#modeling---linear",
    "href": "posts/02_areas/machine_learning/notes/10.html#modeling---linear",
    "title": "Support Vector Machine",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#predict",
    "href": "posts/02_areas/machine_learning/notes/10.html#predict",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 4 35]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#modeling---non-linear",
    "href": "posts/02_areas/machine_learning/notes/10.html#modeling---non-linear",
    "title": "Support Vector Machine",
    "section": "Modeling - non-linear",
    "text": "Modeling - non-linear\n\nclassifier = SVC(kernel='rbf')\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/10.html#predict-1",
    "href": "posts/02_areas/machine_learning/notes/10.html#predict-1",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 4 35]]\n\n\n0.9",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/05.html#preprocessing",
    "title": "Support Vector Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\ny = y.reshape(len(y), 1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#train",
    "href": "posts/02_areas/machine_learning/notes/05.html#train",
    "title": "Support Vector Regression",
    "section": "Train",
    "text": "Train\n\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='rbf')\nregressor.fit(x, y)\n\n/home/cryscham123/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n\n\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#visualize",
    "href": "posts/02_areas/machine_learning/notes/05.html#visualize",
    "title": "Support Vector Regression",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(sc_x.inverse_transform(x), sc_y.inverse_transform(regressor.predict(x).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/05.html#high-resolution",
    "href": "posts/02_areas/machine_learning/notes/05.html#high-resolution",
    "title": "Support Vector Regression",
    "section": "High resolution",
    "text": "High resolution\n\nx_grid = np.arange(min(sc_x.inverse_transform(x)), max(sc_x.inverse_transform(x)), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(x_grid, sc_y.inverse_transform(regressor.predict(sc_x.transform(x_grid)).reshape(-1, 1)))\nplt.show()\n\n/tmp/ipykernel_12503/1939094151.py:1: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/07.html#preprocessing",
    "title": "random forest",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/07.html#modeling",
    "title": "random forest",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\n\n# 모델 학습\nregressor.fit(x, y)\n\nRandomForestRegressor(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=10, random_state=0)",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/07.html#visualization",
    "href": "posts/02_areas/machine_learning/notes/07.html#visualization",
    "title": "random forest",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#preprocessing",
    "href": "posts/02_areas/machine_learning/notes/06.html#preprocessing",
    "title": "Decision Tree Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#modeling",
    "href": "posts/02_areas/machine_learning/notes/06.html#modeling",
    "title": "Decision Tree Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor()\n\n# 모델 학습\nregressor.fit(x, y)\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/machine_learning/notes/06.html#visualization",
    "href": "posts/02_areas/machine_learning/notes/06.html#visualization",
    "title": "Decision Tree Regression",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "PARA",
      "Areas",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#dag-skeleton",
    "href": "posts/02_areas/air_flow/notes/01.html#dag-skeleton",
    "title": "Coding pipeline",
    "section": "DAG skeleton",
    "text": "DAG skeleton\n\nfrom airflow import DAG\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='example_dag',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    pass\n\n\nDAG는 start_date / last_execution time + schedule_interval에 실행된다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#operator",
    "href": "posts/02_areas/air_flow/notes/01.html#operator",
    "title": "Coding pipeline",
    "section": "Operator",
    "text": "Operator\n\noperator 하나 당 하나의 task만 실행하는게 좋다.\n\n\noperator type\n\nAction operators\n\nBashOperator\nPythonOperator\n\nTransfer operators\nSensor operators",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#providers",
    "href": "posts/02_areas/air_flow/notes/01.html#providers",
    "title": "Coding pipeline",
    "section": "Providers",
    "text": "Providers\n\nAirflow providers are a set of packages that contain operators, sensors, hooks, and other utilities to interact with external platforms and services.\nProviders are installed separately from Airflow and can be added to your environment as needed.\nIn Airflow core, Bash and Python operators, … are included\n\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nwith DAG(\n    dag_id='example_db',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    create_table = PostgresOperator(\n        task_id='create_table',\n        postgres_conn_id='postgres',\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS example_table (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(50)\n            );\n        \"\"\",\n    )\n\nDB에 접속하기 위해서 connection을 설정해야 한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/notes/01.html#hook",
    "href": "posts/02_areas/air_flow/notes/01.html#hook",
    "title": "Coding pipeline",
    "section": "Hook",
    "text": "Hook",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html",
    "href": "posts/02_areas/air_flow/index.html",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#details",
    "href": "posts/02_areas/air_flow/index.html#details",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#tasks",
    "href": "posts/02_areas/air_flow/index.html#tasks",
    "title": "AirFlow",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#참고-자료",
    "href": "posts/02_areas/air_flow/index.html#참고-자료",
    "title": "AirFlow",
    "section": "참고 자료",
    "text": "참고 자료\n\nUdemy 강의",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/02_areas/air_flow/index.html#related-posts",
    "href": "posts/02_areas/air_flow/index.html#related-posts",
    "title": "AirFlow",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html",
    "href": "posts/04_archives/adp_필기/notes/16.html",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "",
    "text": "시험 결과",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#intro",
    "href": "posts/04_archives/adp_필기/notes/16.html#intro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Intro",
    "text": "Intro\nadp 필기시험 34회차 결과가 나왔습니다. 당연히 합격했고요. 제가 말했죠? 서술형을 몰라도 객관식을 70개 이상 맞추면 합격이라고요. 저는 69개를 맞췄지만 말입니다. 하하.\n..사실 뭐 딱히 자랑스러운 결과는 아니긴 합니다. 서술형 1.5점 받은 주제에 데이터 분석 전문가라고 말 할 수 있을까요? 하지만 제가 합격한건 오직 필기. 전 아직 데이터 분석 전문가가 아닙니다. (제목을 봐주세요. 데이터 분석 전문가(진)입니다.)\n처음 adp 시험을 준비하려고 생각한 것은 데이터 분석 분야에 입문을 하기 위해서였기 때문에, 그런 측면에서 저의 목적은 달성했다고 생각합니다. 실기 시험은 더 구체적인 통계론이나 머신러닝 로직 등을 꽤 오랜시간 자세히 공부해서 치루려고 합니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#리뷰",
    "href": "posts/04_archives/adp_필기/notes/16.html#리뷰",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "리뷰",
    "text": "리뷰\n생각보다 시험에서 다루는 내용의 범위가 넓었습니다. 암기식 공부가 주가되는 자격증 시험의 특성상, 이는 굉장히 힘든 일이 아닐 수 없었습니다. 그래도 데이터 분석 분야의 내용들을 포괄적으로 다루다 보니, 확실히 입문을 하기에는 괜찮은 resource라고 생각합니다.\n그래도 몇 가지 아쉬운 점을 꼽자면, 2 단원 데이터 처리 기술들이 조금 old한 내용들을 다루는게 아닌가 하는 생각이 듭니다. 또, 5 단원 마지막 부분의 데이터 시각화 tool을 구체적으로 다루는 부분은 이걸 꼭 다 외워야하나 싶은 느낌이 들었고요. (근데 이번 시험에서 이 부분은 안 나왔습니다.) 사실 이 정도만 아는 것으로도 앞으로 학습을 이어나갈 수 있는 좋은 발판이 되어준다고 생각합니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/16.html#outro",
    "href": "posts/04_archives/adp_필기/notes/16.html#outro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Outro",
    "text": "Outro\n이제 실기 시험을 준비해야죠. 올해 볼까.. 내년에 볼까.. 고민입니다. 처음 목표는 졸업 전 까지 adp 자격증을 따는 거였고, 졸업까지 아직 2년이 남았으니까.. 여유롭게 준비해보려 합니다.\n그럼 저는 adp 실기 준비로 다시 돌아오겠습니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html",
    "href": "posts/04_archives/adp_필기/notes/15.html",
    "title": "시험을 보고 왔습니다.",
    "section": "",
    "text": "높게 솟은 수원 공고의 모습",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#intro",
    "href": "posts/04_archives/adp_필기/notes/15.html#intro",
    "title": "시험을 보고 왔습니다.",
    "section": "Intro",
    "text": "Intro\n수원 공고에서 진행된 adp 시험을 보고 왔습니다.\n고등학교 치고는 왠만한 대규모 성당급으로 상당히 넓다는 느낌이 들었습니다. 그냥 제가 나온 인문계 고등학교가 좁아서 그렇게 느껴진 것일 수도 있고요.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#시험",
    "href": "posts/04_archives/adp_필기/notes/15.html#시험",
    "title": "시험을 보고 왔습니다.",
    "section": "시험",
    "text": "시험\n저번 시험에서는 서술형에서 20점 만점에 4점을 받아서 굉장히 아쉬운 성적이 나와버렸죠. 사실 그렇다고 이번 시험에서 서술형을 엄청 잘 준비하거나 하진 않았습니다. 제가 게을러서 그렇다는걸 딱히 부정하는건 아니지만, 가장 큰 이유는 뭐가 나올지 예상을 할 수 없기 때문입니다.\n서술형에 나올 수 있는 주제 자체는 참고서에 나와있는 내용 안에서 등장하지만, 시험에서는 더 구체적인 수식과 구현 과정을 요구하는 경우가 많습니다. 아마도 adp 시험에서 서술형을 더 잘 대비하려면, 더 많은 외부 자료를 참고하거나, 배경지식을 더 쌓은 상태에서 도전해봐야 할 것 같습니다.\n하지만, 만약 서술형이 0점이 나온다고 해도 객관식을 80 문제중 70개 이상만 맞춘다면 통과할 수 있다는 사실. 뭐.. 그렇게까지 불가능한 것도 아니긴 합니다. 객관식은 참고서 내용으로 잘 커버할 수 있으니까요.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/15.html#outro",
    "href": "posts/04_archives/adp_필기/notes/15.html#outro",
    "title": "시험을 보고 왔습니다.",
    "section": "Outro",
    "text": "Outro\n실제로 꽤 느낌이 좋긴 합니다. 이번에도 서술형은 모르는 내용이 나오긴 했지만 말입니다.\n결과가 3월 중순에 나올 예정인데, 그 전까지는 smart contract 쪽으로 공부를 해볼 예정입니다.\n더 자세한 감상은 결과가 나온 후에 작성해보겠습니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/12.html#텍스트-마이닝",
    "href": "posts/04_archives/adp_필기/notes/12.html#텍스트-마이닝",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "텍스트 마이닝",
    "text": "텍스트 마이닝\n\n비정형 데이터를 구조화해서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정\n\n\n기능\n\n목표 기능: 문서 분류, 군집, 정보 추출, 문서 요약\n사용 기술: 자연어 처리, 컴퓨터 언어학\n\n\n\n과정\n\n\n텍스트 수집\n텍스트 전처리\n\ntm 패키지: 문서를 Corpus 객체로 변환해서 관리\n\nVCorpus: 문서를 Corpus로 변환해서 메모리에 저장\nPCorpus: 문서를 Corpus로 변환해서 디스크에 저장\nDirSource, DataframeSource, VectorSource: 데이터 소스 지정\ntm_map(x, FUN): x에 FUN을 적용\nDocumentTermMatrix: 문서-단어 빈도표 생성\nTermDocumentMatrix: 단어-문서 빈도표 생성\n\n전처리\n\n정제: 노이즈 제거\n토큰화\n\n단어 토큰화\n어절 토큰화\n형태소 토큰화\n품사 태깅\n\n불용어 처리: 불필요한 토큰 제거\n정제 / 정규화\n\n표기가 다른 같은 단어 통일\n대소문자 통일\n불필요한 단어 제거\n정규표현식으로 특수문자 제거\n\n어간 / 어근 추출\n텍스트 인코딩\n\none hot 인코딩\n말뭉치(BoW): 단어의 빈도수를 벡터로 표현\nTF-IDF: 문서 내 단어의 빈도 수 / 단어가 등장한 문서 수\n워드 임베딩\n\n\n텍스트 분석\n\n토픽 모델링\n감성 분석\n텍스트 분류\n텍스트 군집화\n\n텍스트 시각화\n\n워드 클라우드\n의미 연결망 분석\n\n\n\n\n\n정보 검색의 적절성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/12.html#사회연결망-분석",
    "href": "posts/04_archives/adp_필기/notes/12.html#사회연결망-분석",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "사회연결망 분석",
    "text": "사회연결망 분석\n\n2. 기법\n\n개인을 노드, 관계를 엣지로 해서 그래프 생성\n아래의 기준에 따라 구조 파악",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "href": "posts/04_archives/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "시각화 인사이트 프로세스의 의미",
    "text": "시각화 인사이트 프로세스의 의미\n\n1. 인사이트란 무엇인가\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해해야 한다.\n이를 위해 시각화 인사이트 방법이 필요하다.\n\n\n\nDIKW 피라미드와 시각화 관계\n\n\n\n\n2. 시각화와 인사이트",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#탐색",
    "href": "posts/04_archives/adp_필기/notes/13.html#탐색",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "탐색",
    "text": "탐색\n\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해하는 과정\n객관적인 패턴을 찾는 용도\n\n\n1. 사용 가능한 데이터 확인\n\n데이터 접근\n\n이벤트 기록으로서 접근: 데이터로부터 통찰을 이끌어 내기 위해서 데이터 생성 원리를 파악해야 한다고 간주\n객체지향 관점에서의 접근: 데이터로부터 통찰을 이끌어 내기 위해서 전체 구조를 파악해야 한다고 간주\n\n데이터 명세화\n\n모든 데이터는 하나 이상의 차원과 측정값을 가지고 있다.\n이는 분석 형태에 따라, 차원이 될 수도 있고, 측정값이 될 수도 있다.\n\n\n\n\n2. 연결 고리의 확인\n데이터 명세서를 이용해 2개 이상의 데이터간 연결 고리를 확인해 봄\n\n공통 요소 찾기\n공통 요소로 변환하기: 데이터 타입이 달라도 공통 요소로 묶을 수 있다 (더 자세한 데이터를 덜 자세한 데이터로 변환. 반대는 불가)\n\n시간 데이터의 변환\n공간 데이터의 변환(지오코딩, 코로플레스 지도, X-Ray Map 사용 가능)\n계층 관계 변환: 상위 수준(덜 자세한)이라는 공통 요소로 변환. replace, lookup, vlookup 함수 사용 가능\n\n탐색 범위 설정: 차원과 측정값의 전체 조합 종류가 탐색 범위가 됨. 데이터를 구성하는 항목이 늘어날 수록 탐색 범위가 늘어남\n\n여러 데이터를 보유한 경우, 개별 데이터 안에서 먼저 탐색\n측정값 하나의 차원만 연결해 탐색\n같은 데이터 안에서 차원과 측정값을 맞바꾸면 다른 통찰을 얻을 수 있음\n어떤 통찰을 얻기 위해 비주얼 인사이트 프로세스를 사용하는 것인지 살펴본 후, 목표와 관련 있을 법한 조합을 만듦\n상식적으로 의미나 연계성이 없는 조합은 배제\n\n\n\n\n3. 관계의 탐색\n상관관계와 인과관계를 탐색\n\n이상값 처리: 시각화 도구를 통해 전체 구조를 파악한 후 처리\n차원과 측정값 유형에 따른 관계 파악 시각화\n\n시각화 도구 선정\n시간 데이터에서의 관계 파악: 구글 모션차트 사용 가능\n공간 데이터에서의 관계 파악: Arc GIS, X-Ray Map, 파워 맵 사용 가능\n비정형 데이터에서의 관계 파악\n\n워들: 주어진 텍스트에서 형태소 단위를 추출(NLP)해 빈도에 따라 시각화\n\n\n잘라보고 달리보기: 둘 이상의 차원과 측정값으로 이루어진 데이터를 여러 관점으로 살펴본다.\n\n잘라보기(slice): ex) 연령별, 성별 평균 체중 데이터 → 20세 이상, 40세 미만 남성들의 체중 패턴\n달리보기(dice): ex) 연령별, 성별 평균 체중 데이터 → 남성의 연령별 체중 패턴, 여성의 연령별 체중 패턴\nMS excel의 pivot, powerview, spreadsheet의 pivot table report 사용 가능\n\n내려다보고 올려보기\n\n내려다보기(Drill Down): 데이터를 하위 계층으로 세분화한다.\n올려보기(Reverse Driil Down): 데이터를 상위 계층으로 통합한다.\nTree map, Hyperbolic Tree\n\n척도의 조정: 스파크라인 차트 사용 가능",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#분석",
    "href": "posts/04_archives/adp_필기/notes/13.html#분석",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "분석",
    "text": "분석\n\n탐색을 통해 발견된 패턴을 분석하는 과정\n\n\n1. 분석 대상의 구체화\n\n2차 탐색: 관계들의 분석 우선순위 결정. 궁극적인 목표는 그냥 다시 한 번 더 검토하는 것\n분석 목표에 따른 분석 기법\n\n \n\n\n2. 분석 시각화 도구\n통계적 도구와 시각적 도구는 상호보완 관계\n\n\n3. 지표 설정과 분석\n\n지표: 어떤 현상의 강도를 평가하는 기준이 되는 수치\n\nex) KPI(Key Performance Indicator): 핵심 성과 지표. 목표 달성을 위한 세부적인 활동 결과물의 추진 정도나 수준을 측정하고 평가\n주로 함수식 구조를 가짐 (ex. 매출액 = 판매단가 * 판매량)\n요인 분석(factor analysis)를 통해 지표가 다른 요인과 설명력이 겹치는지 여부 확인할 수 있다.\n어떤 변화요인에 의해 지표의 흐름에 영향을 미쳤는지 파악하기 어렵다는 단점이 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/13.html#활용",
    "href": "posts/04_archives/adp_필기/notes/13.html#활용",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "활용",
    "text": "활용\n\n도출한 인사이트를 활용하는 과정\n\n\n1. 내부에서 적용\n\n기존 문제 해결 방식이나 설명 모델의 수정\n새로운 문제 해결 방식의 도입\n새롭게 발견한 가능성에 대한 구체적인 탐색과 발전\n\n\n\n2. 외부에 대한 설명, 설득과 시각화 도구\n설득이 필요하기 때문에 스토리텔링이 감미된 시각화 자료나, 인터렉티브 인포그래픽 활용\n\n\n3. 인사이트의 발전과 확장\n계속 잘 검토해 나가야함",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#통계분석의-이해",
    "href": "posts/04_archives/adp_필기/notes/10.html#통계분석의-이해",
    "title": "4 - 통계분석",
    "section": "통계분석의 이해",
    "text": "통계분석의 이해\n\n1. 표본 추출 방법\n\n단순랜덤 추출법\n계통추출법: k개씩 띄어서 랜덤으로 추출\n집락 추출법: 군집을 나눈 후, 군집 안에서 단순랜덤 추출\n층화 추출법: 이질적인 모집단에서, 비슷한 특성을 가진 층을 나눈 후, 각 층에서 단순랜덤 추출\n\n\n\n2. 척도\n\n명목척도\n순서척도\n구간척도: 더하기, 빼기 가능. 곱셈 나눗셈 불가능\n비율척도: 절대적 기준인 0이 존재, 사칙연산 가능\n\n\n\n3. 비모수 검정\n\n모집단에 대한 가정이 없이, 서열관계나 차이를 검정하는 방법\n분포의 형태가 동일하다, 동일하지 않다로 가정\n관측값들의 순위나 차이의 부호에 의존\n\n\n\n\n비모수 검정 예시",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#기초-통계분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#기초-통계분석",
    "title": "4 - 통계분석",
    "section": "기초 통계분석",
    "text": "기초 통계분석\n\n\n\n상관 분석 유형",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#통계분석의-방법론",
    "href": "posts/04_archives/adp_필기/notes/10.html#통계분석의-방법론",
    "title": "4 - 통계분석",
    "section": "통계분석의 방법론",
    "text": "통계분석의 방법론\n\nt 검정\n\n일표본\n대응표본\n독립표본\n\nANOVA\n\n일원분산분석\n이원분산분석\n다원분산분석\n\n다변량분석\n실험계획법\n\n요인배치법\n분할법\n교락법\n난괴법\n\n교차분석\n\n적합성 검정: k개의 범주들에 대한 관측값 갯수가 기댓값과 일치하는지 검정\n\n자유도: k-1\n각 집단의 \\(\\frac{(관측도수 - 기대도수)^2}{기대도수}\\)의 합이 카이제곱 분포를 따름\n\n독립성 검정\n\n자유도: (r - 1)(c - 1)\n\n동질성 검정: 독립성 검정이랑 유사",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#회귀분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#회귀분석",
    "title": "4 - 통계분석",
    "section": "회귀분석",
    "text": "회귀분석\n\n1. 가정\n\n선형성\n정규성: qq-plot, 대각선에 가까워야함\n등분산성: 수평선에 가까워야함\n독립성: 더빈 왓슨 검정(0~4), 2에 가까울수록 독립성이 있다.\n\n→ 가정을 충족하지 않을 경우, 회귀모델을 수정해야함\n\n이상치 → 관측값 제거\n선형성 → 독립변수 변환\n정규성, 등분산성 미충족 → 종속변수 변환\n\n변환: \\(x\\) → \\(x^λ\\)\n\n\n2. 회귀식\n\n\n\\(R^2 = \\frac{SSR}{SST}\\)\n\\(R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\)\n\n\n\n3. 다중공선성\n\n독립변수들 간에 강한 상관관계가 존재하는 경우\n\n상관계수: 변수간 상관계수를 직접 계산\n허용오차: 1 - \\(R^2\\). 0.1 이하면 다중공선성이 존재한다고 판단\nVIF: 허용 오차의 역수. 10 이상이면 다중공선성이 존재한다고 판단 → 변수 제거\n\n\n\n\n4. 최적화 회귀방정식\n\nAIC, BIC나 F-value를 크게 만드는 변수 제거\n\n\n전진 선택법: 상수항부터 시작해, 한번에 한개씩 독립변수 추가\n\n전체 변수 사용할 수 있지만 안정성이 낮음\n\n후진 선택법: 모든 독립변수를 포함한 후, 하나씩 제거. AIC가 더 이상 작아지지 않을 때까지\n\n안정성이 높지만 변수가 많을 때 시간이 오래 걸림\n\n단계 선택법: 전진, 후진 선택법을 혼합.\n\n이미 선택된 변수를 제거할 수 있음\n변수가 많으면 시간이 오래 걸림",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#고급-회귀분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#고급-회귀분석",
    "title": "4 - 통계분석",
    "section": "고급 회귀분석",
    "text": "고급 회귀분석\n\n1. 패널티 회귀분석\n지나치게 많은 독립변수를 갖는 모델에 페널티를 부과하는 방식\n\n릿지: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0에 근접하게 축소 (\\(l_2\\) 규제)\n\n회귀 계수가 비슷하고, 독립변수가 많을 때 효과가 좋다.\n\n라쏘: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0으로 만듦 (\\(l_1\\) 규제)\n\n회귀 계수 차이가 클 때 효과가 좋다.\n\n엘라스틱넷: 릿지와 라쏘를 혼합한 방법 (\\(l_1\\) + \\(l_2\\) 규제)\n\n\n\n2. 일반화 회귀분석\n\n종속변수가 연속형이면서 정규분포를 따르지 않을 때 사용\n\n\nlogistic 회귀모형\npoisson 회귀모형",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/10.html#시계열-분석",
    "href": "posts/04_archives/adp_필기/notes/10.html#시계열-분석",
    "title": "4 - 통계분석",
    "section": "시계열 분석",
    "text": "시계열 분석\n\n시계열 데이터 생성\n탐색적 분석을 통해 데이터 이해\n\n시각화 작업으로 변통 패턴 관찰\n성분분해 작업으로 추세, 계절성분, 불규칙성분 분리\n\n추세(장기)\n계절(단기)\n순환(중장기)\n불규칙(설명 불가)\n\n\n미래 관측값에 대한 예측\n\n이동 평균법\n지수 평활법\nARIMA 기법\n\nAR모델: P시점 전의 자료가 현재에 주는 영향을 시계열 모형으로 구축. 과거 관측값을 이용하여 예측모델 생성. 감절\nMA모델: 시간이 지날수록 관측치의 평균값이 지속적으로 증가하거나 감소하는 경향 표현. 과거 오차항을 이용하여 예측모델 생성. 절감\nARIMA모델: 비정상 시계열. 차분이나 변환을 통해 정상시계열로 변환",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "href": "posts/04_archives/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "빅데이터 분석과 전략 인사이트",
    "text": "빅데이터 분석과 전략 인사이트\n빅테이터 분석은 분석을 통해 가치를 창출하는 것이 목적이다.\n\n일차원적인 분석: 해당 부서나 업무 영역에만 효과가 있다. 변화하는 환경에서 새로운 기회를 포착하기 어려움.\n전략도출 가치기반 분석: 일차원적인 분석을 통해 얻은 가치를 기반으로 활용 범위를 더 넓고 전략적으로 확장해야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "href": "posts/04_archives/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "전략 인사이트 도출을 위한 필요 역량",
    "text": "전략 인사이트 도출을 위한 필요 역량\n\n\n\n데이터 사이언티스트의 요구 역량\n\n\n외부 환경이 다음과 같이 변화함에 따라 인사이트 도출을 위한 인문학적 역량이 요구됨.\n\n컨버전스 → 디버전스\n생산 → 서비스\n생산 → 시장창조",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "href": "posts/04_archives/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "title": "3 - 분석 마스터 플랜",
    "section": "마스터 플랜 수립 프레임 워크",
    "text": "마스터 플랜 수립 프레임 워크\n\n\n\n마스터 플랜 수립 개요\n\n\n\n2. 수행 과제 도출 및 우선순위 평가\n\n\n\n일반적인 IT 프로젝트 우선순위 평가\n\n\n\n\n\nROI 관점\n\n\n위 기준에 따라 시급성과 난이도를 평가한 후, 아래 그림에 맞게 우선순위를 정한다.\n\n우선순위 기준을 시급성에 둔다면, 3 → 4 → 2 순, 난이도에 둔다면 3 → 1 → 2 순으로 우선순위를 정한다.\n\n\n3. 이행계획 수립\n\n\n\n로드맵 수립\n\n\n\n세부 이행계획 수립",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "href": "posts/04_archives/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "title": "3 - 분석 마스터 플랜",
    "section": "분석 거버넌스 체계 수립",
    "text": "분석 거버넌스 체계 수립\n\n1. 거버넌스 체계\n\n\n\n2. 데이터 분석 수준진단\n\n\n분석 준비도\n\n\n\n분석 성숙도\n\n\n\n\nCMMI(Capability Maturity Model Integration)\n\n\n분석 준비도와 성숙도를 통해 현재 분석 수준을 파악한다. 이후 아래의 그림에 맞춰 목표 방향을 설정한다.\n\n\n\n4. 데이터 거버넌스 체계 수립\n\n구성 요소:\n\n원칙\n조직\n프로세스\n\n\n\n\n\n체계\n\n\n\n데이터 표준화: 규칙같은거 통일하는거\n데이터 관리 체계: 라이프사이클 같은거 관리하는거\n레포지토리: 너가 아는 그거\n표준화 활동: 잘 지켜지는지 지속적으로 모니터링하는거\n\n\n\n5. 데이터 조직 및 인력방안 수립\n\n\n\n분석 조직 구조\n\n\n\n\n\n분석 조직 인력 구성\n\n\n\n\n6. 분석과제 관리 프로세스 수립\n\n\n\n분석 과제 관리 프로세스",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html",
    "href": "posts/04_archives/adp_필기/index.html",
    "title": "ADP 필기 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-02-02\n        종료일: 2025-02-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#details",
    "href": "posts/04_archives/adp_필기/index.html#details",
    "title": "ADP 필기 준비",
    "section": "Details",
    "text": "Details\n1회차 시도는 실패했지만, 이번엔 잘 되겠죠",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#참고-자료",
    "href": "posts/04_archives/adp_필기/index.html#참고-자료",
    "title": "ADP 필기 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#tasks",
    "href": "posts/04_archives/adp_필기/index.html#tasks",
    "title": "ADP 필기 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2025.01.20 10 am)\n                \n                2025.02.22 10:00 수원공업고등학교\n            \n\n            \n            \n                \n                    \n                    경기도 자격증 응시료 지원 신청\n                \n                아마도 5월 쯤 뜨지 않을까",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_필기/index.html#related-posts",
    "href": "posts/04_archives/adp_필기/index.html#related-posts",
    "title": "ADP 필기 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html",
    "href": "posts/04_archives/vault/index.html",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#details",
    "href": "posts/04_archives/vault/index.html#details",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#tasks",
    "href": "posts/04_archives/vault/index.html#tasks",
    "title": "vault",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#참고-자료",
    "href": "posts/04_archives/vault/index.html#참고-자료",
    "title": "vault",
    "section": "참고 자료",
    "text": "참고 자료\n\nvault Udemy 강의",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/vault/index.html#related-posts",
    "href": "posts/04_archives/vault/index.html#related-posts",
    "title": "vault",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "vault"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html",
    "href": "posts/04_archives/k8s/notes/5_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html#persistant-volume",
    "href": "posts/04_archives/k8s/notes/5_storage.html#persistant-volume",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/5_storage.html#storage-class",
    "href": "posts/04_archives/k8s/notes/5_storage.html#storage-class",
    "title": "김형훈의 학습 블로그",
    "section": "storage class",
    "text": "storage class\n\ndynamically provisioned persistant volumes.",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#manual-scheduling",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#manual-scheduling",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "title": "김형훈의 학습 블로그",
    "section": "taints and tolerations",
    "text": "taints and tolerations\n\ntaints: a taint is a key-value pair that is applied to a node\ntolerations: a toleration is a key-value pair that is applied to a pod  \nnot garantee that the pod will be scheduled to the node",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#node-affinity",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#node-affinity",
    "title": "김형훈의 학습 블로그",
    "section": "node affinity",
    "text": "node affinity\n\n\n\nnode affinity",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#resource-limits-requests",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#resource-limits-requests",
    "title": "김형훈의 학습 블로그",
    "section": "resource limits, requests",
    "text": "resource limits, requests\n\nresource limits: the maximum amount of resources that a container can use\nresource requests: the amount of resources that a container is guaranteed to have\nresource quotas: the maximum amount of resources that a namespace can use\nlimit range: the minimum and maximum amount of resources that a container can use when it is created",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#static-pod",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#static-pod",
    "title": "김형훈의 학습 블로그",
    "section": "static pod",
    "text": "static pod\n\nstatic pod is a pod that is created by the kubelet on a node\nif kube-api is available, the kubelet will create the mirror pod in the api server. that is read-only  or in /etc/kubernetes/manifests",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#multiple-shedulers",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#multiple-shedulers",
    "title": "김형훈의 학습 블로그",
    "section": "multiple shedulers",
    "text": "multiple shedulers",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "href": "posts/04_archives/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "title": "김형훈의 학습 블로그",
    "section": "configuring sheduler profile",
    "text": "configuring sheduler profile\n: single sheduler, multi profile",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/7_design_cluster.html",
    "href": "posts/04_archives/k8s/notes/7_design_cluster.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "href": "posts/04_archives/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "href": "posts/04_archives/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "title": "김형훈의 학습 블로그",
    "section": "cluster upgrade process",
    "text": "cluster upgrade process\n - k8s supports up to recent 3 minor versions  ### kubeadm upgrade 1. upgrade kubeadm 2. command: kubeadm upgrade apply 3. upgrade kubelet and kubectl",
    "crumbs": [
      "PARA",
      "Archives",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "href": "posts/04_archives/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "title": "EDA와 시각화",
    "section": "EDA(Exploratory Data Analysis)",
    "text": "EDA(Exploratory Data Analysis)\n: 데이터의 특징과 데이터에 내재된 관계를 알아내기 위해 그래프와 통계적 분석 방법을 활용하여 탐구하는 것\n\n주제\n\n저항성 강조: 부분적 변동(이상치 등)에 대한 민감성 확인\n잔차 계산\n자료변수의 재표현: 변수를 적당한 척도로 바꾸는 것\n그래프를 통한 현시성",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#막대-그래프",
    "href": "posts/04_archives/adp_실기/notes/02.html#막대-그래프",
    "title": "EDA와 시각화",
    "section": "막대 그래프",
    "text": "막대 그래프\n범주형 데이터를 요약하고 시각적으로 비교하는 데 활용\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine_load\nwine['Class'] = wine_load.target\nwine['Class'] = wine['Class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nwine_type = wine['Class'].value_counts()\nwine_type\n\nClass\nclass_1    71\nclass_0    59\nclass_2    48\nName: count, dtype: int64\n\n\n\n# 수직 막대\nplt.bar(wine_type.index, wine_type.values, width=0.8, bottom=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 수평 막대\nplt.barh(wine_type.index, wine_type.values, height=0.8, left=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n각 범주의 값의 갯수 차이가 극단적인지 확인한다. 극단적일 경우, 전처리 과정에서 업/다운 샘플링 등을 통해 갯수가 유사해지도록 조정해야한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#히스토그램",
    "href": "posts/04_archives/adp_실기/notes/02.html#히스토그램",
    "title": "EDA와 시각화",
    "section": "히스토그램",
    "text": "히스토그램\n연속형 데이터의 분포를 확인하는 데 활용\n\nplt.title('Wine alcohol histogram')\nplt.hist('alcohol', bins=8, range=(11, 15), color='purple', data=wine)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#box-plot",
    "href": "posts/04_archives/adp_실기/notes/02.html#box-plot",
    "title": "EDA와 시각화",
    "section": "box plot",
    "text": "box plot\n수치형 변수의 분포를 확인하는 그래프\n\nfrom sklearn.datasets import load_iris\n\niris_load = load_iris()\niris = pd.DataFrame(iris_load.data, columns=iris_load.feature_names)\niris['class'] = iris_load.target\niris['class'] = iris['class'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\nplt.boxplot(iris.drop(columns='class'))\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.boxplot(x=\"class\", y=\"sepal width (cm)\", data=iris)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#산점도",
    "href": "posts/04_archives/adp_실기/notes/02.html#산점도",
    "title": "EDA와 시각화",
    "section": "산점도",
    "text": "산점도\n두 개의 수치형 변수의 분포와 관계를 확인하는 그래프\n\nplt.title('iris scatter')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\n\nplt.scatter('sepal length (cm)', 'sepal width (cm)', data=iris, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', hue='class', data=iris, style='class')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#선그래프",
    "href": "posts/04_archives/adp_실기/notes/02.html#선그래프",
    "title": "EDA와 시각화",
    "section": "선그래프",
    "text": "선그래프\n\n수평 / 수직 선\n\nplt.hlines(y=-6, xmin=-10, xmax=10, colors='red', linestyles='solid')\nplt.vlines(x=0, ymin=-10, ymax=10, colors='blue', linestyles='dashed')\n\n\n\n\n\n\n\n\n\n\n함수식\n\ndef linear_func(x):\n    return 2*x + 1\n\nX = iris['sepal length (cm)']\nplt.plot(X, linear_func(X), c='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n회귀선\n\nimport numpy as np\n\nX, Y = iris['sepal length (cm)'], iris['sepal width (cm)']\nplt.scatter(X, Y, alpha=0.5)\na, b = np.polyfit(X, Y, 1)\nplt.plot(X, a*X + b, c='red')\nplt.show()\n\n\n\n\n\n\n\n\n2차 이상의 그래프는 X값에 대하여 정렬해야 한다.\n\niris2 = iris.sort_values(by='sepal length (cm)')\nX, Y = iris2['sepal length (cm)'], iris2['petal length (cm)']\nb2, b1, b0 = np.polyfit(X, Y, 2)\nplt.scatter(X, Y, alpha=0.5)\nplt.plot(X, b0 + b1*X + b2*X**2, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n꺾은선\n\nplt.plot('sepal length (cm)', 'petal length (cm)', data=iris2)\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#상관관계-시각화",
    "href": "posts/04_archives/adp_실기/notes/02.html#상관관계-시각화",
    "title": "EDA와 시각화",
    "section": "상관관계 시각화",
    "text": "상관관계 시각화\n\n산점도 행렬\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(iris, alpha=0.5, figsize= (8, 8), diagonal='hist')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(iris, diag_kind='auto', hue='class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n상관계수 행렬 그래프\n\niris_corr = iris.drop(columns='class').corr(method='pearson')\nsns.heatmap(iris_corr, xticklabels=iris_corr.columns, yticklabels=iris_corr.columns, cmap=\"RdBu_r\", annot=True)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/02.html#pandas-profiling",
    "href": "posts/04_archives/adp_실기/notes/02.html#pandas-profiling",
    "title": "EDA와 시각화",
    "section": "Pandas Profiling",
    "text": "Pandas Profiling\n\n# from pandas_profiling import ProfileReport\n#\n# ProfileReport(iris)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/notes/04.html",
    "href": "posts/04_archives/adp_실기/notes/04.html",
    "title": "머신 러닝",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비",
      "Notes",
      "머신 러닝"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html",
    "href": "posts/04_archives/adp_실기/index.html",
    "title": "ADP 실기 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-02-05\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석python",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#details",
    "href": "posts/04_archives/adp_실기/index.html#details",
    "title": "ADP 실기 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#tasks",
    "href": "posts/04_archives/adp_실기/index.html#tasks",
    "title": "ADP 실기 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    파이썬 한권으로 끝내기 완독\n                \n                \n            \n            \n            \n                \n                    \n                    원서 접수 (2025.03.24 10 am)",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#why-failed",
    "href": "posts/04_archives/adp_실기/index.html#why-failed",
    "title": "ADP 실기 준비",
    "section": "Why failed?",
    "text": "Why failed?\nTOFEL이 더 급하다.\n4학년 때 도전하자.",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/adp_실기/index.html#related-posts",
    "href": "posts/04_archives/adp_실기/index.html#related-posts",
    "title": "ADP 실기 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "ADP 실기 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/12_database.html",
    "href": "posts/04_archives/aws_saa/notes/12_database.html",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/12_database.html#database-types",
    "href": "posts/04_archives/aws_saa/notes/12_database.html#database-types",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "title": "김형훈의 학습 블로그",
    "section": "AWS WAF(Web Application Firewall)",
    "text": "AWS WAF(Web Application Firewall)\n\ndeploy on\n\nCloudFront (global)\nApplication Load Balancer (regional)\nAPI Gateway (regional)\nAppSync GraphQL API (regional)\ncognito (regional)\n\n\n\nfeatures\n\nprotect from SQL injection, cross-site scripting, and other web attacks\nIP blacklisting and whitelisting\nfilter HTTP headers / body / URI\nlimit the size of requests\ngeo-blocking\nrate limiting (DDoS protection)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Shield",
    "text": "AWS Shield\n\nDDoS protection service\nStandard and Advanced plan",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Firewall Manager",
    "text": "AWS Firewall Manager\n\ncentral management service to configure and manage WAF rules across accounts and applications",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon GuardDuty",
    "text": "Amazon GuardDuty\n\nthreat detection service\ngood for detect crypto currency mining",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Inspector",
    "text": "Amazon Inspector\n\nsecurity assessment service\ncontinuous assessment of applications for vulnerabilities and deviations from best practices",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "href": "posts/04_archives/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Macie",
    "text": "Amazon Macie\n\ndata security and data privacy service\ndetect and protect sensitive data",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/08_cloudfront.html",
    "href": "posts/04_archives/aws_saa/notes/08_cloudfront.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/08_cloudfront.html#cloudfront",
    "href": "posts/04_archives/aws_saa/notes/08_cloudfront.html#cloudfront",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html",
    "title": "AWS Organization",
    "section": "",
    "text": "global service\ncontrol over multiple AWS accounts\nconsolidated billing\nshared reserved instances and savings plans across accounts ## service control policies (SCPs)\nIAM policy applied to OU or account except management account\n\n\n\n\ncloudwatch agent\n\n\n\n\n\ncloudwatch agent\n\n\n\n\n\nIAM role: cross-account access\nResource-based policy: cross-service access \n\n\n\n\n\nsupported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions \n\n\n\n\n\n\n\n\nAD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "title": "AWS Organization",
    "section": "",
    "text": "IAM role: cross-account access\nResource-based policy: cross-service access",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "title": "AWS Organization",
    "section": "",
    "text": "supported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/16_IAM.html#aws-directory-service",
    "href": "posts/04_archives/aws_saa/notes/16_IAM.html#aws-directory-service",
    "title": "AWS Organization",
    "section": "",
    "text": "AD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet\n\n\n\n: stateless, allow/deny traffic in/out of subnet default, it allows all traffic \n\n\n\n\n\n\nVPC\n\n\n\n\n\n: private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.\n\n\n\n: VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC\n\n\n\n: connect on-premises network to AWS VPC \n\n\n\n: dedicated network connection between on-premises and AWS\n\n\n\n: IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#nat-gateway",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#nat-gateway",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#nacls",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#nacls",
    "title": "VPC",
    "section": "",
    "text": ": stateless, allow/deny traffic in/out of subnet default, it allows all traffic",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-peering",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-peering",
    "title": "VPC",
    "section": "",
    "text": "VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "title": "VPC",
    "section": "",
    "text": ": private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "title": "VPC",
    "section": "",
    "text": ": VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "title": "VPC",
    "section": "",
    "text": ": connect on-premises network to AWS VPC",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#direct-connectdx",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#direct-connectdx",
    "title": "VPC",
    "section": "",
    "text": ": dedicated network connection between on-premises and AWS",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "href": "posts/04_archives/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "title": "VPC",
    "section": "",
    "text": ": IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)\n\n\n\n\n\n\ndistribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer\n\n\n\n\n\n\n\nServer Name Indication\nALB and NLB and cloudFront support SNI\n\n\n\n\n\n\nALB and NLB support connection draining (deregestration delay)\n\n\n\n\n\n\n\nTarget tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "title": "ELB",
    "section": "",
    "text": "distribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#ssltls",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#ssltls",
    "title": "ELB",
    "section": "",
    "text": "Server Name Indication\nALB and NLB and cloudFront support SNI",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#connection-draining",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#connection-draining",
    "title": "ELB",
    "section": "",
    "text": "ALB and NLB support connection draining (deregestration delay)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/04_elb_asg.html#asg",
    "href": "posts/04_archives/aws_saa/notes/04_elb_asg.html#asg",
    "title": "ELB",
    "section": "",
    "text": "Target tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "title": "Amazon RDS",
    "section": "",
    "text": "Amazon RDS is a managed relational database service that provides a highly available, scalable, and secure database.\nAmazon RDS supports multiple database engines:\n\nAmazon Aurora\nMySQL\nMariaDB\nPostgreSQL\nOracle\nMicrosoft SQL Server\n\nAmazon RDS provides the following features:\n\nAutomated backups\nMulti-AZ deployments\nRead replicas\nMonitoring\nSecurity\nScalability\nHigh availability ### auto scaling\n\nmust set maximum storage threshold\n\n\nfree storage is less then 10% of allocated storage\nlow-storage lasts at least 5 minutes\n6 hours have passed since last modification\n\n\n\n\nup to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance\n\n\n\n\n\n\nMySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second\n\n\n\n\n\n\n\n\nAutomated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore\n\n\n\n\n\n\n\n\n\nin-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "title": "Amazon RDS",
    "section": "",
    "text": "up to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "title": "Amazon RDS",
    "section": "",
    "text": "MySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "title": "Amazon RDS",
    "section": "",
    "text": "Automated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "href": "posts/04_archives/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "title": "Amazon RDS",
    "section": "",
    "text": "in-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#define-iam",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#define-iam",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iampolicies",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iampolicies",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Policies",
    "text": "IAM:Policies\n\nUsers or Groups can be assigned JSON documents called policies\npolicies define permissions of the users(inline) or groups\nAWS apply the least privilege principle\n\n\nJSON exe\n{\n    {\n        \"Version\": \"2012-10-17\",\n        // optional: \"id\": \"...\",\n        \"Statement\": [\n            {\n                // optional: \"Sid\": \"...\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"s3:ListBucket\",\n                \"Resource\": \"arn:aws:s3:::example-bucket\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"s3:GetObject\",\n                    \"s3:PutObject\"\n                ],\n                \"Resource\": \"arn:aws:s3:::example-bucket/*\"\n            }\n        ]\n    }\n}\n\nEffect: Allow/Deny\nPrinciple: who can perform the action (account, user, role)\nAction: list of actions that are allowed or denied\nResource: list of resources that are allowed or denied\nCondition: when the policy is in effect (optional)",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Roles for Services",
    "text": "IAM:Roles for Services\n\nRoles are used to delegate permissions to entities that you trust\n\n\ntrusted entities\n\nAWS account\n\nAWS services\n\nEC2, Lambda, CodeBuild, CodePipeline, etc.",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "href": "posts/04_archives/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Security Tools",
    "text": "IAM:Security Tools\n\nIAM Credentials Report (account level):\nlist of all users and their various credentials\nIAM Access Advisor (user level):\nhow long each service has been active and when it was last used",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/00_region.html",
    "href": "posts/04_archives/aws_saa/notes/00_region.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "href": "posts/04_archives/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time\n\n\n\n\n\nlog data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3\n\n\n\n\n\ncollect more system-level metrics \n\n\n\n\n\nalarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm\n\n\n\n\n\ncron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#matrics",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#matrics",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#logs",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#logs",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "log data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "collect more system-level metrics",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "alarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "cron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/aws_saa/notes/15_monitoring.html#insights-events",
    "href": "posts/04_archives/aws_saa/notes/15_monitoring.html#insights-events",
    "title": "Amazon CloudWatch",
    "section": "Insights Events",
    "text": "Insights Events\n\ninsights events provide insights into the performance and availability of your AWS Account",
    "crumbs": [
      "PARA",
      "Archives",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "title": "Introduction to Human Factors",
    "section": "what is human factors",
    "text": "what is human factors\n\nhuman factors = Ergonomics\na human-centered design philosophy &lt;-&gt; technology-centered design",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "component of human factors",
    "text": "component of human factors\n\nhuman: physical, cognitive, group\ntask: physical + cognitive + group\nenvirnment: working environment, systems",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "Goal of human factors",
    "text": "Goal of human factors\n\nReduce errors\nIncrease productivity\nEnhance safety\nEnhance comfort",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "title": "Research Method in Human Factors",
    "section": "reaserch meathods",
    "text": "reaserch meathods\n\ndescriptive Research\n관찰을 통해 데이터 묘사\n\n무엇을 측정할지\n어떻게 숫자로 표현할지\n\n\n대부분 평균, 표준편차를 대푯값으로 사용\n변수들 간의 관계를 파악하기 위해 상관분석, 회귀분석을 사용\n\n\ntypes of descriptive research\n\nobservational research\n\n\n관찰 연구를 계획할 때, 측정할 변수, 각 변수를 기록할 방법, 관찰이 이루어지는 조건, 관찰 기간 등을 식별\n\n\nsurvey research\n\n\n설문조사를 통해 데이터 수집\n\n\nincident and accident analysis\n\n\n사고나 오류를 분석하여 원인을 찾음\n사고나 오류를 줄이기 위한 대책을 마련\n\n\n\n\nexperimanetal Research\n하나 이상의 독립변수에 의도적인 변화를 주고, 그 변화가 하나 이상의 종속변수에 미치는 인과관계를 측정\n이때 다른 변수들은 통제한다\n\n예시\n\n휴대전화를 사용하는 것이 운전에 미치는 영향\n인센티브를 미리 주고 잘못 할 때마다 차감하는 것과, 잘할 때마다 인센티브를 주는 것의 차이\n\n\n\ntype of variables\n\nindependent(predictor, stratification) variable\ndependent(descriptive, criterion) variable\ncontrol variable: 이 값은 고정시키고 실험을 진행한다. 일반화하기 어렵게 한다.\nrandom variable (sigma): 통제할 수 없는 변수. 일반화하기 용이하다.\nconfounding variable: 수식에는 포함되지 않지만 주의해야하는 변수.\n\n\ndecide variables\noperational definition: 변수를 관찰 가능하고 측정 가능한 형태로 정의\n\nindependent variable\n\nRange: realistic / select a range taht will show the effect / pilot experiment\n\ndependent variable\n\nreliability: consistent. solution: increase the number of observations\nvalidity: measure what was intended\n\n\n\n\n\n\nwhy use experimental research?\n\nhumans are variable\n\n\nintra individual variability\ninter individual variability\n\n\nways to handle variability\n\n\nuse statistical techniques\ncontrol variability as much as possible\n\n\n\ntypes of experimental design\n\nsingle variable experiment\n\n\ntwo levels\nmulti levels\n\n\nfactorial design: 두개 이상의 독립변수를 조합하여 실험군을 만든다.\n\n\n변수 간 interaction effect을 확인할 수 있다.\nmore difficult to analyze\n2 x 2, 3 x 3, 2 x 2 x 2 등으로 설계한다.\nbetween-subject, within-subject를 모두 사용하는 mixed designs를 사용할 수 있음.\n\n\nbetween-subjects design: 각각의 실험군에 다른 사람들을 넣는다.\n\n\ngeneralibility 높다, intra person variability를 제거할 수 있다.\n\n\nwithin-subjects design: 같은 사람들을 다른 실험군에 넣는다.\n\n\ncost-effective, less variability, inter person variability를 제거할 수 있다.\n\n\n\n\nevaluation research\n시스템이나 제품이 목적을 충족하는지 평가\n\nusability testing: 사용자가 제품을 실제로 사용하면서 발생하는 문제점 파악\n\n태스크 완료 시간, 오류율, 사용자 만족도 등을 측정\n\ncost-benefit analysis: 제품 또는 시스템 도입의 경제성 평가\n\n직접 비용(하드웨어, 소프트웨어 구입비, training cost 등)\n예상되는 이익(생산성 향상, 오류 감소 등)을 비교 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "title": "Research Method in Human Factors",
    "section": "research design",
    "text": "research design\n\nqualitative research\n\n보통 마케팅에서 진행.\n\n\n\nquantitative research\n\nexperiments\ncorrelational observation\nsurveys and questionnaires\n\nsample: 랜덤하게 샘플링하는게 중요\nrating\nbias: 질문의 순서, 질문의 내용, 질문의 방향\n\narchival research\n\n\nfield study\n\nuncontrolled\nresults may be more generalizable to real-world situations\nhigher cost\ndifficult to replicate\ndifficult to control extraneous variables\n\n\n\nlab experiment\n\ncontrolled\nprecise replication\nlower cost\nmore flexibility\nreal-world generalizability may be limited",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "title": "Attention",
    "section": "Attention의 정의",
    "text": "Attention의 정의\n\nAttention acts as a means of focusing limited mental resources on the information and cognitive processes that are most salient at a given moment\nFocusing most salient at a given moment:\n\n주의는 Search light로 비유됨\n한 영역에 집중하면 다른 부분은 배제.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "title": "Attention",
    "section": "Attention의 네 가지 주요 측면",
    "text": "Attention의 네 가지 주요 측면\n\nSelective, Focused, Divided, Sustained Attention은 독립적이지 않으며, 상호작용하여 주의 과정 형성.\n\n\nFocused Attention (집중적 주의)\n\n특정 과업에 집중하고, 외부 방해 요인을 배제하는 능력.\n방해 요소(Distraction)를 최소화하여 현재 작업에 주의 집중.\n예시:\n\n냉장고에서 음식을 꺼내려는 도중 질문을 받으면 집중력이 분산되어 원래 작업을 잊어버릴 수 있음.\n\n\n\n\nDivided Attention (분할 주의)\n\n여러 작업을 동시에 수행하며 주의를 분배.\nSelective Attention과의 차이:\n\nSelective Attention: 특정 자극을 선택적으로 받아들임.\nDivided Attention: 여러 작업 간 우선순위를 메기고 주의 자원 분배.\n\n예시:\n\n운전 중 대화하며 라디오 듣기.\n각 작업에 필요한 시간과 노력을 어떻게 배분할지를 결정.\n\n\n\n\nSustained Attention (지속적 주의)\n\n주의가 높거나 낮거나보다는, 장시간 동안 주의를 유지하는 능력.\n높은 주의 레벨 필요 시:\n\n정보를 놓치는 경우가 발생할 가능성이 높음.\n여러 정보와 자극을 동시에 처리.\n예: 주식 거래에서 여러 종목을 모니터링.\n\n낮은 주의 레벨 시:\n\n자극 부족으로 주의 산만 발생.\n예: CCTV 감시 업무.\n\n\n\n\nSelective Attention (선택적 주의)\n\n여러 감각 자극 중 중요한 정보를 선택적으로 처리.\n시각, 청각, 촉각 등 다양한 감각 경로를 통해 들어오는 자극에서 의미 있는 정보 선별.\n예시:\n\n운전 중:\n\n표지판, 신호등, 앞차의 움직임 → 중요한 정보.\n옆 보행자의 얼굴이나 주변 불필요한 자극 → 중요하지 않은 정보.\n\n\n\n\n\nMental Workload (정신적 작업 부하)\n\n동시에 수행할 수 있는 과업이 몇 개인지 혹은 이 과업이 수행하기에 attention scale을 넘어가는 것인지 분석 용도\n측정 방법:\n\n주관적 설문:\n\n작업자가 느끼는 주관적 부담을 평가.\n\n생체 반응 분석:\n\n심박수, 뇌파 등 생리적 데이터를 활용.\n\n부과 과업(parallel tasking):\n\n추가 과업을 부여하여 작업 부하 평가.\n예시:\n\n운전 중 숫자 거꾸로 세기.\n특정 숫자를 기억하고 응답(예: N-back 테스트).\n\n\n\n워크로드 증가의 결과 특정 작업의 실패 확률 증가한다.\n\n\n\nAttention의 결정 요인\n\n의지\n\n개인의 목표와 필요에 따라 주의 집중.\n예: 차선 변경 시 후방 차량 확인.\n\ncaptured by salience and grouping\n\n공간, 강도, 색상, 크기, 음조 등 외부 요인.\n강렬한 자극이 주의를 끌 가능성 높음.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "title": "Attention",
    "section": "Selective Attention",
    "text": "Selective Attention\n\n특정한 자극(예: 시각적 또는 청각적 정보)에 주의를 집중하며, 다른 자극을 배제하는 과정.\n중요도에 따라 특정 정보를 선택적으로 처리하며 불필요한 정보는 억제.\nattention이 sensory memory로 부터 들어온 정보의 filter나 gateway나 bottle neck으로 작용한다고 봄\n\n\n작동 원리\n\nTop-down Processing (Mental model):\n\n개인의 경험과 목표에 기반하여 주의 집중 전략을 개발.\n예: 초보 운전때 앞만 보고 가다가 숙련이 되면 사이드미러 같은 주변도 보게 됨.\n\nBottom-up Processing (자극 기반 처리):\n\n강렬하거나 눈에 띄는 자극에 주의가 끌림.\n예: 갑작스러운 소리나 반짝이는 신호등.\n결정 요인\n\nSalience Source: 자극의 강도(밝기, 소리 크기 등).\n\nInformation Access Trade-offs: 특정 정보를 처리함으로써 얻는 이득.\n\n\n\n\nBottleneck Model\nEarly Selection Theory\n- sensory memory까지는 잘 오지만, Attention filter에 선택이 된게 처리가 되고 나머지는 처리가 안 된다.\n- 감각 단계에서 물리적 특성(의미가 아닌)을 기준으로 정보 필터링.\n- 폐기된 정보는 행동에 미치는 영향 없음.\n- 한계: 칵테일 파티 현상(의미 정보 처리 설명 불가).\n\nLate Selection Theory\n- 모든 정보가 cognition / working memory까지 전달 후 선택.\n- 식별되지 않은 정보는 작업 기억의 제한된 용량으로 인해 빠르게 잊혀짐.\n- 선택되지 않은 정보도 행동에 영향을 미침.\n- 광고 실험 - 인지하지 못하는 정보(빠르게 잊어버려서)에 의해서도 행동의 변화가 있을 것이다.\n\n\nTask\nGeneral orientation and scene scanning\n- 그림을 보거나 웹 브라우징\n감독 제어 (Supervisory Control)\n- 자동화된 시스템에서 이상 징후를 탐지.\n- 주로 AOI(Area of Interests)를 스캐닝 함.\nAOI는 여러개가 있음. 시간, 중요도, 과업의 컨텍스트에 따라서 다르게 설정됨\nspecific task-related information이 있는 물리적 위치\nAOI를 몇개를 만들고, 이들에 대한 시선의 이동을 어떻게 만들것인가가 중요한 issue - 예시:\n자율주행 차량 또는 산업 기계 감독.\n제어 패널에서 비정상적인 지표 확인 (예: 전력 공급 문제, 자원 부족).\n탐지 (Noticing)\n- 예상치 못한 사건이나 환경 변화 감지.\n- 예시:\n- CCTV로 비정상적인 활동 탐지.\n- 주요 시스템 성능의 갑작스러운 변화 인식.\n탐색 (Searching)\n- 방해 요소 속에서 특정 목표를 찾는 활동.\n- 예시:\n- 공항에서 수하물의 X-ray 검색.\n읽기 (Reading)\n- 책이나 디스플레이에서 정보를 읽고 이해.\n- 예시:\n- 계기판의 게이지 읽기.\n확인 (Confirming)\n- 작업이나 과정의 결과를 확인.\n- 예시:\n- 비행기 바퀴가 잘 내려왔는지 확인\n선택적 주의 과업 실패는 중요 정보를 놓치거나 잘못 해석하는 경우 발생할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "title": "Attention",
    "section": "SEEV Model",
    "text": "SEEV Model\nvisual attention에 영향을 주는 요소들을 설명\n\nBottom-up factors\n\nSalience: cue의 특징\nEffort: AOI로 이동하는데 드는 비용\n선형적으로 증가하는건 아니고, 그룹핑 할 수 있음.\n(Within foveal vision) 중심시에서 초점 변화. 멀리있는거에서 가까이 있는거 보는거 &lt; Eye movement &lt; Head movement &lt; Body\n중요한 정보는 cost가 작은 쪽에 배치를 해야함.\n\n\n\nTop-down factors\n\nExpectancy: 일어날 것 같은거에 주의를 더 많이 집중. mental model에 의해 예측 능력이 생길 수 있음.\nValue: 이것에 집중했을 때 얻는 이득, 보지 않았을 때 지불하는 비용\n\n\n\nGuidline\n\n중요한 AOI는 salience가 높아야함\n사용 빈도가 높은 AOI 사이의 거리는 가까워야함\n순차적인 디스플레이도 서로 가깝게 배치해야함.\n\n\n\nChange Blindness\n\n발생 원인\n\n멘탈 워크로드가 높은 경우\n눈에 띄는 변화(Slient change)는 발견하기 쉬움\n중심시에서 멀리 떨어진 곳에서 변화가 발생하면 탐지하기 어렵다.\n시야 밖에서 일어나는 변화는 인지하기 어려움(화면이 깜빡이면서 변하면 animation 효과가 안나타남)\n예상치 못한 변화는 탐지하기 어려움.(top-down processing)\n특정 위치를 응시(fixation)하고 있어도 집중(attention)이 부족하면 변화를 인지하지 못함.\n\n\n\n\nSearch Task의 유형\n\nSerial Search\n\n하나씩 순차적으로 탐색, 탐색 시간이 항목 수에 비례.\n\n예: 긴 텍스트 리스트에서 특정 단어 찾기. 같은 그림 2개 찾기\n\nParallel Search\n\n눈에 띄는 단서(pop-out effect)를 이용해 한 번에 탐색.\n\n5 search items is the same for 50 search items\n\npreattentive process로 유발됨\n\nParallel Search를 유도하는 방법\n\n색상, 크기, 대비(contrast), 회전\n\nmotion\n\nfeature를 adding하는건 찾기 쉬운데 missing하는건 찾기 어려움\n\nO안에서 Q 찾기 vs Q안에서 O 찾기\n\n깜빡이는 곳에서 안깜빡이는거 찾기 vs 안깜빡이는거에서 깜빡이는거 찾기",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "title": "Attention",
    "section": "Divided Attention",
    "text": "Divided Attention\n\n개념\n\n일반적으로 단순 작업보다는 멀티 태스킹이 많이 일어남.\n\n단순 작업: 라면 끓일 때 진짜 라면만 순서대로 끓임. (멀티테스킹 x)\n\n복잡 작업: 라면을 끓이면서 설거지도 하고, 반찬도 만들고, 카톡도 하고, … (멀티테스킹 o)\n\n여러 작업을 동시에 수행하면서 주의를 분배.\nAttention을 한계가 있는 자원으로 바라봄\n\n\n\nResource Model\n\nCentral Resource Theory:\n주의 자원을 단일 통으로 간주. 예: 교차로 진입 시 운전에만 집중, 라디오 듣기같은 다른 작업은 집중을 못함.\nMultiple Resource Theory:\n주의 자원이 감각기관의 특성, 과업의 특성에 따라 별개로 존재\n예: 시각(도로), 청각(라디오) 자원을 분리 사용.\n작업 간 유사성이 높을수록 분배 어려움.\n예: 운전 중 영화 감상(둘 다 시각 자원 사용).\n\n\n\n주의 자원과 훈련의 문제\n주의 자원은 고정인가, 훈련으로 확장 가능한가?\n\n리소스 차이는 명확히 증명되지 않았음.\n전략을 통한 과업 배분 및 우선순위 설정 → 성능 향상.\n반복적 학습과 자동화 → 개별 과업 및 주의 배분이 자연스럽게 효율화.\n\n\n\nAttention as capacity\n\n어떤 정보를 얼마나 주의 깊게 받아들일지 결정\n어떤 정보를 선택할 것인지는 disposition(형태), intentions, arousal, evaluation에 의존\n받아들이는 정보의 특성(visual, auditory), 반응(manual, vocal)에 따라 리소스 풀을 나눌 수 있다.\ntasks interfere to the degree that they tap into the same pool of resources\n\n\n\nUnitary Resource Model (단일 자원 모델)\n\n\n주의(attention)를 제한된 자원으로 봄\n과업 수행에 필요한 자원의 양이 가용 자원을 초과하면 성능 저하\n\n\n\nMultiple Resource Model (다중 자원 모델)\n\n서로 다른 유형의 과업은 다른 자원을 사용\n하지만 한쪽의 workload가 높으면 다른쪽에 영향을 미칠 수 있음.\n비주얼 테스트 두 개를 수행하는 것이 비주얼-청각 테스트보다 더 어려움\n한 객체의 두 가지 특징에 주의를 기울이는 것이 두 객체의 한 가지 특징에 주의를 기울이는 것보다 쉬움\nEx) 특성을 여러 막대로 보여주는것보다 육각형으로 보여주는게 더 보기 쉬움\n\n\n\nPerceptual Modalities\n\nAuditory,Visual, and Tactile Perceptual modalities에 사용하는 resource가 전부 다름\nVisual은 Focal과 Ambient가 서로 다른 자원을 사용함\nCross-modality가 15%정도 더 효과가 있음\ntactile은 auditory랑 비슷함.\n\n\n\ncoding\n\nspatial, verbal\nauditory verbal verbal and visual spatial manual is efficient\nverbal은 단 너무 길면 좋지 않다.\n모든 채널에서 다 쓸 수는 없다. (tactile 같은 경우에는 verbal 코딩이 없음)\n\n\n\nAttentional Allocation during Time-sharing: Skill or Ability?\n\nIf skill:\nAttentional allocation should be trainable\nSkills developed in one task transfer to unrelated tasks.\nIf ability:\nNo evidence supports the existence of a universal “multitasking ability.”\nPeople excel at specific tasks due to familiarity and automation, not inherent multitasking talent.\n\n\nPractical Implications\n\nOperator training:\nTraining must develop automaticity in single-task skills to reduce resource demand\nTraining of attentional allocation and time-sharing will help dual-task performance\nOperator selection:\ntime-sharing ability가 좋은 사람을 선택하는 것보다는 single-task performance가 좋고 자동화가 잘 사람을 선택하는 것이 더 좋음\n\n\n\n\nSystem design이나 multi-task performance를 측정할 때 좋은 것\n\nTask analysis나 multiple resource model를 사용하는 것이 좋다.\nTask의 어떤 면이 효율적으로 time-shared 될 것인가?\nTask의 어떤 면이 interference를 일으킬 것인가?\ninterference를 최소화하기 위해 어떻게 디자인 해야하나?\nex) driving할 때 손과 발을 따로 사용하게 하기\nTime-sharing efficiency, task performance, and mental workload를 고려해야한다\n(멘탈 워크로드 측정은 아직도 쉽지 않다.)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "title": "Control",
    "section": "Basic Control Task and Device",
    "text": "Basic Control Task and Device\n\n\n\n상태를 체크할 때는 toggle switch, 눌렀다 떼는 건 push button\n레버도 상태를 체크할 때 사용. 그 중 큰 힘이 필요한 경우. continuous setting에서는 slider가 쓰임\nselector switch는 lever랑은 다르게 discrete한 상태가 있음 (선풍기 버튼)\n조이스틱은 보통 가속도(2D, 멀리 밀면 빨리 가는 애)를 제어하거나 속도(1D, 버튼 조이스틱)를 제어하는데 사용. 마우스는 위치를 제어.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "title": "Control",
    "section": "15 principles to design of control device",
    "text": "15 principles to design of control device\n\nAttention principles\n\nProximity compatibility\n\n컨트롤 하고자 하는 대상과 컨트롤이 가까워야 한다.\n비상 스위치는 가까이 있어야 한다.\n\n\n\nAvoid resource competition\n\n같은 physical or cognitive resource를 사용하는 control은 피해야한다.\nex) 레버로 속도, 방향 모두 제어\n\n\n\n\nPerceptual principles\n\nMake accessible\n\nphysical accessibility: 손이 닿아야 한다.\ncognitive accessibility: 뭐 하는 control인지 이해하기 쉬워야 한다.\nex) 미는 손잡이는 flat하게 만든다.\n다양한 환경을 고려해야한다. (빛이나 소음이 많은 환경)\n\n\n\nMake discriminable\n\nvisual differentiation: 각각의 컨트롤 장비를 구분할 수 있게\nlogical grouping: 비슷한 기능을 하는 것끼리 묶어놓기\n\n\n\nExploit(활용) redundancy gain\n\n두개의 독립적인 정보를 제공하면 성능이 좋아진다.\n한 가지 정보가 없어도 다른 정보로 대체할 수 있다.\n\n\n\nAvoid absolute judgement limits\n\nworking memory limit(7)를 넘기지 말라.\ncontinuous vs with detents: 연속적인 조절에서 anchor point를 만들어주면 좋다.\n\n\n\n\nMemory Principles\n\nKnowledge in the world\n보편적으로 아는 표현을 사용\n\n\nBe consistent\n\n다른 상황에서도 예상 가능하고 일정한 방법으로 control이 가능해야한다.\n\n\n\nmake discriminable vs be consistent\n\n\n\n\nMental model principles\n\nLocation Compatibility\n\nSpatial Compatibility / physical similarity\n\n\n\nMovement Compatibility\n\n\n\n\nPopulation Stereotypes\n\nrotary controls: 시계방향으로 돌리면 커진다\nUp is on\nIncrease is right, Forward is faster\n\n\n\nResponse selection principles\n\nAvoid accidental activation\n\n사고로 눌리는 것을 방지해야한다.\n\n\n\nHick-Hyman Law\n\\(RT = a + b \\log_2(n+1)\\)\nN is the number of choices\n종류가 많아져도 그냥 몇개만 고민함\n\n\nDecision complexity advantage\n\n일반적으로 복잡한 선택을 적게 하는게 간단한 선택을 여러번 하는것보다 효율적이다\n\n\n\nFitt’s Law\n\n\nIndex of Difficulty: \\(ID = \\log_2(\\frac{2A}{W})\\)\nMovement Time: \\(MT = a + bID\\)\n\navoid accidental vs Fitt’s Law - target width가 구석에 있으면 width가 무한대가 된다.\n\n\nprovide feedback\ntouch screen은 haptic feedback이 없어서 불편함",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "title": "Signal Detection Theory",
    "section": "Overview",
    "text": "Overview\n\n인간의 정보 처리 과정 중 perception에 관련된 것\nperception 단계에서 자극 뿐 아니라 노이즈도 같이 들어옴\n여러가지 신호 중 무엇이 중요한지 판단하는 것\nsiganal 탐지 과정을 정량적 모델로 분석하고 성능 평가가 목표\n인공지능 분야에서 중요성이 대두되고 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "title": "Signal Detection Theory",
    "section": "Example",
    "text": "Example\n\nQuality control inspector\n빵이나 과자가 찌그러졌는지 검사, 반도체 품질 검사. 요즘에는 기계가 대부분 담당\nDetection of a flashing warning light (or cctv)\n거수자 탐지\nAirport security guard\nDetecting peculiar patterns in medical imaging (x-ray)\n종양, 암세포 탐지\nMobile phone rings (sound)\nphantoms vibration\nMorning alarm is active or not (visual)\n\n주변의 제품, 서비스 문제 파악, 해결 디자인 제시, 검증",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Theory",
    "text": "Signal Detection Theory\n\nTrials\n\nSignal case(signal + noise): target이 존재\nNoise case(noise only): target이 없음\n\nResponse\n\nYes\nNo\n\n\n\n\nHit rate: P(Hit) = Number of Hits / Number of Signal Trials\nFalse alarm rate: P(FA) = Number of False Alarms / Number of Noise Trials\nMiss rate: P(Miss) = 1 - P(Hit)\nCorrect rejection rate: P(CR) = 1 - P(FA)\n\n\nWhat does it mean to detect?\n\nsignal is digital (exist / not exist)\nAbsolute threshold is exist\n\n\n\nAssumptions\n\n관찰자가 관찰할 수 있는 signal은 숫자나 변수로 표현할 수 있어야함\nsignal이 random variation이 있다\n피험자가 signal이 있는지 없는지 단순하게 표시할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "title": "Signal Detection Theory",
    "section": "Distribution of signal and noise",
    "text": "Distribution of signal and noise\n\n\nsensitivity index (d')\n\n값이 작으면 분간 힘듦\n값이 크면 분간 쉬움\nsignal의 성격에 따라 결정됨\n\nresponse bias (β)\n\ncriterion에 따라 yes라고 대답하는 비중과 no라고 대답하는 비중\n평가자에 따라 결정됨\n\nd′이 0, β가 50%면 그냥 랜덤으로 대답한 것과 같음\n\n\nd′ 계산\n\nP(M), P(CR) 계산\n표준 정규분포를 그림\nM과 CR의 z값을 찾음\nd′ = (0 - z(M)) + (z(CR) - 0)\n\n\n\nβ 계산\n\nd′과 관계 없이 조절\n\n\\(β = \\frac{P(X/(S+N))}{P(X/N)}\\)\n\\(\\ln β = d′λ_{center}\\)\nβ ~ 1: neutral\n\n\n\n\\(λ_{center}\\) 계산\n\\(λ_{center} = -\\frac{1}{2}(Z(FA)+Z(H))\\)\n\n\\(λ_{center}\\) = 0: ideal observer\n\\(λ_{center}\\) &lt; 0: liberal. yes라고 대답하는 비중이 늘어, hit rate가 높아지지만 false alarm rate도 높아짐\nex) 용의자를 찾는 경찰, 암세포 탐지\n\\(λ_{center}\\) &gt; 0: conservative. no라고 대답하는 비중이 늘어, correct rejection rate가 높아지지만 miss rate도 높아짐\nex) 억울한 죄인을 만들지 않으려는 범원 판결",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "title": "Signal Detection Theory",
    "section": "Optimal Response Criterion",
    "text": "Optimal Response Criterion\nsignal이 더 많은 환경, noise가 더 많은 환경이 있음. 즉, probability가 다를 수 있음\n또, Effects of payoffs가 있음\n\nsignal이 많은 환경 -&gt; criterion을 낮추는게 좋음. \\(β_{opt} &lt; 1\\)\nnoise가 많은 환경 -&gt; criterion을 높이는게 좋음. \\(β_{opt} &gt; 1\\)\n\\(β_{opt} = \\frac{P(N)}{P(S)} * \\frac{V(CR) + C(FA)}{V(H) + C(M)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "title": "Signal Detection Theory",
    "section": "Sluggish β",
    "text": "Sluggish β\n\n\n\nprobability 혹은 payoffs의 변화에 따라 bias가 optimal이랑 다르게 나옴\n\n\n\n\\(β_{opt}\\)가 낮은 경우, ideal보다 덜 conservative함.\n\\(β_{opt}\\)가 높은 경우, ideal보다 덜 risky함.\n확률에 의해 b가 조정될 때 더 많이 발생함.\n\n확률에 대한 계산이 잘못되는 경우\n평가자가 반복되는 반응에 bored해지는 경우",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "title": "Signal Detection Theory",
    "section": "ROC Curve",
    "text": "ROC Curve\n\n\n\nd′이 높아질 수록 false alarm 비중이 낮아지고, hit 비중이 높아짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "href": "posts/04_archives/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Performance",
    "text": "Signal Detection Performance\n\nResponse Bias (β)\n\n잘 맞추면 보상을 준다\nfalse signals to raise signal rate\nFalse Alarm에서도 incentive를 준다.\n\n\n\nSensitivity (d′)\n\ngive feedback\nsignal을 조금 더 오래 보여줌\nsignal을 강조\nsignal을 움직이게\n휴식 시간을 충분히 줌\nsignal이 어떠넌지 잘 보여줌\n온갖 감각으로 signal을 보여줌",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "title": "중심 극한 정리",
    "section": "중심 극한 정리",
    "text": "중심 극한 정리\n평군이 μ이고, 분산이 \\(σ^2\\)인 모집단으로부터 추출한 확률표본 \\(X_1, X_2, ..., X_n\\)의 표본평균 \\(\\bar{X}\\)의 분포\n\n모집단의 분포와 상관 없이 \\(E(\\bar{X}) = μ\\), \\(Var(\\bar{X}) = \\frac{σ^2}{n}\\)\n정규 모집단일 경우 \\(\\bar{X}\\)가 정규분포를 따름\n정규 모집단이 아닐 경우\n\\(n \\geq 30\\) 이면 중심극한정리에 의해 \\(\\bar{X}\\)는 정규분포에 근사됨. (모집단의 skewed에 따라 더 큰 n이 필요할 수 있음)\n∴ \\(\\bar{X} \\sim N(μ, \\frac{σ^2}{n}), \\frac{\\bar{X} - μ}{σ/\\sqrt{n}} \\sim N(0, 1^2)\\)\n모집단의 분포가 이산, 연속 분포일 때 모두 적용 가능하다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "title": "중심 극한 정리",
    "section": "이항분포의 정규근사",
    "text": "이항분포의 정규근사",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "title": "확률변수와 확률분포",
    "section": "확률변수",
    "text": "확률변수\nsample space의 원소를 상호 배반인 event들로 분할하여 실수 값으로 대응시키는 함수\n\n이산확률변수: 확률변수가 취할 수 있는 값이 유한개 또는 무한개이지만 셀 수 있는 경우\n연속확률변수: 확률변수가 취할 수 있는 값이 실수의 구간이고 셀 수 없는 경우\n\n이산 표본공간 -&gt; 이산 확률변수\n연속 표본공간 -&gt; 연속 확률변수\n연속 표본공간 -&gt; 이산 확률변수",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "title": "확률변수와 확률분포",
    "section": "확률 분포",
    "text": "확률 분포\n\n표본공간 S에 정의된 확률변수 X의 모든 함수값들이 발생할 확률. 모집단의 확률구조를 나타냄\n확률 실험 -&gt; 표본공간 -&gt; 확률변수 -&gt; 확률분포\n\n\n이산확률분포\n확률 질량 함수(pmf): P(X=x) = f(x) =&gt; X가 x일 확률\n- 기하분포: 성공확률 p인 베르누이 시행을 독립적으로 반복했을 때 첫 번째 성공이 나타날 때까지의 시행횟수\n\n\n연속확률분포\n\n확률 밀도 함수(pdf): \\(\\int{f(x)}dx = 1\\)\n\\(\\int_a^b {f(x)}dx = P(a ≤ x ≤ b)\\) =&gt; x가 a와 b사이에 있을 확률\nP(X=x) = 0 (연속형 데이터여서 특정값을 가질 확률은 0)\nf(x) ≠ P(X=x)\nf(x)는 1보다 큰 값을 가질 수 있음\n누적분포함수(cdf): \\(F(x) = P(X ≤ x)\\) =&gt; \\(\\int_{-∞}^x{f(y)}dy\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "title": "확률변수와 확률분포",
    "section": "결합 확률분포",
    "text": "결합 확률분포\n\npmf: \\(P(X=x, Y=y) = f(x, y)\\)\npdf: \\(P(a ≤ X ≤ b, c ≤ Y ≤ d) = \\int_{a}^{b}\\int_{c}^{d}{f(x, y)}dydx\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "title": "확률변수와 확률분포",
    "section": "주변 확률분포",
    "text": "주변 확률분포\n\npmf: \\(f_X(x) = \\sum_y{f(x,y)}\\)\npdf: \\(f_X(x) = \\int_{-∞}^{∞}{f(x, y)}dy\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "title": "확률변수와 확률분포",
    "section": "조건부 확률분포",
    "text": "조건부 확률분포\n\n\\(f(x|y)\\) = \\(\\frac{joint}{marginal}\\) = \\(\\frac{f(x, y)}{f_Y(y)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "title": "확률변수와 확률분포",
    "section": "독립 확률변수",
    "text": "독립 확률변수\n\n모든 \\(x, y\\)에 대해 \\(f(x, y) = f_X(x)f_Y(y)\\)\n\n\n\n\\(f(x, y) = g(x) * h(y)\\)\n\nx, y 의 구간이 서로 간섭받지 않는다.\nX,Y는 독립이다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "title": "확률변수와 확률분포",
    "section": "확률변수의 변환",
    "text": "확률변수의 변환\n\ncdf를 이용한 변환\ncdf를 미분해서 pdf\n\n\n역함수가 존재할 경우\n\\(g(y) = f(u^{-1}(y)) * |\\frac{du^{-1}}{dy}|\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "title": "이산형 확률분포",
    "section": "",
    "text": "확률분포 정의 단계",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n각 시행의 결과는 성공(A) 또는 실패(B)\n성공 확률은 p, 실패 확률은 1-p\n각 시행은 서로 독립적 → 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면, 비복원 추출에서도 유효\n∴ S = {A,B}, f(1) = P(X=1) = p, f(0) = P(X=0) = 1-p\n\n\n\n베르누이 시행 예시",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(B(1,p)\\)\n\\(f(x) = p^x(1-p)^{1-x}, x = 0, 1\\)\n\\(E(x) = p\\)\n\\(Var(x) = p(1-p)\\)\n\\(m(t) = 1 - p + pe^t\\)\np = 0.5일 때, 분산은 0.25로 가장 큰 값을 가짐",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\nn번의 독립적인 베르누이 시행을 했을 때 성공 횟수 X\n서로 독립인 n개의 베르누이 분포의 합과 같다.",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim B(n,p)\\)\n\\(f(x) = {_n}C_x\\) \\(p^x(1-p)^{n-x}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = np\\)\n\\(Var(x) = np(1-p)\\)\n\\(m(t) = (1-p + pe^t)^n\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n성공 확률 p인 베르누이 시행을 반복하여 처음 성공할 때까지의 시행 횟수 X\n지수분포와 유사하다\n기하분포는 비기억 속성을 가진다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim G(p)\\)\n\\(f(x) = (1-p)^{x-1}p, x = 1, 2, 3, ...\\)\n\\(E(x) = \\frac{1}{p}\\)\n\\(Var(x) = \\frac{1-p}{p^2}\\)\n\\(m(t) = \\frac{pe^t}{1-qe^t}, (qe^t&lt;1), (q=1-p)\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = (1-p)^y\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n모집단의 크기에 비해 샘플의 크기가 작지 않은 경우, 비 복원 추출시 각각의 선택이 베르누이 시행이라 할 수 없다.\n\\(\\frac{r}{N} = p\\)로 일정할 때, N을 증가시키면, \\(HG(n, N, r)\\)은 \\(B(n, p)\\)로 수렴한다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim HG(n, N, r)\\)\n\\(f(x) = \\frac{\\binom{r}{x}\\binom{N-r}{n-x}}{\\binom{N}{n}}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = \\frac{nr}{N}\\)\n\\(Var(x) = \\frac{nr(N-r)(N-n)}{N^2(N-1)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수 X\n임의의 기간을 n 등분하여 각 등분에서 사건이 발생할 확률이 p라고 할 때, 발생횟수 기댓값 λ를 고정시킨 채로 n을 무한히 증가시킴\nn이 매우 크고 p가 매우 작을 때 이항분포를 포아송분포로 근사할 수 있다\n포아송 분포 + 포아송 분포 = 포아송 분포: \\(P(λ) + P(λ) = P(2λ)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim P(\\lambda)\\)\n\\(f(x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, x = 0, 1, 2, ...\\)\n\\(E(x) = \\lambda\\)\n\\(Var(x) = \\lambda\\)\n\\(m(t) = e^{\\lambda(e^t-1)}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "title": "연속형 확률분포",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\nμ = 0, σ = 1인 정규분포",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "title": "연속형 확률분포",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\nα = ν/2, θ = 2인 감마분포\n자유도 ν에 따라 모양이 변함: 커질수록 정규분포에 가까워짐\n표기: \\(X \\sim χ^2(ν)\\)\n\\(E(x) = ν\\)\n\\(Var(x) = 2ν\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "href": "posts/04_archives/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "title": "연속형 확률분포",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nα = 1, \\(λ = \\frac{1}{\\theta}\\)인 감마분포\nPoisson 분포에서 사건 발생 사이의 시간을 나타낼 수 있음\n표기: \\(X \\sim Exp(λ)\\)\n\\(f(x) = λe^{-λx}, x \\geq 0\\)\n\\(E(x) = \\frac{1}{λ}\\)\n\\(Var(x) = \\frac{1}{λ^2}\\)\n\\(P(X &gt; x) = e^{-λx}\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = e^{-λy}\\)\n포아송분포에서의 \\(\\frac{1}{λ}\\)와 동일\n비기억 특성을 가짐\n독립적으로 동일한 지수분포의 합은 감마분포 \\(Γ(n, \\frac{1}{\\lambda})\\)를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html",
    "title": "Database Design",
    "section": "",
    "text": "MS access is prototyping tool for mock-ups",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "title": "Database Design",
    "section": "Purpose of a Database Design",
    "text": "Purpose of a Database Design\nset of database specifications that can be implemented as a database in a DBMS\n\nconceptual design: non-DBMS specific\nlogical design: DBMS specific\nphysical design: DBMS specific but not implemented directly by humans",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "title": "Database Design",
    "section": "Logical Design(Relational Design)",
    "text": "Logical Design(Relational Design)\n\nCreate a table(relation) for each entity\n\nspecify primary key\nspecify properties for each column\n\ndata type\nconstraints\ndefault value\nnull status\n\nverify normalization: data structure의 complexity를 증가시킬 수도 있다 → denormalization: 조인 불필요, 조회 시 성능 향상 → datastructure complexity vs modification problems\n\nCreate relationships by placing foreign keys:\n\nStrong entity relationships\nID-dependent / non-ID-dependent weak entity relationships\nSubtypes\nRecursive",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "title": "Database Design",
    "section": "Representing Relationships",
    "text": "Representing Relationships\nid-dependent의 경우 부모의 primary key로 composite key 생성\nMaximum cardinality의 유형에 따라 관계 표현 방법이 달라짐\n\n1:1: foreign key를 어디에 두어도 상관 없음\nCREATE UNIQUE INDEX idx_1_1 ON table(foriegn_key);\n1:N: many(child) 쪽에 foreign key를 두는 것이 일반적\n1 side is called parent, many side is called child\nM:N\nData Modeling에서만 쓰임. database design에서는 intersection table을 사용하여 표현. intersection table은 두 entity의 primary key를 포함하는 composite key를 가짐\n만약 두 primary key 외의 attribute를 가진다면, association entity로 표현\nSupertype / Subtype: Supertype의 primary key를 Subtype의 primary key로 사용\nRecursive Relationship: 방향 이거 다시 보자\nN:M의 경우 virtual table을 생성하여 표현\n\n설문조사는\ndescriptive statistics\n남녀 비율, 경험 비율 등등도 포함되어야 한다.\n가중 평균으로 보여준다\n도서관 예약 시스템\n\n퇴설 처리 미흡\n좌석 이용 정보 파악\n앱 알림\n\n좌석 배치도 감이 안온다. 잔여시간도 안뜬다",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "title": "ASP.NET",
    "section": "3-Tier Layers of Database System",
    "text": "3-Tier Layers of Database System\n\npresentation layer: user interface\napplication layer: web server(IIS)\ndata layer: database server",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "title": "ASP.NET",
    "section": "API Interface Standards for DB Access",
    "text": "API Interface Standards for DB Access\nDBMS에 접근하기 위한 표준 API\n\nODBC Open Database Connectivity\nDBMS-independent API\nJDBC: Java Database Connectivity\n\n&lt;a target=\"_blank\"&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "title": "ASP.NET",
    "section": "ASP (Active Server Pages)",
    "text": "ASP (Active Server Pages)\nserver side scripting(VBScript) language\nCGI: &lt;% %&gt;는 server에서 실행되는 코드",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "title": "ASP.NET",
    "section": "ASP 데이터베이스 연동",
    "text": "ASP 데이터베이스 연동\n&lt;%\n  Dim conn, connCmd, rs\n  Set connCmd = \"DSN=dsn_name; Database=dbname; UID=user;PWD=password\"\n  Set conn = Server.CreateObject(\"ADODB.Connection\")\n  Set rs = Server.CreateObject(\"ADODB.Recordset\")\n  conn.Open connCmd\n  rs.Open \"SELECT * FROM table_name\", conn\n%&gt;\n\n&lt;%\n  rs.getRows()\n\n  conn.Execute SQL\n%&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "title": "ASP.NET",
    "section": "오류 메세지 한글 설정",
    "text": "오류 메세지 한글 설정\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;%\n  Session.CodePage = 949\n  Response.CharSet = \"euc-kr\"\n  Response.AddHeader \"Pragma\",\"no-cache\"\n  Response.AddHeader \"cache-control\", \"no-staff\"\n  Response.Expires = -1\n%&gt;\n\nform tag 한글 깨짐 문제\n&lt;%\nSession.CodePage=\"65001\"\nResponse.CharSet=\"UTF-8\"\n%&gt;",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "title": "An Overview of Database",
    "section": "The Importance of DBs Today",
    "text": "The Importance of DBs Today\n\nDepend upon database: Internet, Web 2.0, IOT",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "title": "An Overview of Database",
    "section": "Why and How Databases are Used?",
    "text": "Why and How Databases are Used?\n\nThe purpose of a database is to keep track of thing\ndb store information that is more complicated than a simple spread sheet",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "title": "An Overview of Database",
    "section": "Problems with Lists (spread sheet)",
    "text": "Problems with Lists (spread sheet)\n\nRedundancy\n\n\n\n\n필요없는 column들이 중복됨\n\n\n\nMultiple Themes\n\n\n그 결과로, list에 나타날 때만 존재하는 informartion이 생김\n\n\nList Modification Issues\n\n\n\n\ndeletion problems, update problems, insertion problems",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "title": "An Overview of Database",
    "section": "Relational Databases",
    "text": "Relational Databases\n\nRelationa Model is methodology used as a solution for database design\nA relational database stores information in tables\n\nEach informational topic is stored in its own table\n\nEach theme in the list can be stored in a table\n\nTable = file = relation\ncolumn = fields = attribute\nrow = record = tuple",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "title": "An Overview of Database",
    "section": "SQL (Structured Query Language)",
    "text": "SQL (Structured Query Language)\n\ninternational standard for creating, processing, querying databases and their tables\ndb applications use SQL to retrieve, format, report, insert, delete, modify data for users\ncan combine table by join operation\n\nSELECT  CUSTOMER.CustomerLastName, \n        CUSTOMER.CustomerFirstName, \n        CUSTOMER.Phone,\n        COURSE.CourseDate, \n        ENROLLMENT.AmountPaid,\n        COURSE.Course, \n        COURSE.Fee\nFROM    CUSTOMER, ENROLLMENT, COURSE\nWHERE   CUSTOMER.CustomerNumber = ENROLLMENT.CustomerNumber -- join condition\n        AND  COURSE.CourseNumber = ENROLLMENT.CourseNumber; -- join condition",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "title": "An Overview of Database",
    "section": "Database System (DBS)",
    "text": "Database System (DBS)\n\n\n\nThe four components of database system\n\n\n\nUser: Employ database application to keep track of things\nUse forms to read, enter, query data\nproduce reports\nDatabase Application: web/mobile database applications, Forms, Reports\nDBMS: used to create, process, administer the database\nDatabase: self-describing collection of related tables\nuser data, metadata, index and other overhead data, application metadata(form, reports) are stored in db\nmetadata = about the structure of the database. &lt;-&gt; user data\n\n\nFunction of DBMS\n\nDB administration\n\nControl concurrency\nProvide security\nPerform backup and recovery\n\n\n\n\nReferential Integrity Constraints",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "title": "An Overview of Database",
    "section": "Personal vs Enterprise-class Database Systems",
    "text": "Personal vs Enterprise-class Database Systems\n\nPersonal: Access\nEnterprise-class(Organizational): Microsoft SQL server",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "title": "An Overview of Database",
    "section": "NoSQL databases",
    "text": "NoSQL databases\n\nNoSQL database = non-relational database",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "title": "An Overview of Database",
    "section": "Cloud databases",
    "text": "Cloud databases\nMain frame -&gt; Client/server -&gt; Cloud",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#data-and-information",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#data-and-information",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Data and information",
    "text": "Data and information\n\nData: raw facts. recorded facts\nInformation: meaningful context\nKnowledge: information + 가치",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "What is information system?",
    "text": "What is information system?\n\nSystem: a set of components that interact to achieve some purpose or goal\nInformation System: composed of hardware, software, data, procedures, people",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "System Analysis and Design",
    "text": "System Analysis and Design\n\nSystem analysis and design: process of creating and maintaining information systems\nclassic methodology: SDLC\n\n\nSDLC (System Development Life Cycle)\n\n\n\nSDLC\n\n\n\nSystem definitions: 예산 편상, 위험 분석, …\nRequirements analysis\nComponent design\nImplementation\nSystem maintenance\n\n\ndatabase development process\n\nRequirements analysis\ninput: the project plan\noutput: a set of approved requirements -&gt; data model (ER model로 conceptual design)\nsource: Use cases, Business rules\nComponent Design: Relational Database Design (상세 설계)\nImplementation",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#er-model",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#er-model",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "ER model",
    "text": "ER model\n\nEntities\n\nEntity class\nEntity instance\n\nAttributes: Data type, Properties(default, constraints)\nIdentifiers\n\nunique\nNonunique: identifies a set of instances\n\nRelationships\n\nbinary relationship\n\nMaximum cardinality: 1:1(A has a B), 1:N(A has a set of B), M:N\nMinimum cardinality: 0, 1\n\nternary relationship",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "href": "posts/04_archives/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Entit-Relationship Diagram",
    "text": "Entit-Relationship Diagram\n\nEntity classes: rectangle\nRelationships: diamond\nmaximum cardinality: inside the diamond\nminimum cardinality: oval or hash mark next to diamond\nstrong entity: 독자적으로 존재 가능. 강한개체 관계는 점선\nNon-ID-dependent: identifier에 다른 entity의 identifier가 포함되어 있지 않음. 점선으로 표기(non-identifying relationship)\nweak entity: 약, 강 관계는 실선. IS: rounded square, traditional: 2 layer square\nID-dependent: identifier에 다른 entity의 identifier가 포함되어 있음. 실선으로 표기(identifying relationship)\nassociative entity: relationship이 entity로 변환된 것.\nMany-to-many relationship을 2개의 1:N으로 변환\nsuper type, sub type: 상속관계. sub type is a super type\n\nexclusive: Discriminator attribute가 필요함\ninclusive\n\nrecursive relationship\nBusiness rule: build-in constraints, trigger, stored procedure, application code로 구현 가능\ndata model validation: form, report를 이용한 prototyping",
    "crumbs": [
      "PARA",
      "Archives",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#single-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#single-linked-list",
    "title": "시험 범위",
    "section": "single linked list",
    "text": "single linked list\n\n안 나오지만 그냥 외우셈",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "title": "시험 범위",
    "section": "doubly linked list",
    "text": "doubly linked list",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "href": "posts/01_projects/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "title": "시험 범위",
    "section": "circular linked list",
    "text": "circular linked list\n\n\n\n\n\n5~6주차\n\nstack\n\nt자 철로 출력\nstack을 이용하는 응용\nstack을 이용하지 않는 중위 -&gt; 후위, 전위 표기\n\n\n7주차\n\n원형 queue",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#통계적-추론",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#통계적-추론",
    "title": "통계적 추정",
    "section": "통계적 추론",
    "text": "통계적 추론\n모집단에서 추출된 표본의 통계량으로부터 모수를 추론하는 것\n\n추정\n\n점추정\n구간추정\n\n가설 검정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#추정",
    "title": "통계적 추정",
    "section": "추정",
    "text": "추정\n\n불편성\n\n\\(E(\\hat{\\theta}) = θ\\)\nbias = \\(E(\\hat{\\theta}) - \\theta\\)\n\n보통 sample size가 커질수록 bias는 0에 수렴\n\n\\(\\bar{X}, X_n\\)은 μ의 불편추정량이다.\n\n\n\n최소분산\n\n\\(Var(\\bar{X})\\)가 \\(Var(X_n)\\)보다 분산이 작아서 더 좋은 추정량\n\\(MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2\\)\n\n큰 오차에 더 큰 페널티를 주기 위해 제곱\n\n\n\n\n대표적인 불편추정량\n\n전부 중심극한의정리를 적용할 수 있다.\n\n\n모평균\n모비율\n모평균 차이 (독립이라는 가정 필요)\n모비율 차이 (독립이라는 가정 필요)\n\n\n이때, 이들의 평균과 표준편차는 모집단의 분포와 관계없이 일정하다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#구간-추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#구간-추정",
    "title": "통계적 추정",
    "section": "구간 추정",
    "text": "구간 추정\n\nα: 유의수준\n1 - α: 신뢰수준\n(θ_L, θ_U) = (1 - α) × 100% 신뢰구간\n\n\n(\\(θ_L, θ_U\\)) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다\n구간이 충분히 좁아야 한다\n\n표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.\n고로 신뢰구간이 대칭임\n\n\n\n신뢰 구간의 확률적인 의미\n\n샘플링을 무한히 반복했을 때, 이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함한다. → 구간이 확률 변수이다.\n\n\n\n표본의 크기 결정\n특정 오차 아래로 하는 표본의 수 구하는 법\n\n그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.\n모비율을 모를 때는 일단 0.5로 보수적으로 놓고 계산",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "title": "통계적 추정",
    "section": "소표본 신뢰구간",
    "text": "소표본 신뢰구간\n\n표본이 작다. → 크면 정규분포\n모집단 정규분포를 따른다. → 비모수 검정\nσ 모름 → 알면 그냥 정규 분포\n\\(σ_1 = σ_2\\)\n\n→ t분포\n\n정규분포에 비해 신뢰구간은 더 길어짐",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/01.html#모분산-추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/01.html#모분산-추정",
    "title": "통계적 추정",
    "section": "모분산 추정",
    "text": "모분산 추정\n\n카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음\n\n그냥 쉽게 구하기 위해 \\((x^2_{α/2}, x^2_{1-α/2})\\)를 사용\n\n모분산의 신뢰구간: \\((\\frac{(n-1)s^2}{x^2_{(1-\\alpha)/2}(n-1)}, \\frac{(n-1)s^2}{x^2_{\\alpha/2}(n-1)})\\)\n표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#통계적-검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#통계적-검정",
    "title": "통계적 가설검정",
    "section": "통계적 검정",
    "text": "통계적 검정\n\n가설 수립\n표본 추출\n통계량 계산\n가설 채택 / 기각",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#가설검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#가설검정",
    "title": "통계적 가설검정",
    "section": "가설검정",
    "text": "가설검정\n\n귀무가설(\\(H_0\\)): α의 값을 상한으로 지정하고 보수적으로 고려\n대립가설(\\(H_1\\))\n검정 통계량: \\(H_0\\)가 참이라고 가정했을 때, 표본에서 계산된 통계량\n기각역: \\(H_0\\)를 기각할 수 있는 범위\nType 1 error를 보통 보수적으로 지정하기 때문에 Type 2 error는 높아진다. (상충 관계)\n\n둘 다 줄이고 싶다면 n을 늘려야 한다.\n일반적으로 α(Type 1 error)를 고정하고 원하는 β(Type 2 error)를 만족하는 표본의 크기를 결정한다.\n\n상단측 검정\n하단측 검정\n양측 검정\n\n→ 기각역 계산에 주의하자\n\n모비율 차이 검정\n\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - D_0}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\)\n\\(\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\\)\n\n\n\nβ 계산\n\\(H_0\\)를 이용해 검정 통계량을 계산하고, 이를 이용해 기각역을 구한 후, \\(H_1\\)을 이용해 계산.\n\n\n대표본 단측 가설검정 표본 크기1\n\n\\(n = \\frac{(z_{1 - \\alpha} + z_{1 - \\beta})^2\\sigma^2}{(\\mu_1 - \\mu_2)^2}\\)\n\n\n\n가설 검정 절차와 신뢰구간의 관계\n\n\\(\\hat{θ} - z_{1-α/2}σ_{\\hat{θ}} ≤ θ_0 ≤ \\hat{θ} + z_{1-α/2}σ_{\\hat{θ}}\\)\n100(1-α)% 신뢰구간은 유의수준 α에서 귀무가설 \\(H_0: θ = θ_0\\)가 채택되는 모든 \\(θ_0\\) 값의 집합.\n\n\n\np-value\n\np-value: \\(H_0\\)를 기각시킬 수 있는 가장 작은 유의수준 α의 값 (즉 확률)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "title": "통계적 가설검정",
    "section": "소표본 가설검정",
    "text": "소표본 가설검정\n\n쌍체표본: 두 집단이 독립이 아니고 서로 연관되어 있는 경우\n\n각 쌍의 차이를 계산하여 단일 표본으로 변환 후 분석할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#분산-검정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#분산-검정",
    "title": "통계적 가설검정",
    "section": "분산 검정",
    "text": "분산 검정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/02.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/statistics/02.html#footnotes",
    "title": "통계적 가설검정",
    "section": "각주",
    "text": "각주\n\n\n양측은?↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "title": "ANOVA",
    "section": "여러 모집단의 평균 비교",
    "text": "여러 모집단의 평균 비교\n\n여러 모집단에 대해 t검정을 2개씩 하면 α가 커져서 원하는 유의수준에 대한 가설검정이 어려움.\n\\(t(n_1 + n_2 - 2)^2 = F(1, n_1 + n_2 -2)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석",
    "title": "ANOVA",
    "section": "분산분석",
    "text": "분산분석\n\n요인, 인자 (factor): class\n수준 (level): class 값\n처리 (treatment): 요인과 수준의 조합\n반응치 (response): 관측치\n\n\n완전 확률화 계획법\n\n일원 분산분석이 공정한 결과를 내기 위한 실험 조건\n처리 i에 해당되는 모집단으로부터 독립인 표본 \\(n_i\\)개를 랜덤으로 샘플링함으로써, k개의 서로 다른 모집단으로부터 독립인 random sample들을 얻는 것과 같음\n반복 수가 같을 필요는 없다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산-분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산-분석",
    "title": "ANOVA",
    "section": "일원 분산 분석",
    "text": "일원 분산 분석\n\n정규성, 독립성, 등분산성\n\n→ 오차의 독, 정, 불편성, 등분산성\n오차(표본 - 잔차): 관심 없는 다른 모든 요인에 의해 발생하는 오차\n효과: \\(τ_i: μ_i - μ\\)\n\n\\(Y_{ij} = μ + τ_i + ε_{ij}\\)\n\\(Y_{ij} - \\bar{Y} = (\\bar{Y_i} - \\bar{Y}) + (Y_{ij} - \\bar{Y_i})\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#data-load",
    "title": "preprocessing",
    "section": "Data Load",
    "text": "Data Load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager, rc\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nID_VAR = \"id\"\nWEIGHT_VAR = \"wt2\"\nOUTCOME_VAR = \"status_category\"\nRANDOM_STATE = 54321\ntarget_pred_var = [ID_VAR, \"wt1\", \"q33a01\", \"q33a02\", \"q33a03\", \"q33a04\", \"q33a05\", \"q33a06\",\n                    \"q48a07\", \"q48a08\", \"q48a09\", \"q48a10\",\n                    \"q49a01\", \"q49a02\", \"q49a03\", \"q49a04\",\n                    \"q33a07\", \"q33a08\", \"q33a09\",\n                    \"q49a15\", \"q49a16\", \"q49a17\",\n                    \"q49a09\", \"q49a10\", \"q49a11\",\n                    \"q48b1\", \"q48b2\", \"q48b3\",\n                    \"q12a01\", \"q12a02\", \"q12a03\",\n                    \"q48a04\", \"q48a05\", \"q48a06\",\n                    \"q49a05\", \"q49a06\", \"q49a08\"]\ncfa_model = \"\"\"\n  # 1. 부모애착\n  parent_attachment =~ q33a01 + q33a02 + q33a03 + q33a04 + q33a05 + q33a06\n  # 2. 일탈적 자아 낙인\n  deviant_esteem =~ q48a07 + q48a08 + q48a09 + q48a10\n  # 3. 부모에 의한 스트레스\n  parent_stress =~ q49a01 + q49a02 + q49a03 + q49a04\n  # 4. 부모감독\n  parent_monitoring =~ q33a07 + q33a08 + q33a09\n  # 5. 물질적 요인으로 인한 스트레스\n  desire_stress =~ q49a15 + q49a16 + q49a17\n  # 6. 친구로 인한 스트레스\n  friend_stress =~ q49a09 + q49a10 + q49a11\n  # 7. 자기신뢰감\n  self_confidence =~ q48b1 + q48b2 + q48b3\n  # 8. 상급학교 의존도\n  higher_school_dependence =~ q12a01 + q12a02 + q12a03\n  # 9. 부정적 자아존중감\n  neg_esteem =~ q48a04 + q48a05 + q48a06\n  # 10. 학업으로 인한 스트레스\n  academic_stress =~ q49a05 + q49a06 + q49a08\n\"\"\"\ndf1_origin = pd.read_csv('_data/student_1.csv')\ndf2_origin = pd.read_csv('_data/student_2.csv')\ndf3_origin = pd.read_csv('_data/student_3.csv')\ndf4_origin = pd.read_csv('_data/student_4.csv')\ndf5_origin = pd.read_csv('_data/student_5.csv')\ndf6_origin = pd.read_csv('_data/student_6.csv')\ndf_origin = [df1_origin, df2_origin, df3_origin, df4_origin, df5_origin]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "title": "preprocessing",
    "section": "CFA (Confirmatory Factor Analysis)",
    "text": "CFA (Confirmatory Factor Analysis)\n\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.conversion import localconverter\nfrom rpy2.robjects.packages import importr\n\npandas2ri.activate()\nlavaan = importr('lavaan')\nbase_r = importr('base')\n\nmerged_df_pd = pd.DataFrame()\nfor i, df in enumerate(df_origin):\n    wave = i + 1\n    df_analysis = df[target_pred_var].copy()\n    df_clean = df_analysis.dropna()\n    with localconverter(ro.default_converter + pandas2ri.converter):\n        df_clean_r = ro.conversion.py2rpy(df_clean)\n    cfa_fit_r = lavaan.cfa(\n        model=cfa_model,\n        data=df_clean_r,\n        sampling_weights=\"wt1\",\n        estimator=\"MLR\",\n        warn=False,\n        verbose=False\n    )\n    print(f\"\\n--- Wave {wave} CFA 적합도 지수 ---\")\n    desired_fit_measures = ro.StrVector([\n        'cfi.scaled', 'tli.scaled',\n        'rmsea.scaled', 'rmsea.ci.lower.scaled', 'rmsea.ci.upper.scaled',\n        'srmr_bentler'\n    ])\n    fit_measures_values_r = lavaan.fitMeasures(cfa_fit_r, fit_measures=desired_fit_measures)\n    fit_values_py = [val if val is not ro.NA_Real else float('nan') for val in list(fit_measures_values_r)]\n    fit_measures_s = pd.Series(fit_values_py, index=list(desired_fit_measures))\n    print(fit_measures_s.to_string())\n    print(\"------------------------------------\")\n    factor_scores_r = lavaan.lavPredict(cfa_fit_r, type=\"lv\")\n    factor_scores_pd = pd.DataFrame(factor_scores_r, columns=[\"parent_attachment\", \"deviant_esteem\", \"parent_stress\", \"parent_monitoring\",\n                                                             \"desire_stress\", \"friend_stress\", \"self_confidence\",\n                                                             \"higher_school_dependence\", \"neg_esteem\", \"academic_stress\"])\n    ids_for_scores = df_clean[ID_VAR].reset_index(drop=True)\n    factor_scores_pd[ID_VAR] = ids_for_scores\n    new_colnames = {col: f\"{col}_w{wave}\" for col in factor_scores_pd.columns if col != ID_VAR}\n    factor_scores_pd = factor_scores_pd.rename(columns=new_colnames)\n    if wave == 1:\n        merged_df_pd = factor_scores_pd\n    else:\n        merged_df_pd = pd.merge(merged_df_pd, factor_scores_pd, on=ID_VAR, how='outer')\n\ndef classify_status(q11_value):\n    if q11_value in [1, 7, 8, 9, 71, 81, 91, 10, 101, 11, 111]:\n        return \"stable\"\n    elif q11_value in [2, 3, 4, 5, 6, 12, 13, 14]:\n        return \"explorative\"\n    else:\n        return None\n\npandas2ri.deactivate()\n\n\n--- Wave 1 CFA 적합도 지수 ---\ncfi.scaled               0.927641\ntli.scaled               0.916400\nrmsea.scaled             0.041374\nrmsea.ci.lower.scaled    0.040245\nrmsea.ci.upper.scaled    0.042511\nsrmr_bentler             0.042360\n------------------------------------\n\n--- Wave 2 CFA 적합도 지수 ---\ncfi.scaled               0.937524\ntli.scaled               0.927819\nrmsea.scaled             0.039718\nrmsea.ci.lower.scaled    0.038552\nrmsea.ci.upper.scaled    0.040894\nsrmr_bentler             0.037124\n------------------------------------\n\n--- Wave 3 CFA 적합도 지수 ---\ncfi.scaled               0.930504\ntli.scaled               0.919708\nrmsea.scaled             0.039424\nrmsea.ci.lower.scaled    0.038270\nrmsea.ci.upper.scaled    0.040586\nsrmr_bentler             0.043117\n------------------------------------\n\n--- Wave 4 CFA 적합도 지수 ---\ncfi.scaled               0.925239\ntli.scaled               0.913625\nrmsea.scaled             0.045518\nrmsea.ci.lower.scaled    0.044364\nrmsea.ci.upper.scaled    0.046681\nsrmr_bentler             0.046338\n------------------------------------\n\n--- Wave 5 CFA 적합도 지수 ---\ncfi.scaled               0.934303\ntli.scaled               0.924098\nrmsea.scaled             0.042005\nrmsea.ci.lower.scaled    0.040850\nrmsea.ci.upper.scaled    0.043169\nsrmr_bentler             0.045346\n------------------------------------\n\n\n\n상관행렬\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nplt.figure(figsize=(25, 22))\ncorr_matrix = merged_df_pd.drop(columns=[ID_VAR]).corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(\n    corr_matrix,\n    annot=False,\n    vmax=1.0, \n    vmin=-1.0,\n    center=0,\n    linewidths=.5,\n    cmap=cmap,\n    cbar_kws={\"shrink\": .5, \"label\": \"Correlation Coefficient\"}\n)\n\nplt.title('Correlation Matrix', fontsize=16, pad=20)\nplt.tight_layout()\nplt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ncorr_pairs = corr_matrix.unstack().reset_index()\ncorr_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n\ncorr_pairs = corr_pairs[corr_pairs['Variable 1'] != corr_pairs['Variable 2']]\ncorr_pairs['Pair'] = corr_pairs.apply(lambda x: tuple(sorted([x['Variable 1'], x['Variable 2']])), axis=1)\ncorr_pairs = corr_pairs.drop_duplicates('Pair')\n\ncorr_pairs = corr_pairs.sort_values(by='Correlation', key=abs, ascending=False)\ntop_corr = corr_pairs[['Variable 1', 'Variable 2', 'Correlation']]\n\nprint(\"Highest Absolute Correlations:\")\nprint(top_corr.head(40))\n\n\n\n\n\n\n\n\nHighest Absolute Correlations:\n                Variable 1            Variable 2  Correlation\n619       parent_stress_w2    academic_stress_w2     0.757157\n109       parent_stress_w1    academic_stress_w1     0.737240\n2043  parent_attachment_w5  parent_monitoring_w5     0.713755\n1533  parent_attachment_w4  parent_monitoring_w4     0.692874\n513   parent_attachment_w2  parent_monitoring_w2     0.689347\n2149      parent_stress_w5    academic_stress_w5     0.663941\n3     parent_attachment_w1  parent_monitoring_w1     0.658326\n1023  parent_attachment_w3  parent_monitoring_w3     0.655997\n1129      parent_stress_w3    academic_stress_w3     0.608676\n1540  parent_attachment_w4  parent_attachment_w5     0.604272\n1030  parent_attachment_w3  parent_attachment_w4     0.601522\n10    parent_attachment_w1  parent_attachment_w2     0.582573\n520   parent_attachment_w2  parent_attachment_w3     0.579379\n719       desire_stress_w2    academic_stress_w2     0.574989\n1639      parent_stress_w4    academic_stress_w4     0.566123\n614       parent_stress_w2      desire_stress_w2     0.559798\n409          neg_esteem_w1    academic_stress_w1     0.559204\n209       desire_stress_w1    academic_stress_w1     0.556277\n1040  parent_attachment_w3  parent_attachment_w5     0.539659\n58       deviant_esteem_w1         neg_esteem_w1     0.534314\n104       parent_stress_w1      desire_stress_w1     0.530970\n715       desire_stress_w2      friend_stress_w2     0.530456\n2144      parent_stress_w5      desire_stress_w5     0.519393\n1693  parent_monitoring_w4  parent_monitoring_w5     0.518281\n1132      parent_stress_w3      parent_stress_w4     0.513876\n20    parent_attachment_w1  parent_attachment_w3     0.512048\n1124      parent_stress_w3      desire_stress_w3     0.507338\n1489    academic_stress_w3    academic_stress_w4     0.507213\n163   parent_monitoring_w1  parent_monitoring_w2     0.507147\n1234      desire_stress_w3      desire_stress_w4     0.505383\n530   parent_attachment_w2  parent_attachment_w4     0.504675\n2     parent_attachment_w1      parent_stress_w1    -0.501466\n673   parent_monitoring_w2  parent_monitoring_w3     0.497952\n540   parent_attachment_w2  parent_attachment_w5     0.492200\n1183  parent_monitoring_w3  parent_monitoring_w4     0.489749\n1642      parent_stress_w4      parent_stress_w5     0.489084\n1744      desire_stress_w4      desire_stress_w5     0.486436\n112       parent_stress_w1      parent_stress_w2     0.481296\n1588     deviant_esteem_w4         neg_esteem_w4     0.480133\n1846    self_confidence_w4    self_confidence_w5     0.478809\n\n\n\nextra_df_pd = df6_origin[[ID_VAR, \"q11\", WEIGHT_VAR, \"sex\", \"yy\", \"area\"]].copy()\nextra_df_pd['status_category'] = extra_df_pd['q11'].apply(classify_status)\nmerged_df = pd.merge(merged_df_pd, extra_df_pd, on=ID_VAR, how='left').dropna()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "인구통계학적 분포 분석",
    "text": "인구통계학적 분포 분석\n\n분포 테이블\n\nsex_dist_py = merged_df['sex'].value_counts().rename_axis('sex').reset_index(name='n')\nsex_dist_py['percentage'] = (sex_dist_py['n'] / sex_dist_py['n'].sum()) * 100\n\nbirth_year_dist_py = merged_df['yy'].value_counts().rename_axis('yy').reset_index(name='n')\nbirth_year_dist_py['percentage'] = (birth_year_dist_py['n'] / birth_year_dist_py['n'].sum()) * 100\nbirth_year_dist_py = birth_year_dist_py.sort_values(by='yy').reset_index(drop=True)\n\narea_dist_py = merged_df['area'].value_counts().rename_axis('area').reset_index(name='n')\narea_dist_py['percentage'] = (area_dist_py['n'] / area_dist_py['n'].sum()) * 100\narea_dist_py = area_dist_py.sort_values(by='area').reset_index(drop=True) # 지역 순으로 정렬\n\nstatus_dist_py = merged_df['status_category'].value_counts(dropna=False).rename_axis('status_category').reset_index(name='n')\nstatus_dist_py['percentage'] = (status_dist_py['n'] / status_dist_py['n'].sum()) * 100\n\nprint(sex_dist_py)\nprint(birth_year_dist_py)\nprint(status_dist_py)\n\n   sex     n  percentage\n0  2.0  1280   52.523595\n1  1.0  1157   47.476405\n     yy     n  percentage\n0  88.0     2    0.082068\n1  89.0  1884   77.308166\n2  90.0   551   22.609766\n  status_category     n  percentage\n0          stable  1369   56.175626\n1     explorative  1068   43.824374\n\n\n\n\n분포 시각화\n\narea_mapping_py = {\n    100: \"서울\", 110: \"서울\", 120: \"서울\", 121: \"서울\", 122: \"서울\",\n    130: \"서울\", 131: \"서울\", 132: \"서울\", 133: \"서울\", 134: \"서울\",\n    135: \"서울\", 136: \"서울\", 137: \"서울\", 138: \"서울\", 139: \"서울\",\n    140: \"서울\", 142: \"서울\", 143: \"서울\", 150: \"서울\", 151: \"서울\",\n    152: \"서울\", 153: \"서울\", 156: \"서울\", 157: \"서울\", 158: \"서울\",\n    200: \"강원\", 209: \"강원\", 210: \"강원\", 215: \"강원\", 217: \"강원\",\n    219: \"강원\", 220: \"강원\", 225: \"강원\", 230: \"강원\", 232: \"강원\",\n    233: \"강원\", 235: \"강원\", 240: \"강원\", 245: \"강원\", 250: \"강원\",\n    252: \"강원\", 255: \"강원\", 269: \"강원\",\n    300: \"대전\", 301: \"대전\", 302: \"대전\", 305: \"대전\", 306: \"대전\",\n    312: \"충남\", 314: \"충남\", 320: \"충남\", 321: \"충남\", 323: \"충남\",\n    325: \"충남\", 330: \"충남\", 336: \"충남\", 339: \"충남\", 340: \"충남\",\n    343: \"충남\", 345: \"충남\", 350: \"충남\", 355: \"충남\", 356: \"충남\",\n    357: \"충남\",\n    360: \"충북\", 361: \"충북\", 363: \"충북\", 365: \"충북\", 367: \"충북\",\n    368: \"충북\", 369: \"충북\", 370: \"충북\", 373: \"충북\", 376: \"충북\",\n    380: \"충북\", 390: \"충북\", 395: \"충북\",\n    400: \"인천\", 401: \"인천\", 402: \"인천\", 403: \"인천\", 404: \"인천\",\n    405: \"인천\", 406: \"인천\", 407: \"인천\", 409: \"인천\", 417: \"인천\",\n    411: \"경기\", 412: \"경기\", 413: \"경기\", 415: \"경기\",\n    420: \"경기\", 421: \"경기\", 422: \"경기\", 423: \"경기\", 425: \"경기\",\n    426: \"경기\", 427: \"경기\", 429: \"경기\", 430: \"경기\", 431: \"경기\",\n    435: \"경기\", 437: \"경기\", 440: \"경기\", 441: \"경기\", 442: \"경기\",\n    443: \"경기\", 445: \"경기\", 447: \"경기\", 449: \"경기\", 456: \"경기\",\n    459: \"경기\", 461: \"경기\", 462: \"경기\", 463: \"경기\", 464: \"경기\",\n    465: \"경기\", 467: \"경기\", 469: \"경기\", 471: \"경기\", 472: \"경기\",\n    476: \"경기\", 477: \"경기\", 480: \"경기\", 481: \"경기\", 482: \"경기\",\n    483: \"경기\", 487: \"경기\",\n    500: \"광주\", 501: \"광주\", 502: \"광주\", 503: \"광주\", 506: \"광주\",\n    513: \"전남\", 515: \"전남\", 516: \"전남\", 517: \"전남\", 519: \"전남\",\n    520: \"전남\", 525: \"전남\", 526: \"전남\", 527: \"전남\", 529: \"전남\",\n    530: \"전남\", 534: \"전남\", 535: \"전남\", 536: \"전남\", 537: \"전남\",\n    539: \"전남\", 540: \"전남\", 542: \"전남\", 545: \"전남\", 546: \"전남\",\n    548: \"전남\", 550: \"전남\",\n    560: \"전북\", 561: \"전북\", 565: \"전북\", 566: \"전북\", 567: \"전북\",\n    568: \"전북\", 570: \"전북\", 573: \"전북\", 576: \"전북\", 579: \"전북\",\n    580: \"전북\", 585: \"전북\", 590: \"전북\", 595: \"전북\", 597: \"전북\",\n    600: \"부산\", 601: \"부산\", 602: \"부산\", 604: \"부산\", 606: \"부산\",\n    607: \"부산\", 608: \"부산\", 609: \"부산\", 611: \"부산\", 612: \"부산\",\n    613: \"부산\", 614: \"부산\", 616: \"부산\", 617: \"부산\", 618: \"부산\",\n    619: \"부산\",\n    621: \"경남\", 626: \"경남\", 627: \"경남\", 631: \"경남\", 635: \"경남\",\n    636: \"경남\", 637: \"경남\", 638: \"경남\", 641: \"경남\", 645: \"경남\",\n    650: \"경남\", 656: \"경남\", 660: \"경남\", 664: \"경남\", 666: \"경남\",\n    667: \"경남\", 668: \"경남\", 670: \"경남\", 676: \"경남\", 678: \"경남\",\n    680: \"울산\", 681: \"울산\", 682: \"울산\", 683: \"울산\", 689: \"울산\",\n    690: \"제주\", 695: \"제주\", 697: \"제주\", 699: \"제주\",\n    700: \"대구\", 701: \"대구\", 702: \"대구\", 703: \"대구\", 704: \"대구\",\n    705: \"대구\", 706: \"대구\", 711: \"대구\",\n    712: \"경북\", 714: \"경북\", 716: \"경북\", 717: \"경북\", 718: \"경북\",\n    719: \"경북\", 730: \"경북\", 740: \"경북\", 742: \"경북\", 745: \"경북\",\n    750: \"경북\", 755: \"경북\", 757: \"경북\", 760: \"경북\", 763: \"경북\",\n    764: \"경북\", 766: \"경북\", 767: \"경북\", 769: \"경북\", 770: \"경북\",\n    780: \"경북\", 790: \"경북\", 791: \"경북\", 799: \"경북\",\n    999: \"국외\"\n}\n\nsns.barplot(x='sex', y='percentage', data=sex_dist_py, palette='pastel')\nplt.title('Gender Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(ticks=[0, 1], labels=['Male', 'Female'])\nplt.tight_layout()\nplt.savefig('sex_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='yy', y='percentage', data=birth_year_dist_py, palette='viridis')\nplt.title('Birth Year Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('birth_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nmerged_df['area_name'] = merged_df['area'].map(area_mapping_py)\narea_name_dist_py = merged_df['area_name'].value_counts(dropna=False).rename_axis('area_name').reset_index(name='n')\narea_name_dist_py['percentage'] = (area_name_dist_py['n'] / area_name_dist_py['n'].sum()) * 100\narea_name_dist_py = area_name_dist_py.sort_values(by='percentage', ascending=False).reset_index(drop=True)\nsns.barplot(x='area_name', y='percentage', data=area_name_dist_py, palette='colorblind')\nplt.title('Regional Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('area_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='status_category', y='percentage', data=status_dist_py, palette='Set2')\nplt.title('Dependent Variable Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.tight_layout()\nplt.savefig('status_dis.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "title": "preprocessing",
    "section": "데이터 전처리 및 train test split",
    "text": "데이터 전처리 및 train test split\n\nimport re\nfrom sklearn.model_selection import train_test_split\n\npred_vars = [col for col in merged_df.columns if re.search(r\"_w[1-5]$\", col)]\nmerged_df[OUTCOME_VAR] = merged_df[OUTCOME_VAR].astype('category')\n\nX = merged_df[pred_vars]\ny = merged_df[OUTCOME_VAR]\ncomposite_stratify_key = y.astype(str) + '_' + \\\n                         merged_df['area_name'].astype(str) + '_' + \\\n                         merged_df['sex'].astype(str)\nweights = merged_df[WEIGHT_VAR]\nX_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, test_size=0.3, random_state=RANDOM_STATE, stratify=composite_stratify_key)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "title": "preprocessing",
    "section": "데이터 저장",
    "text": "데이터 저장\n\ntrain_data_to_save = X_train.copy()\ntrain_data_to_save.insert(0, 'y', y_train)\ntrain_data_to_save.insert(1, 'weights', weights_train)\ntrain_data_to_save.to_csv('_data/train_data.csv', index=False)\ntest_data_to_save = X_test.copy()\ntest_data_to_save.insert(0, 'y', y_test)\ntest_data_to_save.insert(1, 'weights', weights_test)\ntest_data_to_save.to_csv('_data/test_data.csv', index=False)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#데이터-load",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#데이터-load",
    "title": "analysis",
    "section": "데이터 load",
    "text": "데이터 load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nRANDOM_STATE = 54321\nnp.random.seed(RANDOM_STATE)\n\nX_train_df = pd.read_csv(\"_data/train_data.csv\")\nX_test_df = pd.read_csv(\"_data/test_data.csv\")\ny_train = X_train_df['y'].astype('category')\ny_test = X_test_df['y'].astype('category')\nweights_train = X_train_df['weights']\nweights_test = X_test_df['weights']\nX_train = X_train_df.drop(columns=['y', 'weights'])\nX_test = X_test_df.drop(columns=['y', 'weights'])",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "title": "analysis",
    "section": "무작위 분류",
    "text": "무작위 분류\n\nclass_proportions_py = y_train.value_counts(normalize=True)\ntest_categories = list(y_test.cat.categories)\nprop_values_ordered = class_proportions_py.reindex(test_categories, fill_value=0).values\nrandom_predictions = np.random.choice(\n    a=test_categories,\n    size=len(y_test),\n    replace=True,\n    p=prop_values_ordered\n)\nrandom_predictions_cat = pd.Categorical(random_predictions, categories=test_categories, ordered=False)\ncm_random = confusion_matrix(y_test, random_predictions_cat, sample_weight=weights_test, labels=test_categories)\n\nprint(\"\\n=== 무작위 분류기 혼동 행렬 ===\\n\")\ncm_df_random = pd.DataFrame(cm_random, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_random.round(2))\n\naccuracy_random = accuracy_score(y_test, random_predictions_cat, sample_weight=weights_test)\nprecision_random_sk = precision_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_random_sk = recall_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_random_sk = f1_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== 무작위 분류기 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_random:.4f}\")\n\nprint(\"\\n무작위 분류기 범주별 정밀도\")\nprint(pd.Series(precision_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 재현율\")\nprint(pd.Series(recall_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 F1-score\")\nprint(pd.Series(f1_score_random_sk, index=test_categories).round(4))\n\n\n=== 무작위 분류기 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37372.17           44867.14\nActual: stable                     41616.52           49155.97\n\n=== 무작위 분류기 성능 지표 ===\n\n정확도: 0.5001\n\n무작위 분류기 범주별 정밀도\nexplorative    0.4731\nstable         0.5228\ndtype: float64\n\n무작위 분류기 범주별 재현율\nexplorative    0.4544\nstable         0.5415\ndtype: float64\n\n무작위 분류기 범주별 F1-score\nexplorative    0.4636\nstable         0.5320\ndtype: float64",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#random-forest",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#random-forest",
    "title": "analysis",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nbase_rf = RandomForestClassifier(\n    random_state=RANDOM_STATE,\n    oob_score=True,\n    class_weight='balanced_subsample',\n    n_jobs=-1\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'max_features': ['sqrt', 'log2', None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    estimator=base_rf,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nrf_model_py = grid_search.best_estimator_\nprint(f\"OOB Score: {rf_model_py.oob_score_:.4f}\")\n\nimportances = rf_model_py.feature_importances_\nfeature_names = X_train.columns\n\nmax_importance = importances.max()\nscaled_importances = (importances / max_importance) * 100\nforest_importances = pd.Series(scaled_importances, index=feature_names).sort_values(ascending=False)\n\nprint(\"\\n=== Random Forest 중요도 ===\")\ntop_10_importances = forest_importances\ntop_10_df = pd.DataFrame({'Feature': top_10_importances.index, 'Importance': top_10_importances.values})\nprint(top_10_df.round(2))\n\nsns.barplot(x=top_10_importances.values, y=top_10_importances.index)\nplt.title('Feature Importances (Random Forest)', fontsize=10)\nplt.xlabel('Importance Score', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('ran_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_rf = rf_model_py.predict(X_test)\ny_pred_proba_rf = rf_model_py.predict_proba(X_test)\n\ny_pred_rf_cat = pd.Categorical(y_pred_rf, categories=test_categories, ordered=False)\n\ncm_rf = confusion_matrix(y_test, y_pred_rf_cat, sample_weight=weights_test, labels=test_categories)\nprint(\"\\n=== Random Forest 혼동 행렬 ===\\n\")\ncm_df_rf = pd.DataFrame(cm_rf, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_rf.round(2))\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf_cat, sample_weight=weights_test)\nprecision_rf_sk = precision_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_rf_sk = recall_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_rf_sk = f1_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== Random Forest 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_rf:.4f}\")\nprint(\"\\nRandom Forest 범주별 정밀도:\")\nprint(pd.Series(precision_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 재현율:\")\nprint(pd.Series(recall_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 F1-score:\")\nprint(pd.Series(f1_score_rf_sk, index=test_categories).round(4))\n\n\n최적의 하이퍼파라미터: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n최적 교차 검증 점수: 0.6129\nOOB Score: 0.5865\n\n=== Random Forest 중요도 ===\n                        Feature  Importance\n0            self_confidence_w4      100.00\n1              desire_stress_w1       89.19\n2          parent_attachment_w5       83.50\n3          parent_monitoring_w5       83.40\n4              friend_stress_w4       82.51\n5          parent_monitoring_w4       81.66\n6             deviant_esteem_w3       80.23\n7                 neg_esteem_w4       79.71\n8          parent_monitoring_w3       79.27\n9          parent_attachment_w4       78.51\n10  higher_school_dependence_w1       76.60\n11             desire_stress_w4       75.87\n12         parent_attachment_w3       74.15\n13  higher_school_dependence_w2       73.13\n14         parent_monitoring_w1       72.00\n15             friend_stress_w3       71.55\n16           self_confidence_w1       69.76\n17  higher_school_dependence_w4       67.74\n18             desire_stress_w2       67.66\n19            deviant_esteem_w4       66.96\n20           self_confidence_w3       66.83\n21  higher_school_dependence_w5       66.33\n22         parent_monitoring_w2       64.70\n23           self_confidence_w2       64.55\n24             parent_stress_w2       64.32\n25             friend_stress_w2       63.39\n26             parent_stress_w3       63.12\n27             parent_stress_w4       62.80\n28            deviant_esteem_w5       62.53\n29         parent_attachment_w1       62.00\n30             friend_stress_w1       61.78\n31                neg_esteem_w5       61.55\n32           academic_stress_w3       60.54\n33            deviant_esteem_w2       60.54\n34         parent_attachment_w2       60.46\n35  higher_school_dependence_w3       60.34\n36           academic_stress_w2       59.86\n37             parent_stress_w5       59.49\n38            deviant_esteem_w1       59.29\n39                neg_esteem_w3       58.54\n40           academic_stress_w4       58.47\n41           self_confidence_w5       57.98\n42                neg_esteem_w2       57.81\n43           academic_stress_w5       55.73\n44             desire_stress_w5       55.68\n45           academic_stress_w1       55.06\n46             friend_stress_w5       54.55\n47                neg_esteem_w1       52.13\n48             parent_stress_w1       52.07\n49             desire_stress_w3       51.97\n\n\n\n\n\n\n\n\n\n\n=== Random Forest 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37334.03           44905.28\nActual: stable                     27479.79           63292.70\n\n=== Random Forest 성능 지표 ===\n\n정확도: 0.5816\n\nRandom Forest 범주별 정밀도:\nexplorative    0.576\nstable         0.585\ndtype: float64\n\nRandom Forest 범주별 재현율:\nexplorative    0.4540\nstable         0.6973\ndtype: float64\n\nRandom Forest 범주별 F1-score:\nexplorative    0.5078\nstable         0.6362\ndtype: float64",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#xgboost",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#xgboost",
    "title": "analysis",
    "section": "XGBoost",
    "text": "XGBoost\n\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_numeric = le.fit_transform(y_train)\ny_test_numeric = le.transform(y_test)\n\nclass_labels_ordered = list(le.classes_) \nnum_classes = len(class_labels_ordered)\n\nprint(f\"Label Encoder 클래스: {class_labels_ordered} -&gt; {list(range(num_classes))}\")\n\nxgb_model = xgb.XGBClassifier(\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    num_class=num_classes,\n    seed=RANDOM_STATE,\n    use_label_encoder=False,\n    verbosity=0\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'learning_rate': [0.05, 0.1],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train_numeric, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nxgb_model_py = grid_search.best_estimator_\n\nimportance_scores = xgb_model_py.feature_importances_\nxgb_importance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': importance_scores\n})\n\nmax_importance = xgb_importance_df['Importance'].max()\nxgb_importance_df['Importance'] = (xgb_importance_df['Importance'] / max_importance) * 100\nxgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nXGBoost 변수 중요도\")\nprint(xgb_importance_df)\n\nsns.barplot(x='Importance', y='Feature', data=xgb_importance_df)\nplt.title('Feature Importances (XGBoost)', fontsize=10)\nplt.xlabel('Importance', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('xg_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_proba_xgb = xgb_model_py.predict_proba(X_test)\ny_pred_xgb_numeric = np.argmax(y_pred_proba_xgb, axis=1)\ny_pred_xgb = le.inverse_transform(y_pred_xgb_numeric)\ny_pred_xgb_cat = pd.Categorical(y_pred_xgb, categories=class_labels_ordered, ordered=False)\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb_cat, sample_weight=weights_test, labels=class_labels_ordered)\nprint(\"\\n=== XGBoost 모델 혼동 행렬 ===\\n\")\ncm_df_xgb = pd.DataFrame(cm_xgb, index=[f\"Actual: {cat}\" for cat in class_labels_ordered], columns=[f\"Predicted: {cat}\" for cat in class_labels_ordered])\nprint(cm_df_xgb.round(2))\n\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb_cat, sample_weight=weights_test)\nprecision_xgb_sk = precision_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nrecall_xgb_sk = recall_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nf1_score_xgb_sk = f1_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\n\nprint(\"\\n=== XGBoost 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_xgb:.4f}\")\nprint(\"\\nXGBoost 모델 범주별 정밀도:\")\nprint(pd.Series(precision_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 재현율:\")\nprint(pd.Series(recall_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_xgb_sk, index=class_labels_ordered).round(4))\n\nLabel Encoder 클래스: ['explorative', 'stable'] -&gt; [0, 1]\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n최적의 하이퍼파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n최적 교차 검증 점수: nan\n\nXGBoost 변수 중요도\n                        Feature  Importance\n40         parent_attachment_w5  100.000000\n38                neg_esteem_w4   89.684937\n36           self_confidence_w4   87.710533\n33         parent_monitoring_w4   84.155891\n30         parent_attachment_w4   83.655487\n35             friend_stress_w4   83.593765\n23         parent_monitoring_w3   80.530685\n15             friend_stress_w2   80.180794\n12             parent_stress_w2   80.104950\n6            self_confidence_w1   78.258446\n17  higher_school_dependence_w2   77.525246\n21            deviant_esteem_w3   75.736404\n22             parent_stress_w3   74.421280\n4              desire_stress_w1   74.244553\n43         parent_monitoring_w5   74.036301\n14             desire_stress_w2   73.580856\n7   higher_school_dependence_w1   73.197128\n25             friend_stress_w3   72.865379\n47  higher_school_dependence_w5   72.309631\n31            deviant_esteem_w4   71.178528\n39           academic_stress_w4   69.950974\n48                neg_esteem_w5   69.804626\n32             parent_stress_w4   69.397591\n3          parent_monitoring_w1   68.932785\n46           self_confidence_w5   68.488029\n45             friend_stress_w5   68.380692\n26           self_confidence_w3   67.708191\n27  higher_school_dependence_w3   66.273537\n19           academic_stress_w2   65.726303\n41            deviant_esteem_w5   65.552612\n20         parent_attachment_w3   65.515610\n11            deviant_esteem_w2   65.489426\n9            academic_stress_w1   64.981789\n34             desire_stress_w4   63.373531\n49           academic_stress_w5   63.285591\n42             parent_stress_w5   62.527847\n13         parent_monitoring_w2   62.383938\n24             desire_stress_w3   62.092834\n18                neg_esteem_w2   61.555267\n29           academic_stress_w3   60.970982\n37  higher_school_dependence_w4   60.602821\n44             desire_stress_w5   59.860622\n5              friend_stress_w1   59.424625\n0          parent_attachment_w1   55.924370\n10         parent_attachment_w2   55.653919\n8                 neg_esteem_w1   52.370869\n28                neg_esteem_w3   50.224579\n2              parent_stress_w1   50.090385\n1             deviant_esteem_w1   49.659233\n16           self_confidence_w2   49.203815\n\n\n\n\n\n\n\n\n\n\n=== XGBoost 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                40510.53           41728.78\nActual: stable                     25198.53           65573.96\n\n=== XGBoost 모델 성능 지표 ===\n\n정확도: 0.6132\n\nXGBoost 모델 범주별 정밀도:\nexplorative    0.6165\nstable         0.6111\ndtype: float64\n\nXGBoost 모델 범주별 재현율:\nexplorative    0.4926\nstable         0.7224\ndtype: float64\n\nXGBoost 모델 범주별 F1-score:\nexplorative    0.5476\nstable         0.6621\ndtype: float64",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "title": "analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\n\nbase_log_reg = LogisticRegression(\n    solver='lbfgs',\n    max_iter=5000,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nparam_grid = [\n    {\n        'penalty': ['l1'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['l2'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['lbfgs', 'liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['elasticnet'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['saga'],\n        'l1_ratio': [0.2, 0.5, 0.8],\n        'class_weight': ['balanced', None]\n    }\n]\n\ngrid_search = GridSearchCV(\n    estimator=base_log_reg,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nlog_reg_model_py = grid_search.best_estimator_\nclass_labels_logreg = log_reg_model_py.classes_\ncoef_series = pd.Series(log_reg_model_py.coef_[0], index=X_train.columns)\nsorted_coefs = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\ny_train_binary = (y_train == class_labels_logreg[1]).astype(int)\n\nlogit_model = sm.Logit(y_train_binary, X_train_sm)\nlogit_result = logit_model.fit(disp=0)\n\ncoef_summary = logit_result.summary2().tables[1]\ncoef_summary_df = pd.DataFrame(coef_summary)\ncoef_summary_sorted = coef_summary_df.sort_values('Coef.', key=abs, ascending=False)\nprint(\"\\n계수 및 p-value (절댓값이 큰 순서):\")\nprint(coef_summary_sorted[['Coef.', 'P&gt;|z|']])\n\ny_pred_logistic_py = log_reg_model_py.predict(X_test)\n\ntest_categories_logreg = list(y_test.cat.categories) if hasattr(y_test, 'cat') else sorted(list(y_test.unique()))\ny_pred_logistic_cat = pd.Categorical(y_pred_logistic_py, categories=test_categories_logreg, ordered=False)\n\ncm_logistic = confusion_matrix(y_test, y_pred_logistic_cat, sample_weight=weights_test, labels=test_categories_logreg)\nprint(\"\\n=== 로지스틱 회귀 모델 혼동 행렬 ===\\n\")\ncm_df_logistic = pd.DataFrame(cm_logistic, index=[f\"Actual: {cat}\" for cat in test_categories_logreg], columns=[f\"Predicted: {cat}\" for cat in test_categories_logreg])\nprint(cm_df_logistic.round(2))\n\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic_cat, sample_weight=weights_test)\nprecision_logistic_sk = precision_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nrecall_logistic_sk = recall_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nf1_score_logistic_sk = f1_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\n\nprint(\"\\n=== 로지스틱 회귀 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_logistic:.4f}\")\nprint(\"\\n로지스틱 회귀 모델 범주별 정밀도:\")\nprint(pd.Series(precision_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 재현율:\")\nprint(pd.Series(recall_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_logistic_sk, index=test_categories_logreg).round(4))\n\n\n최적의 하이퍼파라미터: {'C': 0.1, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n최적 교차 검증 점수: 0.6006\n\n계수 및 p-value (절댓값이 큰 순서):\n                                Coef.     P&gt;|z|\ndesire_stress_w1             0.499608  0.000015\nself_confidence_w4          -0.332916  0.010294\nparent_stress_w3             0.271418  0.057369\nneg_esteem_w4                0.263964  0.031803\ndesire_stress_w2             0.255400  0.018759\nself_confidence_w3          -0.254064  0.030035\nfriend_stress_w4            -0.247249  0.024386\nconst                        0.245302  0.000002\ndeviant_esteem_w3           -0.237610  0.068700\nparent_monitoring_w3         0.211999  0.050412\nparent_attachment_w3         0.184476  0.274346\nparent_monitoring_w4         0.180603  0.106346\nhigher_school_dependence_w5 -0.164889  0.024659\nacademic_stress_w2          -0.156225  0.223868\nparent_attachment_w5         0.151586  0.345721\ndesire_stress_w3            -0.149660  0.181854\nself_confidence_w2          -0.140414  0.185298\ndeviant_esteem_w1           -0.138070  0.263940\nparent_stress_w4            -0.137391  0.383677\nfriend_stress_w1            -0.136979  0.134193\nacademic_stress_w5           0.118084  0.270881\nparent_monitoring_w1         0.113677  0.264741\nacademic_stress_w1          -0.112715  0.410824\ndesire_stress_w4             0.110917  0.295629\nself_confidence_w1           0.109299  0.356000\nparent_stress_w1            -0.104439  0.436255\nparent_stress_w2            -0.101390  0.460526\nparent_monitoring_w2        -0.098915  0.363357\ndesire_stress_w5             0.096226  0.359037\nneg_esteem_w1                0.096029  0.462255\nacademic_stress_w4           0.091028  0.451824\nhigher_school_dependence_w2 -0.086012  0.175999\nhigher_school_dependence_w1 -0.082377  0.182057\nparent_attachment_w4        -0.078615  0.648722\ndeviant_esteem_w4           -0.077995  0.525561\nneg_esteem_w5               -0.077245  0.532354\nparent_monitoring_w5         0.075278  0.495239\nfriend_stress_w5            -0.073577  0.475849\nfriend_stress_w2            -0.069986  0.466378\nhigher_school_dependence_w4 -0.059155  0.445485\nparent_stress_w5            -0.053443  0.710032\nneg_esteem_w3               -0.045219  0.687428\nparent_attachment_w1        -0.035101  0.822711\nhigher_school_dependence_w3  0.033887  0.623319\ndeviant_esteem_w5           -0.024602  0.843547\nneg_esteem_w2                0.021549  0.824973\nacademic_stress_w3          -0.017071  0.879205\nself_confidence_w5           0.015841  0.893445\nfriend_stress_w3            -0.010240  0.924892\nparent_attachment_w2         0.006187  0.967933\ndeviant_esteem_w2            0.002349  0.982333\n\n=== 로지스틱 회귀 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                42190.16           40049.15\nActual: stable                     26537.42           64235.07\n\n=== 로지스틱 회귀 모델 성능 지표 ===\n\n정확도: 0.6151\n\n로지스틱 회귀 모델 범주별 정밀도:\nexplorative    0.6139\nstable         0.6160\ndtype: float64\n\n로지스틱 회귀 모델 범주별 재현율:\nexplorative    0.5130\nstable         0.7076\ndtype: float64\n\n로지스틱 회귀 모델 범주별 F1-score:\nexplorative    0.5589\nstable         0.6586\ndtype: float64",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "title": "analysis",
    "section": "모델 성능 비교",
    "text": "모델 성능 비교\n\nimport matplotlib.cm as cm\n\nmodel_metrics_list = []\nclass_labels_ordered = list(y_test.cat.categories)\n\ndef compile_metrics(model_name, accuracy, precision_arr, recall_arr, f1_arr):\n    metrics = {'Model': model_name, 'Accuracy': accuracy}\n    for i, label in enumerate(class_labels_ordered):\n        metrics[f'Precision ({label})'] = precision_arr[i]\n        metrics[f'Recall ({label})'] = recall_arr[i]\n        metrics[f'F1-score ({label})'] = f1_arr[i]\n    return metrics\n\nmodel_metrics_list.append(compile_metrics(\n    \"Random Classifier\",\n    accuracy_random,\n    precision_random_sk,\n    recall_random_sk,\n    f1_score_random_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Random Forest\",\n    accuracy_rf,\n    precision_rf_sk,\n    recall_rf_sk,\n    f1_score_rf_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"XGBoost\",\n    accuracy_xgb,\n    precision_xgb_sk,\n    recall_xgb_sk,\n    f1_score_xgb_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Logistic Regression\",\n    accuracy_logistic,\n    precision_logistic_sk,\n    recall_logistic_sk,\n    f1_score_logistic_sk\n))\n\ncomparison_df = pd.DataFrame(model_metrics_list).set_index('Model')\nmodels = comparison_df.index.tolist()\nall_categories = ['Accuracy']\nfor label in class_labels_ordered:\n    all_categories.extend([\n        f'Precision ({label})',\n        f'Recall ({label})',\n        f'F1-score ({label})'\n    ])\nangles = np.linspace(0, 2*np.pi, len(all_categories), endpoint=False).tolist()\nangles += angles[:1]\nax = plt.subplot(111, polar=True)\ncolors = cm.tab10(np.linspace(0, 1, len(models)))\nfor i, model in enumerate(models):\n    values = comparison_df.loc[model, all_categories].values.flatten().tolist()\n    values += values[:1]\n    ax.plot(angles, values, 'o-', linewidth=2, color=colors[i], label=model, alpha=0.8)\n    ax.fill(angles, values, color=colors[i], alpha=0.1)\nax.set_xticks(angles[:-1])\nax.set_xticklabels(all_categories, fontsize=10)\n\nax.set_ylim(0, 1)\nax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\nax.grid(True, linestyle='-', alpha=0.3)\nplt.title('모델 성능 비교', size=15, y=1.1)\nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\nplt.savefig('model_met.png', dpi=300, bbox_inches='tight')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/15.html#roc-커브",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/15.html#roc-커브",
    "title": "analysis",
    "section": "ROC 커브",
    "text": "ROC 커브\n\nrandom_pred_proba = np.zeros((len(y_test), len(class_labels_ordered)))\nfor i, cls in enumerate(class_labels_ordered):\n    random_pred_proba[:, i] = prop_values_ordered[i]\n\npred_probas = {\n    \"Random Classifier\": random_pred_proba,\n    \"Random Forest\": y_pred_proba_rf,\n    \"XGBoost\": y_pred_proba_xgb,\n    \"Logistic Regression\": log_reg_model_py.predict_proba(X_test)\n}\n\ncolors = {\n    \"Random Classifier\": \"grey\",\n    \"Random Forest\": \"forestgreen\",\n    \"XGBoost\": \"darkorange\",\n    \"Logistic Regression\": \"navy\"\n}\n\ny_test_numeric = y_test.cat.codes if hasattr(y_test, 'cat') else y_test\n\nfor model_name, proba in pred_probas.items():\n    if proba.shape[1] &gt; 1:\n        y_score = proba[:, 1]\n    else:\n        y_score = proba.ravel()\n    fpr, tpr, _ = roc_curve(y_test_numeric, y_score, sample_weight=weights_test)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})', color=colors[model_name])\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC 커브', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.gcf().set_size_inches(7, 7)\nplt.tight_layout()\nplt.savefig('roc_curve_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html",
    "title": "classification with trees",
    "section": "",
    "text": "gini 지수가 낮을수록, misclassification error가 낮을수록, entropy가 낮을수록 Information gain이 높을수록, gain ratio가 클수록 순도가 높고 좋다. 최댓값은 0.5. entropy는 최댓값 1.\n과적합 pruning 할 때 misclassification error를 기준으로 한다.\n의사결정 트리는 데이터 마이닝에서 가장 널리 사용되는 분류 기법 중 하나로, 데이터의 패턴을 트리 구조로 표현하여 예측 모델을 구축한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "title": "classification with trees",
    "section": "What is Data minig",
    "text": "What is Data minig\n\n데이터 마이닝은 대량의 데이터에서 암시적이고 이전에 알려지지 않았던 잠재적으로 유용한 지식이나 패턴을 추출하는 과정입니다.\n\n\n종류\n\n지도 학습1: 주어진 학습 데이터를 이용하여 목표 속성의 값을 예측하는 모델을 생성하는 과정으로, 입력(속성)과 출력(정답)이 모두 주어진 데이터를 바탕으로 학습\n비지도 학습",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#분류",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#분류",
    "title": "classification with trees",
    "section": "분류",
    "text": "분류\n\n목표\n\n새로운 데이터에 대해서도 정확한 예측이 가능한 일반화된 모델을 만드는 것\n이를 위해 과거 데이터를 학습용과 테스트용으로 나누어 모델의 성능을 검증",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "title": "classification with trees",
    "section": "의사결정 트리의 구조와 원리",
    "text": "의사결정 트리의 구조와 원리\n\n의사결정 트리는 노드와 가지로 구성된 계층적 구조로, 각 노드는 특성(attribute)을 나타내며 가지는 테스트 결과를 표현.\n잎 노드는 클래스 레이블이나 클래스 분포를 나타냄\n의사결정 트리 구축은 주로 탐욕적 전략(Greedy strategy)을 사용하며, 각 단계에서 가장 좋은 분할 기준을 선택함.\n대표적인 의사결정 트리 알고리즘으로는 CART, ID3, C4.5, SLIQ, SPRINT 등이 있다.\n\n\n노드 불순도 측정 방법\n의사결정 트리에서 최적의 분할을 결정하기 위해 다양한 불순도 측정 방법이 사용됩니다:\n\nGini Index: 노드의 불순도를 측정하는 방법으로, 1-∑[p(j|t)]²로 계산됩니다. 값이 0에 가까울수록 순수한 노드를 의미합니다.\nEntropy(엔트로피): 노드의 동질성을 측정하는 방법으로, -∑p(j|t)log₂p(j|t)로 계산됩니다. 0일 때 완전히 동질적인 노드를 의미합니다.\nInformation Gain(정보 이득): 분할 전후의 엔트로피 차이로, 분할로 인해 얻어지는 불확실성 감소량을 의미합니다. 높은 정보 이득은 해당 속성이 데이터를 잘 나누는 것을 의미합니다.\nGain Ratio(이득 비율): 정보 이득을 분할의 내재 정보량(Split Information)으로 나눈 값으로, 분기가 많은 속성에 대한 편향을 줄이기 위해 고안되었습니다.\n\n\n\n트리 분할 기준\n트리 분할 시 고려해야 할 주요 이슈는 다음과 같습니다: - 데이터 분할 방법 선택 - 속성의 테스트 조건 명시 - 최고의 분할 정의 - 트리 분기 종료 시점 결정\n최적의 분할은 불순도를 최소화하는 방향으로 이루어지며, CART는 Gini 기반 분할을, ID3와 C4.5는 Information Gain 기반 분할을 주로 사용합니다.\n\n\n모델 평가 기준\n의사결정 트리 모델의 평가는 다음과 같은 기준으로 이루어집니다: - 테스트 세트에서의 정확도(%) - 오류율 - 혼동 행렬(Confusion Matrix) - 속도와 확장성 - 노이즈와 결측값 처리 능력",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#결론",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#결론",
    "title": "classification with trees",
    "section": "결론",
    "text": "결론\n의사결정 트리는 직관적이고 이해하기 쉬운 분류 모델을 제공하지만, 과적합(overfitting)이나 데이터 단편화와 같은 문제가 발생할 수 있습니다. 이를 해결하기 위해 C4.5와 같은 알고리즘은 Gain Ratio를 도입하여 분기가 많은 속성에 대한 편향을 줄이는 방법을 제시했습니다.\n의사결정 트리의 성공적인 구축을 위해서는 적절한 불순도 측정 방법 선택, 가지치기(pruning), 그리고 다양한 속성 선택 기준의 이해가 필요합니다. 이러한 방법들을 통해 보다 정확하고 일반화된 모델을 구축할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/13.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/13.html#footnotes",
    "title": "classification with trees",
    "section": "각주",
    "text": "각주\n\n\n규칙 기반 시스템 != 연관 규칙 학습↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/10.html#train-test-split",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/10.html#train-test-split",
    "title": "Data Transformation",
    "section": "train test split",
    "text": "train test split\n\nHold out method: sub sampling\n\n\nvalidation data set: 하이퍼파라미터를 튜닝\n층화추출법: stratified sampling\n\n\nresampling method: Hold out을 여러번 반복. variance 극복 bias 극복 x\n\ncross validation\n\n\nk-fold\nleave-one-out:\n\n\nbootstrap",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Data Transformation"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/10.html#data의-특성",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/10.html#data의-특성",
    "title": "Data Transformation",
    "section": "data의 특성",
    "text": "data의 특성\nvolume - Data size big - Data size small(long data) velocity variety veracity(정확성) value\n\nimport numpy as np\nfrom sklearn.model_selection import LeaveOneOut\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array(['a', 'b', 'c'])\nloo = LeaveOneOut()\nloo.get_n_splits(X)\n\n3\n\n\n\nfrom sklearn.model_selection import KFold\n\nX = np.array([[10, 20], [30, 40], [15, 19], [34, 41], [11, 21], [33, 39]])\ny = np.array([0, 1, 0, 1, 0, 1])\nkf = KFold(n_splits=3)\n\nkf.get_n_splits(X)\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nTRAIN: [2 3 4 5] TEST: [0 1]\nTRAIN: [0 1 4 5] TEST: [2 3]\nTRAIN: [0 1 2 3] TEST: [4 5]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Data Transformation"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/11.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/11.html",
    "title": "Homework - 2",
    "section": "",
    "text": "import pandas as pd\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv(\"./_data/class/FFvote.csv\", encoding='utf-8')\ndataset.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngender_female\ngender_male\nregion_Chuncheung\nregion_Honam\nregion_Sudo\nregion_Youngnam\nregion_others\nedu\nincome\nage\nscore_gov\nscore_progress\nscore_intention\nvote\nparties\n\n\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1.0\n0.666667\n0.666667\n0.25\n0.25\n0.75\n1\n2\n\n\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0.5\n0.666667\n0.666667\n0.25\n0.75\n0.50\n0\n3\n\n\n2\n2\n0\n1\n0\n1\n0\n0\n0\n0.0\n0.333333\n1.000000\n0.00\n0.50\n0.45\n1\n4\n\n\n3\n3\n1\n0\n0\n0\n1\n0\n0\n0.5\n0.000000\n0.666667\n1.00\n0.75\n0.40\n1\n1\n\n\n4\n4\n0\n1\n0\n0\n1\n0\n0\n0.0\n0.333333\n1.000000\n0.75\n0.50\n0.35\n1\n1\n\n\n\n\n\n\n\n\nX = dataset.loc[:, 'gender_female':'score_intention'].values\ny = dataset['vote'].values\n\n어떻게 하면 scaling을 간단하게 처리할 수 있을까 고민하던 중 sklearn pipeline 문서의 Safety 부분을 참고해서 작성해봤습니다.\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n      StandardScaler(),\n      KNeighborsClassifier(n_neighbors=42)\n)\n\nfor k in (3, 4, 5):\n      kf = KFold(n_splits=k, shuffle=False)\n      accuracies = cross_val_score(pipeline, X, y, cv=kf, scoring='accuracy')\n      print(f\"Accuracy for K={k}: {round(accuracies.mean(), 2)}\")\n\nAccuracy for K=3: 0.71\nAccuracy for K=4: 0.71\nAccuracy for K=5: 0.72\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Homework - 2"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#business-analytics",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#business-analytics",
    "title": "Intro",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nData Analysis\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\n\nOperations Research\n\nPrescriptive Analytics: What should we do? (Optimization)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#process-of-or-study",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#process-of-or-study",
    "title": "Intro",
    "section": "Process of OR Study",
    "text": "Process of OR Study\n\n\n\n\n\nflowchart LR\n  A(Collect data) --&gt; B(Define the problem)\n  B --&gt; C{Data are sufficient?}\n  C --&gt;|No| A\n  C --&gt;|Yes| D(Formulate a model)\n  D --&gt; E(Solve the model)\n  E --&gt; F{Model is good?}\n  F --&gt;|Yes| G(Interpret results make suggestions)\n  F --&gt;|No| D",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "title": "Intro",
    "section": "LP Model (표준형)",
    "text": "LP Model (표준형)\n\n제한된 자원을 경쟁하는 활동들에게 가능한 최적으로 분배하거나 이와 비슷한 수학적 구조를 가진 문제를 다루는 방법\n\n\\[\\begin{aligned}\nmax & \\sum_{i=1}^{n} c_i x_i \\\\\ns.t. & \\sum_{i=1}^{n} a_{ij} x_i \\leq b_j, j ≤ m \\\\\n& x_1, x_2, ..., x_n ≥ 0\n\\end{aligned}\\]",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/02.html#lp의-가정",
    "href": "posts/01_projects/bs_3_1/notes/OR/02.html#lp의-가정",
    "title": "Intro",
    "section": "LP의 가정",
    "text": "LP의 가정\n\n선형계획은 현실을 단순화한 모델로, 아래의 네 가지 가정이 완벽히 맞지 않을 수 있음.\n작은 불일치는 허용 가능하며, 민감도 분석으로 보완.\n심각한 위반 시 대안 모델(비선형계획, 정수계획 등)을 사용하나, 선형계획의 강력한 알고리즘이 유리하므로 초기 분석에 활용 후 필요 시 복잡한 모델로 전환.\n\n\n비례성(Proportionality)\n\n정의: 목적함수와 제약식에서 활동 수준(예: xx)에 대한 기여도가 선형(비례적)으로 표현됨.\n위반 사례:\n\n초기 투자비용(고정비용)이 있어 \\(Z=3x_1−1\\)이 되는 경우, 비례성이 깨짐.\n규모의 경제로 한계 이익이 증가하면 비례성이 위반됨.\n한계 이익이 감소(예: 마케팅 비용 증가)하면 역시 비례성이 깨짐.\n\n대안: 비례성이 깨지면 비선형계획(12장)이나 혼합정수계획(11장)을 고려.\n\n가합성(Additivity)\n\n정의: 목적함수와 제약식의 값이 각 활동의 개별 기여도의 합으로 표현됨. 즉, 변수 간 교차곱이 없음.\n위반 사례:\n\n제품 간 보완적 상호작용(예: 공동 광고 효과)으로 \\(Z=3x_1+5x_2+x_1x_2\\)가 됨.\n경쟁적 상호작용(예: 설비 공유로 비효율 발생)으로 \\(Z=3x_1+5x_2−x_1x_2\\)가 됨.\n\n대안: 가합성이 위반되면 비선형계획(12장)으로 전환.\n\n가분성(Divisibility)\n\n정의: 의사결정 변수가 실수 값을 가질 수 있음. 즉, 활동 수준이 정수로 제한되지 않음.\n위반 사례: 변수가 정수로 제한되면(예: 배치 단위가 1, 2, 3만 가능) 가분성이 깨짐.\n대안: 정수계획(11장) 사용.\n\n확실성(Certainty)\n\n정의: 모델의 매개변수(예: \\(c_j, a_{ij}, b_i\\))가 알려진 상수로 고정. 해당 상수는 미래 예측에 기반하므로 불확실성이 존재.\n대응: 불확실성이 크면 민감도 분석(6.7절)으로 최적해의 변화를 확인하거나, 확률변수를 도입한 모델(23장) 사용.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "href": "posts/01_projects/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "쌍대 심플렉스 방법",
    "text": "쌍대 심플렉스 방법\n\n쌍대 문제 제약식에 -1 하고 slack 변수 추가해서 반대로 품.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/10.html#상한-기법",
    "href": "posts/01_projects/bs_3_1/notes/OR/10.html#상한-기법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "상한 기법",
    "text": "상한 기법\n\n일단은 대수적으로 푸는 방법만 배움.\n변수가 0일 때 뿐만 아니라 upper bound일 때도 non-basic variable로 취급.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#overview",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#overview",
    "title": "Simplex Method (part 5)",
    "section": "Overview",
    "text": "Overview\n\n심플렉스 방법의 기하학적·대수적 원리, 행렬형 알고리즘, 그리고 그 실용적 응용(민감도 분석 등)을 체계적으로 설명\n심플렉스 방법은 선형계획 문제에서 최적해를 꼭짓점 가능해(CPF)에서 찾으며, 행렬 연산을 통해 컴퓨터로 효율적으로 구현할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "title": "Simplex Method (part 5)",
    "section": "Simplex 방법의 기초",
    "text": "Simplex 방법의 기초\n\n꼭짓점 가능해와 제약식 경계\n\n선형계획 문제의 해는 가능해 영역(feasible region)의 경계에 존재한다.\n이 제약식들을 등호(=)로 바꾼 제약식 경계식을 만들 수 있다.\n\n함수 제약식\n\n≤: slack variable\n=: artificial variable\n≥: surplus variable, artificial variable\n\n비음 제약식\n\nnon-restricted: \\(x_i = x_i^{+} - x_i^{-}\\)\n≤ 0: \\(x_i = -x_i^{+}\\)\n\n\n이 경계식들은 2차원에서는 선, 3차원에서는 평면, n차원에서는 초평면(hyperplane)을 형성한다.\n\n\n\n꼭짓점 가능해의 세 가지 주요 속성\n\n최적해의 위치\n\n최적해가 유일하면, 그것은 꼭짓점 가능해이다.\n최적해가 여러 개라면, 그 중 적어도 두 개는 인접 꼭짓점 가능해이다.\n즉, 최적해는 항상 꼭짓점 가능해(혹은 그 선분)에 존재한다.\n\n유한성\n\n꼭짓점 가능해의 개수는 유한합니다.\nm+n개의 제약식 중 n개를 선택하는 조합의 수는 유한하므로, 이론적으로 모든 꼭짓점 가능해를 열거해 비교할 수도 있습니다. 하지만 실제로는 심플렉스 방법이 훨씬 적은 수만 탐색한다.\n\n최적성의 충분조건\n\n인접 꼭짓점 중 더 좋은 해가 없으면, 현재 해가 최적해임이 보장된다.\n\n\n\n\n심플렉스 방법의 핵심 알고리즘 구조\n심플렉스 방법은 다음과 같은 반복 구조를 가진다\n\n초기 꼭짓점 가능해(기저해) 선택\n인접 꼭짓점으로 이동(목적함수 값이 개선되는 방향)\n더 이상 개선이 불가능하면 종료, 그 해가 최적해임을 보장",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "title": "Simplex Method (part 5)",
    "section": "행렬형의 Simplex",
    "text": "행렬형의 Simplex\n\n행렬형 심플렉스 방법의 기본 구조\n표준형 선형계획 문제를 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\text{Maximize} \\quad &Z=c^Tx \\\\\n\\text{Subject to} \\quad &Ax=b, x≥0 \\\\\n\\end{aligned}\n\\]\n\n여기서 A는 m×n 행렬, x는 n차원 변수 벡터, b는 m차원 상수 벡터, c는 n차원 계수 벡터이다.\n여유변수(slack variable)등을 도입해 모든 제약식을 등식으로 바꾼다.\n\n\n\n행렬 연산을 활용한 반복 과정\n\n(제일 처음 단계의 경우 3, 4단계 먼저 진행)\n\n\n진입기저변수(Entering Variable) 선택\n탈락기저변수(Leaving Variable) 선택\n\n최소비율법(minimum ratio test) 사용\n\n새로운 기저 가능해 결정\n\n기저변수 식별\n기저행렬(Basic Matrix, B): m개의 기저변수에 대해 m×m 행렬 \\(B\\)와 \\(B^{-1}\\)를 만든다. 1\n기저해(Basic Solution) 계산: \\(x_B=B^{-1}b\\)\n목적함수 값 계산: \\(Z=c_B^TB^{−1}b\\)\n\n최적화 검사\n\n비기저변수의 계수(감소계수, reduced cost)를 계산\n\n계산식: \\(c_B^TB^{−1}a_n - c_n\\)\nslack 변수: \\(c_B^TB^{-1}\\)\n\n최적일 경우 종료",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/04.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/OR/04.html#footnotes",
    "title": "Simplex Method (part 5)",
    "section": "각주",
    "text": "각주\n\n\n역행렬 구하는 법. 2차원 말고는 그냥 그 방식으로 풀자.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#general",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#general",
    "title": "Simplex 표 계산",
    "section": "General",
    "text": "General\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\n\n# Convert all elements to Fraction\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 설정\nobj = [5, -5, -13, 0, 0, -10, 0]\nA = [\n    [-1, 1, 3, 1, 0, 3, 20],\n    [12, 4, 10, 0, 1, 5, 90],\n]\nbasic = np.array([4, 5]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3]) - 1  # x1, x2, x3, x4\n\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        min_rc_idx = None\n        min_rc = Fraction(0)\n        for j in range(len(obj) - 1):  # RHS 제외\n            if obj[j] &lt; min_rc:\n                min_rc = obj[j]\n                min_rc_idx = j\n        \n        # 종료 조건: 음수 계수가 없으면 최적\n        if min_rc_idx is None or min_rc &gt;= 0:\n            print(\"Optimal solution reached.\")\n            solution = {f\"x{i+1}\": Fraction(0) for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((float('inf'), i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == float('inf'):\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [x / pivot for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [A[i][j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [obj[j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    5 |   -5 |  -13 |    0 |    0 |  -10 |     0 |\n+----+-----+------+------+------+------+------+------+-------+\n| x4 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   12 |    4 |   10 |    0 |    1 |    5 |    90 |\n+----+-----+------+------+------+------+------+------+-------+\nEntering variable: x3\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+-------+------+------+-------+\n|    |   Z | x1   | x2   |   x3 | x4    |   x5 |   x6 | RHS   |\n+====+=====+======+======+======+=======+======+======+=======+\n| Z  |   1 | 2/3  | -2/3 |    0 | 13/3  |    0 |    3 | 260/3 |\n+----+-----+------+------+------+-------+------+------+-------+\n| x3 |   0 | -1/3 | 1/3  |    1 | 1/3   |    0 |    1 | 20/3  |\n+----+-----+------+------+------+-------+------+------+-------+\n| x5 |   0 | 46/3 | 2/3  |    0 | -10/3 |    1 |   -5 | 70/3  |\n+----+-----+------+------+------+-------+------+------+-------+\nEntering variable: x2\nLeaving variable: x3\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    0 |    0 |    2 |    5 |    0 |    5 |   100 |\n+----+-----+------+------+------+------+------+------+-------+\n| x2 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   16 |    0 |   -2 |   -4 |    1 |   -7 |    10 |\n+----+-----+------+------+------+------+------+------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 0\nx2 = 20\nx3 = 0\nx4 = 0\nx5 = 10\nx6 = 0\nObjective Value = 100",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#big-m-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#big-m-method",
    "title": "Simplex 표 계산",
    "section": "Big M Method",
    "text": "Big M Method\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\nfrom sympy import symbols, simplify, oo\n\nM = symbols('M')\n\n# 초기 설정\nobj = [20, 10, 0, M, 0, M, 0]\nA = [\n    [5, 1, -1, 1, 0, 0, 6],\n    [2, 2, 0, 0, -1, 1, 8],\n]\nbasic = np.array([4, 6]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3, 5]) - 1  # x1, x2, x3, x4\n\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef adjust_obj_for_big_m():\n    global obj\n    print(\"\\nAdjusting Objective Function for Big M Method\")\n    for i in range(len(basic)):\n        basic_var_idx = basic[i]\n        if obj[basic_var_idx] != 0:  # 인공변수일 경우(M이 포함된 경우)\n            factor = obj[basic_var_idx]\n            obj = [simplify(obj[j] - factor * A[i][j]) for j in range(len(obj))]\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    adjust_obj_for_big_m()\n    print_table()\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        eval_obj = [x.evalf(subs={M: 1e6}) if x.has(M) else float(x) for x in obj[:-1]]\n        min_rc_idx = min(range(len(obj)-1), key=lambda j: eval_obj[j])\n        \n        if eval_obj[min_rc_idx] &gt;= 0:\n            print(\"Optimal solution reached.\")\n            # 최적 해 출력\n            solution = {f\"x{i+1}\": 0 for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((oo, i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == oo:\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [simplify(x / pivot) for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [simplify(A[i][j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [simplify(obj[j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nAdjusting Objective Function for Big M Method\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\nEntering variable: x1\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n|    |   Z |   x1 | x2        | x3        | x4        | x5   |   x6 | RHS          |\n+====+=====+======+===========+===========+===========+======+======+==============+\n| Z  |   1 |    0 | 6 - 8*M/5 | 4 - 2*M/5 | 7*M/5 - 4 | M    |    0 | -28*M/5 - 24 |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x1 |   0 |    1 | 1/5       | -1/5      | 1/5       | 0    |    0 | 6/5          |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x6 |   0 |    0 | 8/5       | 2/5       | -2/5      | -1   |    1 | 28/5         |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\nEntering variable: x2\nLeaving variable: x6\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+---------+------+----------+-------+\n|    |   Z |   x1 |   x2 | x3   | x4      | x5   | x6       | RHS   |\n+====+=====+======+======+======+=========+======+==========+=======+\n| Z  |   1 |    0 |    0 | 5/2  | M - 5/2 | 15/4 | M - 15/4 | -45   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x1 |   0 |    1 |    0 | -1/4 | 1/4     | 1/8  | -1/8     | 1/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x2 |   0 |    0 |    1 | 1/4  | -1/4    | -5/8 | 5/8      | 7/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 1/2\nx2 = 7/2\nx3 = 0\nx4 = 0\nx5 = 0\nx6 = 0\nObjective Value = -45",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/01.html#grubi",
    "href": "posts/01_projects/bs_3_1/notes/OR/01.html#grubi",
    "title": "Simplex 표 계산",
    "section": "Grubi",
    "text": "Grubi\n\nfrom gurobipy import *\n\nmodel = Model(\"ex4.4-6\")\nmodel.setParam(GRB.Param.OutputFlag, 0)\n\nxab = model.addVar(vtype=GRB.CONTINUOUS, name=\"xab\")\nxac = model.addVar(vtype=GRB.CONTINUOUS, name=\"xac\")\nxbd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbd\")\nxbe = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbe\")\nxcd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xcd\")\nxce = model.addVar(vtype=GRB.CONTINUOUS, name=\"xce\")\nxde = model.addVar(vtype=GRB.CONTINUOUS, name=\"xde\")\nxdf = model.addVar(vtype=GRB.CONTINUOUS, name=\"xdf\")\nxef = model.addVar(vtype=GRB.CONTINUOUS, name=\"xef\")\nxfa = model.addVar(vtype=GRB.CONTINUOUS, name=\"xfa\")\n\nmodel.setObjective(xfa, GRB.MAXIMIZE)\n\nmodel.addConstr(xab + xac - xfa == 0)\nmodel.addConstr(xbd + xbe - xab == 0)\nmodel.addConstr(xcd + xce - xac == 0)\nmodel.addConstr(xde + xdf - xbd - xcd == 0)\nmodel.addConstr(xef - xbe - xce - xde == 0)\nmodel.addConstr(xfa - xdf - xef == 0)\n\nmodel.addConstr(xab &lt;= 9)\nmodel.addConstr(xac &lt;= 7)\nmodel.addConstr(xbd &lt;= 7)\nmodel.addConstr(xbe &lt;= 2)\nmodel.addConstr(xcd &lt;= 4)\nmodel.addConstr(xce &lt;= 6)\nmodel.addConstr(xde &lt;= 3)\nmodel.addConstr(xdf &lt;= 6)\nmodel.addConstr(xef &lt;= 9)\n\nmodel.optimize()\n\nfor var in model.getVars():\n    print(f\"{var.varName}: {var.x}\")\nprint(\"Obj: \", model.objVal)\n\nRestricted license - for non-production use only - expires 2026-11-23\nxab: 8.0\nxac: 7.0\nxbd: 6.0\nxbe: 2.0\nxcd: 1.0\nxce: 6.0\nxde: 1.0\nxdf: 6.0\nxef: 9.0\nxfa: 15.0\nObj:  15.0",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "href": "posts/01_projects/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "title": "Linear Programming Algorithm",
    "section": "선형 계획을 푸는 알고리즘",
    "text": "선형 계획을 푸는 알고리즘\n\n그래프를 사용하는 방법\n변수가 3개 이하인 경우, 그래프를 그려서 해를 찾을 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/03.html#simplex-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/03.html#simplex-method",
    "title": "Linear Programming Algorithm",
    "section": "Simplex Method",
    "text": "Simplex Method\n\n기하학적 이해\n\n\nInitialization: Collect 1 CFP. 일반적으로 원점을 선택\nOptimality test: find better adj\n\nObj ⋅ (adj - cur) &gt; 0: better\nObj ⋅ (adj - cur) = 0: not changed\nObj ⋅ (adj - cur) &lt; 0: worse\n\n\n\n\n대수적 풀이\n\nbasic solution: 제약식의 변수 중 일부를 기저 변수로 선택하고, 나머지를 0으로 설정하여 얻는 해.\n\n만약 기저변수가 0인 경우, 이를 퇴화라고 부른다.\n\nbasic feasible solution: 모든 변수가 0 이상인 basic solution. 즉, 제약식을 모두 만족하는 해.\n비 기저변수(non basic variable): free variable. 변수의 수 - 방정식의 수 만큼 존재.\n기저 변수(bais variable): pivot variable.\n풀이는 생략\n\n\n\nSimplex Tableau\n\n풀이는 생략\n\n\n\n비 표준형 모델에서의 적용\n\n제약식이 = 인 경우: 인공 변수 추가. 목적 함수에 Big-M 방법을 사용하여 표현.\n\nTableau에서 인공변수의 계수를 0으로 만들어서 진행.\n\n제약식이 ≥ 인 경우: slack 변수랑 surplus variable 추가. surplus variable에 대하여 목적 함수에 Big-M 방법을 사용하여 표현.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/computer/02.html",
    "href": "posts/01_projects/bs_3_1/notes/computer/02.html",
    "title": "컴퓨팅적사고 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "컴퓨팅적사고 발표 ppt"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/11.html#총괄생산계획",
    "href": "posts/01_projects/bs_3_1/notes/product/11.html#총괄생산계획",
    "title": "총괄생산계획",
    "section": "총괄생산계획",
    "text": "총괄생산계획\n\n\n\n중기 범위(6-18개월) 기간에 대한 유사한 제품 묶음 수준에서 수요-공급 균형을 맞추기 위한 러프한 생산 계획\n계획 수립 후 주기적으로 업데이트\n\n\n의사결정\n\n목적: 비용, 인력변동의 최소화, 이윤의 최대화, 바람직한 고객 서비스 수준 유지\n\n보통 trade-off 관계\n\n결정 사항: 고용 수준, 시간당 생산량, 재고수준, 외주 생산량 등\n\n\n\n\n전략\n\n수요대안\n\n가격책정\n판촉\n백오더(납기지연)\n신규수요 창출\n\n공급대안\n\n수요추종전략\n\n생산량을 수요에 일치되도록 조정하는 전략. 재고가 안쌓임\n재고 유지비용이 높고, 생산용량 변경에 따른 비용이 적을 때 효과적\n\n생산평준화전략\n\n생산량을 평균으로 일정하게 유지하는 전략\n안정적 산출량\n재고비용 증가, 납기 지연이나 품절에 따른 고객 서비스 저하\n\n혼합전략",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/00.html",
    "href": "posts/01_projects/bs_3_1/notes/product/00.html",
    "title": "Intro",
    "section": "",
    "text": "과목 목표: 전통적인 생산 시스템 관리 방법론을 학습\n\n생산: 유, 무형의 제품을 만드는 것\n\n제조업, 서비스업 등\n\n시스템: 투입물을 산출물로 만드는 구성 요소와 프로세스의 총체적 집합\n관리: 목표를 달성하기 위한 체계적인 의사결정\n\n목표: 비용, 품질, 납품, 유연성\n\n네 가지 모두 고려해야하고, 이 사이에는 trade-off가 존재.\n\n\n\n중간 시험범위: lec2 ~ lec9\n\n질문:\n\n\n재고 유지의 다섯 가지 이유 - 수송중 vs 안전\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "href": "posts/01_projects/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "title": "기준생산계획 및 자재소요계획",
    "section": "기준생산계획(Master Production Schedule, MPS)",
    "text": "기준생산계획(Master Production Schedule, MPS)\n\n총괄생산계획을 분해한 것\n보통 개별 제품에 대한 계획\n기간은 총괄생산계획에 상대적으로 정함.(분기 - 월, 월 - 주, 주 - 일 등)\n\n부품 주문부터 제품의 최종 조립이 완료될 떼까지의 총 기간은 포함해야함\n\n이어지는 부품의 제조활동이나 자재 조달 활동의 기준이 되는 계획\nRCCP를 통해 초안 생산계획을 수정\n가까운 시기의 계획은 동결하는게 바람직하다. (동결 시작 시기는 agility에 따라 다름)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "href": "posts/01_projects/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "title": "기준생산계획 및 자재소요계획",
    "section": "자재소요계획(Material Requirements Planning, MRP)",
    "text": "자재소요계획(Material Requirements Planning, MRP)\n\n부품에 대한 수요가 발생하는 양상이 완제품인 MPS랑 다름. 종속수요(다른 품목의 수요에 따라 수요가 발생)\n기준생산계획을 지키기 위한 자재로서의 품목 종류, 수량, 시점을 결정\n부품의 공급이 100% 확실하지 않음 (lead time 지연, 품질 등)\n투입물\n\n자재명세서(BOM): 한 단위의 품목 생산에 필요한 자재종류, 품목과 자재의 상하관계, 자재 사용량을 기록한 명세서\n기준생산계획\n재고 기록: 보유량, 주문량, 공급자, 리드타임, 로트사이즈 결정 방침 등 포함\n\n\n\n고려사항\n\n공통부품: LLC(Low Level Coding) 기준으로 MRP를 작성\n로트크기 규칙결정\n\nFixed Order Quantity\nPeriodic Order Quantity: 주문시 향후 T period만큼 필요한 양을 주문\nLot-for-Lot: 주문시 수요량만큼 주문\n\n안전재고: 순소요량에 안전재고 만큼 더함.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "title": "Matching Supply with Demand",
    "section": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지",
    "text": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지\n\n운영하는 관점에서 수요와 공급을 바라볼 예정\n기업을 바라보는 관점: 유/무형의 제품을 생산해서 수요(양, timing, 품질, …)에 맞게 공급하기 위해 노력하는 집단",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급 법칙",
    "text": "수요와 공급 법칙\n\n\n\n파란색 - 수요, 빨간색 - 공급\n\n\n\n경제학 기본적인 법칙. 가격 조정은 건강한 시스템의 증거라고 봄\n운영 관리자(OM)인 우리는 이거랑 다르게 바라봄.\n\nExcess demand = lost revenue\nExcess supply = wasted resources\n가격 조정만으로 수요와 공급 맞추기 어려움\n\n끊임없이 변하는 수요와 비 탄력적인 공급으로 인해 수요와 공급을 맞추기 어렵다.\n과학적 도구로 최대한 수요를 예측하고, 탄력적인 공급을 하는 방법을 찾아야 한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급이 안 맞는 사례",
    "text": "수요와 공급이 안 맞는 사례\n\n푸바오를 보기 위해 사람들이 몰림\n\n수요 공급의 불균형은 안 좋은 효과를 가져옴\n\n마스크, 먹태깡: 수요는 빠르게 변하는데, 공급은 느리게 변함\n\n수요를 예측하고, 설비를 미리 준비하는 과학적 도구가 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소매업\n철광석 공장\n응급실\n심박조율기\n항공 여행\n\n\n\n\n공급\n소비자 전자제품\n철광석\n의료 서비스\n의료 장비\n특정 항공편 좌석\n\n\n수요\n새로운 비디오 시스템을 구매하는 소비자\n제철소\n긴급한 의료 서비스 수요\n심박조율기가 특정 시간과 장소에서 필요한 심장외과 의사\n특정 시간과 목적지로의 여행\n\n\n공급이 수요를 초과\n재고 비용이 높고, 재고 회전율이 낮음\n가격 하락\n의사, 간호사 및 인프라가 충분히 활용되지 않음\n심박조율기가 재고로 남아 있음\n빈 좌석 발생\n\n\n수요가 공급을 초과\n포기한 이익 기회; 소비자 불만족\n가격 상승\n응급실 혼잡 및 지연; 구급차 우회 가능성\n포기된 이익 (일반적으로 의료적 위험과는 관련 없음)\n초과 예약으로 인해 고객이 다른 항공편을 이용해야 함 (이익 손실)\n\n\n공급과 수요를 맞추기 위한 조치\n수요 예측; 신속한 대응\n가격이 지나치게 하락하면 생산 시설이 폐쇄됨\n예측된 수요에 맞춘 인력 배치; 우선순위 설정\n여러 장소에서 심박조율기를 보관하는 유통 시스템\n동적 가격 책정; 예약 정책\n\n\n관리적 중요성\n소비자 전자제품 소매업의 단위당 재고 비용이 종종 순이익을 초과함\n가격 경쟁이 치열하여 주요 초점은 공급 비용 절감에 맞춰짐\n치료 또는 이송 지연이 사망과 연관된 사례 있음\n대부분의 제품(가치 2만 달러)이 사용되기 전에 영업 사원의 차량 트렁크에서 4~5개월 동안 대기함1\n전체 좌석의 약 30%가 빈 채로 운항되며, 좌석 이용률이 1~2%만 증가해도 이익과 손실이 갈림2",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템의 performance",
    "text": "생산 시스템의 performance\n\n서로 상충됨. business 목표에 맞게 balance를 잘 맞춰야함\n\ncost\nquality: 품질이 얼마나 좋고 일관되냐\nvariety: 다양한 사용자의 니즈를 얼마나 잘 맞추냐\ntime",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템 관리를 배우면 할 수 있는 것",
    "text": "생산 시스템 관리를 배우면 할 수 있는 것\n\n비효율성 분석\n상충관계에 대한 의사결정\n신기술 등에 대한 평가",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/01.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/product/01.html#footnotes",
    "title": "Matching Supply with Demand",
    "section": "각주",
    "text": "각주\n\n\n뭔소리지↩︎\n뭔소리지↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "href": "posts/01_projects/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "제품 설계 및 개발",
    "text": "제품 설계 및 개발\n\n설계의 중요성\n\n총 제품 비용 중 설계가 치지하는 비중은 적지만, 설계의 영향을 받는 비중이 높다.\n\n\n\n제품 설계 프로세스\n\n\n\n제품 설계 프로세스\n\n\n\n아이디어 선정: 소비자의 니즈, 경쟁사 제품 등 벤치마킹(reverse engineering)\n제품 선정: 시장 분석, 경제성 분석, 기술 분석\n\n\n\n제조 고려 설계\n\n제조 과정 단순화 및 비용 절감을 고려해서 설계해야 한다.\n\n부품 개수 최소화\n모듈화 표준화\n조립, 재활용, 분해 고려\n\nsubtract manufacturing보단 additive manufacturing(적층제조, DFAM)을 고려\n\n\n\n표준화 모듈화\n\n표준화: 부품 호환성 및 운영 효율성\n모듈화: 표준화된 부품의 집합 &lt;-&gt; integral\n\n\n\n지연전략\n\n차별화 지연 전략: 수요를 알기 전까지 많은 종류 생산은 지연",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "href": "posts/01_projects/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "생산 프로세스의 유형",
    "text": "생산 프로세스의 유형\n\n주문충족 방식에 따른 분류\n\nmake to stock: 수요가 발생하기 전에 생산. 수요가 예측이 쉬울 경우 적합 (push)\nassemble / configure / build to order: 제품 구성요소를 재고로 보유. 고객의 요구에 따라 조립하여 생산. 지연 전략에 맞닿아 있다 (pull)\nmake to order: 주문에 따른 생산. 설계가 완료된 걸 다른 옵션으로 제공. 옵션이 많거나 고가 제품에 적합 (push-pull)\nengineer to order: 고객의 요구에 따라 설계 및 생산. 일회성 프로젝트에 적합 (pull)\n\n위로 갈 수록 제공 시간은 짧아지고, 아래로 갈 수록 유연성이 높아진다.\n\n\n생산 흐름에 따른 분류\n\n프로젝트 프로세스: 일회성 생산. 흐름이라고 할 수는 없다.\n개별작업 프로세스(job shop): 공정별 배치. 높은 유연성, 낮은 규모\n배치 프로세스: job shop과 라인 프로세스의 중간 형태. batch 수가 맞춰지기 전까지 대기 / 유휴시간 존재.\n라인 프로세스: 제품별 배치. 낮은 유연성, 대규모\n연속 흐름 프로세스: 멈춤, 수정, 변경 최소화\n\n\n\n다양한 유형의 프로세스와 설비배치를 혼합 적용하는 것이 일반적\n\n\n\n다품종소량생산, 개인맞춤생산 시대 도래\n\n\nreconfigurable manufacturing system: 다양한 제품을 생산할 수 있는 유연한 생산 시스템\n\n\n\ncell manufacturing\n\nline process랑 job shop이 혼합된거\n비슷한 작업이 필요한 부품들을 하나의 그룹으로 묶어서 전용 셀에서 생산",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#조립공정의-분석",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#조립공정의-분석",
    "title": "인건비 추정과 감축",
    "section": "조립공정의 분석",
    "text": "조립공정의 분석\n\n처리 능력: \\(\\frac{자원의 수}{처리시간}\\)\nbottleneck은 처리능력이 제일 낮은 자원\nX개를 생산하는데 걸리는 시간\n\n가동중인 생산 시스템: \\(\\frac{X}{R}\\)\n비어있는 생산 시스템: 비어있는 시스템을 흘러 가는데 걸리는 시간 + \\(\\frac{X - 1}{R}\\)\n\n\n\n비어있는 시스템을 흘러가는데 걸리는 시간\n\nWorker-paced process: 모든 작업의 처리시간의 합\nMachine-paced process(컨베이어 벨트): 프로세스상의 단계 수 * 병목공정의 처리시간",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "title": "인건비 추정과 감축",
    "section": "노동량과 유휴시간",
    "text": "노동량과 유휴시간\n\n이상적인 노동비: 작업자의 처리시간의 합 * 시간당 평균 임금\n\n유휴시간을 고려하지 않았을 때\n\n직접 노동 인건비= \\(\\frac{단위시간당 총 임금}{단위시간당 흐름률}\\)\n\n실제 노동시간 + 유휴시간(idle time)\n흐름률이 높아지면 유휴시간이 줄어들면서 직접 노동 인건비가 줄어든다.\n\n\n\n유휴시간 종류\n\nbottleneck에 맞추기 위한 유휴시간\n수요에 맞추기 위해 발생하는 유휴시간\n\n이런 경우 작업시간을 줄이는 방법을 생각할 수 있다.\n하지만 flexible하게 맞추기는 어려울 것이다.\n\n\n\n\nCycle time(주기 시간)\n\n프로세스에서 산출되는 연속된 두 제품 간의 시간간격\n프로세스가 얼마나 빨리 생산하는지를 나타내는 지표\nflow rate의 역수\n1인 작업자의 유휴시간 = cycle time - 1인 작업자의 작업시간\n\n\n\n평균 노동 활용률\n\n제품 생산에만 들어가는 노동의 양과 실제 인건비 지불의 기준이 되는 노동의 양(노동량 + 유휴시간)을 비교\n\\(\\frac{노동량}{노동량 + 모든 작업자의 유휴시간 총합}\\)\n\\(\\frac{1}{노동자 수}(활용률의 합)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/05.html#line-balancing",
    "href": "posts/01_projects/bs_3_1/notes/product/05.html#line-balancing",
    "title": "인건비 추정과 감축",
    "section": "Line Balancing",
    "text": "Line Balancing\n\nbottleneck에 맞추기 위한 유휴시간으로 이상적인 노동비를 산출할 수 없음. → line balancing을 맞춰줌\n프로세스 내부적인 수요(요구되는 노동량)와 공급(작업자의 처리 능력)을 맞추는 것\n\n\n대량생산으로의 확장\n\n라인의 병렬적 배치\n프로세스 단계별 작업자 추가\n과업의 분화 및 전문화\n\n전문화될수록 라인 밸런싱이 어려워지고 평균 노동 활용률이 낮아짐(작업이 평탄하지 않음)\n→ 전문화 정도를 감소시켜 라인 밸런싱을 쉬워지게 한다.\n\n작업 셀: 한 명이 모든 과업을 수행함. 한 명의 작업시간이 노동량에 해당하고, 노동 활용률은 100%",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "프로세스 처리 능력 및 활용률",
    "text": "프로세스 처리 능력 및 활용률\n\nprocess capacity: 흐름률의 upper bound(유량).\n병목(bottleneck): 제일 낮은 처리능력의 자원\n전체 프로세스의 처리 능력 = 병목의 처리능력 (단 작업이 일렬로 수행될 때)\nproduct mix:\n\n다양한 제품이 input으로 들어와 처리능력이 달라짐\nbottleneck을 계산하기는 어려움\n\n비율이 매번 달라질 수도 있어서\n작업이 일렬로만 수행되지 않아서\n\n\n실제 생산한 양(흐름률)은 capacity에서만 결정되지 않는다.\n\n수요(market + 계절 / 안전 재고 같은 내부적 수요)\n원자재 투입량\n\n흐름률 = min(시간 당 수요, 프로세스 처리 능력)\n공급능력: 투입량, 처리 능력\n\n\n수요 / 공급 제약적 상황\n\n\n수요(가) 제약적: 수요 &lt; 공급\n\nbottleneck 활용률 &lt; 100%\nflow rate == Demand rate\n\n공급(이) 제약적: 수요 &gt; 공급\n\n투입 제약적\n처리능력 제약적\n\nbottleneck 활용률 == 100%\nflow rate = capacity\n\n\n\n\n\n활용률\n\n\n실제 생산하는 양을 capacity로 나눈 것\n\n\\(\\frac{흐름률}{처리능력}\\)\n활용률을 100% 달성하려면 쉬지 않고 프로세스가 돌아가야하지만 현실적으로 쉽지 않다.\n\n수요가 공급보다 적을 수 있다.\n투입물이 충분하지 않다.\n몇몇 공정의 사용이 공장이나 수리로 제한될 수 있다.\n불확실성\n\n\n병목을 제외한 다른 하위 작업은 활용률이 떨어질 수 있다.\n\n모든 프로세스가 병목인 것이 가장 이상적인 상황 (과도하게 높은 공급능력)\n\n공급 제약적 상황에서 수요가 얼마나 많은지 알 수 없음.\n→ implied utilization\n\n\n\nimplied utilization\n\n\\(U = \\frac{R}{Capacity}\\)\n\\(IU = \\frac{Demand or workload}{Capacity} (≤100% or &gt; 100%)\\)\nif min(demmand, capacity, input) = demand then U = IU\n이 외에도 잠재적 수요 못따라가는 작업도 알 수 있음\n\nIU 100 넘는거 개선 필요\n\n또, 작업이 sequential하게 진행되지 않을 때 병목현상을 확인할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "href": "posts/01_projects/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "여러 종류의 흐름 단위",
    "text": "여러 종류의 흐름 단위\ninput 당 뭐가 다르면 다른 단위로 치환",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/others/2.html",
    "href": "posts/01_projects/bs_3_1/notes/others/2.html",
    "title": "성적 장학금",
    "section": "",
    "text": "오~예~ (남은 등록금 300만원을 대출 받으며)\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "성적 장학금"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html",
    "href": "posts/01_projects/bs_3_1/index.html",
    "title": "학부 3학년 1학기",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-06-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#details",
    "href": "posts/01_projects/bs_3_1/index.html#details",
    "title": "학부 3학년 1학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 1학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#tasks",
    "href": "posts/01_projects/bs_3_1/index.html#tasks",
    "title": "학부 3학년 1학기",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    푸른등대 기부장학금 - 두나무UDC 신청 (~2025-01-20 18:00)\n                \n                불합격\n            \n            \n            \n                \n                \n                    2025 DB 드림리더 장학생 신청 (~2025-01-10)\n                \n                잘할 자신이 없다\n            \n            \n            \n                \n                \n                    경기도 학자금대출 이자 지원 신청 (~2025.02.14 18:00)\n                \n                신청 완료\n            \n            \n            \n                \n                    \n                    학과 근로 신청\n                \n                신청 완료\n            \n\n            \n            \n                \n                    \n                    KMOOC 학점 인정 신청 (2025-03-10~)\n                \n                done\n            \n\n            \n            \n                \n                \n                    봉사활동 계획서 작성 (2025-03-11~)\n                \n                작성 완료\n            \n            \n            \n                \n                \n                    OR 과제 1 (~2025-03-16 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 2 (~2025-03-23 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                    \n                    교수님과 커피\n                \n                나 커피 못 마시는데\n            \n\n            \n            \n                \n                \n                    데이터마이닝 팀과제 script\n                \n                일단 완성\n            \n            \n            \n                \n                \n                    OR 과제 3 (~2025-04-06 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 4 (~2025-04-13 23:59)\n                \n                그만...\n            \n            \n            \n                \n                \n                    data mining 1차 과제 ppt 완성\n                \n                done\n            \n            \n            \n                \n                    \n                    진로 지도 상담 받기\n                \n                \n            \n\n            \n            \n                \n                \n                    컴퓨팅적 사고 발표 ppt 만들기\n                \n                \n            \n            \n            \n                \n                    \n                    데이터마이닝 2차 과제 준비\n                \n                노션에 정리 중\n            \n\n            \n            \n                \n                    \n                    교통비 지원금 신청\n                \n                \n            \n\n            \n            \n                \n                \n                    OR 과제 6 (~2025-05-18 23:59)\n                \n                힘들다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#필요한-자료",
    "href": "posts/01_projects/bs_3_1/index.html#필요한-자료",
    "title": "학부 3학년 1학기",
    "section": "필요한 자료",
    "text": "필요한 자료\n\n자기소개서 작성 1\n교육 이수 증빙자료\nPortfolio: 큰일 났다. 진짜 못만들었다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#참고-자료",
    "href": "posts/01_projects/bs_3_1/index.html#참고-자료",
    "title": "학부 3학년 1학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/index.html#related-posts",
    "href": "posts/01_projects/bs_3_1/index.html#related-posts",
    "title": "학부 3학년 1학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#예측",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#예측",
    "title": "예측",
    "section": "예측",
    "text": "예측\n예측: 과거의 데이터를 사용하여 현재 불확실하고 미래에 실현될 결과에 대해 판단하는 과정\n\n\n단기로 갈 수록 디테일한 예측을 하고, 기법이 달라짐.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/10.html#예측-기법",
    "href": "posts/01_projects/bs_3_1/notes/product/10.html#예측-기법",
    "title": "예측",
    "section": "예측 기법",
    "text": "예측 기법\n\n판단적 기법\n\n전문가의 경험과 직관에 의존하여 예측하는 기법\n비정량적 / 주관적 데이터로 정량적인 예측치를 구함\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석:치전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n시장조사법: 설문지, 인터뷰를 바탕으로 신제품의 생산량 결정이나 기존제품의 수요변화 예측\n전문가 의견 종합법: 여러 전문가로 예측치 수집 후 단순평균 or 가중평균\n사례유추법: 비슷한 제품이랑 비교\n델파이 기법: 여러 전문가들로 패널을 구성하고, 반복적인 질문과 결과 피드백을 통하여 합의된 예측치를 도출\n\n\n\n\n\n델파이 기법\n\n\n\n\n시계열 기법\n\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여\n지수평활법: 과거의 모든 데이터를 가중 평균\n\n지수평활계수(α): 최근의 값을 더 높은 가중치가 부여되도록 추정\n\\(\\hat{y_{t+1}} = αy_t+ (1-α)\\hat{y_t} = \\hat{y_t} + α(y_t - \\hat{y_t}\\)\n예측치와 관측치 중 어디에 중점을 둘 지에 따라서 α 결정\n오차를 어느정도 반영할지에 따라서 α 결정\nα == 1: 최근 자료에 비중을 둠. α == 0: 기존 예측을 따름\n\n\n\n\n\n\n상관관계 기법\n\n회귀분석\n\n\n\n\n선행 지수법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "href": "posts/01_projects/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "title": "시험 범위",
    "section": "6 - 쌍대이론과 민감도 분석",
    "text": "6 - 쌍대이론과 민감도 분석\n\n쌍대이론의 본질\n원-쌍대 관계들\n다른 원형태들에 적용\n민감도 분석",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "href": "posts/01_projects/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "title": "시험 범위",
    "section": "7 - 선형계획을 위한 다른 알고리즘들",
    "text": "7 - 선형계획을 위한 다른 알고리즘들\n\n쌍대 심플렉스 방법\n상한 기법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "href": "posts/01_projects/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "title": "시험 범위",
    "section": "8 - 수송문제와 할당 문제들",
    "text": "8 - 수송문제와 할당 문제들\n\n수송문제의 기초\n수송문제를 위한 능률적인 심플렉스 방법\n할당 문제\n할당문제를 위한 특별한 알고리즘",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "href": "posts/01_projects/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "title": "시험 범위",
    "section": "9 - 네트워크 최적화 모형",
    "text": "9 - 네트워크 최적화 모형\n\n네트워크 용어들\n최단 경로 문제\n최대 흐름 문제\n네트워크 심플렉스 해법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/17.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/17.html",
    "title": "clustering",
    "section": "",
    "text": "k-means\nk-medoids\nPAM\nCLARA\nCLARANS\nAGNES\nDIANA\nDBSCAN\nOPTICS\nGRID\n\n\n주제를 정하고 한게 아니고, 데이터 안에서 통찰을 찾은건데, 막상 우리 분석이 정말 의미가 있는지 모르겠다.\n\n안정형, 탐색형을 분류하는게 무슨 의미가 있고, 이걸 18~19세 데이터만 측정하는게 대체 무슨 의미가 있을까\n\n오늘 발표한거 보니까 변수 해석보다는 분석 방법론(어떻게 분석을 진행했고, 예측률이 얼마나 나오는지)에 더 집중해서 진행한거 같은데, 원래 회귀분석에서는 분석 방법론에 더 집중하는게 맞는가?\n다중 공산성 문제가 있는데 트리기반 모델을 사용했고, l1, l2 엘라스틱넷을 사용했는데, 이게 다중 공산성 문제를 완전히 해결했다고 할 수 있을까\n\nlogistic regression의 결과 중 p value가 낮은 변수로 다시 분석을 진행해보면 괜찮을까\n\ntrain, test set을 분리하는 과정에서 가중치가 왜곡될 수 있어보인다.\n분석 모델을 너무 적게 사용했다고 생각할 수 있을까?\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#트리",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#트리",
    "title": "시험 범위",
    "section": "트리",
    "text": "트리\n\n차수, 노드의 갯수, 높이 물어봄\n특정 노드의 부모 / 조상 노드, 단말 노드 수\n\n\n\n트리의 차수: 트리의 최대 차수\n\n\n전위, 중위, 후위 순회 무조건 냄\n\n\n전위 / 후위 중 하나의 결과와 중위순위를 통해 원래 트리 추측하는 문제\n\n전위의 맨 앞이랑 후위의 맨 뒤가 루트 노드\n루트 노드를 알면 중위 순회에서 루트 노드 기준으로 왼쪽 서브트리와 오른쪽 서브트리로 나눌 수 있음\n\n트리를 주고 나서 전위 / 중위 / 후위 결과를 기술하라\n\n\n레벨 순위 안냄",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#힙트리-1520",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#힙트리-1520",
    "title": "시험 범위",
    "section": "힙트리 (15~20)",
    "text": "힙트리 (15~20)\n\n삽입\n\ndef insert(self, n):\n    self.heap.append(n)\n    i = self.size()\n    while (i != 1 and n &gt; self.Parent(i)):\n        self.heap[i] = self.Parent(i)\n        i = i // 2\n    self.heap[i] = n\n\n삭제\n\ndef delete(self):\n    parent = 1\n    child = 2\n    if not self.is_empty():\n        hroot = self.heap[1]\n        last = self.heap[self.size()]\n        while child &lt;= self.size():\n            if (child &lt; self.size() and self.left(parent) &lt; self.right(parent)):\n                child += 1\n            if last &gt;= self.heap[child]:\n                break;\n            self.heap[parent] = self.heap[child]\n            parent = child\n            child = child * 2\n        self.heap[parent] = last\n        self.heap.pop(-1)\n        return hroot\n\n상향식 힙 만들기\nclass Bheap:\n    def __init__(self, a):\n        self.a = a\n        self.N = len(a) - 1\n\n    def create_heap(self, k):\n        for i in range(N // 2, 0, -1):\n            self.downheap(i)\n\n    def insert(self, k):\n        self.a.append(k)\n        self.N += 1\n        self.upheap(self.N)\n\n    def delete_min(self):\n        if self.N == 0:\n            return None\n        minimum = self.a[1]\n        self.a[1], self.a[-1] = self.a[-1], self.a[1]\n        del self.a[-1]\n        self.downheap(1)\n        return minimum\n\n    def upheap(self, i):\n        while i &gt; 1 and self.a[i][0] &lt; self.a[i // 2][0]:\n            self.a[i], self.a[i // 2] = self.a[i // 2], self.a[i]\n            i = i // 2\n\n    def downheap(self, i):\n        while i * 2 &lt;= self.N:\n            k = i * 2\n            if k &lt; self.N and self.a[k][0] &gt; self.a[k + 1][0]:\n                k = k+1\n            if self.a[i][0] &lt; self.a[k][0]:\n                break\n            self.a[i], self.a[k] = self.a[k], self.a[i]\n            i = k\n삽입, 삭제: O(logN)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#이진-탐색-트리-1520",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#이진-탐색-트리-1520",
    "title": "시험 범위",
    "section": "이진 탐색 트리 (15~20)",
    "text": "이진 탐색 트리 (15~20)\n\n이진 탐색 트리\n\n\nkey 값을 10개정도 쭉 주고 나서 이진 탐색트리로 구현하라\n이진 탐색 트리를 주고 나서, 노드 삭제 후 최종 이진 탐색 트리 기술하라\n\n\n연습 문제: BST 빈칸 채우기 (고급)\n아래 BST 클래스의 메소드들을 완성해보세요:\nclass Node:\n    def __init__(self, key, value, left=None, right=None):\n        self.key = key\n        self.value = value\n        self.left = left\n        self.right = right\n\nclass BST:\n    def __init__(self):\n        self.root = None\n\n    def get(self, k):\n        return self.get_item(self.root, k)\n\n    def get_item(self, i, k):\n        if i == None:\n            return None\n        if k &lt; i.key:\n            return self.get_item(i.left, k)\n        if i.key &lt; k:\n            return self.get_item(i.right, k)\n        return i.value\n\n    def put(self, k, val):\n        self.root = self.put_item(self.root, k, val)\n\n    def put_item(self, i, k, val):\n        if self.root == None:\n            return Node(k, val)\n        if k &lt; i.key:\n            self.left = self.put_item(self,left, k, val)\n        elif i.key &lt; k:\n            self.right = self.put_item(self.right, k, val)\n        else:\n            i.val = val\n        return i\n\n    def min(self):\n        if self.root == None:\n            return None\n        return self.min_item(self.root)\n\n    def min_item(self, i):\n        if i.left == None:\n            return i\n        return self.min_item(i.left)\n\n    def del_minimum(self):\n        if self.root == None:\n            return None\n        return self.del_min(self.root)\n\n    def del_min(self, i):\n        if i.left == None:\n            return i.right\n        i.left = del_min(self.left)\n        return i\n\n    def del(self, k):\n        if self.root == None:\n            return None\n        return self.del_item(self.root, k)\n\n    def del_item(i, k):\n        if k &lt; i.key:\n            i.left = self.del_item(i.left, k)\n        elif i.key &lt; k:\n            i.right = self.del_item(i.right, k)\n        else:\n            if i.right == None:\n                return i.left\n            if i.left == None:\n                return i.right\n            target = i\n            i = self.min_item(target.right)\n            i.left = target.left\n            i.right = self.del_min(target.right)\n        return i\n\nAVL 안냄",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/dsa/01.html#그래프",
    "href": "posts/01_projects/bs_3_1/notes/dsa/01.html#그래프",
    "title": "시험 범위",
    "section": "그래프",
    "text": "그래프\n\n그래프 차수: 정점에 인접한 정점의 수\nkruscal, prim(둘 중 하나) 30~40\n\n\nkruscal\nweights = [(0, 1, 9), (0, 2, 10)]\nweights.sort(key = lambda t: t[2])\nmst = []\nN = 7\np = [] * N\ndef find(a):\n    if a != p[a]:\n        p[a] = find(p[a])\n    return p[a]\n\ndef union(u, v):\n    root1 = find(u)\n    root2 = find(v)\n    p[root1] = root2\n\ne = 0\ncost = 0\nwhile True:\n    if e == N - 1:\n        break\n    u, v, wt = weights.pop(0)\n    if find(u) != find(v):\n        union(u, v)\n        mst.append((u, v))\n        cost += wt\n        e += 1\nprint(cost)\nprint(mst)\nO(MlogN)\n\n\nprim\nimport sys\nN=8\ns=0\ng = [None] * N\ng[0] = [(1, 1), (3, 2)]\ng[1] = []\n\nvisited = [False] * N\nD = [sys.maxsize] * N\nD[s] = 0\nprevious = [None] * N\nprevious[s] = s\n\nfor k in range(N):\n    m = -1\n    min_val = sys.maxsize\n    for j in range(N):\n        if not visited[j] and D[j] &lt; min_val:\n            m = j\n            min_val = D[j]\n    visited[m] = True\n    for v, wt in list(g[m]):\n        if not visited[v]:\n            if wt &lt; D[v]:\n                D[v] = wt\n                previous[v] = m\n그래프가 희소그래프이고, 이진힙 쓰면 O(NlogN) 아니면 O(N^2)\n\n다익스트라(필수)\n\nimport sys\nN=8\ns=0\ng = [None] * N\ng[0] = [(1, 1), (3, 2)]\ng[1] = []\n\nvisited = [False] * N\nD = [sys.maxsize] * N\nD[s] = 0\nprevious = [None] * N\nprevious[s] = s\nfor k in range(N):\n    m = -1\n    min_val = sys.maxsize\n    for j in range(N):\n        if not visited[j] and D[j] &lt; min_val:\n            min_val = D[j]\n            m = j\n    visted[m] = True\n    for v, wt in list(g[m]):\n        if not visted[v]:\n            if D[m] + wt &lt; D[v]:\n                D[v] = D[m] + wt\n                previous[v] = m\nO(N^2)\n\nBFS, DFS (5~10)\n\n\nBFS\n\nadj_list = [[2, 1], [3, 0]]\nN = len(adj_list)\nvisited = [None] * N\n\ndef bfs(i):\n    queue = []\n    visited[i] = True\n    queue.append(i)\n    while len(queue) != 0:\n        v = queue.pop(0)\n        print(v, ' ', end='')\n        for i in adj[v]:\n            if not visited[i]:\n                queue.append(i)\n                visited[i] = True\n\nfor i in range(N):\n    if not visited[i]:\n        bfs(i)\nO(N + M)\n\nDFS\n\nadj_list = [[2, 1], [3, 0]]\nN = len(adj_list)\nvisited = [None] * N\n\ndef dfs(v):\n    visited[v] = True\n    print(v, ' ', end='')\n    for i in adj_list[v]:\n        if not visited[i]:\n            dfs(i)\n\nfor i in range(N):\n    if not visited[i]:\n        dfs(i)\nO(N + M)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#통계학습-및-회귀분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#통계학습-및-회귀분석",
    "title": "Regression Analysis",
    "section": "통계학습 및 회귀분석",
    "text": "통계학습 및 회귀분석\n\n통계 학습: 관측된 X, Y 데이터로부터 X와 Y의 관계를 추정하는 것\n\\(Y = f(X) + ϵ\\)\n선형 회귀 모형: 독립변수와 종속변수 사이의 관계를 선형으로 가정하는 모형",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#단순-선형회귀모형",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#단순-선형회귀모형",
    "title": "Regression Analysis",
    "section": "단순 선형회귀모형",
    "text": "단순 선형회귀모형\n\n\\(Y_i = β_0 + β_1x_i + ϵ_i\\)\n\\(β_0 + β_1x_i\\): 회귀 계수, 선형 모형으로 설명 가능한 부분\n\\(ϵ_i\\): 오차항, 선형 모형으로 설명할 수 없는 부분\n\n\\(ϵ_i ~^{iid} N(0, σ^2) → Y_i ~ N(β_0 + β_1x_i, σ^2)\\)\n\n\n\n\n\n\\(Y_i\\)는 iid는 아니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀계수의-추정",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀계수의-추정",
    "title": "Regression Analysis",
    "section": "회귀계수의 추정",
    "text": "회귀계수의 추정\n\n최소 제곱법: 잔차 제곱합을 최소화하는 방법\n\n→ \\(SS_E\\)를 \\(\\hat{β}_0, \\hat{β}_1\\)에 대해 미분하여 0으로 만드는 \\(\\hat{β}_0, \\hat{β}_1\\).\n\\(β_0 = \\bar{Y} - β_1\\bar{X}\\), \\(β_1 = \\frac{S_{XY}}{S_{XX}}\\)\n\\(Σ_{i=1}^n e_i = 0\\), \\(Σ_{i=1}^n x_ie_i = 0\\)\n\n회귀계수 추정 시에는 오차항의 분포에 대한 가정이 사용되지 않는다.\n독립변수의 단위가 달라지면, 회귀계수의 단위도 달라진다. (설명력은 동일하다)\n표본의 수가 늘어나면, \\(β_1\\)은 수렴한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀모형의-적합성",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀모형의-적합성",
    "title": "Regression Analysis",
    "section": "회귀모형의 적합성",
    "text": "회귀모형의 적합성\n\n\\(SS_T = S_{YY}\\)\n\\(SS_R = \\frac{S_{XY}^2}{S_{XX}}\\)\n\\(SS_E = SS_T - SS_R\\)\n\n - 결정계수: \\(R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_E}{SS_T}\\) - \\(R^2\\)는 모형이 종속변수의 변동을 얼마나 설명하는지 나타내는 지표 - 오차를 최소화하면 \\(R^2\\) 값 증가 - \\(\\frac{S_{XY}^2}{S_{XX}S_{YY}} = r(X, Y)^2\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀계수에-대한-추론",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#회귀계수에-대한-추론",
    "title": "Regression Analysis",
    "section": "회귀계수에 대한 추론",
    "text": "회귀계수에 대한 추론\n\n\\(H_0: β_1 = 0\\) (독립변수와 종속변수 사이에 관계가 없다)\n\\(β_1\\)은 불편추정량, 정규분포를 따른다.\n\\(Var(β_1) = \\frac{MS_E}{S_{XX}}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#종속변수의-신뢰구간과-예측구간",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#종속변수의-신뢰구간과-예측구간",
    "title": "Regression Analysis",
    "section": "종속변수의 신뢰구간과 예측구간",
    "text": "종속변수의 신뢰구간과 예측구간\n\n\\(\\hat{Y_0} = \\hat{β}_0 + \\hat{β}_1x_0\\)\n\\(Var(\\hat{Y_0}) = MS_E(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{XX}})\\)\n예측 구간에서의 오차의 분산은 \\(MS_E(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{XX}})\\)\n\n\n\n\n종속변수의 예측 / 신뢰구간은 평균일 때 가장 짧음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/04.html#잔차-분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/04.html#잔차-분석",
    "title": "Regression Analysis",
    "section": "잔차 분석",
    "text": "잔차 분석\n\n불편성\n정규성\n독립성\n등분산성\n\n오차의 추정치인 잔차를 통해 위의 가정을 검증할 수 있다.\n\n선형성: 잔차그림이 특정 패턴을 보이지 않는다.\n\n만족하지 않을 경우: 독립변수 변환\n\n등분산성: 잔차그림의 분산이 일정하다.\n\n만족하지 않는 경우: 반응치를 log 변환할 수 있다.\n\n독립성: 시계열 데이터의 경우, 자기상관을 고려해야 한다.\n정규성: normal Q-Q plot / shapiro-wilk test\n\n만족하지 않는 경우: 반응치를 log 변환할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석-anova",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#분산분석-anova",
    "title": "ANOVA",
    "section": "분산분석 (ANOVA)",
    "text": "분산분석 (ANOVA)\n\n요인, 인자 (factor): class\n수준 (level): class 값\n처리 (treatment): 요인과 수준의 조합\n반응치 (response): 관측치",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#완전-확률화-계획법completely-randomized-design",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#완전-확률화-계획법completely-randomized-design",
    "title": "ANOVA",
    "section": "완전 확률화 계획법(Completely Randomized Design)",
    "text": "완전 확률화 계획법(Completely Randomized Design)\n\n일원 분산분석이 공정한 결과를 내기 위한 실험 조건\n처리 i에 해당되는 모집단으로부터 독립인 표본 \\(n_i\\)개를 랜덤으로 샘플링함으로써, k개의 서로 다른 모집단으로부터 독립인 random sample들을 얻는 것과 같음\n반복 수가 같을 필요는 없다\n각 모집단으로부터 랜덤으로 표본을 추출하는 것\n== 실험 대상을 랜덤으로 그룹으로 나눈 후, 각 그룹에 대해 서로 다른 처리를 작용하는 것",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산분석-one-way-anova",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#일원-분산분석-one-way-anova",
    "title": "ANOVA",
    "section": "일원 분산분석 (One-Way ANOVA)",
    "text": "일원 분산분석 (One-Way ANOVA)\n\n정규성, 독립성, 등분산성\n\n→ 오차의 독, 정, 불편성, 등분산성\n오차(표본 - 잔차): 관심 없는 다른 모든 요인에 의해 발생하는 오차\n효과: \\(τ_i: μ_i - μ\\)\n\n\\(Y_{ij} = μ + τ_i + ε_{ij}\\)\n\\(Y_{ij} - \\bar{Y} = (\\bar{Y_i} - \\bar{Y}) + (Y_{ij} - \\bar{Y_i})\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#사후-검정-post-hoc-test",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#사후-검정-post-hoc-test",
    "title": "ANOVA",
    "section": "사후 검정 (Post-Hoc Test)",
    "text": "사후 검정 (Post-Hoc Test)\n\nf검정 결과 \\(H_0\\)가 기각되는 경우 어떤 처리 사이에 차이가 있는지 검정해야 함.\n\\(T_0 = \\frac{\\bar{Y}_i - \\bar{Y}_j}{\\sqrt{MS_E}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}} ~ t(n-k)\\)\nbonferroni 방법: α를 검정 횟수 m으로 나눈 \\(\\frac{\\alpha}{m}\\)으로 사용\n\n이는 매우 보수적인 검정 방법. f 검정시 \\(H_0\\)를 기각해도, 사후 검정에서 \\(H_0\\)를 기각 못할 수도 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#무작위-블록-계획법-randomized-block-design",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#무작위-블록-계획법-randomized-block-design",
    "title": "ANOVA",
    "section": "무작위 블록 계획법 (Randomized Block Design)",
    "text": "무작위 블록 계획법 (Randomized Block Design)\n\n처리 외 독립적으로 영향을 미치는 변수의 변동을 분석하기 위해, 각 요인을 처리 모두에 대해 관찰하는 것\n이때 처리의 순서는 random\n\\(SS_T = SS_A + SS_B + SS_E\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/03.html#randomized-block-design에서의-분산분석",
    "href": "posts/01_projects/bs_3_1/notes/statistics/03.html#randomized-block-design에서의-분산분석",
    "title": "ANOVA",
    "section": "Randomized Block Design에서의 분산분석",
    "text": "Randomized Block Design에서의 분산분석\n\n사후 검정: 자유도가 n - b - k + 1",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "href": "posts/01_projects/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "title": "총괄생산계획",
    "section": "최적화 기법을 통한 총괄생산계획 수립",
    "text": "최적화 기법을 통한 총괄생산계획 수립",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/15.html#overview",
    "href": "posts/01_projects/bs_3_1/notes/OR/15.html#overview",
    "title": "수송문제와 할당 문제들",
    "section": "Overview",
    "text": "Overview\n\n수송문제: 여러 공급지로부터 여러 수요지까지 상품을 운송하는 최적의 방법을 결정하는 문제를 다루지만, 그 적용 범위는 생산 일정 계획과 같이 실제 수송과 직접적인 관련이 없는 경우까지 확장된다.\n할당문제: 주로 인력이나 자원을 특정 과업(tasks)에 배정하는 문제를 다룬다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "href": "posts/01_projects/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "title": "수송문제와 할당 문제들",
    "section": "2. 수송문제를 위한 능률적인 심플렉스 방법",
    "text": "2. 수송문제를 위한 능률적인 심플렉스 방법\n\n가정\n\n공급량과 수요량은 일치\n\n일치하지 않으면 dummy 공급지를 추가, 비용은 0으로 설정\n불가능한 연결은 무한대 비용으로 설정\n수요가 정수가 아닌 범위일 경우\n\n비용은 분배되는 상품 양(\\(x_{ij}\\))에 비례\n\n\n\n수식\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n창고 (트럭당 운송비용, $)\n\n생산량 (트럭분)\n\n\n\n\n\n\n새크라멘토\n솔트레이크시티\n래피드시티\n앨버커키\n\n\n\n공장 1 (벨링햄)\n\n464\n513\n654\n867\n75\n\n\n공장 2 (유진)\n\n352\n416\n690\n791\n125\n\n\n공장 3 (앨버트 리)\n\n995\n682\n388\n685\n100\n\n\n창고 배정량 (트럭분)\n\n80\n65\n70\n85\n(총 300)\n\n\n\n\n\n\n수송문제 example\n\n\n\n\\(Z= \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} x_{ij}\\)\n공급 제약식: \\(\\sum_{j=1}^{n} x_{ij} = s_i \\quad \\text{for } i=1,2,\\ldots,m\\)\n수요 제약식: \\(\\sum_{i=1}^{m} x_{ij} = d_j \\quad \\text{for } j=1,2,\\ldots,n\\)\n비음 제약식: \\(x_{ij} \\geq 0 \\quad \\text{for } i=1,2,\\ldots,m, j=1,2,\\ldots,n\\)\n\n\n\n수송문제를 위한 매개변수 표\n\n\n일반적인 심플렉스 방법도 수송문제를 푸는 데 적용할 수 있지만, 수송문제의 규모가 크고 제약식 행렬이 대부분 0과 일부 1로 구성된 희소 행렬(sparse matrix)이라는 특성 때문에 비효율적이다. 일반 심플렉스 방법은 초기화 단계에서 많은 인공 변수를 필요로 할 수 있다.\n수송문제에서 기저가능해는 항상 m+n−1개의 기저 변수를 가진다.\n\n\n\n\n초기 bfs를 만들기 위한 절차\n\n\n\n문제 예시\n\n\n\n여전히 고려중인 행들과 열들로부터, 어떤 기준에 따라 다음 기저변수(할당)를 선택한다.\n\n북서모서리법\n\n할당을 충분히 크게 하여 그 행에 있는 남은 공급이나 그 열에 있는 남은 수요를 다 써버리게 한다(둘 중 더 작은 것).\n더 이상의 고려에서 그 행이나 열(둘 중에서 더 작게 남은 공급 혹은 수요를 가진 것)을 제거한다(만약 행과 열이 같은 남은 공급과 수요를 가지면, 임의로 행을 선택하여 제거한다. 열은 후에 퇴화기저변수, 즉 할당 0으로 표시됨을 제공하는 것으로 사용될 것이다).\n만약 단지 하나의 행 혹은 하나의 열이 고려 대상으로 남아 있으면, 가능한 할당과 함께 기저가 되는 그 행이나 열과 연관된 모든 남아 있는 변수(즉 전에 기저로 선택되지 않았고 행 과 열을 제거함으로써 고려 대상에서 제외되지 않은 변수들)를 선택함으로써 절차는 종결된다.\n\n\n\n\n최적화 검사 절차\n\n가장 많은 할당이 일어난 행의 변수 하나를 0으로 설정\n기저인 \\(x_{ij}\\)의 \\({i, j}\\)에 대해 \\(c_{ij} = u_i + v_j\\)를 만족한다는 성질로 \\(u_i\\)와 \\(v_j\\)를 계산한다.\n비기저 변수들의 \\(c_{ij} - u_i - v_j\\)를 계산한다.\n모두 양수이면 최적.\n\n\n\n반복\n\n진입기저변수를 결정하라: 가장 큰(절댓값으로) 음의 값 \\(C_{jj} - u_i - v_j\\)를 가지는 비기저변수 \\(x_{ij}\\)를 선택하라.\n탈락기저변수를 결정하라: 진입기저변수가 증가할 때 가능을 유지하기 위해 요구되는 연쇄반응을 식별하라. 기증셀들 중에서, 가장 작은 값을 가지는 기저변수를 선택하라.\n새 기저가능해를 결정하라: 탈락변수의 값을 각 수신셀의 할당에 더하라. 그 값을 각 기증 셀의 할당에서 빼어라.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/15.html#할당-문제",
    "href": "posts/01_projects/bs_3_1/notes/OR/15.html#할당-문제",
    "title": "수송문제와 할당 문제들",
    "section": "3. 할당 문제",
    "text": "3. 할당 문제",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/15.html#할당문제를-위한-특별한-알고리즘",
    "href": "posts/01_projects/bs_3_1/notes/OR/15.html#할당문제를-위한-특별한-알고리즘",
    "title": "수송문제와 할당 문제들",
    "section": "4. 할당문제를 위한 특별한 알고리즘",
    "text": "4. 할당문제를 위한 특별한 알고리즘",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/06.html#one-way-anova",
    "href": "posts/01_projects/bs_3_1/notes/statistics/06.html#one-way-anova",
    "title": "확률과 통계 R 실습 과제",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\na\n\nval1 &lt;- c(1.84, 2.67, 2.61, 2.95, 2.25, 2.35, 2.85, 2.58, 3.08, 2.25)\nval2 &lt;- c(1.79, 1.98, 2.01, 1.93, 2.24, 2.03, 2.22, 1.44, 2.24, 2.05)\nval3 &lt;- c(3.31, 2.24, 2.74, 2.36, 2.79, 2.20, 2.32, 2.82, 2.73, 2.78)\ndf &lt;- data.frame(\n  group = c(rep(\"일반 마우스\", 10), rep(\"트랙 패트\", 10), rep(\"버티컬 마우스\", 10)),\n  finger = c('왼중지', '왼검지', '오른약지', '왼약지', '왼엄지', '오른약지', '오른새끼', '오른엄지', '왼중지', '왼새끼', '오른중지', '오른엄지', '오른중지', '오른검지', '왼엄지', '왼검지', '오른새끼', '오른검지', '왼엄지', '오른중지', '왼검지', '왼새끼', '오른검지', '왼약지', '오른새끼', '오른엄지', '왼약지', '왼중지', '왼새끼', '오른약지'),\n  value = c(val1, val2, val3)\n)\ndf\n\n           group   finger value\n1    일반 마우스   왼중지  1.84\n2    일반 마우스   왼검지  2.67\n3    일반 마우스 오른약지  2.61\n4    일반 마우스   왼약지  2.95\n5    일반 마우스   왼엄지  2.25\n6    일반 마우스 오른약지  2.35\n7    일반 마우스 오른새끼  2.85\n8    일반 마우스 오른엄지  2.58\n9    일반 마우스   왼중지  3.08\n10   일반 마우스   왼새끼  2.25\n11     트랙 패트 오른중지  1.79\n12     트랙 패트 오른엄지  1.98\n13     트랙 패트 오른중지  2.01\n14     트랙 패트 오른검지  1.93\n15     트랙 패트   왼엄지  2.24\n16     트랙 패트   왼검지  2.03\n17     트랙 패트 오른새끼  2.22\n18     트랙 패트 오른검지  1.44\n19     트랙 패트   왼엄지  2.24\n20     트랙 패트 오른중지  2.05\n21 버티컬 마우스   왼검지  3.31\n22 버티컬 마우스   왼새끼  2.24\n23 버티컬 마우스 오른검지  2.74\n24 버티컬 마우스   왼약지  2.36\n25 버티컬 마우스 오른새끼  2.79\n26 버티컬 마우스 오른엄지  2.20\n27 버티컬 마우스   왼약지  2.32\n28 버티컬 마우스   왼중지  2.82\n29 버티컬 마우스   왼새끼  2.73\n30 버티컬 마우스 오른약지  2.78\n\n\n\n요인: 클릭하는 장치 종류\n수준: 일반 마우스, 트랙 패드, 버티컬 마우스\n반응치: 클릭 반응속도\n\n10개의 손가락에 대해 3개의 장치를 랜덤으로 사용하여 클릭 반응속도를 측정한다.\n각 손가락의 실험에 대해 랜덤으로 그룹을 나눈 후, 그룹별로 클릭하는 장치 종류를 배정하였다.\n\n\nb\n\ndf.means &lt;- tapply(df$value, INDEX=df$group, FUN=mean)\nboxplot(df$value ~ df$group, col=c(\"lightblue\", \"mistyrose\", \"lightcyan\"))\npoints(1:3, df.means, col=\"red\", pch=4, cex=1.5)\n\n\n\n\n\n\n\n\n결과는 다음과 같이 나온다.\n\n\nc\n\ndf.sd &lt;- tapply(df$value, INDEX=df$group, FUN=sd)\ndf.sd.diff &lt;- max(df.sd) / min(df.sd)\nif (df.sd.diff &gt; 2) \n{\n  print(\"모집단의 분산이 동일하지 않음\")\n} else \n{\n  print(\"모집단의 분산이 동일하다고 가정\")\n}\n\n[1] \"모집단의 분산이 동일하다고 가정\"\n\n\n결과는 다음과 같이 나온다.\n\n\nd\n\nlibrary(ggpubr)\n\nLoading required package: ggplot2\n\nggqqplot(val1)\n\n\n\n\n\n\n\nggqqplot(val2)\n\n\n\n\n\n\n\nggqqplot(val3)\n\n\n\n\n\n\n\n\n그림을 보니 반응치 값들이 정규분포를 따르는 것 같다!\n\n\ne\n\ndf.anova &lt;- aov(value ~ group, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  2.381  1.1907   11.17 0.000292 ***\nResiduals   27  2.878  0.1066                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nis_significant &lt;- summary(df.anova)[[1]][[\"Pr(&gt;F)\"]][1] &lt; 0.05\nif (is_significant)\n{\n  print(\"95%에서 처리별 모평균의 차이가 있음\")\n} else\n{\n  print(\"95%에서 처리별 모평균의 차이가 없음\")\n}\n\n[1] \"95%에서 처리별 모평균의 차이가 있음\"\n\n\n결과는 다음과 같이 나온다.\n\n\nf\n\nif (is_significant)\n{\n  df.bon &lt;- pairwise.t.test(df$value, df$group, p.adjust.method = \"bonferroni\")\n  diff_pair &lt;- which(df.bon$p.value &lt; 0.05, arr.ind = TRUE)\n  if (length(diff_pair) &gt; 0)\n  {\n    cat(\"모평균의 차이가 있는 쌍\", \"\\n\")\n    for (i in 1:nrow(diff_pair)) \n    {\n      cat(colnames(df.bon$p.value)[diff_pair[i, \"col\"]], \"-\", rownames(df.bon$p.value)[diff_pair[i, \"row\"]], \"\\n\")\n    }\n  } else \n  {\n    print(\"모평균의 차이가 있는 처리 쌍 없음\")\n  }\n}\n\n모평균의 차이가 있는 쌍 \n버티컬 마우스 - 트랙 패트 \n일반 마우스 - 트랙 패트 \n\n\n결과는 다음과 같이 나온다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/06.html#anova-for-randomized-block-design",
    "href": "posts/01_projects/bs_3_1/notes/statistics/06.html#anova-for-randomized-block-design",
    "title": "확률과 통계 R 실습 과제",
    "section": "ANOVA for Randomized Block Design",
    "text": "ANOVA for Randomized Block Design\n\na\n\ndf &lt;- data.frame(\n  block = rep(c(\"아스팔트\", \"트랙\", \"모래밭\", \"잔디밭\"), each=3),\n  treatment = rep(c(\"런닝화\", \"축구화\", \"크록스\"), times=4),\n  value = c(6.1, 6.4, 6.5, 5.5, 5.9, 6.0, 6.9, 7.1, 7.5, 6.3, 6.9, 7.0)\n)\ndf\n\n      block treatment value\n1  아스팔트    런닝화   6.1\n2  아스팔트    축구화   6.4\n3  아스팔트    크록스   6.5\n4      트랙    런닝화   5.5\n5      트랙    축구화   5.9\n6      트랙    크록스   6.0\n7    모래밭    런닝화   6.9\n8    모래밭    축구화   7.1\n9    모래밭    크록스   7.5\n10   잔디밭    런닝화   6.3\n11   잔디밭    축구화   6.9\n12   잔디밭    크록스   7.0\n\n\n\n요인: 운동화 종류\n수준: 런닝화, 축구화, 크록스\nblock: 달리는 땅\n반응치: 50m 달리기 시간(초)\n\n각 block에 대해 3개의 운동화 종류를 사용하는 순서를 랜덤으로 결정해서 달리기 시간을 측정한다.\n\n\nb\n\ndf.anova.rbd &lt;- aov(value ~ treatment + block, data=df)\nsummary(df.anova.rbd)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    2 0.6317  0.3158   27.73  0.00093 ***\nblock        3 3.0492  1.0164   89.24 2.28e-05 ***\nResiduals    6 0.0683  0.0114                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n처리에 의한 변동이 통계적으로 유의미하다.\nblock에 의한 변동 역시 통계적으로 유의미하다.\n\n\nc\n\ndf.anova &lt;- aov(value~treatment, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ntreatment    2 0.6317  0.3158   0.912  0.436\nResiduals    9 3.1175  0.3464               \n\n\n처리에 의한 변동이 통계적으로 유의미하지 않다고 나온다.\nblock에 의한 변동이 통계적으로 유의미하기 때문에, block에 의한 변동을 제거하고 처리에 의한 변동을 분석해야 하기 때문이다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/06.html#simple-linear-regression",
    "href": "posts/01_projects/bs_3_1/notes/statistics/06.html#simple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\na\n\nx &lt;- runif(100, 0, 10)\nerror &lt;- rnorm(100, 0, 4)\ny &lt;- 3 * x - 2 + error\nmodel &lt;- lm(y ~ x)\nmodel\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -2.248        2.933  \n\n\n결과는 다음과 같이 나온다.\n\n\nb\n\ncat(\"B1의 추정 기울기: \", coef(model)[2])\n\nB1의 추정 기울기:  2.933282\n\ntrust &lt;- confint(model, level=0.95)[2, ]\ncat(\"신뢰구간: \", trust, \"\\n\")\n\n신뢰구간:  2.652931 3.213632 \n\nif (3 &gt; trust[1] && 3 &lt; trust[2]) \n{\n  print(\"신뢰구간에 3 포함\")\n} else \n{\n  print(\"신뢰구간에 3 안 포함\")\n}\n\n[1] \"신뢰구간에 3 포함\"\n\n\n결과는 다음과 같이 나온다.\n\n\nc\n\nplot(x, y, cex=0.7)\nabline(model, col = \"red\", lwd = 2)\nabline(-2, 3, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"실제 회귀선\", \"추정 회귀선\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n결과는 다음과 같이 나온다.\n\n\nd\n\ncnt &lt;- 0\nfor (i in 1:100) \n{\n  x &lt;- runif(100, 0, 10)\n  error &lt;- rnorm(100, 0, 4)\n  y &lt;- 3 * x - 2 + error\n  model &lt;- lm(y ~ x)\n  trust &lt;- confint(model, level=0.95)[2, ]\n  cnt &lt;- cnt + (3 &gt; trust[1] && 3 &lt; trust[2])\n}\ncnt\n\n[1] 98\n\n\n95에 가깝게 나온다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/06.html#multiple-linear-regression",
    "href": "posts/01_projects/bs_3_1/notes/statistics/06.html#multiple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\na\n\ndata(iris)\nmodel &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, data = iris)\nsummary(model)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88045 -0.20945  0.01426  0.17942  0.78125 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.04309    0.27058   3.855 0.000173 ***\nSepal.Length  0.60707    0.06217   9.765  &lt; 2e-16 ***\nPetal.Length -0.58603    0.06214  -9.431  &lt; 2e-16 ***\nPetal.Width   0.55803    0.12256   4.553  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3038 on 146 degrees of freedom\nMultiple R-squared:  0.524, Adjusted R-squared:  0.5142 \nF-statistic: 53.58 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nF 값의 p-value가 매우 작으므로, 모델이 통계적으로 유의미하다고 할 수 있다.\n각각의 독립변수에 대한 p-value 역시 모두 매우 작으므로, 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\n하지만 r-squared 값이 크지 않아서 모델이 종속변수의 변동을 잘 설명하지 못한다고 할 수 있다.\n\n\nb\n\nplot(model, 1)\n\n\n\n\n\n\n\n\n잔차가 U자 패턴을 보이므로, 선형성이 만족되지 않는 것 같다.\n\n\nc\n\nmodel2 &lt;- lm(Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), data = iris)\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + \n    Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82277 -0.16843 -0.00315  0.15300  0.77761 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.10783    1.37460  -2.261 0.025275 *  \nSepal.Length       2.32243    0.50141   4.632 8.08e-06 ***\nI(Sepal.Length^2) -0.15699    0.04264  -3.682 0.000327 ***\nPetal.Length      -0.89155    0.20300  -4.392 2.17e-05 ***\nI(Petal.Length^2)  0.06925    0.02336   2.965 0.003549 ** \nPetal.Width       -0.03666    0.38214  -0.096 0.923708    \nI(Petal.Width^2)   0.13095    0.10529   1.244 0.215648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2755 on 143 degrees of freedom\nMultiple R-squared:  0.6167,    Adjusted R-squared:  0.6006 \nF-statistic: 38.35 on 6 and 143 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nd\nSepal.Length, \\(\\text{Sepal.Length}^2\\), Petal.Length의 p-value가 모두 0.05보다 작으므로, 이 변수들이 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\nr-squared 값도 더 작아져서, c 모델이 b 모델보다 종속변수의 변동을 잘 설명한다고 할 수 있다.\n\n\ne\n\nplot(model2, 1)\n\n\n\n\n\n\n\n\nU자 패턴이 완만해진걸로 보인다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/statistics/05.html",
    "href": "posts/01_projects/bs_3_1/notes/statistics/05.html",
    "title": "Analysis of categorical data",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Analysis of categorical data"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/18.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/18.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "18"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "href": "posts/01_projects/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "title": "네트워크 최적화 모형",
    "section": "3. 최단 경로 문제",
    "text": "3. 최단 경로 문제\n\n그냥 다익스트라",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "href": "posts/01_projects/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "title": "네트워크 최적화 모형",
    "section": "5. 최대 흐름 문제 (augmenting path method)",
    "text": "5. 최대 흐름 문제 (augmenting path method)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "href": "posts/01_projects/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "title": "네트워크 최적화 모형",
    "section": "7. 네트워크 심플렉스 해법",
    "text": "7. 네트워크 심플렉스 해법\n모든 실행가능 해는 n-1개의 기저변수를 가지고, spanning tree를 형성한다.\n\n최대흐름문제 심플렉스 (9.7.2) 이거 어케 품\nmingamdo variable add?",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "href": "posts/01_projects/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer를 둘 수 없는 상황: 병원외상센터",
    "text": "buffer를 둘 수 없는 상황: 병원외상센터\n\n\n대기해야 할 상황이 있으면 다른 병원으로 이동\n\ndiversion 상태, loss(service를 못 받음)\n\n\n\nDiversion 상태 확률\n\nD &lt; C 가정하지 않음\n도착 간격은 지수분포 가정 (processing time 분포는 가정 안함)\n대기하지 않고 바로 이탈한다고 가정\n\\(P_m\\): 내재활용률과 자원의 수에 의해 결정됨\n\\(r = um = \\frac{p}{a}\\), 해야하는 일의 양을 의미\n\n단위: Erlang\n\n\n\n\n\nErlang Loss Table\n\n\n\n들어온 인원(흐름률): \\(\\frac{1}{a}(1 - P_m(r))\\)\n안 들어온 인원: \\(\\frac{1}{a}P_m(r)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/13.html#방법",
    "href": "posts/01_projects/bs_3_1/notes/product/13.html#방법",
    "title": "일정 계획",
    "section": "방법",
    "text": "방법\n\nFCFS\nLCFS\nSPT (Shortest Processing Time)\nLPT (Longest Processing Time)\nEDD (Earliest Due Date)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/product/13.html#평가-척도",
    "href": "posts/01_projects/bs_3_1/notes/product/13.html#평가-척도",
    "title": "일정 계획",
    "section": "평가 척도",
    "text": "평가 척도\n\nmakespan: 모든 작업이 완료되는 시간\n\n전체 소요시간은 달라지지 않음.\nsequential dependent한 set up이 있는 경우는 달라짐\n\nlateness: \\(C_i - d_i\\) (작업이 완료된 시간 - 작업의 마감 시간)\ntardiness: 지연된 시간\nearliness: 일찍 완료된 시간\nflow time: \\(C_i - r_i\\) (작업이 완료된 시간 - 작업이 시작된 시간)\n흐름률: \\(\\frac{작업량}{makespan}\\)\n\n제고와 flow time을 제일 많이 줄이는 것은 SPT rule\n\n정확한 processing time을 알아야 하고\nprocessing time의 예측은 편향될 수 있고\n공정성 문제가 있을 수 있다.\n\nweighted SPT Rule: \\(\\frac{weight}{processing time}\\)이 높은 작업을 우선적으로 처리",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "title": "association rule mining",
    "section": "Pattern minig",
    "text": "Pattern minig\n\nBasic Concepts\n\npattern: dataset 안에서 함께 자주 발생하는 subsequences, substructures, set of items\n\n이 pattern은 인과관계를 의미하진 않는다.\n\nAssociation rule minig: 최소 지지도나 신뢰도를 넘는 모든 항목에 대해 pattern을 찾는다.\n\n\n\nApplications\n\nassociation rule, correlation, classification, clustering data mining의 기반이 될 수 있다.\n장바구니 분석\n연속 구매 분석\n\n\n\nTerminologies\n\n지지도(Support): 전체 거래 중 특정 항목 집합이 포함된 거래의 비율.\n신뢰도(Confidence): 항목 X를 포함하는 거래 중에서 항목 Y도 함께 포함하는 거래의 비율.\n빈발 패턴(frequent): 최소 지지도를 넘는 pattern\n\n빈발 항목 집합(frequent itemset): 단순한 묶음\n빈발 시퀀스\n\n\n\n\nclosed pattern\n\nx가 빈발이고, 지지도가 상위 집합들과 다른 집합 (지지도는 상위로 갈 수록 떨어짐)\n지지도 정보를 유지할 수 있다.\n\n신뢰도 계산할 때 사용할 수 있음\n\n\n\n\nmax patterns\n\nx가 빈발이고, 상위 집합들 모두가 빈발 집합이 아닌 집합\n지지도 정보는 유지되지 않음.\n\n신뢰도 계산할 때 사용할 수 없어서 사실 상 결과 요약 외의 용도는 없음\n\nDownward closure property: 어떤 itemset이 빈발하지 않으면, 그 모든 superset은 무조건 빈발하지 않는다. 대우도 성립\n\n교수님은 anti-monotone property로 설명하셨지만 이게 더 자주 사용되는 용어",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/05.html#association-rule",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/05.html#association-rule",
    "title": "association rule mining",
    "section": "Association Rule",
    "text": "Association Rule\n\nfind frequent itemsets\n\nApriori (breadth-first search)\nFP-Growth\nEclat (depth-first search)\n\ngenerate association rules\n\n모든 빈발 itemset I에 대해 모든 I의 subset s로 ‘s -&gt; (I - s)’ 규칙을 생성\n최소 신뢰도 조건을 만족하는 규칙만 남김",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/05.html#algorithm",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/05.html#algorithm",
    "title": "association rule mining",
    "section": "Algorithm",
    "text": "Algorithm\n\nApirori\n\nMonotone 성질을 이용하여 빈발하지 않는 집합은 후보에서 제거\n\n\nscan DB once to get 1-itemsets\n반복\n\nk개의 itemset에 대해 k+1-itemset의 후보를 생성\n\nself-join: k-itemset을 두 개 합쳐서 k+1-itemset을 생성\nprune: k+1-itemset을 생성할 때, k-itemset의 subset이 모두 빈발해야 k+1-itemset이 빈발할 수 있다.\n\nk+1-itemset 후보에 대해 DB를 scan하여 빈발한 itemset을 찾는다.\nk += 1\n빈발 itemset이 없으면 종료\n\n\n\nApriori의 단점: DB scan을 여러 번 해야함, 후보가 많아질 수 있음\n\n후보 수를 줄이는 방법: Hashing\n\n\n\n\nDHP (Direct Hashing and Pruning)\n\nHash값이 같은 itemset의 count를 합하고, minimum support를 넘는 itemset만 남김\nHash table을 매번 만드는 번거로움이 있지만, apriori보다 빠름\n반복\n\n빈발 항목 찾기, 후보 해시 테이블 생성\n\n데이터베이스를 scan하여 최소 지지도를 넘는 1-itemset 후보를 찾음\n동시에 조합 가능한 2-itemset을 만들어 mapping된 해시 테이블 bucket에 count += 1\n\n가지치기\n\n1-itemset 후보를 이용해 self-join, prune으로 2-itemset 후보 생성\n완성된 후보를 1단계에서 만든 hash table에 매핑해서 최소 지지도를 넘는 2-itemset bucket이 아닐 경우 배제",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/01.html#scaling",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/01.html#scaling",
    "title": "데이터 전처리",
    "section": "scaling",
    "text": "scaling\n\nmin-max scaling: \\(x' = \\frac{x - min(x)}{max(x) - min(x)}\\)\n\n0과 1 사이의 값으로 변환\n\nstandardization: \\(x' = \\frac{x - \\mu}{\\sigma}\\), \\(\\mu\\): 평균, \\(\\sigma\\): 표준편차",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "title": "데이터 전처리",
    "section": "Train Test Resampling",
    "text": "Train Test Resampling\n\nresampling\n\nresampling: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\ntrain set: 모델을 학습하는 데이터\nvalidation data set: 하이퍼파라미터를 튜닝\ntest set: 모델의 성능 평가\n층화추출법: stratified sampling\n\n\n\nmethods\n\nhold out method: data를 train set과 test set으로 나누는 방법\n\n어떤 데이터가 train, test set에 포함되는지에 따라 결과가 달라질 수 있다.\n→ train set과 test set을 여러 번 나누어 모델을 학습하고 평가하는 방법이 필요하다.\n\ncross validation: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\n\nk-fold cross validation: 데이터를 k개의 fold로 나누고, 각 fold를 test set으로 사용하여 모델을 학습하고 평가한다.\n\nrandom하게 sampling할 수 있고, 안할 수도 있음\n\nleave-one-out cross validation: 데이터의 개수가 n개일 때, n-1개의 데이터를 train set으로 사용하고 1개의 데이터를 test set으로 사용하는 방법\nerror는 각 train set, fold 에서 계산된 error의 평균으로 구한다.\n\nbootstrap: 데이터를 중복을 허용하여 샘플링하는 방법\n\n원본 데이터에서 n개의 데이터를 랜덤하게 선택하여 train set을 만들고, 나머지 데이터를 test set으로 사용한다.\n여러 번 반복하여 모델을 학습하고 평가한다.\n실제 오류 추정치의 편향과 분산 모두에 대한 정확한 측정값을 얻을 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/07.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/07.html",
    "title": "ensemble",
    "section": "",
    "text": "다수의 모델을 학습시켜 결과를 종합하여 예측 성능을 높이는 방법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/07.html#techniques",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/07.html#techniques",
    "title": "ensemble",
    "section": "Techniques",
    "text": "Techniques\n\nstacking:\n\n여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.\n메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.\n\nbagging\n\nvs cross validation:\n\ncross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님\nbagging은 분산을 줄이기 위해 사용함\n\nbagging의 voting, averaging은 unsupervised learning\n\nboosting: sequentially 학습\n\n이전 모델의 오차를 보완하는 방식으로 학습한다.\nsupervised learning",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/04.html",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/04.html",
    "title": "Support vector machine",
    "section": "",
    "text": "bias variance tradeoff\n\nmargin ↑, bias ↑, variance ↓\nmargin ↓, bias ↓, variance ↑\n\nsuppot vector: 임계값에 가까운 데이터 포인트\nlinear classification fomulation:\n\n\n\n\nif not linear solvable → kernel functions\nsoft margin: 이상치를 포함할 수 있는 마진\n\ncross validation으로 최적의 soft margin을 찾는다.\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Support vector machine"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#k-nn",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#k-nn",
    "title": "분류",
    "section": "K-NN",
    "text": "K-NN\n\n새로운 데이터 포인트에 대해 k개의 가장 가까운 이웃을 찾고, 그 이웃들의 클래스를 투표하여 다수결로 분류한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "title": "분류",
    "section": "의사결정 트리",
    "text": "의사결정 트리\n불순도가 가장 낮은(한쪽의 class가 더 많은) leaves를 root에 두고, 그 다음 불순도가 낮은 leaf를 그 아래에 두는 방식으로 트리를 구성한다. leaf 노드의 과반수가 같은 클래스를 가지면 그 클래스를 리턴한다. overfit을 방지하기 위해 pruning을 하거나 max depth를 설정한다.\n\ngood split: 불순도가 낮고, 분할된 각 leave의 비율이 비슷한 경우\n\n\n과정\n\n루트 노드에서 시작전체 데이터셋을 기준으로 시작하여 가장 좋은 분할속성(feature)을 선택\n분할 기준 평가각 속성에 대해 데이터를 분할했을 때의 분할 평가함수 적용\n\ngini index\n\ngini: \\(1 - \\sum_{i=1}^{c} p_i^2\\).\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nmisclassification error\n\nerror: \\(1 - \\max(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nentropy: 유용하지 못한 정보를 포함하고 있는 정도.\n\nentropy: \\(-\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 1\n\ninformation gain: 어떤 속성을 기준으로 분할했을 때, 얻을 수 있는 불확실성의 감소량\n\n부모의 엔트로피 - 자식의 엔트로피 가중평균\n목표: 최대화\n단점: 고유한 값을 많이 갖는 변수를 선호하는 경향이 있음\n알고리즘: ID3, C4.5\n\ngain ratio: information gain의 단점을 보완한 방법\n\ngain ratio: \\(\\frac{information\\ gain}{entropy}\\)\n분기의 갯수와 각 분기의 크기를 함께 고려\n목표: 최대화\n단점: entropy가 낮은 것을 선택하는 경향이 있음\n\n→ 평균이 넘는 entorpy만 선택해서 gain ratio를 계산\n\n알고리즘: C4.5\n\n\n최적의 분할 선택 평가된 기준 중 가장 순도가 높은 점수를 갖는 속성을 선택 (greedy)\n재귀적으로 하위 노드 분할. 분할된 하위 데이터에 대해 위의 과정을 반복\n종료 조건 만족 시 정지\n가지치기 수행\n\n\n\nAlgorithm\n\nCART\n\n분할 기준\n\n분류: gini index\n회귀: MSE\n\n모든 분할에서 이진 트리로 분할\n사후 가지치기\n\n비용 복잡도 가지치기: \\(Total SSR + α(leaf size)\\)이 제일 작은 트리 선택\nα는 cross validation으로 결정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#평가",
    "title": "분류",
    "section": "평가",
    "text": "평가\n\nconfusion matrix\n\nTP, TN, FP, FN\n\naccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nprecision(정밀도): true로 예측한 것 중 실제 true인 것의 비율\n\n\\(\\frac{TP}{TP + FP}\\)\n\nrecall(민감도, 재현율): 실제 true인 것 중 true로 예측한 것의 비율\n\n\\(\\frac{TP}{TP + FN}\\)\n\nF1 score: \\(2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\)\nmacro 평균: 그냥 각 클래스의 score를 평균\nweighted 평균: 가중 평균\nmicro 평균: 전체 TP, TN, FP, FN을 합쳐서 계산\nROC curve: y축: TPR(민감도), x축: FPR(1 - 특이도)\n\nAUC: ROC curve 아래 면적\n\n1에 가까울수록 좋은 모델\n0.5는 랜덤 추측과 같음\n\n\n(1,1): 무작위 추측과 동일. 모든 샘플을 무조건 양성으로 예측\n(0,0): 모든 샘플을 무조건 음성으로 예측\n(1,0): 완벽한 모델\n(0,1): 반대로 예측하는 모델",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/02.html#learning-curve",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/02.html#learning-curve",
    "title": "분류",
    "section": "learning curve",
    "text": "learning curve\n\n\nsample 수가 적으면 정확도와 신뢰 구간에 부정적 영향을 미침\ntest set에 대한 오류가 증가하는 구간(cross 되는 구간)은 과적합 구간. 여기서 stop",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/06.html#전제-조건",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/06.html#전제-조건",
    "title": "clustering",
    "section": "전제 조건",
    "text": "전제 조건\n\nscalability\n다양한 타입의 속성을 처리해야 함\n\nk-means는 수치형만 처리 가능\n\n인위적인 형상의 군집도 발견할 수 있어야 함\n\nk-means는 non-convex 형태는 잘 못찾음\n\n파라미터 설정에 전문지식을 요하지 않아야함\nnoise와 outliers를 처리해야 함\n데이터가 입력되는 순서에 민감하면 안됨\n차원 수가 높아도 잘 처리할 수 있어야함\n사용자 정의 제약조건도 수용할 수 있어야함\n해석과 사용이 용이해야함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/06.html#전처리",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/06.html#전처리",
    "title": "clustering",
    "section": "전처리",
    "text": "전처리\n\nscaling 필요\none hot encoding",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/06.html#model",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/06.html#model",
    "title": "clustering",
    "section": "model",
    "text": "model\n\nDistance-based methods\n\nPartitioning methods\n\nk-means1:\n\npolinominal 시간 안에 해결 가능\nnoise, outlier에 민감함\n수치형만 처리 가능\nnon-convex 형태는 잘 못찾음\n\nk-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\nk-prototype: 범주형, 수치형 섞인거 처리 가능\nk-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n\nPAM: Partitioning Around Medoids\n\nscalability 문제 있음\n\nCLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n\n샘플링 과정에서 biased될 수 있음\n\nCLARANS: medoid 후보를 랜덤하게 선택함\n\nk-means++: 초기 centroids를 더 잘 잡음\n\nHierarchical methods\n\ntop-down: divisive, dia\nbottom-up: agglomerative\n\nward’s distance: 군집 간의 거리 계산을 군집 내의 분산을 최소화하는 방식으로 계산\n\nESS: 각 군집의 중심으로 부터의 거리 제곱합\n\n\n\n\nDensity-based methods\n\n다양한 모양의 군집을 찾을 수 있음\nnoise, outlier에 강함\nDBSCAN: 잡음 포인트는 군집에서 제외\n\ncore point를 찾음(eps 이내에 minPts 이상 있는 점)\ncore point를 중심으로 군집을 확장\n\ncore point가 아닌 경우 확장 종료\n\n\n\n고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n군집간 계층관계를 인식하기 어렵다\n\nOPTICS: DBSCAN의 단점을 보완\n\n군집의 밀도가 다를 때도 잘 처리함\n군집의 계층 구조를 인식할 수 있음\neps, minPts 파라미터가 필요함\n\n\nGrid-based methods: 대표만(각 grid를 대표) 가지고 군집분석 하는거\n\n속도와 메모리 측면에서 효율적\n\nModel-based clustering methods\n\n\n거리기반 군집의 단점:\n\n군집의 모양이 구형이 아닐 경우 찾기 어려움\n군집의 갯수 결정하기 어려움\n군집의 밀도가 높아야함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/06.html#평가",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/06.html#평가",
    "title": "clustering",
    "section": "평가",
    "text": "평가\n\nsilhuette score: \\(\\frac{\\sum_{i=1}^{n} s(i)}{n}\\)\n\ns(i): \\(\\frac{b(i) - a(i)}{max((a(i), b(i)))}\\)\n\na(i): 군집 내 노드간의 평균 거리\nb(i): 가장 가까운 군집과의 노드 간 평균 거리\n\n1에 가까울 수록 좋음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/06.html#footnotes",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/06.html#footnotes",
    "title": "clustering",
    "section": "각주",
    "text": "각주\n\n\n장단점 기말고사 언급하심↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/08.html#회귀",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/08.html#회귀",
    "title": "XGBoost",
    "section": "회귀",
    "text": "회귀\n\nsimilarity score: \\(\\frac{(sum of residuals)^2}{number of residuals + λ}\\)\nλ를 높이면 잔차의 수에 따른 민감도를 완화할 수 있다. overfitting 방지\ngain: \\(Left_{similarity} + Right_{similarity} - Parent_{similarity}\\)\ngain값이 높은 속성을 선택\ngain 값이 특정 γ보다 낮으면 가지치기\nleaf value: \\(\\frac{sum of residuals}{number of residuals + λ}\\)\n예측값 갱신: \\(기존 값 + learning rate(eta) * leaf value\\)로 예측",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "XGBoost"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/08.html#분류",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/08.html#분류",
    "title": "XGBoost",
    "section": "분류",
    "text": "분류\n\nsimilarity score: \\(\\frac{(sum of residuals)^2}{∑_{i=1}^{n}(previous probability_i * (1 - previous probability_i)) + λ}\\)\nleaf value: \\(\\frac{sum of residuals}{∑_{i=1}^{n}(previous probability_i * (1 - previous probability_i)) + λ}\\)\n예측값 갱신: log(odds) = \\(log \\frac{p}{1 - p} + learning rate * leaf value\\)\n\nnew probability = \\(\\frac{e^{log(odds)}}{1 + e^{log(odds)}}\\)\n\n잔차는 점점 0에 수렴하고, probability는 실제 값(1, 0)에 수렴한다.\nmin_child_weight: leaf node의 최소 가중치 합(\\(∑_{i=1}^{n} (previous probability * (1 - previous probility\\))이 이 값보다 작으면 분할하지 않음",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "XGBoost"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/03.html#장점",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/03.html#장점",
    "title": "random forest",
    "section": "장점",
    "text": "장점\n\nClassification, Regression문제 모두 해결 가능\nAccuracy, out-of-bag error에 우수한 결과\nValidation을 위한 별도의 data set이 필요하지 않음\nBuilt-in validation set\nOverfitting이 없다\nOutlier에 강함\nMissing data를 잘 처리\n선처리 작업을 최소화\nFeature의 선택을 자동처리\n변수 삭제 없이 수천 개의 입력 변수를 처리",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_1/notes/data_mining/03.html#단점",
    "href": "posts/01_projects/bs_3_1/notes/data_mining/03.html#단점",
    "title": "random forest",
    "section": "단점",
    "text": "단점\n\n속도가 느림\n해석이 어렵다",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/01.html",
    "href": "posts/01_projects/adp_실기/notes/01.html",
    "title": "pandas data 구조",
    "section": "",
    "text": "pandas: numpy를 라벨링한거",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "코드 snippet"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/01.html#before",
    "href": "posts/01_projects/adp_실기/notes/01.html#before",
    "title": "pandas data 구조",
    "section": "Before",
    "text": "Before\n데이터를 호출하고, 데이터 내용과 요약 / 통계 정보를 확인해야함\n칼럼명이 칼럼 타입을 변경해야할 때도 있음\n\nPandas 사용 준비\n\n라이브러리 설치\n라이브러리 호출\n\n\nimport pandas as pd\n\npd.set_option('display.max_rows', 10)\n\n\n\nDataFrame 선언\n\nimport numpy as np\ndataset = np.array([['kor', 70], ['math', 80]])\n# declare df 1\ndf = pd.DataFrame(dataset, columns=['class', 'score'])\n# declare df 2\ndf = pd.DataFrame([['kor', 70], ['math', 80]], columns=['class', 'score'])\n# declare df 3\ndf = pd.DataFrame({'class': ['kor', 'math'], 'score': [70, 80]})\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\n\nDataFrame 읽고 저장\n\n# filepath = '../book/data/data.csv'\n# data = pd.read_csv(filepath, na_values='NA', encoding='utf8')\n# data.to_csv('result.csv', header=True, index=True, encoding='utf8')\n\n\n\nDataFrame 출력\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.8 KB\n\n\n\niris.describe()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\nsepal length와 petal width의 값의 차이가 크다.\n전처리 과정에서 변수 정규화 수행의 근거가 된다.\n\n\nindex / column 명 변경\n\ndf.index\n\nRangeIndex(start=0, stop=2, step=1)\n\n\n\nlist(df.index)\n\n[0, 1]\n\n\n\ndf.index = ['A', 'B']\ndf.index\n\nIndex(['A', 'B'], dtype='object')\n\n\n\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\nA\nkor\n70\n\n\nB\nmath\n80\n\n\n\n\n\n\n\n\ndf.set_index('class', drop=True, append=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nscore\n\n\nclass\n\n\n\n\n\nkor\n70\n\n\nmath\n80\n\n\n\n\n\n\n\n\ndf.reset_index(drop=False, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n\n\n\n\n\n\niris.columns\n\nIndex(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)'],\n      dtype='object')\n\n\n\niris.columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\niris\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris.columns = iris.columns.str.replace(' ', '_')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\n데이터 타입 변경\n사용 가능한 타입\n\nint\nfloat\nbool\ndatetime\ncategory\nobject\n\n\niris.dtypes\n\nsepal_length    float64\nsepal_width     float64\npetal_length    float64\npetal_width     float64\ndtype: object\n\n\n\niris['sepal_length'] = iris['sepal_length'].astype('int')\niris[['sepal_width', 'petal_length']] = \\\niris[['sepal_width', 'petal_length']].astype('int')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n4\n5\n3\n1\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6\n3\n5\n2.3\n\n\n146\n6\n2\n5\n1.9\n\n\n147\n6\n3\n5\n2.0\n\n\n148\n6\n3\n5\n2.3\n\n\n149\n5\n3\n5\n1.8\n\n\n\n\n150 rows × 4 columns",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "코드 snippet"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "href": "posts/01_projects/adp_실기/notes/01.html#row-coumn-선택-추가-삭제",
    "title": "pandas data 구조",
    "section": "row / coumn 선택 추가 삭제",
    "text": "row / coumn 선택 추가 삭제\n\nrow 선택\n\niris[0:4]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5\n3\n1\n0.2\n\n\n1\n4\n3\n1\n0.2\n\n\n2\n4\n3\n1\n0.2\n\n\n3\n4\n3\n1\n0.2\n\n\n\n\n\n\n\n\n\ncolumn 선택\nSeries 형식으로 출력\n\niris['sepal_length']\n\n0      5\n1      4\n2      4\n3      4\n4      5\n      ..\n145    6\n146    6\n147    6\n148    6\n149    5\nName: sepal_length, Length: 150, dtype: int64\n\n\nDataFrame 형식으로 출력\n\niris[['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n...\n...\n...\n\n\n145\n6\n3\n\n\n146\n6\n2\n\n\n147\n6\n3\n\n\n148\n6\n3\n\n\n149\n5\n3\n\n\n\n\n150 rows × 2 columns\n\n\n\n\n\ncolumn, row 선택\n\niris.loc[0:4, ['sepal_length', 'sepal_width']]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\n\n0\n5\n3\n\n\n1\n4\n3\n\n\n2\n4\n3\n\n\n3\n4\n3\n\n\n4\n5\n3\n\n\n\n\n\n\n\n\niris.iloc[0:4, [1, 2]]\n\n\n\n\n\n\n\n\nsepal_width\npetal_length\n\n\n\n\n0\n3\n1\n\n\n1\n3\n1\n\n\n2\n3\n1\n\n\n3\n3\n1\n\n\n\n\n\n\n\n\n\nrow 추가\n\n# 방법 1: concat 사용\n# df = pd.concat([df, pd.DataFrame([{'class': 'eng', 'score': 90}])], ignore_index=True)\n\n# 방법 2: loc 사용 \ndf.loc[len(df)] = {'class': 'eng', 'score': 90}\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80\n\n\n2\neng\n90\n\n\n\n\n\n\n\n\n\ncolumn 추가\n\ndf['yo'] = df['score'] + 10\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n2\neng\n90\n100\n\n\n\n\n\n\n\n\n\nrow 삭제\n\ndf.drop(2, inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\nyo\n\n\n\n\n0\nkor\n70\n80\n\n\n1\nmath\n80\n90\n\n\n\n\n\n\n\n\n\ncolumn 삭제\n\ndf.drop(columns=['yo'], inplace=True)\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n\n\n\n\n0\nkor\n70\n\n\n1\nmath\n80",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "코드 snippet"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/01.html#조건-선택",
    "href": "posts/01_projects/adp_실기/notes/01.html#조건-선택",
    "title": "pandas data 구조",
    "section": "조건 선택",
    "text": "조건 선택\n\niris[(iris['sepal_length'] &gt; 5) & (iris['sepal_width'] &lt; 3)]\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n54\n6\n2\n4\n1.5\n\n\n58\n6\n2\n4\n1.3\n\n\n62\n6\n2\n4\n1.0\n\n\n63\n6\n2\n4\n1.4\n\n\n68\n6\n2\n4\n1.5\n\n\n...\n...\n...\n...\n...\n\n\n130\n7\n2\n6\n1.9\n\n\n132\n6\n2\n5\n2.2\n\n\n133\n6\n2\n5\n1.5\n\n\n134\n6\n2\n5\n1.4\n\n\n146\n6\n2\n5\n1.9\n\n\n\n\n29 rows × 4 columns\n\n\n\n\ndf.loc[df['score'] &gt; 70, '합격'] = 'Pass'\ndf.loc[df['합격'] != 'Pass', '합격'] = 'Fail'\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\n\n\n\n\n0\nkor\n70\nFail\n\n\n1\nmath\n80\nPass\n\n\n\n\n\n\n\n\nimport numpy as np\n\ncondition_list = [(df['score'] &gt;= 70), \n                  (df['score'] &lt; 70) & (df['score'] &gt;= 60),\n                  (df['score'] &lt; 60)]\ngrade_list = ['A', 'B', 'C']\ndf['grade'] = np.select(condition_list, grade_list, default='F')\ndf\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n결측치 탐색\n\ndf.isna().sum()\n\nclass    0\nscore    0\n합격       0\ngrade    0\ndtype: int64\n\n\n\ndf.notna().sum(1) # 행 기준\n\n0    4\n1    4\ndtype: int64\n\n\n\n\n결측치 제거\n\n# dropna(axis=0, how='any' or 'all', thresh=None, subset=None, inplace=False)\ndf.dropna()\n\n\n\n\n\n\n\n\nclass\nscore\n합격\ngrade\n\n\n\n\n0\nkor\n70\nFail\nA\n\n\n1\nmath\n80\nPass\nA\n\n\n\n\n\n\n\n\n\n결측치 대체\n\n# fillna(value=None, method=None ('pad', 'ffill', 'backfill', 'bfill'), axis=None, inplace=False, limit=None)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "코드 snippet"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#데이터-전처리의-의미",
    "href": "posts/01_projects/adp_실기/notes/03.html#데이터-전처리의-의미",
    "title": "데이터 전처리",
    "section": "데이터 전처리의 의미",
    "text": "데이터 전처리의 의미\n\n데이터 클리닝\n데이터 통합\n데이터 변환\n데이터 축소\n불균형 데이터 처리\n데이터 분할",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#이상치-확인-및-정제",
    "href": "posts/01_projects/adp_실기/notes/03.html#이상치-확인-및-정제",
    "title": "데이터 전처리",
    "section": "이상치 확인 및 정제",
    "text": "이상치 확인 및 정제\n\n이상치 확인\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.core.common import random_state\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine['class'] = wine_load.target\nwine['class'] = wine['class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nplt.boxplot(wine['color_intensity'], whis=1.5)\nplt.title('Boxplot of color_intensity')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef outliers_iqr(dt, col):\n    q1, q3 = np.percentile(dt[col], [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return dt[(dt[col] &lt; lower_bound) | (dt[col] &gt; upper_bound)]\n\noutliers = outliers_iqr(wine, 'color_intensity')\noutliers\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nclass\n\n\n\n\n151\n12.79\n2.67\n2.48\n22.0\n112.0\n1.48\n1.36\n0.24\n1.26\n10.80\n0.48\n1.47\n480.0\nclass_2\n\n\n158\n14.34\n1.68\n2.70\n25.0\n98.0\n2.80\n1.31\n0.53\n2.70\n13.00\n0.57\n1.96\n660.0\nclass_2\n\n\n159\n13.48\n1.67\n2.64\n22.5\n89.0\n2.60\n1.10\n0.52\n2.29\n11.75\n0.57\n1.78\n620.0\nclass_2\n\n\n166\n13.45\n3.70\n2.60\n23.0\n111.0\n1.70\n0.92\n0.43\n1.46\n10.68\n0.85\n1.56\n695.0\nclass_2\n\n\n\n\n\n\n\n\n\n이상치 정제\n\n이상치 제거\n\n\ndrop_outliers = wine.drop(index=outliers.index)\n\nprint(\"Original:\", wine.shape)\nprint(\"Drop outliers:\", drop_outliers.shape)\n\nOriginal: (178, 14)\nDrop outliers: (174, 14)\n\n\n\n이상치 대체\n\n이상치를 NULL로 만든 후, 결측치와 함께 대체\n\nwine.loc[outliers.index, 'color_intensity'] = np.NaN\n\nwine['color_intensity'].fillna(wine['color_intensity'].mean(), inplace=True)\nwine.loc[outliers.index, 'color_intensity']\n\n151    4.908678\n158    4.908678\n159    4.908678\n166    4.908678\nName: color_intensity, dtype: float64",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#범주형-데이터-처리",
    "href": "posts/01_projects/adp_실기/notes/03.html#범주형-데이터-처리",
    "title": "데이터 전처리",
    "section": "범주형 데이터 처리",
    "text": "범주형 데이터 처리\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris = pd.DataFrame(iris.data, columns=iris.feature_names)\niris['Class'] = load_iris().target\niris['Class'] = iris['Class'].map({0: 'Setosa', \n                                   1:'Versicolour', \n                                   2: 'Virginica'})",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#데이터-분할",
    "href": "posts/01_projects/adp_실기/notes/03.html#데이터-분할",
    "title": "데이터 전처리",
    "section": "데이터 분할",
    "text": "데이터 분할\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(iris.drop(\n  columns='Class'), iris['Class'], test_size=0.2, random_state=1004)\nprint('X_train: ', X_train.shape, 'X_test: ', X_test.shape)\nprint('y_train: ', y_train.shape, 'y_test: ', y_test.shape)\n\nX_train:  (120, 4) X_test:  (30, 4)\ny_train:  (120,) y_test:  (30,)\n\n\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n87\n6.3\n2.3\n4.4\n1.3\n\n\n67\n5.8\n2.7\n4.1\n1.0\n\n\n131\n7.9\n3.8\n6.4\n2.0\n\n\n\n\n\n\n\n\ny_train.head(3)\n\n87     Versicolour\n67     Versicolour\n131      Virginica\nName: Class, dtype: object\n\n\n\niris['Class'].value_counts()\n\nClass\nSetosa         50\nVersicolour    50\nVirginica      50\nName: count, dtype: int64\n\n\n\ny_train.value_counts()\n\nClass\nVersicolour    41\nSetosa         40\nVirginica      39\nName: count, dtype: int64",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#데이터-스케일링",
    "href": "posts/01_projects/adp_실기/notes/03.html#데이터-스케일링",
    "title": "데이터 전처리",
    "section": "데이터 스케일링",
    "text": "데이터 스케일링\n\nStandard Scaler\n\n평균이 0, 분산이 1이 되도록 변환\n이상치에 민감하다.\n회귀분석보다는 분류분석에 적합\n\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\nStdScaler = StandardScaler()\n\nStdScaler.fit(X_train)\nX_train_sc = StdScaler.transform(X_train)\nX_test_sc = StdScaler.transform(X_test)\n\n\n\nMin-Max Scaler\n\n0 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nMinMaxScaler = MinMaxScaler()\n\nMinMaxScaler.fit(X_train)\nX_train_sc = MinMaxScaler.transform(X_train)\n\nX_test_sc = MinMaxScaler.transform(X_test)\n\n\n\nMax Abs Scaler\n\n-1 ~ 1 사이의 값으로 변환\n이상치에 민감하다.\n회귀분석에 적합\n\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nMaxAbsScaler = MaxAbsScaler()\n\nMaxAbsScaler.fit(X_train)\nX_train_sc = MaxAbsScaler.transform(X_train)\n\nX_test_sc = MaxAbsScaler.transform(X_test)\n\n\n\nRobust Scaler\n\n중앙값을 0으로 설정하고, IQR을 사용하여 잉상치 영향을 최소화함\n\n\nfrom sklearn.preprocessing import RobustScaler\n\nRobustScaler = RobustScaler()\n\nRobustScaler.fit(X_train)\nX_train_sc = RobustScaler.transform(X_train)\n\nX_test_sc = RobustScaler.transform(X_test)\n\n\n\n다시 완본으로 변경\n\nscaler.inverse_transform()\n\n\npd.DataFrame(X_train_sc).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n0.384615\n-1.4\n0.028369\n0.000000\n\n\n1\n0.000000\n-0.6\n-0.056738\n-0.200000\n\n\n2\n1.615385\n1.6\n0.595745\n0.466667\n\n\n\n\n\n\n\n\nX_original = RobustScaler.inverse_transform(X_train_sc)\n\npd.DataFrame(X_original).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n6.3\n2.3\n4.4\n1.3\n\n\n1\n5.8\n2.7\n4.1\n1.0\n\n\n2\n7.9\n3.8\n6.4\n2.0",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#차원-축소",
    "href": "posts/01_projects/adp_실기/notes/03.html#차원-축소",
    "title": "데이터 전처리",
    "section": "차원 축소",
    "text": "차원 축소\n\nfeatures = []\nx = iris.drop(columns='Class')\n\nx = StandardScaler().fit_transform(x)\n\npd.DataFrame(x).head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n-0.900681\n1.019004\n-1.340227\n-1.315444\n\n\n1\n-1.143017\n-0.131979\n-1.340227\n-1.315444\n\n\n2\n-1.385353\n0.328414\n-1.397064\n-1.315444\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\npca_fit = pca.fit(x)\n\nprint(pca.singular_values_)\nprint(pca.explained_variance_ratio_.cumsum())\n\n[20.92306556 11.7091661   4.69185798  1.76273239]\n[0.72962445 0.95813207 0.99482129 1.        ]\n\n\n\nplt.title('Scree Plot')\nplt.plot(pca.explained_variance_ratio_, 'o-')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "href": "posts/01_projects/adp_실기/notes/03.html#데이터-불균형-문제-처리",
    "title": "데이터 전처리",
    "section": "데이터 불균형 문제 처리",
    "text": "데이터 불균형 문제 처리",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/tofel_준비/index.html",
    "href": "posts/04_archives/tofel_준비/index.html",
    "title": "TOFEL 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: None\n        종료일: None\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        English",
    "crumbs": [
      "PARA",
      "Archives",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/tofel_준비/index.html#details",
    "href": "posts/04_archives/tofel_준비/index.html#details",
    "title": "TOFEL 준비",
    "section": "Details",
    "text": "Details\nTOFEL을 준비해 봅시다.",
    "crumbs": [
      "PARA",
      "Archives",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/tofel_준비/index.html#tasks",
    "href": "posts/04_archives/tofel_준비/index.html#tasks",
    "title": "TOFEL 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/tofel_준비/index.html#why-failed",
    "href": "posts/04_archives/tofel_준비/index.html#why-failed",
    "title": "TOFEL 준비",
    "section": "Why failed?",
    "text": "Why failed?\n자격증식 영어 공부에 너무 시간을 많이 투자하지 말자.\nopic정도만 시도해보자",
    "crumbs": [
      "PARA",
      "Archives",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/tofel_준비/index.html#related-posts",
    "href": "posts/04_archives/tofel_준비/index.html#related-posts",
    "title": "TOFEL 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "href": "posts/01_projects/adp_실기/notes/02.html#edaexploratory-data-analysis",
    "title": "EDA와 시각화",
    "section": "EDA(Exploratory Data Analysis)",
    "text": "EDA(Exploratory Data Analysis)\n: 데이터의 특징과 데이터에 내재된 관계를 알아내기 위해 그래프와 통계적 분석 방법을 활용하여 탐구하는 것\n\n주제\n\n저항성 강조: 부분적 변동(이상치 등)에 대한 민감성 확인\n잔차 계산\n자료변수의 재표현: 변수를 적당한 척도로 바꾸는 것\n그래프를 통한 현시성",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#막대-그래프",
    "href": "posts/01_projects/adp_실기/notes/02.html#막대-그래프",
    "title": "EDA와 시각화",
    "section": "막대 그래프",
    "text": "막대 그래프\n범주형 데이터를 요약하고 시각적으로 비교하는 데 활용\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\n\nwine_load = load_wine()\nwine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\nwine_load\nwine['Class'] = wine_load.target\nwine['Class'] = wine['Class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n\nwine_type = wine['Class'].value_counts()\nwine_type\n\nClass\nclass_1    71\nclass_0    59\nclass_2    48\nName: count, dtype: int64\n\n\n\n# 수직 막대\nplt.bar(wine_type.index, wine_type.values, width=0.8, bottom=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 수평 막대\nplt.barh(wine_type.index, wine_type.values, height=0.8, left=None, align = 'center')\nplt.show()\n\n\n\n\n\n\n\n\n각 범주의 값의 갯수 차이가 극단적인지 확인한다. 극단적일 경우, 전처리 과정에서 업/다운 샘플링 등을 통해 갯수가 유사해지도록 조정해야한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#히스토그램",
    "href": "posts/01_projects/adp_실기/notes/02.html#히스토그램",
    "title": "EDA와 시각화",
    "section": "히스토그램",
    "text": "히스토그램\n연속형 데이터의 분포를 확인하는 데 활용\n\nplt.title('Wine alcohol histogram')\nplt.hist('alcohol', bins=8, range=(11, 15), color='purple', data=wine)\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#box-plot",
    "href": "posts/01_projects/adp_실기/notes/02.html#box-plot",
    "title": "EDA와 시각화",
    "section": "box plot",
    "text": "box plot\n수치형 변수의 분포를 확인하는 그래프\n\nfrom sklearn.datasets import load_iris\n\niris_load = load_iris()\niris = pd.DataFrame(iris_load.data, columns=iris_load.feature_names)\niris['class'] = iris_load.target\niris['class'] = iris['class'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\nplt.boxplot(iris.drop(columns='class'))\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.boxplot(x=\"class\", y=\"sepal width (cm)\", data=iris)\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#산점도",
    "href": "posts/01_projects/adp_실기/notes/02.html#산점도",
    "title": "EDA와 시각화",
    "section": "산점도",
    "text": "산점도\n두 개의 수치형 변수의 분포와 관계를 확인하는 그래프\n\nplt.title('iris scatter')\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\n\nplt.scatter('sepal length (cm)', 'sepal width (cm)', data=iris, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', hue='class', data=iris, style='class')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#선그래프",
    "href": "posts/01_projects/adp_실기/notes/02.html#선그래프",
    "title": "EDA와 시각화",
    "section": "선그래프",
    "text": "선그래프\n\n수평 / 수직 선\n\nplt.hlines(y=-6, xmin=-10, xmax=10, colors='red', linestyles='solid')\nplt.vlines(x=0, ymin=-10, ymax=10, colors='blue', linestyles='dashed')\n\n\n\n\n\n\n\n\n\n\n함수식\n\ndef linear_func(x):\n    return 2*x + 1\n\nX = iris['sepal length (cm)']\nplt.plot(X, linear_func(X), c='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n회귀선\n\nimport numpy as np\n\nX, Y = iris['sepal length (cm)'], iris['sepal width (cm)']\nplt.scatter(X, Y, alpha=0.5)\na, b = np.polyfit(X, Y, 1)\nplt.plot(X, a*X + b, c='red')\nplt.show()\n\n\n\n\n\n\n\n\n2차 이상의 그래프는 X값에 대하여 정렬해야 한다.\n\niris2 = iris.sort_values(by='sepal length (cm)')\nX, Y = iris2['sepal length (cm)'], iris2['petal length (cm)']\nb2, b1, b0 = np.polyfit(X, Y, 2)\nplt.scatter(X, Y, alpha=0.5)\nplt.plot(X, b0 + b1*X + b2*X**2, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n꺾은선\n\nplt.plot('sepal length (cm)', 'petal length (cm)', data=iris2)\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#상관관계-시각화",
    "href": "posts/01_projects/adp_실기/notes/02.html#상관관계-시각화",
    "title": "EDA와 시각화",
    "section": "상관관계 시각화",
    "text": "상관관계 시각화\n\n산점도 행렬\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(iris, alpha=0.5, figsize= (8, 8), diagonal='hist')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(iris, diag_kind='auto', hue='class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n상관계수 행렬 그래프\n\niris_corr = iris.drop(columns='class').corr(method='pearson')\nsns.heatmap(iris_corr, xticklabels=iris_corr.columns, yticklabels=iris_corr.columns, cmap=\"RdBu_r\", annot=True)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/02.html#pandas-profiling",
    "href": "posts/01_projects/adp_실기/notes/02.html#pandas-profiling",
    "title": "EDA와 시각화",
    "section": "Pandas Profiling",
    "text": "Pandas Profiling\n\n# from pandas_profiling import ProfileReport\n#\n# ProfileReport(iris)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA와 시각화"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/04.html",
    "href": "posts/01_projects/adp_실기/notes/04.html",
    "title": "머신 러닝",
    "section": "",
    "text": "flowchart TD\n    A[검정 방법 선택 시작] --&gt; B(표본의 크기가 충분히 큰가?)\n    B --&gt;|yes| C(모분산을 알고 있는가?)\n    B --&gt;|no| E(모집단이 정규분포를 따르는가?)\n    E --&gt;|yes| C\n    C --&gt;|yes| D[z 검정]\n    C --&gt;|no| G(등분산성을 만족하는가?)\n    E --&gt;|no| F[비모수 검정]\n    G --&gt;|yes| H[t 검정]\n    G --&gt;|no| I[Welch's t 검정]\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "머신 러닝"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/index.html",
    "href": "posts/01_projects/adp_실기/index.html",
    "title": "ADP 실기 준비 - try 1",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-06-16\n        종료일: 2025-10-18\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석python",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/index.html#details",
    "href": "posts/01_projects/adp_실기/index.html#details",
    "title": "ADP 실기 준비 - try 1",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/index.html#tasks",
    "href": "posts/01_projects/adp_실기/index.html#tasks",
    "title": "ADP 실기 준비 - try 1",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2025.09.15 10 am)\n                \n                \n            \n\n            \n            \n                \n                    \n                    사전 점수 공개 (2025.11.07)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/index.html#참고-자료",
    "href": "posts/01_projects/adp_실기/index.html#참고-자료",
    "title": "ADP 실기 준비 - try 1",
    "section": "참고 자료",
    "text": "참고 자료\n\n확률 통계\n  \n위 책은 코드에 오류가 좀 있는거 같다. 단계적으로 잘 설명은 해주지만 오류가 너무 치명적인거 같다. 깃허브에 오류 관련 질문들이 올라와 있다.\n\n\n\n머신 러닝",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/index.html#related-posts",
    "href": "posts/01_projects/adp_실기/index.html#related-posts",
    "title": "ADP 실기 준비 - try 1",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/index.html",
    "href": "posts/01_projects/opic/index.html",
    "title": "OPIc 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-06-16\n        종료일: 2025-07-07\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/index.html#details",
    "href": "posts/01_projects/opic/index.html#details",
    "title": "OPIc 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/index.html#tasks",
    "href": "posts/01_projects/opic/index.html#tasks",
    "title": "OPIc 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/index.html#참고-자료",
    "href": "posts/01_projects/opic/index.html#참고-자료",
    "title": "OPIc 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/index.html#related-posts",
    "href": "posts/01_projects/opic/index.html#related-posts",
    "title": "OPIc 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/statistics/00.html#plot을-통한-자료-요약",
    "href": "posts/01_projects/adp_실기/notes/statistics/00.html#plot을-통한-자료-요약",
    "title": "EDA",
    "section": "plot을 통한 자료 요약",
    "text": "plot을 통한 자료 요약\n\n범주형 자료 요약\n\n도수분포표\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom seaborn import color_palette\n\ndf = pd.DataFrame(range(1, 11)).sample(100, replace=True)\ntable = pd.crosstab(index=df.values.flatten(), colnames=['질병'], columns='도수')\ntable.index = [\"감염\", \"심장\", \"호흡기\", \"소화기\", \"신경\", \"근골격\", \"내분비\", \"정신\", \"피부\", \"기타\"]\nprint(table)\n\n질병   도수\n감염    8\n심장   11\n호흡기   9\n소화기   4\n신경   15\n근골격  12\n내분비  11\n정신    9\n피부    9\n기타   12\n\n\n\n원형 그래프\n\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ntable.plot(kind='bar', color='skyblue', legend=False)\nplt.xlabel('사망 원인')\nplt.ylabel('빈도수')\nplt.title('사망 원인별 빈도수')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n막대 그래프\n\n\nplt.pie(table.iloc[:, 0], labels=list(table.index), colors=color_palette(\"pastel\"), autopct='%1.1f%%')\nplt.title('사망 원인별 빈도수')\nplt.show()\n\n\n\n\n\n\n\n\n\n파레토그림 (명목형)\n\n\n가장 큰 영향을 미치는 범주를 파악할 수 있는 그래프\n라이브러리는 딱히 없는거 같고, 뭐 많이 쓰지도 않는거 같아서 구현은 생략\n\n\n\n이산형 자료 요약\n\n관측값의 종료가 적은 경우 그냥 범주형으로 처리할 수 있다. (단 파레토그림같은 순서가 바뀌는 기법은 사용하지 않는다.)\n관측값의 종류가 많을 경우, 연속형 자료로 처리할 수 있다.\n\n\n\n연속형 자료 요약\n\n점도표\n도수분포표\n\n\n구간을 정하고 각 구간에 속하는 관측값의 개수를 세어 도수분포표를 만든다.\n\n\n히스토그램\n\n\n상대도수를 계급구간의 폭으로 나눈 값을 막대의 높이로 사용하는 그래프\n\n\n도수다각형\n\n\n중심의 위치, 퍼진 정도 등을 파악하는데 유용하다.\n또한 여러 자료를 비교하는 경우 히스토그램보다 유용하다.\n\n\n줄기-잎 그림\n\n\n개개의 관측값에 대한 정보를 유지하면서 분포를 파악할 수 있는 그래프\n하지만 관측값의 갯수가 많거나 지나치게 흩어져 있는 경우 제한된 공간에 그리는 것이 불가능하다.\n\n\n\n분포의 모양\n\n종모양\n이봉형: 두 개의 다른 집단이 섞여 있을 때 종종 나타난다.\n균일형\n오른쪽 편중\n왼쪽 편중",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Statistics",
      "EDA"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html",
    "title": "확률",
    "section": "",
    "text": "import pandas as pd\n\ngss = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/gss_bayes.csv')\nbanker = (gss['indus10'] == 6870)\nprint(f'은행원 수: { banker.sum() }')\nprint(f'은행원 비율: { banker.mean() }')\n\n은행원 수: 728\n은행원 비율: 0.014769730168391155",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/index.html",
    "href": "posts/02_areas/helm/index.html",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/index.html#details",
    "href": "posts/02_areas/helm/index.html#details",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/index.html#tasks",
    "href": "posts/02_areas/helm/index.html#tasks",
    "title": "Helm",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/index.html#참고-자료",
    "href": "posts/02_areas/helm/index.html#참고-자료",
    "title": "Helm",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/index.html#related-posts",
    "href": "posts/02_areas/helm/index.html#related-posts",
    "title": "Helm",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm"
    ]
  },
  {
    "objectID": "posts/02_areas/helm/notes/00.html",
    "href": "posts/02_areas/helm/notes/00.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Helm",
      "Notes",
      "00"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/statistics/00.html#수치를-통한-연속형-자료-요약",
    "href": "posts/01_projects/adp_실기/notes/statistics/00.html#수치를-통한-연속형-자료-요약",
    "title": "EDA",
    "section": "수치를 통한 연속형 자료 요약",
    "text": "수치를 통한 연속형 자료 요약\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n100.000000\n\n\nmean\n5.700000\n\n\nstd\n2.840899\n\n\nmin\n1.000000\n\n\n25%\n3.000000\n\n\n50%\n6.000000\n\n\n75%\n8.000000\n\n\nmax\n10.000000\n\n\n\n\n\n\n\n\n중심위치의 측도\n\n평균, 중앙값, 최빈값\n\n\nfrom scipy import stats\n\nprint(f'평균: {np.mean(df)}, 중앙값: {np.median(df)}, 최빈값: {stats.mode(df)}')\n\n평균: 5.7, 중앙값: 6.0, 최빈값: ModeResult(mode=array([5]), count=array([15]))\n\n\n\n\n퍼진 정도의 측도\n\n분산, 표준편차, 범위, 사분위수, 변동계수\n\n\n# 자유도 -1\nprint(f'분산: {np.var(df, ddof=1)}, 표준편차: {np.std(df, ddof=1)}, 범위: {np.ptp(df)}, 1,3분위수: {np.quantile(df, [0.25, 0.75])}, 변동계수: {np.std(df, ddof=1) / np.mean(df)}')\n\n분산: 0    8.070707\ndtype: float64, 표준편차: 0    2.840899\ndtype: float64, 범위: 9, 1,3분위수: [3. 8.], 변동계수: 0    0.498403\ndtype: float64\n\n\n\n\nbox plot\n\n종모양의 데이터의 분포를 나타내는 데 적절하다.\n사전에 도수분포표, 히스토그램, 줄기-잎 그림으로 봉우리를 파악해야 한다.\n\n\nplt.boxplot(df)\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x74d87129df10&gt;,\n  &lt;matplotlib.lines.Line2D at 0x74d87129e210&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x74d87129e510&gt;,\n  &lt;matplotlib.lines.Line2D at 0x74d87129e7e0&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x74d8713c2450&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x74d87129eab0&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x74d87129ed80&gt;],\n 'means': []}",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Statistics",
      "EDA"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html#확률",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html#확률",
    "title": "확률",
    "section": "확률",
    "text": "확률\n\ndef prob(A):\n    \"\"\"A의 확률 계산\"\"\"\n    return A.mean()\nprob(banker)\n\n0.014769730168391155\n\n\n\nfemale = (gss['sex'] == 2)\nprob(female)\n\n0.5378575776019476\n\n\n\nliberal = (gss['polviews'] &lt;= 3)\nprob(liberal)\n\n0.27374721038750255\n\n\n\ndemocrat = (gss['partyid'] &lt;= 1)\nprob(democrat)\n\n0.3662609048488537",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html#and-논리곱",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html#and-논리곱",
    "title": "확률",
    "section": "And 논리곱",
    "text": "And 논리곱\n\nprob(banker & democrat)\n\n0.004686548995739501",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html#조건부-확률",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html#조건부-확률",
    "title": "확률",
    "section": "조건부 확률",
    "text": "조건부 확률\n\nselected = democrat[liberal]\nprob(selected)\n\n0.5206403320240125\n\n\n진보(liberal) 성향 중 민주당(democrat)의 비율\n\ndef conditional(proposition, given):\n  return prob(proposition[given])\nconditional(liberal, given=female)\n\n0.27581004111500884\n\n\n\nprob(liberal & female) / prob(female)\n\n0.2758100411150089\n\n\n\\(P(A|B) = \\frac{P(A and B)}{P(B)}\\)\n\nprint(prob(female) * conditional(liberal, given=female))\nprint(prob(female & liberal))\n\n0.14834652059241224\n0.14834652059241227\n\n\n\\(P(A and B) = P(B)P(A|B)\\)\n\nconditional(female, given=liberal)\n\n0.5419106203216483\n\n\n조건부 확률은 교환 불가\n\nprint(conditional(female, given=liberal))\nprint(prob(female) * conditional(liberal, given=female) / prob(liberal))\n\n0.5419106203216483\n0.5419106203216482\n\n\n\\(P(A|B) = \\frac{P(A)*P(B|A)}{P(B)}\\)\n\nmale = (gss['sex'] == 1)\nsum(prob(gss['sex'] == i) * conditional(banker, gss['sex'] == i) for i in range(1, 3))\n\n0.014769730168391153\n\n\n\\(P(A) = P(B_1)P(A|B_1) + P(B_2)P(A|B_2)\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html#조건과-논리곱",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html#조건과-논리곱",
    "title": "확률",
    "section": "조건과 논리곱",
    "text": "조건과 논리곱\n\nconditional(female, given=liberal & democrat)\n\n0.576085409252669",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/01.html#이론",
    "href": "posts/01_projects/adp_실기/notes/bayse/01.html#이론",
    "title": "베이즈 정리",
    "section": "이론",
    "text": "이론\n사전확률을 이용해서 사후확률을 구하는 과정을 베이즈 갱신이라고 한다.\n\\(P(H|D) = \\frac{P(D|H)P(H)}{P(D)}\\)\n\n\\(P(H)\\): 가설의 확률. 사전확률\n\\(P(H|D)\\): 데이터가 주어졌을 때 가설의 확률. 사후확률\n\\(P(D|H)\\): 가설하에서 데이터가 나올 확률. 가능도(우도)\n\\(P(D)\\): 데이터의 확률. \\(P(D) = \\sum P(H_i)P(D|H_i)\\)\n\n가설은 상호 배제와 전체 포괄 가정이 필요하다.\n즉, 가설은 서로 배타적이어야 하고, 모든 가능한 가설을 포함해야 한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/01.html#베이즈-테이블",
    "href": "posts/01_projects/adp_실기/notes/bayse/01.html#베이즈-테이블",
    "title": "베이즈 정리",
    "section": "베이즈 테이블",
    "text": "베이즈 테이블\n\n가설과 데이터를 정리한다.\n사전확률을 구한다.\n각 가설 하에서의 데이터 가능도를 구한다.\n베이즈 테이블을 정리한다.\n\n\n쿠키 문제\n\n그릇 1에서 바닐라 쿠키를 집을 확률은 3/4\n그릇 2에서 바닐라 쿠키를 집을 확률은 1/2\n바닐라 쿠키를 집었을 때, 그릇 i에서 집었을 확률은?\n\n\nimport pandas as pd\n\ntable = pd.DataFrame(index=['bowl1', 'bowl2'])\ntable['prior'] = 1/2, 1/2\ntable['likelihood'] = 3/4, 1/2\ntable['unnorm'] = table['prior'] * table['likelihood']\ntable['posterior'] = table['unnorm'] / table['unnorm'].sum()\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nbowl1\n0.5\n0.75\n0.375\n0.6\n\n\nbowl2\n0.5\n0.50\n0.250\n0.4\n\n\n\n\n\n\n\n\n\n주사위 문제\n\n육면체, 팔면체, 십이면체 주사위가 든 상자가 있다.\n이 중 임의로 하나를 집어 굴렸더니 1이 나왔다.\n이 주사위가 육면체일 확률은?\n\n\ndef update(table):\n  table['unnorm'] = table['prior'] * table['likelihood']\n  prob_data = table['unnorm'].sum()\n  table['posterior'] = table['unnorm'] / prob_data\n  return prob_data\n\nfrom fractions import Fraction\n\ntable2 = pd.DataFrame(index=[6, 8, 12])\ntable2['prior'] = Fraction(1, 3)\ntable2['likelihood'] = Fraction(1, 6), Fraction(1, 8), Fraction(1, 12)\nprob_data = update(table2)\ntable2\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n6\n1/3\n1/6\n1/18\n4/9\n\n\n8\n1/3\n1/8\n1/24\n1/3\n\n\n12\n1/3\n1/12\n1/36\n2/9\n\n\n\n\n\n\n\n\n\n몬티홀 문제\n\n1,2,3 번호가 붙은 세 개의 문 중 하나에 상품이 있다.\n참가자는 문 하나를 선택하고, 사회자는 나머지 두 문 중 상품이 없는 문을 열어 보여준다.\n참가자는 처음 선택한 문을 유지하거나 다른 문으로 바꿀 수 있다.\n당신은 1번 문을 선택했고, 사회자는 문 3을 열었다.\n다시 참가자가 문을 선택했을 때 상품이 있는 문을 선택할 확률은?\n\n\ntable3 = pd.DataFrame(index=[1, 2, 3])\ntable3['prior'] = Fraction(1, 3)\ntable3['likelihood'] = Fraction(1, 2), 1, 0\nprob_data = update(table3)\ntable3\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n1/2\n1/6\n1/3\n\n\n2\n1/3\n1\n1/3\n2/3\n\n\n3\n1/3\n0\n0\n0",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/02.html#분포",
    "href": "posts/01_projects/adp_실기/notes/bayse/02.html#분포",
    "title": "분포",
    "section": "분포",
    "text": "분포\n\nfrom empiricaldist import Pmf\nimport pandas as pd\n\nprior = Pmf.from_seq(['Bowl 1', 'Bowl 2'])\nlikelihood_vanilla = [0.75, 0.5]\nposterior = prior * likelihood_vanilla\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.6\n\n\nBowl 2\n0.4",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/02.html#pmf를-이용한-확률",
    "href": "posts/01_projects/adp_실기/notes/bayse/02.html#pmf를-이용한-확률",
    "title": "분포",
    "section": "Pmf를 이용한 확률",
    "text": "Pmf를 이용한 확률\n\n쿠키 문제\n\nfrom empiricaldist import Pmf\nimport pandas as pd\n\nprior = Pmf.from_seq(['Bowl 1', 'Bowl 2'])\nlikelihood_vanilla = [0.75, 0.5]\nposterior = prior * likelihood_vanilla\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.6\n\n\nBowl 2\n0.4\n\n\n\n\n\n\n\n\n만약 같은 그릇에서 한 번 더 쿠키를 꺼냈을 때 바닐라일 경우\n\n\nposterior *= likelihood_vanilla\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.692308\n\n\nBowl 2\n0.307692\n\n\n\n\n\n\n\n\n만약 같은 그릇에서 한 번 더 쿠키를 꺼냈을 때 초코쿠키일 경우\n\n\nlikelihood_chocolate = [0.25, 0.5]\nposterior *= likelihood_chocolate\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.529412\n\n\nBowl 2\n0.470588\n\n\n\n\n\n\n\n\n\n101개의 쿠키 그릇\n\n그릇 i에는 바닐라 쿠키가 i% 있다.\n임의의 그릇을 골라 쿠키를 임의로 꺼냈을 때, 바닐라 쿠키라면 그릇 x에서 쿠키가 나왔을 확률은 얼마일까?\n\n\nimport numpy as np\n\nhypos = np.arange(101)\nprior = Pmf(1, hypos)\nprior.normalize()\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n0\n0.009901\n\n\n1\n0.009901\n\n\n2\n0.009901\n\n\n3\n0.009901\n\n\n4\n0.009901\n\n\n...\n...\n\n\n96\n0.009901\n\n\n97\n0.009901\n\n\n98\n0.009901\n\n\n99\n0.009901\n\n\n100\n0.009901\n\n\n\n\n101 rows × 1 columns\n\n\n\n\nlikelihood_vanilla = hypos / 100\nposterior1 = prior * likelihood_vanilla\nposterior1.normalize()\n\n0.5000000000000001\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\nprior.plot(label='prior', color='C5')\nposterior1.plot(label='posterior', color='C4')\nplt.legend()\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 하나 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 하나 뽑은 후')\n\n\n\n\n\n\n\n\n\n\n한 번 더 뽑았을 때 바닐라 쿠키인 경우\n\n\nposterior2 = posterior1 * likelihood_vanilla\nposterior2.normalize()\n\n0.6699999999999999\n\n\n\nposterior2.plot(label='posterior', color='C4')\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 두 번 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 두 번 뽑은 후')\n\n\n\n\n\n\n\n\n\n\n한 번 더 뽑았는데 초코 쿠키인 경우\n\n\nlikelihood_chocolate = 1 - hypos / 100\nposterior3 = posterior2 * likelihood_chocolate\nposterior3.normalize()\n\n0.2462686567164179\n\n\n\nposterior3.plot(label='posterior', color='C4')\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 두 번 뽑고 초코 쿠키 하나 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 두 번 뽑고 초코 쿠키 하나 뽑은 후')\n\n\n\n\n\n\n\n\n\n\nposterior3.max_prob()\n\n67\n\n\n\nMAP: 사후확률 분포에서 가장 큰 확률값\n\n\n\n주사위 문제\n\ndef update_dice(pmf, data):\n  hypos = pmf.qs\n  likelihood = 1 / hypos\n  likelihood[data &gt; hypos] = 0\n  pmf *= likelihood\n  pmf.normalize()\n\n\n주사위를 굴려서 1이 나왔다.\n\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\nupdate_dice(pmf, 1)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.444444\n\n\n8\n0.333333\n\n\n12\n0.222222\n\n\n\n\n\n\n\n\n주사위를 한번 더 굴렸는데 7이 나왔다.\n\n\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.000000\n\n\n8\n0.692308\n\n\n12\n0.307692",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html",
    "title": "비율 추정",
    "section": "",
    "text": "from scipy.stats import binom\nfrom empiricaldist import Pmf\nimport numpy as np\n\ndef make_binominal(n, p):\n  ks = np.arange(n + 1)\n  ps = binom.pmf(ks, n, p)\n  return Pmf(ps, ks)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html#유로-동전-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html#유로-동전-문제",
    "title": "비율 추정",
    "section": "유로 동전 문제",
    "text": "유로 동전 문제\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\npmf_k = make_binominal(n=250, p=0.5)\npmf_k.plot(label='coin', color='C5')\nplt.title('coin toss 이항분포')\nplt.xlabel('앞면이 나온 횟수(k)')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n동전을 250번 던져서 앞면의 횟수가 140과 같은 극단적인 값이 나올 확률\n\n\npmf_k.prob_ge(140) + pmf_k.prob_le(110)\n\n0.06642115124004333",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html#베이지안-추정",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html#베이지안-추정",
    "title": "비율 추정",
    "section": "베이지안 추정",
    "text": "베이지안 추정\n\n동전 앞면의 비율을 균등분포로 가정하고 시작\n\n\nhypos = np.linspace(0, 1, 101)\nprior = Pmf(1, hypos)\nlikelihood_heads = hypos\nlikelihood_tails = 1 - hypos\nlikelihood = {\n  'H': likelihood_heads,\n  'T': likelihood_tails\n}\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n0.00\n1\n\n\n0.01\n1\n\n\n0.02\n1\n\n\n0.03\n1\n\n\n0.04\n1\n\n\n...\n...\n\n\n0.96\n1\n\n\n0.97\n1\n\n\n0.98\n1\n\n\n0.99\n1\n\n\n1.00\n1\n\n\n\n\n101 rows × 1 columns\n\n\n\n\ndef update_euro(pmf, dataset):\n  for data in dataset:\n    pmf *= likelihood[data]\n  pmf.normalize()\n\n\nposterior = prior.copy()\ndataset = 'H' * 140 + 'T' * 110\n\nupdate_euro(posterior, dataset)\n\nposterior.plot(label='coin', color='C5')\nplt.title('동전 앞면 비율의 사후확률 분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nprint(f'동전의 앞면이 250번 중 140번 등장했다면 앞면의 비율은 {posterior.max_prob()}일 확률이 가장 높다.')\n\n동전의 앞면이 250번 중 140번 등장했다면 앞면의 비율은 0.56일 확률이 가장 높다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html#삼각사전분포",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html#삼각사전분포",
    "title": "비율 추정",
    "section": "삼각사전분포",
    "text": "삼각사전분포\n\n사전확률을 균등분포로 설정했지만, 실제로는 정규분포에 가까울 것이다.\n근데 책에서는 일단 삼각분포를 사용한다.\n\n\nramp_up = np.arange(50)\nramp_down = np.arange(50, -1, -1)\na = np.append(ramp_up, ramp_down)\ntriangle = Pmf(a, hypos, name='triangle')\ntriangle.normalize()\n\nuniform = Pmf(1, hypos, name='uniform')\nuniform.normalize()\n\nuniform.plot(label='균등사전', color='C4')\ntriangle.plot(label='삼각사전', color='C5')\nplt.legend()\nplt.title('삼각사전분포 및 균등사전분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nupdate_euro(uniform, dataset)\nupdate_euro(triangle, dataset)\n\nuniform.plot(label='균등사전', color='C4')\ntriangle.plot(label='삼각사전', color='C5')\nplt.legend()\nplt.title('삼각 및 균등 사후분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n두 확률분포의 사후 분포간 차이는 미미함.\n데이터가 충분하다면 서로 다른 사전확률로 시작한다고 해도 동일한 사후확률로 수렴하는 경향이 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html#이항가능도함수",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html#이항가능도함수",
    "title": "비율 추정",
    "section": "이항가능도함수",
    "text": "이항가능도함수\n\n갱신을 굳이 250번 하지 않아도, 이항분포를 통해 가능도를 한번에 계산할 수 있다.\n\n\ndef update_binomial(pmf, data):\n  k, n = data\n  xs = pmf.qs\n  likelihood = binom.pmf(k, n, xs)\n  pmf *= likelihood\n  pmf.normalize()\n\n\nuniform2 = Pmf(1, hypos, name='uniform2')\ndata = 140, 250\nupdate_binomial(uniform2, data)\nnp.allclose(uniform, uniform2)\n\nTrue",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/04.html#기관차-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/04.html#기관차-문제",
    "title": "수량 추정",
    "section": "기관차 문제",
    "text": "기관차 문제\n\n각 철도를 지나가는 기관차에 1부터 N까지의 순서로 번호를 붙인다.\n60번 번호가 붙은 기관차를 보았다.\n이 철도에 몇 개의 기관차가 지나가는지 추정해보자\n\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos = np.arange(1, 1001)\nprior = Pmf(1, hypos)\n\n\n가정: N은 1부터 1000까지의 값 중 한 값이 동일한 확률로 선택될 수 있다.\n\n\ndef update_train(pmf, data):\n  hypos = pmf.qs\n  likelihood = 1 / hypos\n  likelihood[(data &gt; hypos)] = 0\n  pmf *= likelihood\n  pmf.normalize()\n\n\ndata = 60\nposterior = prior.copy()\nupdate_train(posterior, data)\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nposterior.plot(label='60번 기관차 발견 시 전체 기관차 수의 사후확률', color='C5')\nplt.legend()\nplt.title('사후 확률')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nposterior.max_prob()\n\n60\n\n\n\n당연하다는 듯이 60이 최선의 선택. 하지만 이는 별로 도움이 안됨.\n대안으로 사후확률의 평균을 구해본다.\n\n\nposterior.mean()\n\n333.41989326370776\n\n\n\n해당 값을 선택하는 것이 장기적으로 좋은 선택.\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(columns=['사후확률 분포 평균'])\ndf.index.name = '상한값'\n\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    pmf = Pmf(1, hypos)\n    for data in dataset:\n        update_train(pmf, data)\n    df.loc[high] = pmf.mean()\ndf\n\n\n\n\n\n\n\n\n사후확률 분포 평균\n\n\n상한값\n\n\n\n\n\n500\n151.849588\n\n\n1000\n164.305586\n\n\n2000\n171.338181\n\n\n\n\n\n\n\n\n하지만 상한값의 범위의 변화에 따른 사후확률 분포의 평균값이 크게 달라진다.\n이럴때는 2가지 방법이 있다.\n\n데이터를 더 확보\n배경지식을 더 확보해서 더 나은 사전확률을 선택",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/04.html#멱법칙-사전확률",
    "href": "posts/01_projects/adp_실기/notes/bayse/04.html#멱법칙-사전확률",
    "title": "수량 추정",
    "section": "멱법칙 사전확률",
    "text": "멱법칙 사전확률\n\n기관차 수는 멱법칙을 주로 따르는 것으로 알려져 있음\n더 적합한 사전확률은 안정적인 사전확률을 제공할 수 있다.\n\n\nalpha = 1.0\nps = hypos ** (-alpha)\npower = Pmf(ps, hypos, name='power law')\npower.normalize()\n\n8.178368103610282\n\n\n\nuniform = Pmf(1, hypos, name='uniform')\nuniform.normalize()\n\npower.plot(label='power', color='skyblue')\nuniform.plot(label='uniform', color='pink')\nplt.legend()\nplt.title('사전확률 분포')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nupdate_train(uniform, 60)\nupdate_train(power, 60)\n\npower.plot(label='power', color='skyblue')\nuniform.plot(label='uniform', color='pink')\nplt.legend()\nplt.title('사후확률 분포')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(columns=['사후확률 분포 평균'])\ndf.index.name = '상한값'\n\nalpha = 1.0\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    ps = hypos**(-alpha)\n    power = Pmf(ps, hypos)\n    for data in dataset:\n        update_train(power, data)\n    df.loc[high] = power.mean()\ndf\n\n\n\n\n\n\n\n\n사후확률 분포 평균\n\n\n상한값\n\n\n\n\n\n500\n130.708470\n\n\n1000\n133.275231\n\n\n2000\n133.997463",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/04.html#신뢰구간",
    "href": "posts/01_projects/adp_실기/notes/bayse/04.html#신뢰구간",
    "title": "수량 추정",
    "section": "신뢰구간",
    "text": "신뢰구간\n\npower.credible_interval(0.9)\n\narray([ 91., 243.])",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "05"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#공산",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#공산",
    "title": "공산과 가산",
    "section": "공산",
    "text": "공산\n\ndef odds(p):\n  return p / (1 - p)\n\ndef prob(o):\n  return o / (1 + o)\n\ndef prob2(a, b):\n  return a / (a + b)\n\nodds(0.6)\n\n1.4999999999999998",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#베이즈-규칙",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#베이즈-규칙",
    "title": "공산과 가산",
    "section": "베이즈 규칙",
    "text": "베이즈 규칙\n\\(odds(A|D) = odds(A) \\cdot \\frac{P(D|A)}{P(D|B)}\\)\n\nprior_odds = 1\nlikelihood_ratio = (3/4) / (1/2)\npost_odds = prior_odds * likelihood_ratio\npost_odds\n\n1.5\n\n\n\nprob(post_odds)\n\n0.6\n\n\n\nlikelihood_ratio = (1/4) / (1/2)\npost_odds *= likelihood_ratio\npost_odds\n\n0.75\n\n\n\nprob(post_odds)\n\n0.42857142857142855",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#올리버의-혈액형",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#올리버의-혈액형",
    "title": "공산과 가산",
    "section": "올리버의 혈액형",
    "text": "올리버의 혈액형\n\n범죄 현장에서 두 사람의 혈흔을 발견했다.\n발견된 혈흔은 O형과 AB형이다.\n해당 지역에서 O형의 비율은 0.6, AB형의 비율은 0.01이다.\n올리버는 O형이다.\n범죄 현장의 혈흔(데이터)이 올리버가 범인 중 한 명이라는 질문(가정)에 대한 증거가 될 수 있는가\n\n\\(odds(A|D) = odds(A) \\cdot \\frac{P(D|A)}{P(D|B)}\\) \\(→ \\frac{odds(A|D)}{odds(A)} = \\frac{P(D|A)}{P(D|B)}\\)\n\n베이즈 요인 &gt; 1: A의 가정 하에 존재하는게 더 가깝다.\n베이즈 요인 &lt; 1: B의 가정 하에 존재하는게 더 가깝다.\n베이즈 요인 == 1: 양쪽 가설 하에서 동일한 가능성을 가진다.\n\n\nlike1 = 0.01 # oliver가 혈흔을 남긴 경우\nlike2 = 2 * 0.6 * 0.01 # oliver가 혈흔을 남기지 않은 경우\n\nlikelihood_ratio = like1 / like2\nlikelihood_ratio\n\n0.8333333333333334\n\n\n\n가능도비가 1보다 낮기 때문에, oliver가 혈흔을 남기지 않은 쪽에 더 가깝다.\n\n\npost_odds = 1 * like1 / like2\nprob(post_odds)\n\n0.45454545454545453",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/01.html",
    "href": "posts/01_projects/opic/notes/01.html",
    "title": "script 정리",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/00.html#시험-구조",
    "href": "posts/01_projects/opic/notes/00.html#시험-구조",
    "title": "오픽 구조 파악",
    "section": "시험 구조",
    "text": "시험 구조\n\n시험 시간은 총 40분. 답변 시간은 자유\n2분 정도 지나면 다음 문제로 넘어갈 수 있는 버튼이 활성화됨.\n후반 문제는 조금 늦게 나옴",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/00.html#질문-유형",
    "href": "posts/01_projects/opic/notes/00.html#질문-유형",
    "title": "오픽 구조 파악",
    "section": "질문 유형",
    "text": "질문 유형\n\n자기소개\nbackground survey\n돌발\n\n1번 문제가 자기소개. 점수엔 딱히 영향 없고, 그냥 목소리 푸는 용도\n2/3/4, 5/6/7, 8/9/10, 11/12/13, 14/15 는 background survey 및 돌발 질문\n각각의 묶음은 combo 문제\n\ncombo 별 10개 유형\n\n묘사\n\n일반적으로 이렇다는걸 대답. 현재형으로 대답\n가장 쉽고 배점이 낮기 때문에 적당히 대답\n\n루틴, 단계, 활동\n\n1번과 마찬가지로 현재형으로 대답\n\n처음 / 최근 경험\n\n육하원칙, 과거형\n최초 경험은 시간에 따른 취향 변화도 물어봄\n\n가장 인상적인 경험\n\n구체적인 설명 필요. 문제 경험을 물어보기도 함\n배점이 높으니까 신경써서 대답\n\n에바에게 질문하기\n\n안나옴\n\n주어진 상황에 3-4가지 정보 요청하기\n6번에서의 문제 상황 설명 및 대책 2-3가지 세우기\n이와 같은 본인 경험\n\n7번과 유사한 나의 경험\n\n비교 / 대조\n\n자세한 예를 들어 설명\n마무리는 선호\n\n주제 관련 이슈, 문제점, 걱정거리, 뉴스\n\n예시, 상황, 생각, 의견",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/00.html#돌발-문제",
    "href": "posts/01_projects/opic/notes/00.html#돌발-문제",
    "title": "오픽 구조 파악",
    "section": "돌발 문제",
    "text": "돌발 문제",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/00.html#답변-구조",
    "href": "posts/01_projects/opic/notes/00.html#답변-구조",
    "title": "오픽 구조 파악",
    "section": "답변 구조",
    "text": "답변 구조\n\nmain idea\n\n첫인상, 요지, hook\n\nbody\n\n\n\n\n강조 구간 (2-3 문장 속도감 있게)\n강조 구간\n\nconclusion\n\n역질문 what about you?\n제안하기 why dont we 동사, lets 동사\n급 마무리 thats it for this question\n1 + 3",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/01.html#돌발",
    "href": "posts/01_projects/opic/notes/01.html#돌발",
    "title": "script 정리",
    "section": "돌발",
    "text": "돌발\n\n재활용\n\n집에서 하는 재활용 과정\n\nhow do you recycle at home? when and how often do you recycle? describe each step of the process from beginning to end\n\n재활용을 하며 기억에 남는 경험\n\ntell me about a memorable experience you had while recycling. what happened? what made it so memorable? tell me about it in as much detail as possible.\n\n우리나라의 재활용\n\ntell me about recycling in your country. what kind of items do people usually recycle? please describe the recycling system in your country in detail.\n\n\n\n\n약속\n\n약속을 잡는 경향\n\npeople set up appointments for various reasons. what sort of appointments do you usually make, social or otherwise? who do you usually meet with? where do you usually meet them? give me as many details as possible\n\n기억에 남는 약속\n\ntell me about the most memorable appointment you have ever had. what kind of appointment was it? who did you meet? what did you do? did anything unexpected or interesting occur? why was it so memorable?\n\n약속 접기 전 연락 과정\n\nwhen you want to meet up with someone, how do you get in touch with him or her? do you make phone calls, send an e-mail, or do something else? how does the exchange usually go? tell me about the process from beginning to end\n\n\n\n\n은행\n\n우리나라의 은행\n\ndescribe the banks in your country. where are they usually located, and what are they like? what hours are banks typically open there? give me as many details as possible\n\n과거와 현재의 은행 비교\n\nhave there been any changes to the banks in your country since you were a child? how were they in the past? how are they now? please describe the changes in detail\n\n은행에서 겪은 문제\n\nhave you ever experienced any problems at a bank? for instance, sometimes ATMs malfunction. what sort of problem did you face, and how did you deal with it?\n\n\n\n\n휴대폰\n\n\n지형 야외활동\n\n우리나라의 지형\n\ni would like to know about the geographic features of your country. what makes them different from other countries? please describe them in as much detail as possible.\n\n기억에 남는 야회 활동 경험\n\ndescribe the most memorable experience you have had outdoors. what is a beautiful place you have been to? please provide s many details as possible\n\n우리나라 사람들이 즐겨하는 야외 활동\n\nwhat kind of outdoor activities do people in your country do? do they enjoy things like jogging, cycling, or hiking? why do the like to do those activities?\n\n\n\n\n모임, 기념일\n\n\n외식 음식\n\n자주가는 식당\n\ni would like to know about a restaurant you often visit. what kind of dishes does it serve? what do you like about the restaurant? what does it look like?\n\n식당에서 겪은 경험\n\nhave you ever had a special experience at a restaurant? who were you with? what happened? tell me about the experience in detail, and explain what made it memorable.\n\n유명한 한국 요리\n\ntell me about the most famous dish of your country. what are the main ingredients in it? do you know how yo make it? what is special about the dish?\n\n\n\n\n기술\n\n과거와 현재의 기술 비교\n\ntechnology is advancing more rapidly than ever. can you tell me about the way technology has been changing? what changes have occurred since you were child? provide me with as many details as possible.\n\n기술과 관련되어 생긴 문제\n\nhave you ever experienced a problem related to technology? for instance, sometimes a device does not work properly or is difficult to use. describe your technological problem in detail. how did you handle it?\n\n우리나라에서 인기 있는 기술\n\ntell me about the technologies that are popular in your country. which technology do people use the most there? what is it used for? can you tell me why they like to use it?\n\n\n\n\n날씨 계절\n\n우리나라의 계절\n\ni would like to know about the seasons in your country. how many seasons are there? how are they different? what is the weather like in each season?\n\n과거와 현재의 날씨 비교\n\ndo you think that the weather has changed over the past few years? what was it like in the past? how has it changed? please describe the differences in detail.\n\n이상 기후로 인해 겪은 경험\n\nhave you ever had a memorable or unexpected experience because of unusual weather conditions? when was it? what happened? tell me about it in as much detail as possible\n\n\n\n\n휴일",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/01.html#roleplaying",
    "href": "posts/01_projects/opic/notes/01.html#roleplaying",
    "title": "script 정리",
    "section": "roleplaying",
    "text": "roleplaying\n\n면접관에게 질문하기\n\ni live in an apartment. please ask me three or four questions about the place i live in.\ni enjoy traveling too. please ask me three or four questions about traveling.\ni live in Canada. ask me three or four questions about the geographic features of my country.\n\n\n\n주어진 상황에서 직접 질문하기\n\ni’m going yo give you a situation to act out. pretend you are overseas on vacation and you need a car to get around, so you have gone to a car rental agency. ask the agent three or four questions about renting a car.\ni am going to give you a situation to act out. imagine that you have gone to a store to buy new furniture. ask the salesperson three or four questions about the furniture you are looking for\n\n\n\n주어진 상황에서 전화로 질문하기\n\ni would like to give you a situation to act out. imagine you are planning a vacation. call a travel agency, and ask three or four questions about potential destinations and itineraries.\n\n\n\n상황 설명하고 대안 제시하기\n\ni have a problem for you to solve. you found out you purchased the wrong tickets at the movie theater. talk to the person at the ticket window about your situation and offer suggestions to solve the problem.\nthere is a problem i’d like you to solve. you’ve ordered some furniture and its just been delivered. however, the furniture thats arrived is not what you ordered. call the furniture store to explain the situation and suggest some alternatives to the problem.\nthere is a problem i need you to solve. pretend that you’ve gone to the library to look for a book, but the one you want has been checked out. explain the situation to a librarian and offer two or theree alternatives to the problem.",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/01.html#주제",
    "href": "posts/01_projects/opic/notes/01.html#주제",
    "title": "설문 script 정리",
    "section": "주제",
    "text": "주제\n일 경험 없음 학생: 아니요 수강 후 5년 이상 지남\n\n개인주택이나 아파트에 홀로 거주\nQ. I would like to know where you live. Describe your house in detail. What does it look like? How many rooms do you have?\n\nI live in a pretty cozy studio apartment, and I love this place because it feels like my own personal space. It’s not that big, but it’s just the right size for me. I have a small kitchen area, a living room that doubles as my bedroom, and a bathroom. living room에는 안락한 소파와 조명이 있는데, 이곳에 앉아 있으면 편안함을 느낍니다. 이것들이 없었다면 제 집은 정말 단조로웠을 거예요. so, even though it’s small, it has everything I need to feel at home.\n\n\nQ. I would like to know where you live. Which is your favorite room at home? What does it look like? What do you mostly do there?\n\nYou asked about my favorite room. Well, 저는 원룸에 살고 있기 때문에, 제일 좋아한다고 할 수 있는 공간은 거실밖에 없어요. 저는 거실에서 가장 많은 시간을 보냅니다. 거실에는 편안한 소파와 따뜻한 조명이 있어요. I watch movies, read books, and listen to music there. I find this space so comfortable because I’ve filled it with all my favorite things, like a cozy sofa and a nice lamp. For these reasons, it’s truly my favorite part of the house.\n\n\nQ. Can you tell me about the household appliances in your house? What is your favorite one among them? Why do you like it most?\n\nRegarding my household appliances, my favorite is definitely my robot vacuum cleaner. For me, it’s a very important part of my daily life. For example, I can schedule it to clean my apartment automatically while I’m at work, and it always keeps my floor clean without any effort from me. This is really important because it saves me a lot of time and frees me from a chore I really don’t like. So, because of this convenience, it’s my absolute favorite.\n\n\nQ. Tell me about the house or apartment you lived in when you were a child. How was it different from the one you live in now? What are the similarities and differences?\n\n답변 예시: &gt; Talking about my childhood home, there’s a pretty big difference between it and my current apartment. My childhood home was a large apartment that was always bustling with my family, whereas my current place is a small and quiet studio just for me. I think the most important difference is that my old home was about ‘sharing’ with family, while my current home is about ‘independence’. Reflecting on this, I appreciate both the warmth of my family home and the freedom I have now.\n\n\n10. Now, tell me about the problems that happen at your home. What are those problems? Why do they occur? How do people deal with those problems? How do you personally deal with those problems?\n\nThe biggest problem I’ve ever had at home was when my air conditioner broke down during a really hot summer. The main issue was that my apartment became like an oven, which was a big deal because I couldn’t sleep or even relax. To deal with this problem, I had to call a repairman, but I had to wait for a few days because they were so busy. During that time, I spent most of my days at nearby cafes to escape the heat. This whole experience taught me that it’s very important to check my appliances before the season starts.\n\n\n\n영화/공연/콘서트/음악\nQ1. What is your favorite genre of movies? Why do you like those types of movies? (당신이 좋아하는 영화 장르는 무엇인가요? 왜 그 장르를 좋아하나요?)\n\nThat’s an interesting question. While I enjoy many genres, my absolute favorite type of movie these days is music films, especially live concert films. The reason is simple: they combine the best of both worlds. You get the immersive sound and giant screen of a movie theater, but you also get to feel the incredible energy and passion of a live concert. It’s a very powerful and emotional experience for me, and I find it’s the best way to really appreciate an artist’s performance.\n\n\nQ2. I’d like you to tell me about one of the most memorable movies you’ve seen. (당신이 본 영화 중 가장 기억에 남는 영화는 무엇인가요?)\n\nThe most memorable movie I’ve ever seen was actually a concert film called ‘Coldplay: Music of the Spheres’. I’m a huge fan of Coldplay, but I couldn’t get tickets to their actual show in my country. So when I heard it was broadcasting live in theaters, I went to see it immediately. The sound quality in the theater was just amazing, and seeing the artists’ expressions up close on a huge screen was incredible. The most special part was that everyone in the audience was singing and dancing together, almost like a real concert. It truly showed me a whole new way to enjoy music.\n\n\nQ3. Tell me about a time when you went to listen to some live music. (라이브 음악을 들으러 갔을 때에 대해 말해주세요.)\n\nYou asked about a time I went to listen to live music. Well, one of the most unique live music experiences I’ve had was, interestingly, at a movie theater. I went to see a live broadcast of a Coldplay concert. The mood in the theater was electric. Even though we were in our cinema seats, people were standing up, singing along to every song, and waving light sticks. The music itself was fantastic, of course, but what made it so special was sharing that “live” energy with all the other fans in the same space. It was a very powerful and modern way to enjoy a live performance.\n\n\nQ4. Could you compare the movies made today to movies you saw while you were growing up? (요즘 영화와 당신이 자라날 때 보았던 영화를 비교해주세요.)\n\nI think the biggest difference between movies today and movies from the past is the technology and the variety of content. Of course, today’s movies have amazing special effects and sound systems, which makes everything more realistic and immersive. But more importantly, the type of content has expanded. When I was young, a movie was usually just a fictional story. But now, as I’ve mentioned, we can watch live concerts, musicals, and even operas in the movie theater. This kind of “alternative content” was almost unimaginable in the past. So I would say movies have become a much broader entertainment platform.\n\n\nQ5. How have the performances in your country changed or developed over the last several years? (지난 몇 년간 당신의 나라의 공연들은 어떻게 변해왔나요?)\n\nIn my opinion, the biggest change in how people enjoy performances in Korea is how technology has made them much more accessible. In the past, you had to physically be at the concert hall or theater, and getting a ticket was often very competitive. But now, things have changed a lot. Many popular concerts and musicals are broadcast live in movie theaters nationwide. I think this is a fantastic development because it allows more people to enjoy high-quality performances, even if they live far away or couldn’t get a ticket. It has really opened up a new market and a new way for people to experience culture.\n\n\n\n공원가기 캠핑하기\nQ1. You indicated that you like to go to parks. Tell me about one of the parks that you often visit. What does it look like? (자주 가는 공원에 대해 말해주세요. 어떻게 생겼나요?)\n\nSure. One of the parks I love to visit is Nanji Hangang Park, located right next to the Han River in Seoul. It’s a huge, open park with a lot of green grass, which is perfect for picnics or just relaxing. Many people ride bikes along the river or fly kites on windy days. But what makes this park truly special for me is that it has a large, well-equipped camping site inside. It’s not every day you find a place where you can enjoy both a regular park atmosphere and a real camping experience right in the middle of the city.\n\n\nQ2. Tell the place you usually go camping. What does it look like? Why do you like to go there? (자주 캠핑가는 장소에 대해 말해주세요. 왜 그곳을 좋아하나요?)\n\nMy go-to place for camping is, as I just mentioned, the Nanji Camping Site inside Hangang Park. It’s the perfect combination of nature and convenience. The campsite itself is a large grassy area with designated spots where you can park your car right next to your tent. It has clean public restrooms and even a convenience store, which is amazing. I love going there because I can enjoy an outdoor barbecue and a real camping vibe without having to drive for hours out of the city. It’s the easiest way to escape from my daily routine.\n\n\nQ3. Do you have any memorable experience when you went camping? When was it? Who did you go with? What happened? (캠핑을 갔을 때 기억에 남는 경험이 있나요?)\n\nMy most memorable camping experience was actually my first time going solo camping, and it was at Nanji Camping Site last fall. I wanted to have some time to myself and clear my head. To be honest, I struggled a bit to set up my new tent all by myself. But after I finally did, I was sitting in my chair, enjoying a cup of coffee and watching the beautiful sunset over the river. Suddenly, it started to rain really hard. At first, I was worried it would ruin my trip, but I went inside my tent. Listening to the sound of the rain hitting the tent was incredibly peaceful and cozy. It turned a potentially bad situation into a very calm and special memory.\n\n\nQ4. I would like to know things you usually bring when you go camping. Why do you take those? (캠핑갈 때 주로 가져가는 것들에 대해 말해주세요.)\n\nWhen I go camping, I have a few essential items I always bring. I would group them into three categories. First is for shelter, so of course, I bring my tent, a comfortable sleeping bag, and a thick mat to put underneath it. Next is for comfort. For this, I always take a foldable chair and a small table. They are important because sitting on a chair is much more comfortable than sitting on the ground. Lastly, and most importantly, is for food. I bring a portable gas stove and a cooler box filled with food. These items are essential because having a delicious barbecue is one of the biggest joys of camping for me.\n\n\nQ5. Tell me about your most memorable experience that have happened at the park. (공원에서 발생했던 당신의 가장 기억에 남는 경험에 대해 말해주세요.)\n\nMy most memorable experience at a park actually happened while I was camping at Nanji Hangang Park. As I mentioned, I was on my first-ever solo camping trip there. After setting up my tent, I was just enjoying the park’s general atmosphere, watching people walk by and seeing the sunset over the river. But then, a sudden summer rain began to pour. Instead of ruining my day at the park, it actually made it more special. I sat inside my tent, listening to the rain and feeling very cozy and close to nature. It was a really peaceful moment that I’ll never forget, all thanks to that park. (Note: 3번 캠핑 경험과 동일한 이야기를, ’공원에서 있었던 일’이라는 관점으로 살짝 바꾸어 답변하는 전략입니다. 매우 효율적입니다.)\n\n\n\n조깅 걷기 운동을 전혀 하지 않음\nQ1. Where do you often go to for jogging? Describe your favorite place for jogging in detail. (주로 어디로 조깅을 하러 가나요? 그곳을 자세히 묘사해주세요.)\n\nI usually go jogging along a stream near my house called Tancheon. It’s a very popular spot for local residents, and for a good reason. There’s a long, well-maintained path that runs right alongside the water, so you can run for kilometers without worrying about cars. On one side you have the stream, and on the other, there are lots of trees and grassy areas. You can always see many people walking their dogs, riding bikes, or jogging like me. I especially love it because it’s a peaceful escape from the busy city streets, and the scenery is beautiful and changes with the seasons.\n\n\nQ2. How did you get into jogging initially? Tell me about your motivation to try and continue. (처음 조깅을 시작하게 된 계기는 무엇인가요? 동기에 대해 말해주세요.)\n\nI actually got into jogging a few years ago when I was going through a really stressful period in my life. I was preparing for an important exam and spent most of my time indoors, and I felt my mind was getting really cluttered and heavy. A friend suggested that I should just go for a walk to get some fresh air. At first, I just walked along the stream near my house. Soon, I started jogging a little, and I found that focusing on my breathing and the rhythm of my steps really helped to clear my head. That feeling of mental clarity became my main motivation, and it’s the biggest reason why I’ve continued to jog ever since.\n\n\nQ3. Please describe a memorable experience you had while walking. Explain what happened and why it was so memorable. (걷는 동안 있었던 기억에 남는 경험을 묘사해주세요.)\n\nMy most memorable experience while walking happened a few months ago. I woke up very early one morning, right after it had rained all night. The air felt incredibly fresh and clean, and I could smell the wet grass and the trees. I decided to go for a walk along my usual path at Tancheon Stream. Because it was so early, the path was almost empty and very quiet. As I was walking, the sun began to rise, and the morning light shining on the wet leaves was just beautiful. That peaceful and quiet moment, just me and the calm morning scenery, was so special. It made me really appreciate the simple beauty of nature that you can find even in a busy city.\n\n\nQ4. What do you have to consider when you go jogging? What can you do to avoid an injury? (조깅하러 갈 때 어떤 걸 고려하나요? 부상을 피하기 위해 무엇을 할 수 있나요?)\n\nWhen I go jogging, there are a few important things I always consider to avoid getting injured. First and foremost, before I start running, I always do some light stretching for at least five minutes. It’s really important to warm up your muscles, especially in your legs and ankles. Second, during the jog, I try to listen to my body. If I feel any sharp pain, I don’t push myself; I slow down to a walk or just stop. I think pushing too hard is the easiest way to get hurt. Lastly, choosing the right footwear is crucial. I make sure to wear proper running shoes that provide good cushioning and support for my feet. I believe these simple steps are the key to enjoying jogging safely for a long time.\n\n\n\n집에서 보내는 휴가\nQ1. Most people want to travel during vacations. Tell me why you prefer staying at home. What makes your vacation at home enjoyable? (대부분 여행을 원하는데, 당신이 집에 머무는 걸 더 좋아하는 이유는 무엇인가요?)\n\nThat’s a great question. While I do enjoy traveling sometimes, I often prefer staying at home for my vacation because it’s a form of true rest for me. To be honest, traveling can be quite stressful. You have to deal with the hassle of packing, booking flights, and navigating crowded airports. But when I’m at home, there’s zero pressure. What makes it so enjoyable is the complete freedom to do whatever I want, whenever I want. I can sleep in late, binge-watch an entire movie series, or just relax with good music in my own comfortable and private space. It’s the best way for me to truly recharge my batteries.\n\n\nQ2. Describe what you did during your last vacation that you spent at home. Give me a description from the first to the last day. (집에서 보낸 마지막 휴가에 대해 첫날부터 마지막 날까지 묘사해주세요.)\n\nSure. My last vacation at home was for three days, and I had a very simple and relaxing plan. On the first day, I focused on doing absolutely nothing. I turned off my alarm, slept in until noon, and ordered my favorite food for delivery. It was my official ‘de-stress’ day. The next day was my hobby day. I dedicated the entire afternoon to binge-watching the entire Lord of the Rings trilogy, which I had wanted to do for ages. On the final day, I did a little ‘reset’ for the week ahead. I cleaned my apartment, did my laundry, and planned my schedule, all while listening to a good playlist. It was a perfect balance of rest and productivity.\n\n\nQ3. I would like to know about the memorable vacation you have spent at home. What did you do and who were you with? Why was it so memorable? (집에서 보낸 휴가 중 가장 기억에 남는 휴가에 대해 말해주세요.)\n\nMy most memorable vacation at home was last spring when I decided to use the time for a personal project: redecorating my room. I was by myself for the whole vacation. I had always wanted to paint one wall, so I made it my vacation goal. I spent the first couple of days carefully painting the wall a deep green color. In the evenings, I would order some nice food and watch movies in my newly transforming room. It was so memorable because I wasn’t just passively resting; I was actively creating a space that I loved. By the end of the vacation, I felt not only refreshed from the break but also incredibly proud and satisfied with what I had accomplished.\n\n\nQ4. Why do you think people need vacations? What purposes do you think vacations have for different people? (사람들에게 왜 휴가가 필요하다고 생각하나요? 휴가의 목적은 무엇일까요?)\n\nI believe vacations are absolutely essential for everyone in modern society. From my perspective, the main reason is to prevent burnout and take care of our mental health. We work so hard and live such busy lives, and without a proper break, it’s very easy to become stressed and exhausted. I think the purpose of a vacation can be different for different people. For some, it’s about adventure and having new experiences in foreign countries. For others, like me, it’s more about getting deep rest and having quiet time. But regardless of the style, I think the ultimate purpose is to step away from our daily routine, hit the ‘reset’ button, and come back with renewed energy and a fresh perspective on life.\n\n\n\n국내여행 해외여행\nQ1. What are some things that you do to prepare for trips? (여행을 준비할 때 당신이 하는 일들은 무엇인가요?)\n\nWhen I prepare for a trip, whether it’s domestic or overseas, I usually follow three simple steps. First, I take care of the most important bookings, like flights and accommodation. I always try to do this well in advance to get reasonable prices. Second, I make a rough itinerary. I don’t plan every single minute because I like to be flexible, but I do research some famous restaurants and attractions I definitely want to visit. Finally, a day or two before I leave, I start packing. I always check the weather forecast to make sure I pack the right clothes. Following these steps helps me feel organized and start my trip without any stress.\n\n\nQ2. You indicated that you enjoy domestic travel. Where do you like to visit? Describe the place and why you love to go there. (국내 여행지 중 어디를 좋아하나요? 그곳을 묘사하고 왜 좋아하는지 설명해주세요.)\n\nMy favorite place to visit for domestic travel is definitely Jeju Island. To be honest, even though it’s in Korea, it feels like a completely different country to me. The scenery is very exotic, with its unique volcanic mountains, black rocks, and beautiful emerald-colored beaches. Also, since it’s an island, you have to take a plane to get there, which adds to the feeling of going on a real adventure. The local food there is also very special and different from the mainland. For all these reasons, visiting Jeju Island is my favorite way to feel like I’m traveling overseas without needing a passport.\n\n\nQ3. Tell me a story that you experienced when traveling overseas. What happened and what made that incident memorable? (해외여행 경험에 대해 얘기해주세요.) (전략: 제주도 경험을 ‘해외여행 같은’ 경험으로 자연스럽게 연결하여 이야기합니다.)\n\nI have a memorable story from a trip to Jeju Island, which, as I said, felt just like an overseas trip to me. My flight back to Seoul was suddenly canceled at the last minute because a powerful typhoon hit the island. All the flights for the next day were already fully booked. At first, I really started to panic, thinking about my schedule and the extra cost. But then, I realized there was nothing I could do, so I decided to just embrace the situation. I found a cozy guesthouse near the coast and spent my unexpected extra day just reading a book and listening to the sound of the storm outside. It turned a stressful problem into a surprisingly peaceful and relaxing experience.\n\n\nQ4. Unexpected things can happen while you are traveling. Talk about an unforgettable happening on a trip. (여행 중 겪은 잊지 못할 일에 대해 말해주세요.) (전략: 3번 질문과 동일한 ’태풍 스토리’를 재활용하되, ’예상치 못한 일’이라는 관점에 초점을 맞춰 답변합니다.)\n\nAn unforgettable thing happened on my trip to Jeju Island. I was supposed to fly back home, but a powerful typhoon hit the island, and all flights were canceled. I was essentially stranded there for an extra day. After the initial panic, I found a small guesthouse to stay in. The amazing thing was how peaceful it became. Just sitting inside my room, listening to the sound of the wind and rain, felt like a special, unplanned part of the vacation. This experience was so memorable because it taught me that unexpected problems can sometimes turn into the best memories if you just change your perspective and go with the flow.",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/00.html#공부-방법",
    "href": "posts/01_projects/opic/notes/00.html#공부-방법",
    "title": "오픽 구조 파악",
    "section": "공부 방법",
    "text": "공부 방법\n주제별로 공부\n묘사 -&gt; 루틴 -&gt; 비교 -&gt; 과거 경험 -&gt; 롤플레이 -&gt; 어드밴스",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#가산",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#가산",
    "title": "공산과 가산",
    "section": "가산",
    "text": "가산\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\ndef make_die(sides):\n    outcomes = np.arange(1, sides+1)\n    die = Pmf(1/sides, outcomes)\n    return die\n\ndie = make_die(6)\n\ndef add_dist_seq(seq):\n    total = seq[0]\n    for other in seq[1:]:\n        total = total.add_dist(other)\n    return total\ndice = [die] * 3\ndice[0]\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n1\n0.166667\n\n\n2\n0.166667\n\n\n3\n0.166667\n\n\n4\n0.166667\n\n\n5\n0.166667\n\n\n6\n0.166667\n\n\n\n\n\n\n\n\nthrice = add_dist_seq(dice)\nthrice\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n3\n0.004630\n\n\n4\n0.013889\n\n\n5\n0.027778\n\n\n6\n0.046296\n\n\n7\n0.069444\n\n\n8\n0.097222\n\n\n9\n0.115741\n\n\n10\n0.125000\n\n\n11\n0.125000\n\n\n12\n0.115741\n\n\n13\n0.097222\n\n\n14\n0.069444\n\n\n15\n0.046296\n\n\n16\n0.027778\n\n\n17\n0.013889\n\n\n18\n0.004630",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#글루텐-민감도",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#글루텐-민감도",
    "title": "공산과 가산",
    "section": "글루텐 민감도",
    "text": "글루텐 민감도\n\n글루텐에 민감한 사람은 블라이든 검사에서 글루텐 밀가루를 정확히 식별할 확률이 95%다\n글루텐에 민감하지 않은 사람이 우연히 글루텐 밀가루를 식별할 확률은 40%다.\n\n\nn = 35\nnum_sensitive = 10\nnum_insensitive = n - num_sensitive\n\n\nfrom scipy.stats import binom\nfrom empiricaldist import Pmf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef make_binomial(n, p):\n  ks = np.arange(n+1)\n  ps = binom.pmf(ks, n, p)\n  return Pmf(ps, ks)\n\ndist_sensitive = make_binomial(num_sensitive, 0.95)\ndist_insensitive = make_binomial(num_insensitive, 0.40)\n\ndist_total = Pmf.add_dist(dist_sensitive, dist_insensitive)\n\ndist_sensitive.plot(label='sensitive', ls=':')\ndist_insensitive.plot(label='insensitive', ls='--')\ndist_total.plot(label='total')\nplt.legend()\n\n\n\n\n\n\n\n\n\n역산\n\n35명의 피험자중 12명이 글루텐이 있다고 했을 때, 글루텐에 민감한 사람의 비율은?\n\n\nimport pandas as pd\n\ntable = pd.DataFrame()\nfor num_sensitive in range(0, n+1):\n  num_insensitive = n - num_sensitive\n  dist_sensitive = make_binomial(num_sensitive, 0.95)\n  dist_insensitive = make_binomial(num_insensitive, 0.40)\n  dist_total = Pmf.add_dist(dist_sensitive, dist_insensitive)\n  table[num_sensitive] = dist_total\n\nfor dist in table:\n  table[dist].plot(label=f'num_sensitive = {dist}')\n\n\n\n\n\n\n\n\n\nlikelihood1 = table.loc[12]\nhypos = np.arange(n+1)\nprior = Pmf(1, hypos)\n\nposterior1 = prior * likelihood1\nposterior1.normalize()\n\nlikelihood2 = table.loc[20]\nposterior2 = prior * likelihood2\nposterior2.normalize()\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nposterior1.plot(label='12개가 정확히 분류한 경우의 사후분포', color='C4')\nposterior2.plot(label='20개가 정확히 분류한 경우의 사후분포', color='C5')\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint(f'12명이 정확히 분류했을 때 글루텐 민감한 사람은 {posterior1.max_prob()}명, 20명이 정확히 분류했을 때 {posterior2.max_prob()}명일 확률이 높음')\n\n12명이 정확히 분류했을 때 글루텐 민감한 사람은 0명, 20명이 정확히 분류했을 때 11명일 확률이 높음",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/06.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/06.html",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/06.html#누적분포함수",
    "href": "posts/01_projects/adp_실기/notes/bayse/06.html#누적분포함수",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "누적분포함수",
    "text": "누적분포함수\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos = np.linspace(0, 1, 101)\npmf = Pmf(1, hypos)\ndata = 140, 250\n\n\nfrom scipy.stats import binom\n\ndef update_binomial(pmf, data):\n  k, n = data\n  xs = pmf.qs\n  likelihood = binom.pmf(k, n, xs)\n  pmf *= likelihood\n  pmf.normalize()\n\nupdate_binomial(pmf, data)\ncumulative = pmf.cumsum()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/06.html#넷-중-높은-값",
    "href": "posts/01_projects/adp_실기/notes/bayse/06.html#넷-중-높은-값",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "넷 중 높은 값",
    "text": "넷 중 높은 값\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nn = 10000\na = np.random.randint(1, 7, size=(n, 4))\na.sort(axis=1)\nt = a[:, 1:].sum(axis=1)\npmf_best3 = Pmf.from_seq(t)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/07.html#팽귄-데이터",
    "href": "posts/01_projects/adp_실기/notes/bayse/07.html#팽귄-데이터",
    "title": "분류",
    "section": "팽귄 데이터",
    "text": "팽귄 데이터\n\nimport pandas as pd\n\ndf = pd.read_csv('https://github.com/allisonhorst/palmerpenguins/raw/main/inst/extdata/penguins_raw.csv')\ndf.shape\n\n(344, 17)\n\n\n\ndef make_cdf_map(df, colname, by='Species2'):\n  cdf_map = {}\n  grouped = df.groupby(by)[colname]\n  for species, group in grouped:\n    cdf_map[species] = Cdf.from_seq(group, name=species)\n  return cdf_map",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/00.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/00.html#연습문제",
    "title": "확률",
    "section": "연습문제",
    "text": "연습문제\n\n1-1\n\nprob(female), prob(liberal), prob(democrat)\n\n(0.5378575776019476, 0.27374721038750255, 0.3662609048488537)\n\n\n\n\n1-2\n\nconditional(liberal, given=democrat), conditional(democrat, given=liberal)\n\n(0.3891320002215698, 0.5206403320240125)\n\n\n\n\n1-3\n\nyoung = (gss['age'] &lt; 30)\nold = (gss['age'] &gt;= 65)\nconservative = (gss['polviews'] &gt;= 5)\nprob(young & liberal), conditional(liberal, given=young), prob(old & conservative), conditional(old, given=conservative)\n\n(0.06579427875836884,\n 0.338517745302714,\n 0.06701156421180766,\n 0.19597721609113564)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/01.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/01.html#연습문제",
    "title": "베이즈 정리",
    "section": "연습문제",
    "text": "연습문제\n\n2-1\n\ntable = pd.DataFrame(index=['normal', 'weird'])\ntable['prior'] = Fraction(1, 2)\ntable['likelihood'] = Fraction(1, 2), 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nnormal\n1/2\n1/2\n1/4\n1/3\n\n\nweird\n1/2\n1\n1/2\n2/3\n\n\n\n\n\n\n\n\n\n2-2\n\ntable = pd.DataFrame(index=['bb', 'bg', 'gb', 'gg'])\ntable['prior'] = Fraction(1, 4)\ntable['likelihood'] = 0, 1, 1, 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nbb\n1/4\n0\n0\n0\n\n\nbg\n1/4\n1\n1/4\n1/3\n\n\ngb\n1/4\n1\n1/4\n1/3\n\n\ngg\n1/4\n1\n1/4\n1/3\n\n\n\n\n\n\n\n\n\n2-3\n\ntable = pd.DataFrame(index=[1, 2, 3])\ntable['prior'] = Fraction(1, 3)\ntable['likelihood'] = 1, 0, 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n1\n1/3\n1/2\n\n\n2\n1/3\n0\n0\n0\n\n\n3\n1/3\n1\n1/3\n1/2\n\n\n\n\n\n\n\n\ntable = pd.DataFrame(index=[1, 2, 3])\ntable['prior'] = Fraction(1, 3)\ntable['likelihood'] = 0, 1, 0\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n0\n0\n0\n\n\n2\n1/3\n1\n1/3\n1\n\n\n3\n1/3\n0\n0\n0\n\n\n\n\n\n\n\n\n\n2-4\n\ntable = pd.DataFrame(index=['1994_1996', '1996_1994'])\ntable['prior'] = Fraction(1, 2)\ntable['likelihood'] = Fraction(2, 10) * Fraction(2, 10), Fraction(14, 100) * Fraction(1, 10)\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1994_1996\n1/2\n1/25\n1/50\n20/27\n\n\n1996_1994\n1/2\n7/500\n7/1000\n7/27",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/02.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/02.html#연습문제",
    "title": "분포",
    "section": "연습문제",
    "text": "연습문제\n\n3-1\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\nupdate_dice(pmf, 1)\nupdate_dice(pmf, 3)\nupdate_dice(pmf, 5)\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.000000\n\n\n8\n0.835052\n\n\n12\n0.164948\n\n\n\n\n\n\n\n\n\n3-2\n\nhypos = [4, 6, 8, 12, 20]\npmf = Pmf([1/15, 2/15, 3/15, 4/15, 5/15], hypos)\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n4\n0.000000\n\n\n6\n0.000000\n\n\n8\n0.391304\n\n\n12\n0.347826\n\n\n20\n0.260870\n\n\n\n\n\n\n\n\n\n3-3\n\npmf = Pmf.from_seq(['서랍 1', '서랍 2'])\nlikelihood = [1/2, 1/9]\nposterior = pmf * likelihood\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n서랍 1\n0.818182\n\n\n서랍 2\n0.181818",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/03.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/03.html#연습문제",
    "title": "비율 추정",
    "section": "연습문제",
    "text": "연습문제\n\n4-1\n\nhypos = np.linspace(0.2, 0.33, 101)\nuniform = Pmf(1, hypos)\nprior = uniform.copy()\nprior.normalize()\n\ndata = 3, 3\nupdate_binomial(uniform, data)\nprior.plot(label='사전', color='C4')\nuniform.plot(label='사후', color='C5')\nplt.legend()\nplt.title('사전 vs 사후 분포')\nplt.xlabel('안타 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n\n4-2\n\ndef update_randomized_response(pmf, data):\n    yes_count, total = data\n    ps = pmf.qs\n    prob_yes = 0.5 + 0.5 * ps\n    likelihood = binom.pmf(yes_count, total, prob_yes)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.linspace(0, 1, 101)\nuniform = Pmf(1, hypos)\nprior = uniform.copy()\nprior.normalize()\n\ndata = 80, 100\nupdate_randomized_response(uniform, data)\n\nprior.plot(label='사전', color='C4')\nuniform.plot(label='사후', color='C5')\nplt.legend()\nplt.title('Randomized Response - 사전 vs 사후 분포')\nplt.xlabel('탈세자 비율 (p)')\nplt.ylabel('PMF')\nplt.grid(True, alpha=0.3)\n\n\n\n\n\n\n\n\n\nmost_likely_rate = uniform.max_prob()\nprint(f'가장 가능성이 높은 탈세자 비율: {most_likely_rate:.3f}')\n\ncredible_interval = uniform.credible_interval(0.95)\nprint(f'95% 신뢰구간: [{credible_interval[0]:.3f}, {credible_interval[1]:.3f}]')\n\n가장 가능성이 높은 탈세자 비율: 0.600\n95% 신뢰구간: [0.420, 0.730]\n\n\n\n\n4-3\n\ndef update_machine_response(pmf, data, y):\n    k, n = data\n    xs = pmf.qs * (1 - y) + (1 - pmf.qs) * y\n    likelihood = binom.pmf(k, n, xs)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.linspace(0, 1, 101)\nuniform = Pmf(1, hypos)\n\ndata = 140, 250\nfor y in np.linspace(0, 0.5, 6):\n    dist = uniform.copy()\n    update_machine_response(dist, data, y)\n    dist.plot(label=f'y={y:.1f}')\nplt.legend()\nplt.title('y에 따른 앞면의 비율')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n\n4-4\n\nhypos = np.linspace(0.1, 0.4, 101)\nuniform = Pmf(1, hypos)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nprob_both_0 = (1 - hypos) ** 4\nprob_both_1 = (2 * hypos * (1 - hypos)) ** 2\nprob_both_2 = hypos ** 4\n\nlikelihood = prob_both_0 + prob_both_1 + prob_both_2\nposterior = uniform * likelihood\nposterior.normalize()\n\nprior.plot(label='사전분포', color='C1')\nposterior.plot(label='사후분포', color='C2')\nplt.legend()\nplt.title('사전 분포와 사후분포')\nplt.xlabel('우주선을 맞출 확률 (x)')\nplt.ylabel('PMF')\n\nprior_mean = prior.mean()\nposterior_mean = posterior.mean()\n\nprint(\"\\n=== 연습문제 4-4 답변 ===\")\nif posterior_mean &gt; prior_mean:\n    print(\"✅ 데이터는 좋은 소식입니다 (GOOD)\")\n    print(f\"✅ x의 추정값이 {prior_mean:.3f}에서 {posterior_mean:.3f}로 증가했습니다 (INCREASE)\")\nelse:\n    print(\"❌ 데이터는 나쁜 소식입니다 (BAD)\")\n    print(f\"❌ x의 추정값이 {prior_mean:.3f}에서 {posterior_mean:.3f}로 감소했습니다 (DECREASE)\")\n\n\n=== 연습문제 4-4 답변 ===\n❌ 데이터는 나쁜 소식입니다 (BAD)\n❌ x의 추정값이 0.250에서 0.235로 감소했습니다 (DECREASE)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/04.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/04.html#연습문제",
    "title": "수량 추정",
    "section": "연습문제",
    "text": "연습문제\n\n5-1\n\nfrom scipy.stats import binom\n\ndef update(pmf, k, p):\n    likelihood = binom.pmf(k, pmf.qs, p)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.arange(1, 2001)\nprior = Pmf(1, hypos)\n\nposterior = prior.copy()\n\nupdate(posterior, 2, 1/365)\nprint(f\"5월 11일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\nupdate(posterior, 1, 1/365)\nprint(f\"5월 23일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\nupdate(posterior, 0, 1/365)\nprint(f\"8월 1일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\n5월 11일 데이터 적용 후 평균 인원수: 957.1\n5월 23일 데이터 적용 후 평균 인원수: 721.7\n8월 1일 데이터 적용 후 평균 인원수: 486.2\n\n\n\nestimated_people = posterior.mean()\nprint(f\"추정된 강당 인원수: {estimated_people:.1f}명\")\n\nprob_over_1200 = posterior[posterior.qs &gt; 1200].sum()\nprint(f\"1200명을 초과할 확률: {prob_over_1200:.4f} ({prob_over_1200*100:.2f}%)\")\n\nci_90 = posterior.credible_interval(0.9)\nprint(f\"90% 신뢰구간: [{ci_90[0]:.0f}, {ci_90[1]:.0f}]\")\n\n추정된 강당 인원수: 486.2명\n1200명을 초과할 확률: 0.0112 (1.12%)\n90% 신뢰구간: [166, 942]\n\n\n\n\n5-2\n\ndef rabbit_likelihood(n):\n    return ((n - 1) / n) * (1/n) * (1/n) * 3\n\nhypos = np.arange(4, 11)\nprior = Pmf(1, hypos)\n\nlikelihood = [rabbit_likelihood(n) for n in hypos]\n\nposterior = prior.copy()\nposterior *= likelihood\nposterior.normalize()\n\nprint(f\"\\n추정 토끼 수 (평균): {posterior.mean():.2f}마리\")\n\n\n추정 토끼 수 (평균): 5.92마리\n\n\n\n\n5-3\n\ndef update_remain(pmf, data):\n    hypos = pmf.qs\n    likelihood = 1 / hypos\n    likelihood[(hypos &lt; data)] = 0\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.arange(0, 1096)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nupdate_remain(prior, 1095)\nprior.plot(label='사후확률', color='pink')\nplt.legend()\nplt.show()\n\n\n\n5-5\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos_short = np.arange(0, 201) # 10억 단위\nprior_short = Pmf(1, hypos_short, name=\"short\")\nprior_short.normalize()\n\nhypos_long = np.arange(0, 2001)\nprior_long = Pmf(1, hypos_long, name=\"long\")\nprior_long.normalize()\n\nlikelihood_ps = {}\nfor pmf in [prior_short, prior_long]:\n    likelihood = 1 / pmf.qs\n    likelihood[(pmf.qs &lt; 108)] = 0\n    pmf *= likelihood\n    pmf.normalize()\n    likelihood_ps[pmf.name] = pmf(108)\n    pmf.plot(label=f\"{pmf.name}: {pmf(108)}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprior = Pmf.from_seq(['short', 'long'])\nfor hypos in prior.index:\n    prior.loc[hypos] *= likelihood_ps[hypos]\n\nprior.normalize()\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nlong\n0.175733\n\n\nshort\n0.824267",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/05.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/05.html#연습문제",
    "title": "공산과 가산",
    "section": "연습문제",
    "text": "연습문제\n\n6-1\n\nfor trust in [0.9, 0.5, 0.1]:\n  prior_odds = odds(trust)\n  post_odds =  prior_odds * likelihood_ratio\n  print(f'{trust}, {prior_odds}: {prob(post_odds)}')\n\n0.9, 9.000000000000002: 0.8823529411764706\n0.5, 1.0: 0.45454545454545453\n0.1, 0.11111111111111112: 0.08474576271186442\n\n\n\n\n6-2\n\nprior_odds = odds(1/3)\npost_odds = prior_odds * 2\npost_odds *= 1.25\n\nprob(post_odds)\n\n0.5555555555555555\n\n\n\n\n6-3\n\nprior_odds = odds(1/10)\npost_odds = prior_odds * (2 ** 3)\n\nprob(post_odds)\n\n0.4705882352941177\n\n\n\n베이즈 방법을 이용하나 빈도주의적 방법을 이용하나, 확률을 구하는 문제에 있어서는 차이가 없다.\n하지만 통계적 추론 방법을 선택하는데 있어서는 차이가 있다.\n\n\n\n6-4\n\nprior_odds = odds(14/100)\npost_odds = prior_odds * 25\n\nprob(post_odds)\n\n0.8027522935779816\n\n\n\n\n6-5\n\ndice6 = Pmf.from_seq(np.arange(1, 7))\ngoblin_health = dice6.add_dist(dice6)\nremaining_health = Pmf.sub_dist(goblin_health, 3)\nattack_damage = dice6\n\ndefeat_probability = 0\nfor damage, damage_prob in attack_damage.items():\n    for remaining, remaining_prob in remaining_health.items():\n        if remaining &gt; 0 and damage &gt; remaining:\n            defeat_probability += damage_prob * remaining_prob\n\nprint(f\"\\n고블린을 물리칠 확률: {defeat_probability:.3f}\")\n\n\n고블린을 물리칠 확률: 0.292\n\n\n\n\n6-6\n\nimport numpy as np\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\ndice = [Pmf([1/6] * 6, np.arange(1, 7)),\n        Pmf([1/8] * 8, np.arange(1, 9)),\n        Pmf([1/12] * 12, np.arange(1, 13))]\nlike = [d.mul_dist(d) for d in dice]\ndf = pd.DataFrame(like).fillna(0).transpose()\ndf *= pmf.ps\ndf[1][12]\n\n0.020833333333333332\n\n\n\n\n6-7\n\npmf = Pmf(1, ['long', 'zost', 'bell'])\npmf.normalize()\ndice = Pmf([1/3] * 3, np.arange(0, 3))\nlike = [dice.add_dist(dice).add_dist(dice).add_dist(dice).add_dist(dice),\n        dice.add_dist(dice).add_dist(dice).add_dist(dice),\n        dice.add_dist(dice).add_dist(dice)]\ndf = pd.DataFrame(like).fillna(0).transpose()\nfor i in [3, 4, 5]:\n  pmf *= np.array(df.loc[i])\n  pmf.normalize()\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nlong\n0.235762\n\n\nzost\n0.449704\n\n\nbell\n0.314534\n\n\n\n\n\n\n\n\n\n6-8\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom\nfrom empiricaldist import Pmf\n\nn_total = 538\nn_outperform = 312\n\nhypos = np.arange(n_total + 1)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nlikelihoods = np.zeros(prior.shape[0])\nfor n_honest in hypos:\n    n_dishonest = n_total - n_honest\n    honest_dist = make_binomial(n_honest, 0.5)\n    dishonest_dist = make_binomial(n_dishonest, 0.9)\n    total_dist = Pmf.add_dist(honest_dist, dishonest_dist)\n    likelihood = total_dist[n_outperform]\n    likelihoods[n_honest] = likelihood\n\nposterior = prior * likelihoods\nposterior.normalize()\n\nmost_likely_honest = posterior.max_prob()\nposterior_prob = posterior[most_likely_honest]\n\nprint(f\"가장 가능성이 높은 정직한 의원 수: {most_likely_honest}명\")\nprint(f\"해당 확률: {posterior_prob:.4f}\")\n\n# 사후분포 시각화\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\nposterior.plot()\nplt.xlabel('정직한 의원 수')\nplt.ylabel('확률')\nplt.title('정직한 의원 수에 대한 사후분포')\nplt.axvline(most_likely_honest, color='red', linestyle='--', \n           label=f'최빈값: {most_likely_honest}명')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 95% 신용구간\ncredible_interval = posterior.credible_interval(0.95)\nprint(f\"95% 신용구간: {credible_interval[0]}명 - {credible_interval[1]}명\")\n\n가장 가능성이 높은 정직한 의원 수: 430명\n해당 확률: 0.0147\n\n\n\n\n\n\n\n\n\n95% 신용구간: 380.0명 - 486.0명",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/07.html#a의-키",
    "href": "posts/01_projects/adp_실기/notes/bayse/07.html#a의-키",
    "title": "비교",
    "section": "A의 키",
    "text": "A의 키\n\n미국 성인 남성 중 두명을 임의로 골랐다.\nA가 B보다 큰 것 같을 때, A의 키는 얼마인가\n\n\nimport numpy as np\nfrom scipy.stats import norm\nfrom empiricaldist import Pmf\n\nmean = 178\nstd = 7.7\n\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\nprior = Pmf(ps, qs)\nprior.normalize()\n\n1.9963309462450582\n\n\n\nimport pandas as pd\n\ndef make_joint(pmf1, pmf2):\n  X, Y = np.meshgrid(pmf1, pmf2)\n  return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\njoint = make_joint(prior, prior)\nplt.contour(joint.columns, joint.index, joint, linewidths=2)\nplt.xlabel('A의 키(cm)')\nplt.ylabel('B의 키(cm)')\n\nText(0, 0.5, 'B의 키(cm)')\n\n\n\n\n\n\n\n\n\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X &gt; Y), 1, 0)\nlikelihood = pd.DataFrame(a, index=x, columns=y)\nposterior = joint * likelihood\n\n\ndef normalize(pdf):\n  prob_data = joint.to_numpy().sum()\n  pdf /= prob_data\n  return prob_data\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_B = Pmf(posterior.sum(axis=1))\nmarginal_A.normalize()\nmarginal_B.normalize()\nmarginal_A.plot()\nmarginal_B.plot()\nprior.plot()\n\n\n\n\n\n\n\n\n\nmarginal_A.mean(), marginal_B.mean()\n\n(182.38728123421686, 173.60286000233393)\n\n\n\nA의 키가 170이라면?\n\n\npmf = Pmf(posterior[170])\npmf.normalize()\npmf.plot()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/07.html#연습-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/07.html#연습-문제",
    "title": "비교",
    "section": "연습 문제",
    "text": "연습 문제\n\n11-1\n\npmf = Pmf(posterior.loc[180])\npmf.normalize()\npmf.plot()\n\n\n\n\n\n\n\n\n\n\n11-2\n\nmean = 163\nstd = 7.3\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\ngirl_prior = Pmf(ps, qs)\ngirl_prior.normalize()\njoint = make_joint(marginal_A, girl_prior)\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X - Y &gt;= 15), 1, 0)\nlikelihood = pd.DataFrame(a, index=y, columns=x)\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A.plot(label='c 정보 이전')\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\nmarginal_A.plot(label='c 정보 이후')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n11-3\n\nmean_A = 1600\nstd_A = 100\nqs_A = np.arange(mean_A - 300, mean_A + 300, 10)\nps_A = norm(mean_A, std_A).pdf(qs_A)\nprior_A = Pmf(ps_A, qs_A)\nprior_A.normalize()\n\nmean_B = 1800\nstd_B = 100\nqs_B = np.arange(mean_B - 300, mean_B + 300, 10)\nps_B = norm(mean_B, std_B).pdf(qs_B)\nprior_B = Pmf(ps_B, qs_B)\nprior_B.normalize()\n\njoint = make_joint(prior_A, prior_B)\n\n\ndef logistic_prob(r_a, r_b):\n    return 1 / (1 + 10**((r_b - r_a) / 400))\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = 1 / (1 + 10**((Y - X) / 400))\nlikelihood = pd.DataFrame(a, index=y, columns=x)\n\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\n\nprint(marginal_A.max_prob(), marginal_A.mean())\nmarginal_A.plot()\n\n1640 1636.648345528236",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/08.html#팽귄-데이터",
    "href": "posts/01_projects/adp_실기/notes/bayse/08.html#팽귄-데이터",
    "title": "분류",
    "section": "팽귄 데이터",
    "text": "팽귄 데이터\n\nimport pandas as pd\n\ndf = pd.read_csv('https://github.com/allisonhorst/palmerpenguins/raw/main/inst/extdata/penguins_raw.csv')\ndf.shape\n\n(344, 17)\n\n\n\ndef make_cdf_map(df, colname, by='Species2'):\n  cdf_map = {}\n  grouped = df.groupby(by)[colname]\n  for species, group in grouped:\n    cdf_map[species] = Cdf.from_seq(group, name=species)\n  return cdf_map",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "의사결정분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/09.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/09.html",
    "title": "검정",
    "section": "",
    "text": "import numpy as np\nfrom empiricaldist import Pmf\nfrom scipy.stats import binom\n\nk, n = 140, 250\nlike_fair = binom.pmf(k, n, 0.5)\n\n\nramp_up = np.arange(50)\nramp_down = np.arange(50, -1, -1)\na = np.append(ramp_up, ramp_down)\n\nxs = np.linspace(0, 1, 101)\ntriangle = Pmf(a, xs)\ntriangle.normalize()\nbiased_triangle = triangle.copy()\nbiased_triangle[0.5] = 0\nbiased_triangle.normalize()\n\nlikelihood = binom.pmf(k, n, xs)\nlike_triangle = np.sum(biased_triangle * likelihood)\n\n\nk = like_fair / like_triangle\nk, k / (k + 1)\n\n(1.1970766535647133, 0.5448497446015277)\n\n\n\n250번 중 140번이 앞면이 나왔는데도 여전히 공정할 확률이 높다. (왜?)\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "검정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/09.html#우주-왕복선-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/09.html#우주-왕복선-문제",
    "title": "로지스틱 회귀",
    "section": "우주 왕복선 문제",
    "text": "우주 왕복선 문제\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv')\npred = df.iloc[-1]\ndf = df[:-1].dropna()\noffset = df['Temperature'].mean().round()\ndf['x'] = df['Temperature'] - offset\ndf['y'] = df['Damage Incident'].astype(int)\n\ndf\n\n\n\n\n\n\n\n\nDate\nTemperature\nDamage Incident\nx\ny\n\n\n\n\n0\n04/12/1981\n66\n0\n-4.0\n0\n\n\n1\n11/12/1981\n70\n1\n0.0\n1\n\n\n2\n3/22/82\n69\n0\n-1.0\n0\n\n\n4\n01/11/1982\n68\n0\n-2.0\n0\n\n\n5\n04/04/1983\n67\n0\n-3.0\n0\n\n\n6\n6/18/83\n72\n0\n2.0\n0\n\n\n7\n8/30/83\n73\n0\n3.0\n0\n\n\n8\n11/28/83\n70\n0\n0.0\n0\n\n\n9\n02/03/1984\n57\n1\n-13.0\n1\n\n\n10\n04/06/1984\n63\n1\n-7.0\n1\n\n\n11\n8/30/84\n70\n1\n0.0\n1\n\n\n12\n10/05/1984\n78\n0\n8.0\n0\n\n\n13\n11/08/1984\n67\n0\n-3.0\n0\n\n\n14\n1/24/85\n53\n1\n-17.0\n1\n\n\n15\n04/12/1985\n67\n0\n-3.0\n0\n\n\n16\n4/29/85\n75\n0\n5.0\n0\n\n\n17\n6/17/85\n70\n0\n0.0\n0\n\n\n18\n7/29/85\n81\n0\n11.0\n0\n\n\n19\n8/27/85\n76\n0\n6.0\n0\n\n\n20\n10/03/1985\n79\n0\n9.0\n0\n\n\n21\n10/30/85\n75\n1\n5.0\n1\n\n\n22\n11/26/85\n76\n0\n6.0\n0\n\n\n23\n01/12/1986\n58\n1\n-12.0\n1\n\n\n\n\n\n\n\n\n전통 로지스틱\n\nimport statsmodels.formula.api as smf\n\nformula = 'y ~ x'\nresults = smf.logit(formula, data=df).fit(disp=False)\nresults.params\n\nIntercept   -1.208490\nx           -0.232163\ndtype: float64\n\n\n\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ninter = results.params['Intercept']\nslope = results.params['x']\nxs = np.arange(53, 83) - offset\nps = expit(inter + slope * xs)\nplt.plot(xs + offset, ps)\nplt.scatter(df['x'] + offset, df['y'])\n\n\n\n\n\n\n\n\n\n\n사전 분포\n\nfrom empiricaldist import Pmf\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\nqs = np.linspace(-5, 1, 101)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(-0.8, 0.1, 101)\nprior_slope = Pmf.from_seq(qs)\njoint = make_joint(prior_inter, prior_slope)\njoint_pmf = Pmf(joint.stack())\njoint_pmf\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.8\n-5.00\n0.000098\n\n\n-4.94\n0.000098\n\n\n-4.88\n0.000098\n\n\n-4.82\n0.000098\n\n\n-4.76\n0.000098\n\n\n...\n...\n...\n\n\n0.1\n0.76\n0.000098\n\n\n0.82\n0.000098\n\n\n0.88\n0.000098\n\n\n0.94\n0.000098\n\n\n1.00\n0.000098\n\n\n\n\n10201 rows × 1 columns\n\n\n\n\n\n가능도\n\nfrom scipy.stats import binom\n\ngrouped = df.groupby('x')['y'].agg(['count', 'sum'])\nns = grouped['count']\nks = grouped['sum']\nxs = grouped.index\nps = expit(inter + slope * xs)\nlikes = binom.pmf(ks, ns, ps)\nlikes\n\narray([0.93924781, 0.85931657, 0.82884484, 0.60268105, 0.56950687,\n       0.24446388, 0.67790595, 0.72637895, 0.18815003, 0.8419509 ,\n       0.87045398, 0.15645171, 0.86667894, 0.95545945, 0.96435859,\n       0.97729671])\n\n\n\nlikelihood = joint_pmf.copy()\nfor slope, inter in joint_pmf.index:\n    ps = expit(inter + slope * xs)\n    likes = binom.pmf(ks, ns, ps)\n    likelihood[slope, inter] = likes.prod()\n\n\n\n갱신\n\nposterior_pmf = joint_pmf * likelihood\nposterior_pmf.normalize()\njoint_posterior = posterior_pmf.unstack()\n\nmarginal_inter = Pmf(joint_posterior.sum(axis=0))\nmarginal_inter.normalize()\nmarginal_slope = Pmf(joint_posterior.sum(axis=1))\nmarginal_slope.normalize()\n\nmarginal_inter.plot()\n\n\n\n\n\n\n\n\n\nmarginal_slope.plot()\n\n\n\n\n\n\n\n\n\n\n분포 변환\n\nmarginal_probs = marginal_inter.transform(expit)\nmarginal_lr = marginal_slope.transform(np.exp)\n\n\n\n예측 분포\n\nsample = posterior_pmf.choice(101)\ntemps = np.arange(31, 83)\nxs = temps - offset\npred = np.empty((len(sample), len(xs)))\nfor i, (slope, inter) in enumerate(sample):\n    pred[i] = expit(inter + slope * xs)\nlow, median, high = np.percentile(pred, [5, 50, 95], axis=0)\nplt.fill_between(temps, low, high, color='C1', alpha=0.2, label='95% 신뢰구간')\nplt.plot(temps, median, color='C1', label='logistic model')\nplt.legend()\nplt.scatter(df['x'] + offset, df['y'], label='data')",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "검정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/10.html#더-많은-눈이-내렸을까",
    "href": "posts/01_projects/adp_실기/notes/bayse/10.html#더-많은-눈이-내렸을까",
    "title": "회귀",
    "section": "더 많은 눈이 내렸을까?",
    "text": "더 많은 눈이 내렸을까?\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf = pd.read_csv('https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv', parse_dates=[2])\ndf['YEAR'] = df['DATE'].dt.year\nsnow = df.groupby('YEAR')['SNOW'].sum()\nsnow = snow.iloc[1:-1]\nsnow.plot(ls='', marker='o', label='강설량')\nplt.legend()\n\n\n\n\n\n\n\n\n\nfrom empiricaldist import Pmf\nimport statsmodels.formula.api as smf\ndata = snow.reset_index()\noffset = data['YEAR'].mean().round()\ndata['x'] = data['YEAR'] - offset\ndata['y'] = data['SNOW']\n\nformula = 'y ~ x'\nresults = smf.ols(formula, data=data).fit()\nresults.params\n\nIntercept    64.446325\nx             0.511880\ndtype: float64\n\n\n\n사전분포\n\nqs = np.linspace(-0.5, 1.5, 51)\nprior_slope = Pmf.from_seq(qs)\nqs = np.linspace(54, 75, 41)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(20, 35, 31)\nprior_sigma = Pmf.from_seq(qs)\n\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.index, index=pmf2.index)\n\ndef make_joint3(pmf1, pmf2, pmf3):\n    joint2 = make_joint(pmf2, pmf1).stack()\n    joint3 = make_joint(pmf3, joint2).stack()\n    return Pmf(joint3)\n\nprior = make_joint3(prior_slope, prior_inter, prior_sigma)\nprior\n\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.5\n54.0\n20.0\n0.000015\n\n\n20.5\n0.000015\n\n\n21.0\n0.000015\n\n\n21.5\n0.000015\n\n\n22.0\n0.000015\n\n\n...\n...\n...\n...\n\n\n1.5\n75.0\n33.0\n0.000015\n\n\n33.5\n0.000015\n\n\n34.0\n0.000015\n\n\n34.5\n0.000015\n\n\n35.0\n0.000015\n\n\n\n\n64821 rows × 1 columns",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/11.html#월드컵-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/11.html#월드컵-문제",
    "title": "켤레사전분포",
    "section": "월드컵 문제",
    "text": "월드컵 문제\n\nfrom scipy.stats import gamma\nimport numpy as np\nfrom empiricaldist import Pmf\n\nalpha = 1.4\ndist = gamma(alpha)\n\nlams = np.linspace(0, 10, 101)\nprior = Pmf(dist.pdf(lams), lams)\nprior.normalize()\n\n9.889360237140306\n\n\n\nfrom scipy.stats import poisson\n\nk = 4\nlikelihood = poisson(lams).pmf(k)\nposterior = prior * likelihood\nposterior.normalize()\n\n0.05015532557804499",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/11.html#켤레사전분포",
    "href": "posts/01_projects/adp_실기/notes/bayse/11.html#켤레사전분포",
    "title": "켤레사전분포",
    "section": "켤레사전분포",
    "text": "켤레사전분포\n\ndef make_gamma_dist(alpha, beta):\n    dist = gamma(alpha, scale=1/beta)\n    dist.alpha = alpha\n    dist.beta = beta\n    return dist\n\ndef update_gamma(prior, daata):\n    k, t = data\n    alpha = prior.alpha + k\n    beta = prior.beta + t\n    return make_gamma_dist(alpha, beta)\n\nprior_gamma = make_gamma_dist(1.4, 1)\ndata = 4, 1\nposterior_gamma = update_gamma(prior_gamma, data)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "부호검정",
    "text": "부호검정\n\n어떤 분포라도 중앙값이 존재한다.\n표본 중 절반은 중앙값보다 작고, 절반은 크다.\n검정통계량은 \\(\\sum_{i=1}^{n}I(X_i &gt; θ_0)\\)\n영분포는 B(n, 1/2)를 따른다.\n\n\\(P(S≥c_{val} | H_0) = \\sum_{k=c_{val}}^{n}nCk(\\frac{1}{2})^n ≤ α\\)\n\\(c_{val} = Q_{1-α} + 1\\)\n\n만약 이산형 변수여서 \\(θ_0\\)와 같은 값이 m개 이면 n - m개에 대해서만 검정을 진행\n대표본의 경우 이산형 분포가 정규분포로 근사할 수 있기 때문에, 근사해서 검정 진행\n\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\nnp.random.seed(42)\ndata = np.array([8, 12, 15, 9, 11, 13, 10, 14, 7, 16])\ntheta_0 = 10\npositive_count = np.sum(data &gt; theta_0)\nn = len(data)\n\nprint(f\"데이터: {data}\")\nprint(f\"가설검정 중앙값: {theta_0}\")\nprint(f\"총 관측값 개수: {n}\")\n\np_value_binom = stats.binomtest(positive_count, n, 0.5, alternative='two-sided')\nprint(f\"binom_test p-value: {p_value_binom}\")\n\ndef sign_test_normal_approx(data, theta_0):\n    n = len(data)\n    S = np.sum(data &gt; theta_0)\n    mu = n * 0.5\n    sigma = np.sqrt(n * 0.5 * 0.5)\n    # 0.5는 연속성 보정\n    z = (S - 0.5 - mu) / sigma if S &gt; mu else (S + 0.5 - mu) / sigma\n    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n    return S, z, p_value\n\nnp.random.seed(123)\nlarge_data = np.random.normal(12, 3, 100)\nS, z, p_value_norm = sign_test_normal_approx(large_data, 10)\n\nprint(f\"\\n=== 대표본 부호검정 (정규분포 근사) ===\")\nprint(f\"검정통계량 S: {S}\")\nprint(f\"Z-통계량: {z:.4f}\")\nprint(f\"p-value: {p_value_norm:.4f}\")\n\ndef calculate_critical_value(n, alpha=0.05):\n    \"\"\"\n    부호검정의 임계값 계산\n    \"\"\"\n    # 양측검정에서 alpha/2 수준\n    alpha_half = alpha / 2\n    \n    # 이항분포에서 임계값 찾기\n    critical_value = stats.binom.ppf(1 - alpha_half, n, 0.5)\n    \n    return int(critical_value)\n\n# 임계값 계산\nn_sample = 20\ncritical_val = calculate_critical_value(n_sample)\nprint(f\"\\n=== 임계값 계산 ===\")\nprint(f\"표본 크기: {n_sample}\")\nprint(f\"임계값: {critical_val}\")\nprint(f\"귀무가설 기각 조건: S &gt;= {critical_val} 또는 S &lt;= {n_sample - critical_val}\")\n\n데이터: [ 8 12 15  9 11 13 10 14  7 16]\n가설검정 중앙값: 10\n총 관측값 개수: 10\nbinom_test p-value: BinomTestResult(k=6, n=10, alternative='two-sided', statistic=0.6, pvalue=0.75390625)\n\n=== 대표본 부호검정 (정규분포 근사) ===\n검정통계량 S: 67\nZ-통계량: 3.3000\np-value: 0.0010\n\n=== 임계값 계산 ===\n표본 크기: 20\n임계값: 14\n귀무가설 기각 조건: S &gt;= 14 또는 S &lt;= 6\n\n\n\n부호검정 사용 시 주의사항\n\n동일한 값 처리: 중앙값과 정확히 같은 값들은 검정에서 제외\n표본 크기: 작은 표본에서는 정확한 이항분포 사용, 큰 표본에서는 정규분포 근사\n검정 방향: 양측검정 vs 단측검정 선택 중요\n가정: 표본이 연속분포에서 추출되었다고 가정",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정-사용-시-주의사항",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정-사용-시-주의사항",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "부호검정 사용 시 주의사항",
    "text": "부호검정 사용 시 주의사항\n\n동일한 값 처리: 중앙값과 정확히 같은 값들은 검정에서 제외\n표본 크기: 작은 표본에서는 정확한 이항분포 사용, 큰 표본에서는 정규분포 근사\n검정 방향: 양측검정 vs 단측검정 선택 중요\n가정: 표본이 연속분포에서 추출되었다고 가정",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#연속성-보정continuity-correction이란",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#연속성-보정continuity-correction이란",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "연속성 보정(Continuity Correction)이란?",
    "text": "연속성 보정(Continuity Correction)이란?\n연속성 보정은 이산분포를 연속분포로 근사할 때 발생하는 오차를 줄이기 위해 사용하는 기법입니다.\n\n왜 필요한가?\n\n이산분포 vs 연속분포:\n\n부호검정의 검정통계량 S는 이산값 (0, 1, 2, 3, …)\n정규분포는 연속분포 (모든 실수값 가능)\n\n근사 오차 발생:\n\n이산값을 연속분포로 근사할 때 정확도가 떨어짐\n특히 작은 표본에서 오차가 큼\n\n\n\n# 연속성 보정의 효과 비교\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\ndef compare_continuity_correction(n=20, p=0.5):\n    \"\"\"\n    연속성 보정 유무에 따른 근사 정확도 비교\n    \"\"\"\n    # 이항분포의 정확한 확률 계산\n    exact_probs = [stats.binom.pmf(k, n, p) for k in range(n+1)]\n    \n    # 정규분포 근사 (보정 없음)\n    mu = n * p\n    sigma = np.sqrt(n * p * (1-p))\n    no_correction = []\n    \n    # 정규분포 근사 (보정 있음)\n    with_correction = []\n    \n    for k in range(n+1):\n        # 보정 없음: P(X = k) ≈ P(k-0.5 &lt; Z &lt; k+0.5)\n        z1 = (k - 0.5 - mu) / sigma\n        z2 = (k + 0.5 - mu) / sigma\n        no_corr_prob = stats.norm.cdf(z2) - stats.norm.cdf(z1)\n        no_correction.append(no_corr_prob)\n        \n        # 보정 있음: 같은 결과 (이미 보정이 적용됨)\n        with_correction.append(no_corr_prob)\n    \n    return exact_probs, no_correction, with_correction\n\n# 비교 실행\nexact, no_corr, with_corr = compare_continuity_correction(n=20)\n\n# 시각화\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nx = range(21)\nplt.bar(x, exact, alpha=0.7, label='정확한 이항분포')\nplt.title('정확한 이항분포 (n=20, p=0.5)')\nplt.xlabel('k')\nplt.ylabel('확률')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 2)\nplt.bar(x, no_corr, alpha=0.7, color='orange', label='정규분포 근사')\nplt.title('정규분포 근사')\nplt.xlabel('k')\nplt.ylabel('확률')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.bar(x, [abs(e - n) for e, n in zip(exact, no_corr)], alpha=0.7, color='red')\nplt.title('오차 (|정확값 - 근사값|)')\nplt.xlabel('k')\nplt.ylabel('절대오차')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 연속성 보정 적용 예제\nprint(\"=== 연속성 보정 적용 예제 ===\")\nn = 20\nS = 15  # 관측된 검정통계량\nmu = n * 0.5\nsigma = np.sqrt(n * 0.5 * 0.5)\n\n# 보정 없음\nz_no_corr = (S - mu) / sigma\np_no_corr = 2 * (1 - stats.norm.cdf(abs(z_no_corr)))\n\n# 보정 있음 (S &gt; mu이므로 S - 0.5)\nz_with_corr = (S - 0.5 - mu) / sigma\np_with_corr = 2 * (1 - stats.norm.cdf(abs(z_with_corr)))\n\n# 정확한 이항분포 p-value\np_exact = 2 * (1 - stats.binom.cdf(S - 1, n, 0.5))\n\nprint(f\"검정통계량 S: {S}\")\nprint(f\"보정 없음 - Z: {z_no_corr:.4f}, p-value: {p_no_corr:.4f}\")\nprint(f\"보정 있음 - Z: {z_with_corr:.4f}, p-value: {p_with_corr:.4f}\")\nprint(f\"정확한 값 - p-value: {p_exact:.4f}\")\nprint(f\"보정 효과: {abs(p_exact - p_with_corr):.4f} vs {abs(p_exact - p_no_corr):.4f}\")\n\n\n\n\n\n\n\n\n=== 연속성 보정 적용 예제 ===\n검정통계량 S: 15\n보정 없음 - Z: 2.2361, p-value: 0.0253\n보정 있음 - Z: 2.0125, p-value: 0.0442\n정확한 값 - p-value: 0.0414\n보정 효과: 0.0028 vs 0.0160\n\n\n\n\n연속성 보정 공식\n# S &gt; μ일 때: 아래쪽 보정\nz = (S - 0.5 - μ) / σ\n\n# S &lt; μ일 때: 위쪽 보정  \nz = (S + 0.5 - μ) / σ\n\n# S = μ일 때: 보정 불필요\nz = (S - μ) / σ\n핵심 아이디어: 이산값 S를 연속구간 [S-0.5, S+0.5]로 변환하여 더 정확한 근사를 얻습니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#윌콕슨-부호순위검정",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#윌콕슨-부호순위검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "윌콕슨 부호순위검정",
    "text": "윌콕슨 부호순위검정\n\n모집단이 대칭인 경우 사용 가능\nrank가 동점인 경우 평균으로 처리\n영분포는 잘 알려져 있지 않은 분포\n평균: \\(\\frac{n(n+1)}{4}\\)\n분산: \\(\\frac{n(n+1)(2n+1)}{24}\\)\n일반적으로 resampling 방식으로 구하고, 수가 많아지면 표준정규분포로 근사해서 계산(Lyapunov의 중심극한정리)\n\n\n중앙값 추정과 신뢰구간\n\n왈시평균: \\(W_{ij} = \\frac{X_i + X_j}{2}\\)\n\\(SR_{+}(θ) = \\#{W_{ij} &gt; θ}\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#순열-검정",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#순열-검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "순열 검정",
    "text": "순열 검정",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/00.html#preprocessing",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/00.html#preprocessing",
    "title": "Titanic",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nData load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ntrain = pd.read_csv('_data/train.csv', index_col=0)\ntest_X = pd.read_csv('_data/test.csv', index_col=0)\ntrain_X = train.drop('Survived', axis=1)\ntrain_y = train.iloc[:, 0]\n\n\n\n결측치 처리\n\ntrain_X.info(), test_X.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 891 entries, 1 to 891\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    891 non-null    int64  \n 1   Name      891 non-null    object \n 2   Sex       891 non-null    object \n 3   Age       714 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Ticket    891 non-null    object \n 7   Fare      891 non-null    float64\n 8   Cabin     204 non-null    object \n 9   Embarked  889 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 76.6+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 418 entries, 892 to 1309\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    418 non-null    int64  \n 1   Name      418 non-null    object \n 2   Sex       418 non-null    object \n 3   Age       332 non-null    float64\n 4   SibSp     418 non-null    int64  \n 5   Parch     418 non-null    int64  \n 6   Ticket    418 non-null    object \n 7   Fare      417 non-null    float64\n 8   Cabin     91 non-null     object \n 9   Embarked  418 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 35.9+ KB\n\n\n(None, None)\n\n\n\n결측치가 있는 column은 Age, Cabin, Embarked, Fare\nCabin은 너무 많으니까 걍 삭제하자\n\n\ntrain_X.drop('Cabin', axis=1, inplace=True)\ntest_X.drop('Cabin', axis=1, inplace=True)\n\n\nAge, Fare랑 Embarked는 일단 지워보자\n\n\ntrain_X.dropna(subset=['Embarked', 'Age'], inplace=True)\ntest_X.dropna(subset=['Fare', 'Age'], inplace=True)\ntrain_X.info(), test_X.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 1 to 891\nData columns (total 9 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    712 non-null    int64  \n 1   Name      712 non-null    object \n 2   Sex       712 non-null    object \n 3   Age       712 non-null    float64\n 4   SibSp     712 non-null    int64  \n 5   Parch     712 non-null    int64  \n 6   Ticket    712 non-null    object \n 7   Fare      712 non-null    float64\n 8   Embarked  712 non-null    object \ndtypes: float64(2), int64(3), object(4)\nmemory usage: 55.6+ KB\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 331 entries, 892 to 1307\nData columns (total 9 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    331 non-null    int64  \n 1   Name      331 non-null    object \n 2   Sex       331 non-null    object \n 3   Age       331 non-null    float64\n 4   SibSp     331 non-null    int64  \n 5   Parch     331 non-null    int64  \n 6   Ticket    331 non-null    object \n 7   Fare      331 non-null    float64\n 8   Embarked  331 non-null    object \ndtypes: float64(2), int64(3), object(4)\nmemory usage: 25.9+ KB\n\n\n(None, None)\n\n\n\n\nEDA\n\ntrain_X.describe()\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n712.000000\n712.000000\n712.000000\n712.000000\n712.000000\n\n\nmean\n2.240169\n29.642093\n0.514045\n0.432584\n34.567251\n\n\nstd\n0.836854\n14.492933\n0.930692\n0.854181\n52.938648\n\n\nmin\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n1.000000\n20.000000\n0.000000\n0.000000\n8.050000\n\n\n50%\n2.000000\n28.000000\n0.000000\n0.000000\n15.645850\n\n\n75%\n3.000000\n38.000000\n1.000000\n1.000000\n33.000000\n\n\nmax\n3.000000\n80.000000\n5.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nPclass: 명목 1, 2, 3\nSex: 명목 male, female\nAge: 연속형 (0.42~80)\nSibSp: 명목, 0, 1, 2, 3, 4, 5\nParch: 명목, 0, 1, 2, 3, 4, 5, 6\nTicket: 541개 짜리 명목\nFare: 연속 (34.56~512.32)\nEmbarked: 명목 S C Q\n\n\nbindo = pd.DataFrame(train_X['Pclass'].value_counts())\n\nsns.barplot(bindo, x=\"Pclass\", y=\"count\", legend=False)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "Titanic"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/06.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/06.html#연습문제",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "연습문제",
    "text": "연습문제\n\n7-1\n\nn = 10000\na = np.random.randint(1, 7, size=(n, 4))\na.sort(axis=1)\nt = a[:, 1:].sum(axis=1)\npmf_best3 = Pmf.from_seq(t)\ncdf_best3 = pmf_best3.make_cdf()\n\n\nfrom empiricaldist import Cdf\nimport matplotlib.pyplot as plt\n\nstandard = [15,14,13,12,10,8]\nstandard_pmf = Pmf.from_seq(standard)\n\ncdf_best3.plot(label='best 3 of 4', color='C1', ls='--')\ncdf_standard = Cdf.from_seq(standard)\n\ncdf_standard.step(label='standard set', color='C7')\nplt.ylabel('CDF');\n\ncdf_max_dist6 = cdf_best3.max_dist(6)\ncdf_min_dist6 = cdf_best3.min_dist(6)\n\n\n\n\n\n\n\n\n\nprint(f\"Best 3 of 4 - 평균: {pmf_best3.mean():.2f}, 표준편차: {pmf_best3.std():.2f}\")\nprint(f\"Standard array - 평균: {standard_pmf.mean():.2f}, 표준편차: {standard_pmf.std():.2f}\")\n\n\nprob_less_than_8 = cdf_best3(7)\nprint(f\"Best 3 of 4에서 8보다 작은 값이 나올 확률: {prob_less_than_8:.4f}\")\n\nprint(f\"6번 굴렸을 때 최소 하나가 8보다 작을 확률: {(1 - cdf_min_dist6(8)):.4f}\")\n\nprob_greater_than_15 = 1 - cdf_best3(15)\nprint(f\"Best 3 of 4에서 15보다 큰 값이 나올 확률: {prob_greater_than_15:.4f}\")\nprint(f\"6번 굴렸을 때 최소 하나가 15보다 클 확률: {(1 - cdf_max_dist6(15)):.4f}\")\n\nBest 3 of 4 - 평균: 12.25, 표준편차: 2.82\nStandard array - 평균: 12.00, 표준편차: 2.38\nBest 3 of 4에서 8보다 작은 값이 나올 확률: 0.0569\n6번 굴렸을 때 최소 하나가 8보다 작을 확률: 0.5206\nBest 3 of 4에서 15보다 큰 값이 나올 확률: 0.1275\n6번 굴렸을 때 최소 하나가 15보다 클 확률: 0.5588\n\n\n\n\n7-2\n\ndef update_dice(pmf, data):\n    hypos = pmf.qs\n    likelihood = 1 / hypos\n    likelihood[data &gt; hypos] = 0\n    pmf *= likelihood\n    pmf.normalize()\n\npmf = Pmf.from_seq([6, 8, 10])\nupdate_dice(pmf, 1)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.425532\n\n\n8\n0.319149\n\n\n10\n0.255319\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\npmf_6 = Pmf.from_seq(range(1, 7))  # 6면체 주사위\npmf_8 = Pmf.from_seq(range(1, 9))  # 8면체 주사위\npmf_10 = Pmf.from_seq(range(1, 11)) # 10면체 주사위\n\nmixture = make_mixture(pmf, [pmf_6, pmf_8, pmf_10])\nprob_6_damage = mixture[6] if 6 in mixture.qs else 0\nprint(f\"\\nProbability of 6 points of damage: {prob_6_damage:.4f}\")\n\n\nProbability of 6 points of damage: 0.1363\n\n\n\n\n7-3\n\nmean = 950\nstd = 50\n\nsample = np.random.normal(mean, std, size=365)\npmf = Pmf.from_seq(sample)\ncdf = pmf.make_cdf()\n\n\nmeans = []\nn_values = np.arange(1, 20)\n\nfor n in n_values:\n    cdf_max_dist = cdf.max_dist(n)\n    mean_max = cdf_max_dist.mean()\n    means.append(mean_max)\n\nmeans = np.array(means)\ntarget = 1000\nclosest_idx = np.argmin(np.abs(means - target))\noptimal_n = n_values[closest_idx]\nclosest_mean = means[closest_idx]\n\nprint(f\"\\n결과:\")\nprint(f\"1000g에 가장 가까운 평균을 만드는 n: {optimal_n}\")\nprint(f\"해당 n에서의 평균: {closest_mean:.2f}g\")\nprint(f\"목표값과의 차이: {abs(closest_mean - target):.2f}g\")\n\n\n결과:\n1000g에 가장 가까운 평균을 만드는 n: 3\n해당 n에서의 평균: 997.51g\n목표값과의 차이: 2.49g",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/07.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/07.html#연습문제",
    "title": "포아송 과정",
    "section": "연습문제",
    "text": "연습문제\n\n8-1\n\nfrom scipy.stats import gamma\nimport numpy as np\nfrom empiricaldist import Pmf\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nalpha = 1.4\nqs = np.linspace(0, 10, 101)\nps = gamma.pdf(qs, alpha)\nprior = Pmf(ps, qs)\nprior.normalize()\n\n9.889360237140306\n\n\n\ndef expo_pdf(t, lam):\n    return lam * np.exp(-lam * t)\n\nt1 = 11/90\nlikelihood1 = expo_pdf(t1, prior.qs)\npost1 = prior * likelihood1\npost1.normalize()\n\n# 지수분포에서 모든 시행은 독립적\nt2 = 12/90\nlikelihood2 = expo_pdf(t2, post1.qs)\npost2 = post1 * likelihood2\npost2.normalize()\n\nprior.plot(label=\"사전분포\", alpha=0.7)\npost1.plot(label=\"1골 득점 후 사후분포\", alpha=0.7)\npost2.plot(label=\"2골 득점 후 사후분포\", alpha=0.7)\nplt.xlabel(\"λ (게임당 골 수)\")\nplt.ylabel(\"확률밀도\")\nplt.title(\"독일의 골 득점률 분포\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import poisson\nimport pandas as pd\nimport numpy as np\n\ndef make_poisson_pmf(lam, qs):\n    ps = poisson.pmf(qs, lam)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\nremaining_time = (90 - 23) / 90\ngoals = np.arange(15)\npmf_seq = [make_poisson_pmf(lam * remaining_time, goals) for lam in post2.qs]\npred = make_mixture(post2, pmf_seq)\n\nplt.plot(pred)\nplt.title('독일의 남은 시간 동안의 골 득점 예측')\nplt.xlabel('골 수')\nplt.ylabel('확률')\nplt.show()\n\n\n\n\n\n\n\n\n\npred[5:].sum()\n\n0.09386958056810232\n\n\n\n\n8-2\n\ndef update_goal(prior, data):\n    posterior = prior.copy()\n    for goals in data:\n        likelihood = [poisson.pmf(goals, lam) for lam in posterior.qs]\n        posterior = posterior * likelihood\n        posterior.normalize()\n    return posterior\n\n\nfrance = update_goal(prior, [4])\ncroatia = update_goal(prior, [2])\n\nqs = np.linspace(0, 1, 101)\nlikelihood = expo_pdf(qs, prior.qs)\n\nfrance_post = france * likelihood\nfrance_post.normalize()\ncroatia_post = croatia * likelihood\ncroatia_post.normalize()\n\nPmf.prob_lt(france_post, croatia_post)\n\n0.2633977680689847\n\n\n\n\n8-3\n\nalpha = 2.8\nqs = np.linspace(0, 10, 101)\n\nprior = Pmf(gamma.pdf(qs, alpha), qs)\nprior.normalize()\n\n\nboston_posterior = update_goal(prior, [0, 2, 8, 4])\nvancouver_posterior = update_goal(prior, [1, 3, 1, 0])\n\nprior.plot(label='prior')\nboston_posterior.plot(label='boston')\nvancouver_posterior.plot(label='vancouver')\nplt.legend()\n\n\n\n\n\n\n\n\n\npmf_seq = [make_poisson_pmf(lam, goals) for lam in prior.qs]\npred_boston = make_mixture(boston_posterior, pmf_seq)\npred_vancouver = make_mixture(vancouver_posterior, pmf_seq)\nwin = Pmf.prob_gt(pred_boston, pred_vancouver)\nwin\n\n0.703863141313654\n\n\n챔피언십 우승 확률은 아직은 못 구하겠다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "포아송 과정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/15.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/15.html",
    "title": "로지스틱 회귀",
    "section": "",
    "text": "\\(logO(H|x) = β_0 + β_1x\\)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "로지스틱 회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/15.html#우주-왕복선-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/15.html#우주-왕복선-문제",
    "title": "로지스틱 회귀",
    "section": "우주 왕복선 문제",
    "text": "우주 왕복선 문제\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv')\npred = df.iloc[-1]\ndf = df[:-1].dropna()\noffset = df['Temperature'].mean().round()\ndf['x'] = df['Temperature'] - offset\ndf['y'] = df['Damage Incident'].astype(int)\n\ndf\n\n\n\n\n\n\n\n\nDate\nTemperature\nDamage Incident\nx\ny\n\n\n\n\n0\n04/12/1981\n66\n0\n-4.0\n0\n\n\n1\n11/12/1981\n70\n1\n0.0\n1\n\n\n2\n3/22/82\n69\n0\n-1.0\n0\n\n\n4\n01/11/1982\n68\n0\n-2.0\n0\n\n\n5\n04/04/1983\n67\n0\n-3.0\n0\n\n\n6\n6/18/83\n72\n0\n2.0\n0\n\n\n7\n8/30/83\n73\n0\n3.0\n0\n\n\n8\n11/28/83\n70\n0\n0.0\n0\n\n\n9\n02/03/1984\n57\n1\n-13.0\n1\n\n\n10\n04/06/1984\n63\n1\n-7.0\n1\n\n\n11\n8/30/84\n70\n1\n0.0\n1\n\n\n12\n10/05/1984\n78\n0\n8.0\n0\n\n\n13\n11/08/1984\n67\n0\n-3.0\n0\n\n\n14\n1/24/85\n53\n1\n-17.0\n1\n\n\n15\n04/12/1985\n67\n0\n-3.0\n0\n\n\n16\n4/29/85\n75\n0\n5.0\n0\n\n\n17\n6/17/85\n70\n0\n0.0\n0\n\n\n18\n7/29/85\n81\n0\n11.0\n0\n\n\n19\n8/27/85\n76\n0\n6.0\n0\n\n\n20\n10/03/1985\n79\n0\n9.0\n0\n\n\n21\n10/30/85\n75\n1\n5.0\n1\n\n\n22\n11/26/85\n76\n0\n6.0\n0\n\n\n23\n01/12/1986\n58\n1\n-12.0\n1\n\n\n\n\n\n\n\n\n전통 로지스틱\n\nimport statsmodels.formula.api as smf\n\nformula = 'y ~ x'\nresults = smf.logit(formula, data=df).fit(disp=False)\nresults.params\n\nIntercept   -1.208490\nx           -0.232163\ndtype: float64\n\n\n\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ninter = results.params['Intercept']\nslope = results.params['x']\nxs = np.arange(53, 83) - offset\nps = expit(inter + slope * xs)\nplt.plot(xs + offset, ps)\nplt.scatter(df['x'] + offset, df['y'])\n\n\n\n\n\n\n\n\n\n\n사전 분포\n\nfrom empiricaldist import Pmf\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\nqs = np.linspace(-5, 1, 101)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(-0.8, 0.1, 101)\nprior_slope = Pmf.from_seq(qs)\njoint = make_joint(prior_inter, prior_slope)\njoint_pmf = Pmf(joint.stack())\njoint_pmf\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.8\n-5.00\n0.000098\n\n\n-4.94\n0.000098\n\n\n-4.88\n0.000098\n\n\n-4.82\n0.000098\n\n\n-4.76\n0.000098\n\n\n...\n...\n...\n\n\n0.1\n0.76\n0.000098\n\n\n0.82\n0.000098\n\n\n0.88\n0.000098\n\n\n0.94\n0.000098\n\n\n1.00\n0.000098\n\n\n\n\n10201 rows × 1 columns\n\n\n\n\n\n가능도\n\nfrom scipy.stats import binom\n\ngrouped = df.groupby('x')['y'].agg(['count', 'sum'])\nns = grouped['count']\nks = grouped['sum']\nxs = grouped.index\nps = expit(inter + slope * xs)\nlikes = binom.pmf(ks, ns, ps)\nlikes\n\narray([0.93924781, 0.85931657, 0.82884484, 0.60268105, 0.56950687,\n       0.24446388, 0.67790595, 0.72637895, 0.18815003, 0.8419509 ,\n       0.87045398, 0.15645171, 0.86667894, 0.95545945, 0.96435859,\n       0.97729671])\n\n\n\nlikelihood = joint_pmf.copy()\nfor slope, inter in joint_pmf.index:\n    ps = expit(inter + slope * xs)\n    likes = binom.pmf(ks, ns, ps)\n    likelihood[slope, inter] = likes.prod()\n\n\n\n갱신\n\nposterior_pmf = joint_pmf * likelihood\nposterior_pmf.normalize()\njoint_posterior = posterior_pmf.unstack()\n\nmarginal_inter = Pmf(joint_posterior.sum(axis=0))\nmarginal_inter.normalize()\nmarginal_slope = Pmf(joint_posterior.sum(axis=1))\nmarginal_slope.normalize()\n\nmarginal_inter.plot()\n\n\n\n\n\n\n\n\n\nmarginal_slope.plot()\n\n\n\n\n\n\n\n\n\n\n분포 변환\n\nmarginal_probs = marginal_inter.transform(expit)\nmarginal_lr = marginal_slope.transform(np.exp)\n\n\n\n예측 분포\n\nsample = posterior_pmf.choice(101)\ntemps = np.arange(31, 83)\nxs = temps - offset\npred = np.empty((len(sample), len(xs)))\nfor i, (slope, inter) in enumerate(sample):\n    pred[i] = expit(inter + slope * xs)\nlow, median, high = np.percentile(pred, [5, 50, 95], axis=0)\nplt.fill_between(temps, low, high, color='C1', alpha=0.2, label='95% 신뢰구간')\nplt.plot(temps, median, color='C1', label='logistic model')\nplt.legend()\nplt.scatter(df['x'] + offset, df['y'], label='data')",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "로지스틱 회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/14.html#팽귄-데이터",
    "href": "posts/01_projects/adp_실기/notes/bayse/14.html#팽귄-데이터",
    "title": "분류",
    "section": "팽귄 데이터",
    "text": "팽귄 데이터\n\nimport pandas as pd\n\ndf = pd.read_csv('https://github.com/allisonhorst/palmerpenguins/raw/main/inst/extdata/penguins_raw.csv')\ndf.shape\n\n(344, 17)\n\n\n\ndef make_cdf_map(df, colname, by='Species2'):\n  cdf_map = {}\n  grouped = df.groupby(by)[colname]\n  for species, group in grouped:\n    cdf_map[species] = Cdf.from_seq(group, name=species)\n  return cdf_map",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/16.html#더-많은-눈이-내렸을까",
    "href": "posts/01_projects/adp_실기/notes/bayse/16.html#더-많은-눈이-내렸을까",
    "title": "회귀",
    "section": "더 많은 눈이 내렸을까?",
    "text": "더 많은 눈이 내렸을까?\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf = pd.read_csv('https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv', parse_dates=[2])\ndf['YEAR'] = df['DATE'].dt.year\nsnow = df.groupby('YEAR')['SNOW'].sum()\nsnow = snow.iloc[1:-1]\nsnow.plot(ls='', marker='o', label='강설량')\nplt.legend()\n\n\n\n\n\n\n\n\n\nfrom empiricaldist import Pmf\nimport statsmodels.formula.api as smf\ndata = snow.reset_index()\noffset = data['YEAR'].mean().round()\ndata['x'] = data['YEAR'] - offset\ndata['y'] = data['SNOW']\n\nformula = 'y ~ x'\nresults = smf.ols(formula, data=data).fit()\nresults.params\n\nIntercept    64.446325\nx             0.511880\ndtype: float64\n\n\n\n사전분포\n\nqs = np.linspace(-0.5, 1.5, 51)\nprior_slope = Pmf.from_seq(qs)\nqs = np.linspace(54, 75, 41)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(20, 35, 31)\nprior_sigma = Pmf.from_seq(qs)\n\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.index, index=pmf2.index)\n\ndef make_joint3(pmf1, pmf2, pmf3):\n    joint2 = make_joint(pmf2, pmf1).stack()\n    joint3 = make_joint(pmf3, joint2).stack()\n    return Pmf(joint3)\n\nprior = make_joint3(prior_slope, prior_inter, prior_sigma)\nprior\n\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.5\n54.0\n20.0\n0.000015\n\n\n20.5\n0.000015\n\n\n21.0\n0.000015\n\n\n21.5\n0.000015\n\n\n22.0\n0.000015\n\n\n...\n...\n...\n...\n\n\n1.5\n75.0\n33.0\n0.000015\n\n\n33.5\n0.000015\n\n\n34.0\n0.000015\n\n\n34.5\n0.000015\n\n\n35.0\n0.000015\n\n\n\n\n64821 rows × 1 columns",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/13.html#a의-키",
    "href": "posts/01_projects/adp_실기/notes/bayse/13.html#a의-키",
    "title": "비교",
    "section": "A의 키",
    "text": "A의 키\n\n미국 성인 남성 중 두명을 임의로 골랐다.\nA가 B보다 큰 것 같을 때, A의 키는 얼마인가\n\n\nimport numpy as np\nfrom scipy.stats import norm\nfrom empiricaldist import Pmf\n\nmean = 178\nstd = 7.7\n\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\nprior = Pmf(ps, qs)\nprior.normalize()\n\n1.9963309462450582\n\n\n\nimport pandas as pd\n\ndef make_joint(pmf1, pmf2):\n  X, Y = np.meshgrid(pmf1, pmf2)\n  return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\njoint = make_joint(prior, prior)\nplt.contour(joint.columns, joint.index, joint, linewidths=2)\nplt.xlabel('A의 키(cm)')\nplt.ylabel('B의 키(cm)')\n\nText(0, 0.5, 'B의 키(cm)')\n\n\n\n\n\n\n\n\n\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X &gt; Y), 1, 0)\nlikelihood = pd.DataFrame(a, index=x, columns=y)\nposterior = joint * likelihood\n\n\ndef normalize(pdf):\n  prob_data = joint.to_numpy().sum()\n  pdf /= prob_data\n  return prob_data\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_B = Pmf(posterior.sum(axis=1))\nmarginal_A.normalize()\nmarginal_B.normalize()\nmarginal_A.plot()\nmarginal_B.plot()\nprior.plot()\n\n\n\n\n\n\n\n\n\nmarginal_A.mean(), marginal_B.mean()\n\n(182.38728123421686, 173.60286000233393)\n\n\n\nA의 키가 170이라면?\n\n\npmf = Pmf(posterior[170])\npmf.normalize()\npmf.plot()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/13.html#연습-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/13.html#연습-문제",
    "title": "비교",
    "section": "연습 문제",
    "text": "연습 문제\n\n11-1\n\npmf = Pmf(posterior.loc[180])\npmf.normalize()\npmf.plot()\n\n\n\n\n\n\n\n\n\n\n11-2\n\nmean = 163\nstd = 7.3\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\ngirl_prior = Pmf(ps, qs)\ngirl_prior.normalize()\njoint = make_joint(marginal_A, girl_prior)\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X - Y &gt;= 15), 1, 0)\nlikelihood = pd.DataFrame(a, index=y, columns=x)\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A.plot(label='c 정보 이전')\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\nmarginal_A.plot(label='c 정보 이후')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n11-3\n\nmean_A = 1600\nstd_A = 100\nqs_A = np.arange(mean_A - 300, mean_A + 300, 10)\nps_A = norm(mean_A, std_A).pdf(qs_A)\nprior_A = Pmf(ps_A, qs_A)\nprior_A.normalize()\n\nmean_B = 1800\nstd_B = 100\nqs_B = np.arange(mean_B - 300, mean_B + 300, 10)\nps_B = norm(mean_B, std_B).pdf(qs_B)\nprior_B = Pmf(ps_B, qs_B)\nprior_B.normalize()\n\njoint = make_joint(prior_A, prior_B)\n\n\ndef logistic_prob(r_a, r_b):\n    return 1 / (1 + 10**((r_b - r_a) / 400))\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = 1 / (1 + 10**((Y - X) / 400))\nlikelihood = pd.DataFrame(a, index=y, columns=x)\n\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\n\nprint(marginal_A.max_prob(), marginal_A.mean())\nmarginal_A.plot()\n\n1640 1636.648345528236",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/17.html#월드컵-문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/17.html#월드컵-문제",
    "title": "켤레사전분포",
    "section": "월드컵 문제",
    "text": "월드컵 문제\n\nfrom scipy.stats import gamma\nimport numpy as np\nfrom empiricaldist import Pmf\n\nalpha = 1.4\ndist = gamma(alpha)\n\nlams = np.linspace(0, 10, 101)\nprior = Pmf(dist.pdf(lams), lams)\nprior.normalize()\n\n9.889360237140306\n\n\n\nfrom scipy.stats import poisson\n\nk = 4\nlikelihood = poisson(lams).pmf(k)\nposterior = prior * likelihood\nposterior.normalize()\n\n0.05015532557804499",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/17.html#켤레사전분포",
    "href": "posts/01_projects/adp_실기/notes/bayse/17.html#켤레사전분포",
    "title": "켤레사전분포",
    "section": "켤레사전분포",
    "text": "켤레사전분포\n\ndef make_gamma_dist(alpha, beta):\n    dist = gamma(alpha, scale=1/beta)\n    dist.alpha = alpha\n    dist.beta = beta\n    return dist\n\ndef update_gamma(prior, daata):\n    k, t = data\n    alpha = prior.alpha + k\n    beta = prior.beta + t\n    return make_gamma_dist(alpha, beta)\n\nprior_gamma = make_gamma_dist(1.4, 1)\ndata = 4, 1\nposterior_gamma = update_gamma(prior_gamma, data)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정이항검정",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/01.html#부호검정이항검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "부호검정(이항검정)",
    "text": "부호검정(이항검정)\n\n모집단 분포가 정규분포가 아니고, 대칭이 아닐경우 사용\n어떤 분포라도 중앙값이 존재한다.\n표본 중 절반은 중앙값보다 작고, 절반은 크다.\n검정통계량은 \\(\\sum_{i=1}^{n}I(X_i &gt; θ_0)\\)\n영분포는 B(n, 1/2)를 따른다.\n\n\\(P(S≥c_{val} | H_0) = \\sum_{k=c_{val}}^{n}nCk(\\frac{1}{2})^n ≤ α\\)\n\\(c_{val} = Q_{1-α} + 1\\)\n\n만약 이산형 변수여서 \\(θ_0\\)와 같은 값이 m개 이면 n - m개에 대해서만 검정을 진행\n\n\nsign_test &lt;- function(S0, sample, dir='two') {\n    n &lt;- sum(sample != S0)\n    Sobs &lt;- sum(sample &gt; S0)\n    \n    if (dir == 'greater') {\n        pvalue &lt;- 1 - pbinom(Sobs-1, n, 1/2)\n    } else if (dir == 'less') {\n        pvalue &lt;- pbinom(Sobs, n, 1/2)\n    } else {\n        prob_lower &lt;- pbinom(Sobs, n, 1/2)\n        prob_upper &lt;- 1 - pbinom(Sobs-1, n, 1/2)\n        pvalue &lt;- 2 * min(prob_lower, prob_upper)\n    }\n    \n    return(pvalue)\n}\n\nS0 &lt;- 100\niq &lt;- c(98, 121, 110, 89, 109, 108, 102, 92, 131, 114)\nsign_test(S0, iq, 'greater')\n\n[1] 0.171875\n\n\n\nquantile(iq, probs = 0.95)\n\n  95% \n126.5 \n\n\n\nsign_test(127.5, iq, 'greater')\n\n[1] 0.9990234\n\n\n\n부호검정의 영분포 시각화\n\nlibrary(ggplot2)\n\n# 부호검정 영분포 plotting 함수\nplot_sign_test_null &lt;- function(sample, S0, dir = 'two', alpha = 0.05) {\n    n &lt;- sum(sample != S0)\n    Sobs &lt;- sum(sample &gt; S0)\n    \n    # 이항분포 B(n, 0.5)의 가능한 값들\n    x_vals &lt;- 0:n\n    probs &lt;- dbinom(x_vals, n, 0.5)\n    \n    # 데이터프레임 생성\n    df &lt;- data.frame(x = x_vals, prob = probs)\n    \n    # p-value 계산 및 영역 표시를 위한 색상 설정\n    if (dir == 'greater') {\n        critical_region &lt;- x_vals &gt;= Sobs\n        pvalue &lt;- 1 - pbinom(Sobs-1, n, 0.5)\n        title_suffix &lt;- \"(우측검정)\"\n    } else if (dir == 'less') {\n        critical_region &lt;- x_vals &lt;= Sobs\n        pvalue &lt;- pbinom(Sobs, n, 0.5)\n        title_suffix &lt;- \"(좌측검정)\"\n    } else {\n        # 양측검정\n        prob_lower &lt;- pbinom(Sobs, n, 0.5)\n        prob_upper &lt;- 1 - pbinom(Sobs-1, n, 0.5)\n        if (prob_lower &lt;= prob_upper) {\n            critical_region &lt;- x_vals &lt;= Sobs | x_vals &gt;= (n - Sobs)\n        } else {\n            critical_region &lt;- x_vals &gt;= Sobs | x_vals &lt;= (n - Sobs)\n        }\n        pvalue &lt;- 2 * min(prob_lower, prob_upper)\n        title_suffix &lt;- \"(양측검정)\"\n    }\n    \n    df$region &lt;- ifelse(critical_region, \"p-value 영역\", \"채택역\")\n    \n    # 그래프 생성\n    p &lt;- ggplot(df, aes(x = x, y = prob)) +\n        geom_col(aes(fill = region), alpha = 0.7, width = 0.8) +\n        geom_point(data = df[df$x == Sobs, ], \n                   aes(x = x, y = prob), \n                   color = \"red\", size = 4, shape = 19) +\n        scale_fill_manual(values = c(\"p-value 영역\" = \"red\", \"채택역\" = \"lightblue\")) +\n        labs(\n            title = paste0(\"부호검정의 영분포 B(\", n, \", 0.5) \", title_suffix),\n            subtitle = paste0(\"관찰값 S = \", Sobs, \", p-value = \", round(pvalue, 4)),\n            x = \"검정통계량 S (중앙값보다 큰 값의 개수)\",\n            y = \"확률\",\n            fill = \"영역\"\n        ) +\n        theme_minimal() +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 14),\n            plot.subtitle = element_text(hjust = 0.5, size = 12),\n            legend.position = \"bottom\"\n        ) +\n        scale_x_continuous(breaks = x_vals)\n    \n    return(p)\n}\n\nS0 &lt;- 100\niq &lt;- c(98, 121, 110, 89, 109, 108, 102, 92, 131, 114)\n\nplot_sign_test_null(iq, S0, 'greater')\n\n\n\n\n\n\n\n\n\nplot_sign_test_null(iq, S0, 'two')\n\n\n\n\n\n\n\n\n\nplot_sign_test_null(iq, 127.5, 'less')\n\n\n\n\n\n\n\n\n\nα를 정확하게 통제할 수 없다.\n\n대표본의 경우 이산형 분포가 정규분포로 근사할 수 있기 때문에, 근사해서 검정 진행해서 α 통제\n\n\n\n\n신뢰구간\n\n# 신뢰구간 계산\na = 0.05\nn = length(iq)\nl = qbinom(a / 2, n, 1/2)\nu = n + 1 - l\nsorted_iq = sort(iq)\n(ci = sorted_iq[c(l, u)])\n\n[1]  92 121\n\n\n\n# 실제 신뢰도 계산 (이산분포 특성상 정확히 95%가 아님)\n(coverage = sum(dbinom(l:(u-1), n, 1/2)))\n\n[1] 0.9785156\n\n\n왜 비대칭 분포에서도 유효한가?\n\n중앙값의 불변성: 분포가 비대칭이어도 중앙값은 여전히 모집단을 반으로 나누는 점\n부호의 독립성: 각 관측값이 중앙값보다 클 확률은 항상 0.5 (연속분포 가정)\n순서통계량의 분포: 순서통계량들 사이의 확률적 관계는 분포 형태와 무관\n\n시뮬레이션 검증: 비대칭 분포에서의 신뢰구간\n\n# 부호검정 신뢰구간 함수\nsign_ci &lt;- function(sample, alpha = 0.05) {\n    n &lt;- length(sample)\n    l &lt;- qbinom(alpha / 2, n, 0.5)\n    u &lt;- n + 1 - l\n    sorted_sample &lt;- sort(sample)\n    return(c(lower = sorted_sample[l], upper = sorted_sample[u]))\n}\n\n# 비대칭 분포 시뮬레이션\nset.seed(123)\nn_sim &lt;- 1000\nn_sample &lt;- 20\ncoverage_results &lt;- data.frame(\n    distribution = character(),\n    true_median = numeric(),\n    coverage_rate = numeric(),\n    stringsAsFactors = FALSE\n)\n\n# 1. 정규분포 (대칭)\ntrue_median_normal &lt;- 0\ncoverage_normal &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rnorm(n_sample, mean = 0, sd = 1)\n    ci &lt;- sign_ci(sample)\n    if (true_median_normal &gt;= ci[1] && true_median_normal &lt;= ci[2]) {\n        coverage_normal &lt;- coverage_normal + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"정규분포\", \n               true_median = true_median_normal,\n               coverage_rate = coverage_normal / n_sim))\n\n# 2. 지수분포 (오른쪽 비대칭)\ntrue_median_exp &lt;- log(2)  # 지수분포의 중앙값\ncoverage_exp &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rexp(n_sample, rate = 1)\n    ci &lt;- sign_ci(sample)\n    if (true_median_exp &gt;= ci[1] && true_median_exp &lt;= ci[2]) {\n        coverage_exp &lt;- coverage_exp + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"지수분포\", \n               true_median = true_median_exp,\n               coverage_rate = coverage_exp / n_sim))\n\n# 3. 베타분포 (왼쪽 비대칭)\ntrue_median_beta &lt;- qbeta(0.5, shape1 = 5, shape2 = 2)\ncoverage_beta &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rbeta(n_sample, shape1 = 5, shape2 = 2)\n    ci &lt;- sign_ci(sample)\n    if (true_median_beta &gt;= ci[1] && true_median_beta &lt;= ci[2]) {\n        coverage_beta &lt;- coverage_beta + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"베타분포(5,2)\", \n               true_median = true_median_beta,\n               coverage_rate = coverage_beta / n_sim))\n\nprint(coverage_results)\n\n   distribution true_median coverage_rate\n1      정규분포   0.0000000         0.952\n2      지수분포   0.6931472         0.971\n3 베타분포(5,2)   0.7355500         0.961\n\n\n\n# 각 분포의 형태와 중앙값 시각화\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nset.seed(123)\nx_normal &lt;- rnorm(1000)\nx_exp &lt;- rexp(1000)\nx_beta &lt;- rbeta(1000, 5, 2)\n\np1 &lt;- ggplot() + \n    geom_histogram(aes(x = x_normal), bins = 30, alpha = 0.7, fill = \"blue\") +\n    geom_vline(xintercept = 0, color = \"red\", size = 1) +\n    labs(title = \"정규분포 (대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 0.5, y = 80, label = \"중앙값 = 0\", color = \"red\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- ggplot() + \n    geom_histogram(aes(x = x_exp), bins = 30, alpha = 0.7, fill = \"green\") +\n    geom_vline(xintercept = log(2), color = \"red\", size = 1) +\n    labs(title = \"지수분포 (오른쪽 비대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 2, y = 150, label = paste(\"중앙값 =\", round(log(2), 3)), color = \"red\")\n\np3 &lt;- ggplot() + \n    geom_histogram(aes(x = x_beta), bins = 30, alpha = 0.7, fill = \"orange\") +\n    geom_vline(xintercept = qbeta(0.5, 5, 2), color = \"red\", size = 1) +\n    labs(title = \"베타분포(5,2) (왼쪽 비대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 0.5, y = 80, label = paste(\"중앙값 =\", round(qbeta(0.5, 5, 2), 3)), color = \"red\")\n\ngrid.arrange(p1, p2, p3, ncol = 1)\n\n\n\n\n\n\n\n\n결론: 시뮬레이션 결과에서 볼 수 있듯이, 분포가 대칭이든 비대칭이든 부호검정의 신뢰구간 포함률(coverage rate)은 명목 신뢰수준(95%)에 근접합니다. 이는 부호검정 신뢰구간이 분포무관(distribution-free) 성질을 가지기 때문입니다.\n\n\n부호검정 사용 시 주의사항\n\n동일한 값 처리: 중앙값과 정확히 같은 값들은 검정에서 제외\n표본 크기: 작은 표본에서는 정확한 이항분포 사용, 큰 표본에서는 정규분포 근사\n검정 방향: 양측검정 vs 단측검정 선택 중요\n가정: 표본이 연속분포에서 추출되었다고 가정",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/07.html#포아송-모델의-가정과-한계점",
    "href": "posts/01_projects/adp_실기/notes/bayse/07.html#포아송-모델의-가정과-한계점",
    "title": "포아송 과정",
    "section": "포아송 모델의 가정과 한계점",
    "text": "포아송 모델의 가정과 한계점\n포아송 모델은 다음과 같은 가정을 기반으로 합니다:\n\n독립성: 각 골은 서로 독립적으로 발생\n정상성: 골 득점률이 시간에 따라 일정\n희소성: 매우 짧은 시간 간격에서는 최대 1골만 발생\n\n\n실제 하키 경기에서의 위반 사례\n\n팀 전략 변화:\n\n리드하고 있는 팀은 수비적으로 변할 수 있음\n뒤처진 팀은 마지막에 골키퍼를 빼고 공격수를 추가\n\n심리적 요인:\n\n연속 득점으로 인한 모멘텀 효과\n상대팀의 사기 저하\n\n물리적 피로:\n\n경기 후반으로 갈수록 체력 저하\n부상으로 인한 선수 교체\n\n상호 의존성:\n\n한 팀의 득점이 상대팀의 전략에 영향\n파워플레이/페널티킬 상황\n\n\n\n\n예측에 미치는 영향\n이러한 위반은 다음과 같은 영향을 미칠 수 있습니다:\n\n과소추정: 모멘텀 효과를 고려하지 않아 연속 득점 확률을 낮게 예측\n과대추정: 전략 변화를 무시하여 높은 득점 게임의 지속성을 과대평가\n불확실성 증가: 모델이 고려하지 못하는 요인들로 인한 예측 정확도 감소\n\n따라서 실제 베팅이나 중요한 의사결정에서는 이러한 한계점을 고려하여 모델 결과를 해석해야 합니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "포아송 과정"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/08.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/08.html",
    "title": "의사결정분석",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf1 = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/showcases.2011.csv', index_col=0, skiprows=[1]).dropna().transpose()\ndf2 = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/showcases.2012.csv', index_col=0, skiprows=[1]).dropna().transpose()\ndf = pd.concat([df1, df2], ignore_index=True)\ndf\n\n\n\n\n\n\n\n\nShowcase 1\nShowcase 2\nBid 1\nBid 2\nDifference 1\nDifference 2\n\n\n\n\n0\n50969.0\n45429.0\n42000.0\n34000.0\n8969.0\n11429.0\n\n\n1\n21901.0\n34061.0\n14000.0\n59900.0\n7901.0\n-25839.0\n\n\n2\n32815.0\n53186.0\n32000.0\n45000.0\n815.0\n8186.0\n\n\n3\n44432.0\n31428.0\n27000.0\n38000.0\n17432.0\n-6572.0\n\n\n4\n24273.0\n22320.0\n18750.0\n23000.0\n5523.0\n-680.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n308\n25375.0\n31986.0\n36000.0\n32000.0\n-10625.0\n-14.0\n\n\n309\n24949.0\n30696.0\n20500.0\n31000.0\n4449.0\n-304.0\n\n\n310\n23662.0\n22329.0\n26000.0\n20000.0\n-2338.0\n2329.0\n\n\n311\n23704.0\n34325.0\n23800.0\n34029.0\n-96.0\n296.0\n\n\n312\n20898.0\n23876.0\n28000.0\n25000.0\n-7102.0\n-1124.0\n\n\n\n\n313 rows × 6 columns\nfrom scipy.stats import gaussian_kde\nfrom empiricaldist import Pmf\n\ndef kde_from_sample(sample, qs):\n    kde = gaussian_kde(sample)\n    ps = kde(qs)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf\n\nqs = np.linspace(0, 80000, 81)\nprior1 = kde_from_sample(df['Showcase 1'], qs)\nprior2 = kde_from_sample(df['Showcase 2'], qs)\n\nprior1.plot(label='진열대 1번의 사전확률')\nprior2.plot(label='진열대 2번의 사전확률')\nplt.xlabel('진열대 물건 총 금액')\nplt.ylabel('PMF')\nplt.legend()\nsample_diff1 = df['Bid 1'] - df['Showcase 1']\nsample_diff2 = df['Bid 2'] - df['Showcase 2']\n\nqs = np.linspace(-40000, 20000, 61)\nkde_diff1 = kde_from_sample(sample_diff1, qs)\nkde_diff2 = kde_from_sample(sample_diff2, qs)\n\nkde_diff1.plot(label='1번의 차이')\nkde_diff2.plot(label='2번의 차이')\nplt.xlabel('차이 금액')\nplt.ylabel('PMF')\nplt.legend()\nfrom scipy.stats import norm\n\nstd_diff1 = sample_diff1.std()\nstd_diff2 = sample_diff2.std()\n\nerror_dist1 = norm(0, std_diff1)\nerror_dist2 = norm(0, std_diff2)\n\nguess1 = 23000\nerror1 = guess1 - prior1.qs\nlikelihood1 = error_dist1.pdf(error1)\nposterior1 = prior1 * likelihood1\nposterior1.normalize()\n\nguess2 = 38000\nerror2 = guess2 - prior2.qs\nlikelihood2 = error_dist2.pdf(error2)\nposterior2 = prior2 * likelihood2\nposterior2.normalize()\n\nposterior1.plot(label='1번의 사후분포')\nposterior2.plot(label='2번의 사후분포')\nplt.legend()\ndef compute_prob_win(my_diff, op_diff):\n    if my_diff &gt; 0:\n        return 0\n    p1 = np.mean(op_diff &gt; 0)\n    p2 = np.mean(op_diff &lt; my_diff)\n    return p1 + p2\n\nxs = np.linspace(-30000, 5000, 121)\nys1 = [compute_prob_win(x, sample_diff2) for x in xs]\nys2 = [compute_prob_win(x, sample_diff1) for x in xs]\n\nplt.plot(xs, ys1, label='1번 참가자가 이길 확률')\nplt.plot(xs, ys2, label='2번 참가자가 이길 확률')\nplt.legend()\ndef total_prob_win(bid, posterior, op_diff):\n    total = 0\n    for price, prob in posterior.items():\n        diff = bid - price\n        total += prob * compute_prob_win(diff, op_diff)\n    return total\n\nbids1 = posterior1.qs\nprobs1 = [total_prob_win(bid, posterior1, sample_diff2) for bid in bids1]\nprob1_wins = pd.Series(probs1, index=bids1)\n\nbids2 = posterior2.qs\nprobs2 = [total_prob_win(bid, posterior2, sample_diff1) for bid in bids2]\nprob2_wins = pd.Series(probs2, index=bids2)\n\nprob1_wins.plot(label=f'1번 참가자의 이길 확률 (최적: {prob1_wins.idxmax()})')\nplt.axvline(x=prob1_wins.idxmax(), color='b', linestyle=':')\n\nprob2_wins.plot(label=f'2번 참가자의 이길 확률 (최적: {prob2_wins.idxmax()})')\nplt.axvline(x=prob2_wins.idxmax(), color='r', linestyle=':')\n\nplt.legend()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "의사결정분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/08.html#연습문제",
    "href": "posts/01_projects/adp_실기/notes/bayse/08.html#연습문제",
    "title": "의사결정분석",
    "section": "연습문제",
    "text": "연습문제\n\n9-6\n열차 도착 시간에 대한 분포(z)\n\nobserved_gap_times = [\n    428.0, 705.0, 407.0, 465.0, 433.0, 425.0, 204.0, 506.0, 143.0, 351.0, \n    450.0, 598.0, 464.0, 749.0, 341.0, 586.0, 754.0, 256.0, 378.0, 435.0, \n    176.0, 405.0, 360.0, 519.0, 648.0, 374.0, 483.0, 537.0, 578.0, 534.0, \n    577.0, 619.0, 538.0, 331.0, 186.0, 629.0, 193.0, 360.0, 660.0, 484.0, \n    512.0, 315.0, 457.0, 404.0, 740.0, 388.0, 357.0, 485.0, 567.0, 160.0, \n    428.0, 387.0, 901.0, 187.0, 622.0, 616.0, 585.0, 474.0, 442.0, 499.0, \n    437.0, 620.0, 351.0, 286.0, 373.0, 232.0, 393.0, 745.0, 636.0, 758.0]\nzs = np.array(observed_gap_times) / 60\nqs = np.linspace(0, 20, 101)\npmf_z = kde_from_sample(zs, qs)\n\nlikelihood = pmf_z.qs\nposterior_z = pmf_z * likelihood\nposterior_z.normalize()\n\npmf_z.plot(label=f'사전분포 (평균: {pmf_z.mean():.2f})')\nplt.axvline(x=pmf_z.mean(), color='b', linestyle=':')\n\nposterior_z.plot(label=f'사후분포 (평균: {posterior_z.mean():.2f})')\nplt.axvline(x=posterior_z.mean(), color='r', linestyle=':')\nplt.legend()\nplt.title('열차 도착 시간 간격')\nplt.xlabel('시간')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n내가 관찰하는 열차 지연 시간은 실제 열차 지연시간보다 더 길다.\n예를들어 5분 간격의 길이와 15분 간격의 길이가 있다면, 더 긴 간격의 중간에 내가 도착할 확률이 더 높기 때문\n그래서 사후분포가 오른쪽으로 이동함. (신기하네)\n\n내가 도착했을 때 이미 지난 시간에 대한 분포(x)\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\ndef make_elapsed_dist(gap, qs):\n    qs = qs[qs &lt;= gap]\n    n = len(qs)\n    return Pmf(1/n, qs)\n\nqs = posterior_z.qs\npmf_seq = [make_elapsed_dist(gap, qs) for gap in qs]\npmf_x = make_mixture(posterior_z, pmf_seq)\n\n\nz의 각 간격별 지난 시간을 균등분포로 지정\n\n\nfrom scipy.stats import poisson\n\nlam = 2\nnum_passengers = 10\nlikelihood = poisson(lam * pmf_x.qs).pmf(num_passengers)\nposterior_x = pmf_x * likelihood\nposterior_x.normalize()\npmf_x.plot(label='사전분포', color='C1')\nposterior_x.plot(label='사후분포', color='C2')\nplt.legend()\nplt.title('10명이 기다리고 있는것을 관찰한 이전과 이후 분포')\n\nText(0.5, 1.0, '10명이 기다리고 있는것을 관찰한 이전과 이후 분포')\n\n\n\n\n\n\n\n\n\n\n일반적으로 분당 2명의 승객이 기다림. 가능도는 포아송 분포를 따름.\n10명이 기다리고 있는것을 관찰했을 때의 사후분포를 구해준다.\n\n다음 열차 도착까지의 남은 시간에 대한 분포\n\nposterior_y = Pmf.sub_dist(posterior_z, posterior_x)\nnonneg = (posterior_y.qs &gt;= 0)\nposterior_y = Pmf(posterior_y[nonneg])\nposterior_y.normalize()\n\nposterior_x.make_cdf().plot(label='x의 사후분포', color='C2')\nposterior_y.make_cdf().plot(label='y의 사후분포', color='C3')\nposterior_z.make_cdf().plot(label='z의 사후분포', color='C4')\nplt.legend()\n\n\n\n\n\n\n\n\n결정 분석\n\nsample = posterior_z.sample(260)\ndelays = [30, 40, 50]\naugmented_sample = np.append(sample, delays)\n\nqs = np.linspace(0, 60, 101)\naugmented_posterior_z = kde_from_sample(augmented_sample, qs)\naugmented_posterior_z.plot(label='보강된 z의 사후분포', color='C4')\nplt.legend()\n\n\n\n\n\n\n\n\n\n이전 데이터에서는 긴 지연시간에 대한 데이터가 없기 때문에 sampling을 통해 임의로 만들어줌\n\n\nqs = augmented_posterior_z.qs\npmf_seq = [make_elapsed_dist(gap, qs) for gap in qs]\npmf_x = make_mixture(augmented_posterior_z, pmf_seq)\nlam = 2\ndef compute_posterior_y(num_passengers):\n    likelihood = poisson(lam * qs).pmf(num_passengers)\n    posterior_x = pmf_x * likelihood\n    posterior_x.normalize()\n    posterior_y = Pmf.sub_dist(augmented_posterior_z, posterior_x)\n    nonneg = (posterior_y.qs &gt;= 0)\n    posterior_y = Pmf(posterior_y[nonneg])\n    posterior_y.normalize()\n    return posterior_y\n\nnums = np.arange(0, 37, 3)\nposteriors = [compute_posterior_y(num) for num in nums]\nmean_wait = [posterior_y.mean()\n             for posterior_y in posteriors]\nplt.plot(nums, mean_wait)\nplt.title('승객 수에 따른 예상 대기 시간')\nplt.xlabel('승객 수')\nplt.ylabel('예상 다음 도착 시간')\n\nText(0, 0.5, '예상 다음 도착 시간')\n\n\n\n\n\n\n\n\n\n\nprob_late = [1 - posterior_y.make_cdf()(15) \n             for posterior_y in posteriors]\nplt.plot(nums, prob_late)\nplt.xlabel('승객 수')\nplt.ylabel('늦을 확률')\nplt.title('승객 수에 따른 늦을 확률')\n\nText(0.5, 1.0, '승객 수에 따른 늦을 확률')\n\n\n\n\n\n\n\n\n\n\nlam이 만약 알려져 있지 않을 경우, lam에 대해서도 분포를 계산해야함 (쉽지 않네)\n\n\n\n9-7\n\ndef print_cost(printed):\n    if printed &lt; 100:\n        return printed * 5\n    else:\n        return printed * 4.5\n\ndef total_income(printed, orders):\n    sold = min(printed, np.sum(orders))\n    return sold * 10\n\ndef inventory_cost(printed, orders):\n    excess = printed - np.sum(orders)\n    if excess &gt; 0:\n        return excess * 2\n    else:\n        return 0\n\ndef out_of_stock_cost(printed, orders):\n    weeks = len(orders)\n    total_orders = np.cumsum(orders)\n    for i, total in enumerate(total_orders):\n        if total &gt; printed:\n            return (weeks-i) * 50\n    return 0\n\ndef compute_profit(printed, orders):\n    return (total_income(printed, orders) -\n            print_cost(printed)-\n            out_of_stock_cost(printed, orders) -\n            inventory_cost(printed, orders))\n\n\nfrom scipy.stats import gamma\n\nalpha = 9\nqs = np.linspace(0, 25, 101)\nps = gamma.pdf(qs, alpha)\npmf = Pmf(ps, qs)\npmf.normalize()\npmf.mean()\n\n8.998788382371902\n\n\n\nrates = pmf.choice(1000)\nnp.mean(rates)\n\n9.05375\n\n\n\norder_array = np.random.poisson(rates, size=(8, 1000)).transpose()\norder_array[:5, :]\n\narray([[10, 16,  3,  3,  8, 10, 13, 14],\n       [ 9,  8,  5,  3,  6,  8,  7,  9],\n       [10, 12,  9,  7, 14, 12,  7, 10],\n       [ 4, 11,  6, 11,  6,  7,  4,  8],\n       [ 5,  4, 12, 12, 10,  6,  3, 12]])\n\n\n\ndef compute_expected_profits(printed, order_array):\n    profits = [compute_profit(printed, orders)\n               for orders in order_array]\n    return np.mean(profits)\n\n\ncompute_expected_profits(70, order_array)\ncompute_expected_profits(80, order_array)\ncompute_expected_profits(90, order_array)\n\n175.142\n\n\n\nprinted_array = np.arange(70, 110)\nt = [compute_expected_profits(printed, order_array)\n                    for printed in printed_array]\nexpected_profits = pd.Series(t, printed_array)\n\n\nexpected_profits.plot(label='')",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "의사결정분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/18.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/18.html",
    "title": "MCMC",
    "section": "",
    "text": "from scipy.stats import gamma, poisson\nimport numpy as np\nfrom empiricaldist import Pmf\n\nalpha = 1.4\ndist = gamma(alpha)\n\nlams = np.linspace(0, 10, 101)\nprior_pmf = Pmf(dist.pdf(lams), lams)\nprior_pmf.normalize()\n\ndata = 4\nlikelihood = poisson.pmf(data, lams)\nposterior = prior_pmf * likelihood\nposterior.normalize()\n\n0.05015532557804499\n\n\n\nsample_prior = dist.rvs(1000)\nsample_prior\n\narray([1.78245428, 1.45342136, 1.56617409, 2.26968618, 2.48739122,\n       2.04947673, 0.42657469, 1.59840941, 1.61450781, 1.04473805,\n       1.38766769, 1.29342865, 0.10011362, 2.84676252, 2.0907023 ,\n       0.501516  , 1.75463547, 0.75424778, 2.35723675, 0.20373931,\n       1.12665252, 0.3603877 , 0.40185481, 0.14502238, 2.16084403,\n       1.33073581, 2.12610331, 0.47025327, 1.47340163, 4.3584236 ,\n       3.93169673, 1.61656859, 0.32971242, 0.71269559, 1.80662831,\n       1.18335286, 1.46886129, 0.19924373, 1.42013889, 1.16160252,\n       3.44516481, 3.50632314, 0.31767409, 0.10326678, 3.80793668,\n       1.68673128, 2.42240728, 1.10211366, 0.85346076, 0.60510763,\n       0.78364214, 1.01700125, 0.25109979, 1.45602232, 2.53275259,\n       0.67428257, 2.13179357, 0.15602002, 2.40751205, 0.00900921,\n       1.09614156, 1.1789723 , 4.50074634, 2.28033616, 1.20313575,\n       2.03917054, 0.60637777, 1.00250621, 1.08680873, 1.26979198,\n       1.60454441, 0.70143311, 0.20483807, 1.38220482, 0.66860397,\n       1.97347913, 0.85348215, 1.61166464, 0.5694026 , 0.77472185,\n       2.2469975 , 2.59749612, 1.32358443, 1.3492898 , 0.39975522,\n       1.77195451, 0.1514805 , 2.74518891, 1.70943013, 1.06480265,\n       0.15751771, 2.42403873, 0.93600165, 0.37081925, 4.95480763,\n       4.50244729, 0.99897033, 1.75701265, 0.63303305, 0.75107271,\n       1.21914169, 0.56481277, 0.20323012, 4.8281205 , 0.05825477,\n       0.8180459 , 0.39507497, 0.39567062, 1.70882689, 1.22333205,\n       1.25106454, 1.09135668, 0.59421816, 3.10792611, 1.94096089,\n       1.33699676, 3.24004141, 0.20338037, 0.77345422, 0.01109566,\n       0.28726445, 0.5188063 , 0.32004225, 0.56189764, 0.78222717,\n       0.86536462, 0.75354692, 0.36003061, 2.55631833, 5.37122246,\n       1.41387676, 1.31543492, 1.01074796, 3.87677072, 0.5995094 ,\n       1.20237366, 1.57964719, 0.58273492, 4.03528673, 1.4019504 ,\n       0.97618193, 0.15539856, 4.15005826, 1.06660502, 0.7626793 ,\n       1.72684506, 0.0450794 , 0.74543249, 0.09050903, 1.34800622,\n       2.92492755, 2.42968827, 2.60138038, 2.90331673, 0.89979192,\n       1.19012654, 2.53193949, 2.52008528, 1.11941151, 1.85726849,\n       2.76141794, 0.46461223, 2.49662685, 2.35624678, 1.86963077,\n       2.41364535, 0.30807995, 1.4061337 , 0.27264207, 2.44446813,\n       2.24204357, 2.01689348, 1.12378139, 0.69105913, 0.92202871,\n       2.93724754, 0.17141724, 0.60518804, 0.53125454, 1.03206292,\n       0.47439733, 1.75277398, 0.23028169, 0.8280134 , 1.93138644,\n       2.81545321, 1.56380812, 0.18527356, 0.0984316 , 1.99172578,\n       0.81991126, 0.4790343 , 1.90666511, 4.16125993, 0.15915635,\n       0.25560981, 0.77757414, 1.02994745, 0.7296187 , 2.77953051,\n       0.45302316, 2.61951237, 0.82442108, 2.51345799, 0.28933755,\n       0.30471831, 0.63519023, 4.19278344, 0.63981378, 1.50199748,\n       1.53171353, 0.05193196, 0.23522131, 1.19337516, 0.62119419,\n       0.67090562, 4.87535766, 0.12746862, 1.66727365, 0.61737015,\n       0.96103356, 0.63758575, 1.58182367, 1.78805179, 0.14303181,\n       0.90088986, 2.41355835, 2.88119186, 3.62656613, 0.57027875,\n       0.10181002, 2.55424918, 1.17669901, 5.10656878, 0.60856925,\n       2.37935349, 0.97901042, 1.04362086, 1.72180322, 0.56624396,\n       3.19632108, 0.50092852, 1.61691302, 0.91491038, 1.57061131,\n       0.52779649, 1.22280998, 2.5266366 , 0.86564415, 2.48023759,\n       4.8845416 , 0.89407679, 1.22596699, 0.75404211, 0.16527154,\n       0.76870535, 1.99984975, 0.90019864, 0.13392852, 0.16678515,\n       3.25003003, 0.62330084, 3.52010987, 0.60159782, 3.11350541,\n       1.44583984, 0.65523014, 2.8895766 , 0.21520675, 0.66540019,\n       1.29101126, 3.48560161, 1.58849289, 0.35755348, 0.87234375,\n       0.57700548, 0.30313153, 2.42159878, 2.26150262, 0.3824956 ,\n       2.97441566, 0.130601  , 1.1687748 , 0.22204499, 0.11365245,\n       0.14389429, 1.01204286, 2.34458005, 1.15306329, 0.22943553,\n       1.75475892, 0.85484513, 1.95672875, 1.64233556, 1.43204869,\n       0.65810987, 1.11893247, 0.42048471, 1.67412056, 1.72722623,\n       2.14945311, 0.41395448, 0.74699141, 0.52283731, 2.56181903,\n       0.6571291 , 1.08812639, 0.92454495, 1.84451687, 0.5554823 ,\n       0.61896596, 0.30390847, 1.06271335, 0.19606563, 1.52173132,\n       0.85162107, 1.12267304, 1.35894119, 0.85057098, 0.90606912,\n       1.50407866, 0.40416076, 0.63258919, 1.83349409, 3.07069563,\n       3.13453365, 3.36162901, 0.91476133, 2.4607268 , 1.37933747,\n       1.41308005, 1.40098989, 0.09314009, 1.15655232, 1.35956324,\n       0.9562359 , 4.25017262, 1.58582269, 0.88988051, 0.66225242,\n       3.1478352 , 0.1912415 , 0.43079422, 1.60481873, 3.48793288,\n       3.29679857, 1.42527886, 0.14327117, 1.78002169, 1.60884479,\n       1.34999623, 0.41873708, 1.06218268, 3.00648362, 1.35459038,\n       0.62904508, 0.68680944, 0.8259365 , 0.43216401, 0.06517087,\n       0.76928278, 0.29646467, 0.47003658, 0.49210669, 2.21807655,\n       0.8815014 , 1.82491166, 0.38739385, 1.11630756, 2.47541734,\n       3.31111865, 3.00380241, 2.0328079 , 0.77396742, 0.62406382,\n       1.70915789, 1.16173553, 0.29018906, 2.8465662 , 2.12208868,\n       1.23529682, 0.23832301, 0.74280795, 0.42489782, 0.83071628,\n       0.10585368, 0.88082506, 1.1422167 , 0.45012877, 0.96804109,\n       0.69669037, 1.75769205, 1.79921145, 1.0469775 , 1.27638059,\n       1.44456316, 3.18230869, 0.31115153, 2.90534957, 1.09384762,\n       0.73737181, 0.73153375, 0.80130282, 0.93084067, 2.82910308,\n       2.11995971, 3.48271261, 1.2508142 , 0.77078057, 0.27005484,\n       2.30816112, 1.48304143, 2.3197458 , 2.87177339, 0.37235907,\n       0.08439045, 1.80168299, 1.27580988, 0.1376812 , 0.72410959,\n       1.2551965 , 0.52931964, 1.18787009, 0.42532437, 0.7374523 ,\n       0.82102856, 0.82465968, 1.43241383, 1.57478286, 0.98243462,\n       1.41672022, 2.12126751, 0.51028047, 0.264361  , 0.17359669,\n       3.49280959, 0.28799884, 1.93231242, 0.20307447, 0.67579587,\n       0.32516326, 0.66475116, 0.16775069, 1.49053435, 0.48362181,\n       1.88346664, 0.7367098 , 1.28901807, 0.41609368, 2.81952939,\n       1.92616636, 0.3233559 , 0.43425742, 1.20022375, 1.37352912,\n       1.81814333, 0.71787827, 0.20092714, 1.37301219, 1.96938137,\n       0.22785241, 1.92048134, 1.11241357, 1.40346287, 2.21565625,\n       0.70134262, 0.36980475, 0.88515598, 0.39391247, 2.41861256,\n       1.42008427, 0.25944955, 0.98063703, 1.3591506 , 1.06245797,\n       2.18967405, 1.14497289, 2.41920052, 1.22700078, 2.14015952,\n       3.50280141, 0.32285136, 0.12346727, 0.8337781 , 0.13989125,\n       1.66263551, 0.39415498, 2.78120238, 0.24714151, 0.66741164,\n       0.32704671, 1.83887574, 4.45999199, 0.7774285 , 0.41141288,\n       2.02693585, 0.05295953, 1.66546587, 0.32911593, 0.30873302,\n       1.2914885 , 0.14691859, 2.12207068, 0.51315365, 0.80032371,\n       2.34913407, 3.4095474 , 0.33763934, 1.22953585, 0.23075278,\n       0.7374652 , 0.91715679, 0.39236151, 1.58367854, 0.95839352,\n       3.06733169, 0.91288509, 0.19102102, 0.11115259, 3.11374855,\n       1.30481605, 0.74220086, 0.77310277, 0.30451101, 1.30518641,\n       2.30149429, 0.35855988, 0.89185888, 0.37207391, 1.77390799,\n       3.5280598 , 1.96202531, 1.91611586, 0.23112386, 2.80806439,\n       0.10056925, 0.11788967, 0.66522683, 2.87382961, 0.68619677,\n       2.17313528, 0.9558771 , 1.17700941, 0.94207278, 1.53238011,\n       0.99653946, 0.85299825, 0.7120591 , 1.94878342, 0.60649798,\n       0.18151201, 0.72210506, 0.90348217, 1.88356921, 0.06351328,\n       2.9767626 , 0.59317537, 1.80871685, 0.11252428, 1.13556851,\n       0.95026968, 0.58921063, 1.08746956, 0.37980344, 2.12101607,\n       1.59854286, 0.12085816, 0.22008041, 1.64744616, 3.37383142,\n       2.64912841, 1.68066142, 1.78559911, 3.76130028, 3.00240492,\n       0.13001163, 0.04030774, 1.40844236, 0.55993878, 0.19412716,\n       0.87202483, 1.25664671, 1.19941488, 7.17456765, 2.52034946,\n       1.04923498, 1.0045098 , 1.39849648, 1.06626886, 1.81495584,\n       0.22072722, 0.42678823, 0.17427896, 0.31374976, 0.94881118,\n       0.11695164, 0.23673133, 0.54695901, 0.5640094 , 1.31881685,\n       3.02020657, 0.73039081, 3.26285507, 0.47588791, 1.14561038,\n       3.08471669, 1.79722984, 0.71204969, 1.14314614, 0.33993976,\n       0.69468141, 1.13031585, 0.20245588, 0.06412572, 0.7679586 ,\n       1.08179364, 1.74875858, 0.13766512, 1.02392835, 1.29577827,\n       0.09105422, 0.17627404, 1.4013938 , 0.92428193, 1.964099  ,\n       0.48575707, 0.22956195, 2.34847801, 1.47081282, 0.05469313,\n       6.37212418, 1.58318943, 0.6007103 , 0.24392092, 1.54535754,\n       5.88259622, 0.58528123, 0.40431939, 0.20934691, 0.83563943,\n       2.18714582, 0.17075649, 0.68424449, 1.29319222, 0.7661801 ,\n       0.36229985, 3.38192096, 1.21570705, 0.75593531, 0.72948589,\n       0.29429465, 0.24672329, 0.24379611, 5.02651607, 2.94806746,\n       2.21708485, 0.4614484 , 1.71470044, 0.51920825, 3.94565173,\n       0.708636  , 0.57923338, 2.96705406, 0.2944275 , 1.25637226,\n       1.22096943, 0.18290153, 1.77017348, 1.63520181, 1.75808304,\n       0.73410077, 5.82731507, 1.52510495, 0.03736303, 0.24150383,\n       0.82206031, 0.26913806, 2.28554287, 1.45268443, 0.21564075,\n       2.18622272, 0.08782493, 0.54652373, 0.69412775, 1.6556548 ,\n       0.27665459, 0.70821905, 1.30150644, 0.53504482, 2.24914298,\n       3.48785426, 0.82334135, 0.67226148, 0.54208502, 0.84640797,\n       1.11280929, 1.21401206, 1.62551353, 0.20391175, 0.07203695,\n       0.04048055, 2.90846273, 0.3421185 , 2.13861689, 5.09107539,\n       1.40477632, 1.12705544, 1.71155139, 0.44224997, 0.06196968,\n       0.17224229, 0.71676445, 0.86271074, 0.43271951, 1.24696646,\n       0.14112179, 2.08775182, 0.79544149, 1.6654211 , 2.12324338,\n       2.81877089, 1.25563189, 3.48923833, 0.1365598 , 0.39880259,\n       0.20177009, 0.48194506, 1.52881059, 2.32429724, 1.66280629,\n       1.83099818, 1.74067556, 3.30083543, 1.8308614 , 1.1145482 ,\n       3.32710777, 0.66102754, 1.50616104, 2.36224778, 3.02144457,\n       8.86162098, 2.27128422, 1.92403156, 4.45066817, 2.56433866,\n       0.21695367, 1.2403261 , 2.52689656, 5.2685631 , 1.84526002,\n       0.98541833, 1.15096267, 1.03645475, 0.81933851, 0.3934846 ,\n       1.21499941, 1.60832082, 3.18128774, 2.1095381 , 3.8934185 ,\n       1.79109006, 1.3959264 , 0.38306127, 1.32617665, 0.08432029,\n       2.0966402 , 1.8667662 , 3.3558948 , 0.14799817, 2.14552212,\n       1.25160298, 0.91576209, 1.1691564 , 2.29722509, 0.6127987 ,\n       4.17148409, 0.85662604, 2.57904496, 2.9415721 , 0.21121228,\n       1.09210864, 1.78375426, 2.79602724, 1.05356592, 3.57150381,\n       3.26658441, 0.29256895, 1.18776403, 0.87495308, 2.00343635,\n       2.06267891, 2.31240083, 5.67768676, 1.90715169, 0.81179953,\n       0.86165212, 1.6245984 , 3.67096281, 2.10937595, 1.30683289,\n       0.48036824, 3.39090841, 4.63791219, 4.19824051, 2.71398943,\n       0.86266062, 0.1049414 , 0.47116751, 1.87292779, 1.40398141,\n       3.44842299, 2.04297218, 1.23114444, 0.38680855, 2.4343798 ,\n       0.24587911, 1.0012706 , 1.50570632, 2.06932207, 0.68841968,\n       0.48475363, 0.2370504 , 1.85823636, 0.99650605, 0.18724216,\n       0.31248844, 0.51720466, 1.36847445, 4.38848338, 0.40028451,\n       0.57817147, 2.33715831, 0.19275902, 1.14346875, 1.30225192,\n       0.78136161, 0.29358472, 1.98502703, 0.51967865, 0.28196284,\n       0.88626531, 0.33166157, 2.06054915, 0.25269865, 1.33403699,\n       0.0607662 , 0.26593347, 2.59447086, 1.50757733, 0.31869501,\n       1.90951547, 1.18127502, 2.99410786, 2.11297465, 3.37042564,\n       2.64052509, 1.36346991, 0.68539065, 1.03361491, 0.90540307,\n       1.76845903, 2.11128979, 1.78173859, 2.79023275, 0.64396541,\n       0.87610746, 5.23750049, 3.41102649, 3.18833522, 1.20921241,\n       2.07238565, 1.23863862, 5.66405728, 3.07347514, 0.98414824,\n       1.12368196, 1.3060482 , 1.02938722, 1.96038754, 0.81355828,\n       1.65056426, 0.11725298, 2.1493593 , 0.49300763, 1.26171529,\n       0.27856461, 1.83922204, 0.97770842, 1.27751732, 5.5191054 ,\n       2.21460443, 1.72701543, 1.80123051, 1.27982277, 1.69996599,\n       0.41897369, 0.56141662, 1.59944028, 1.28269957, 1.09464645,\n       0.91348711, 0.83844011, 0.06779634, 1.59391636, 0.92909009,\n       0.2249008 , 0.37235769, 3.54665899, 0.69076408, 0.464155  ,\n       0.93546219, 1.72142044, 0.65844905, 4.33013673, 0.08389357,\n       5.05809403, 0.47798695, 0.63148799, 2.26068668, 0.99370813,\n       4.69218164, 2.03889133, 1.06071617, 3.99596801, 2.76045249,\n       2.53195504, 6.34568204, 1.79466828, 1.17214677, 0.63195367,\n       3.12500801, 2.98122107, 2.74994727, 1.85120285, 0.31523191,\n       0.03039504, 2.39494752, 0.05283476, 1.40513227, 0.27310686,\n       2.78295955, 2.78503277, 0.17889362, 0.21993918, 2.47193703,\n       0.09961994, 0.98290001, 2.87897453, 1.04654061, 1.44689139,\n       0.75053196, 1.90072161, 0.07337072, 1.08005232, 1.76350391,\n       1.13444569, 2.38286551, 1.20792944, 2.59251206, 3.83880974,\n       1.4713979 , 0.82968679, 0.96234638, 0.3518984 , 1.53537545,\n       1.53714678, 0.91555435, 0.17847465, 1.04501305, 1.62623627,\n       0.56511931, 0.27666544, 1.02573777, 1.76343823, 0.09543004,\n       3.27791113, 4.09629567, 1.00900773, 3.87704081, 1.24957286,\n       1.9460308 , 0.64492968, 1.87331116, 0.57686789, 0.82118049,\n       1.19346087, 0.42413331, 1.62276897, 0.20517721, 0.98301327,\n       3.5662663 , 0.61798656, 0.6370218 , 1.46742942, 0.94152712,\n       1.78928848, 1.46753359, 1.3924947 , 0.35489384, 9.00032173])\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "MCMC"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/03.html#overview",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/03.html#overview",
    "title": "두 변수의 연관성과 독립성",
    "section": "overview",
    "text": "overview\n표본 상관계수를 대체할 수 있는 비모수적 상관계수와 이를 이용한 두 변수에 대한 독립성 검정법을 알아본다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "두 변수의 연관성과 독립성"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/Nonparametric/03.html#상관계수와-단순선형회귀",
    "href": "posts/01_projects/adp_실기/notes/Nonparametric/03.html#상관계수와-단순선형회귀",
    "title": "두 변수의 연관성과 독립성",
    "section": "상관계수와 단순선형회귀",
    "text": "상관계수와 단순선형회귀\n\n스피어만 순위상관\n\n두 변수를 순위변환 한 뒤 표본상관계수를 계산\n비선형 추세일지라도 순증가 또는 순감소하는 추세를 측정할 수 있다.\n순서가 있는 범주형일 때도 적용할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "두 변수의 연관성과 독립성"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/others/1.html",
    "href": "posts/04_archives/bs_3_1/notes/others/1.html",
    "title": "자기 소개서",
    "section": "",
    "text": "자기소개 및 가치관 (500자 이내)\n\n저는 데이터 분석과 IT 인프라 설계 분야에 깊은 관심을 가지고 있는 산업공학과 학생입니다. 산업공학을 전공하며 시스템 최적화와 데이터 기반 의사결정에 대한 이론을 배우며 데이터 분석 및 IT 인프라 설계 분야에 관심을 가지게 되었고, 이를 실무에 적용할 수 있는 지식을 학습하고자 42서울 교육기관에서 2년 동안 IT 관련 학습을 진행했습니다. 또한 이 기간 동안 AWS와 ADsP(Advanced Data Analytics Semi-Professional) 자격증을 취득하며 클라우드 컴퓨팅과 데이터 분석에 대한 기초 역량을 쌓았습니다.\n저는 효율적이고 신뢰할 수 있는 시스템을 구축하는 것을 가장 중요한 가치로 삼고 있습니다. 이러한 시스템은 데이터 손실과 보안 위협을 방지할 뿐만 아니라, 장기적인 성장의 토대가 되기 때문입니다. 현재는 데이터와 블록체인 기술을 활용하여 복잡한 문제를 단순화하고, 효율적인 해결책을 찾는 데 큰 관심을 가지고 있습니다. 앞으로도 지속적인 학습과 경험을 통해 해당 분야에서 전문성을 키워가고자 합니다.\n\n졸업 후 IT 및 블록체인 분야에 관련해서 이루고자 하는 꿈과 선정 사유 (500자 이내)\n\n저는 데이터 분석, IT 인프라, 블록체인 기술을 융합하여 현실의 복잡한 문제들을 해결하고 혁신적인 가치를 창출하는 데 기여하고 싶습니다. 전공 수업과 프로젝트를 통해 데이터가 지닌 잠재력을 배워가면서, 동시에 데이터의 신뢰성과 보안이라는 중요한 과제에 대해서도 깊이 고민하게 되었습니다. 특히 42서울에서의 학습 경험을 통해, 안전하고 효율적인 데이터 활용을 위해서는 IT 인프라와 블록체인 기술의 역할이 매우 중요하다는 것을 깨달았습니다. 이러한 경험들을 바탕으로 IT 인프라와 블록체인 기술에 더욱 관심을 가지게 되었고, 관련 기술 서적과 온라인 자료를 통해 꾸준히 학습하며 이해의 폭을 넓혀가고 있습니다. 앞으로도 끊임없이 배우고 성장하여 데이터의 가치를 안전하게 실현할 수 있는 시스템을 만드는 데 기여하고 싶습니다.\n\n목표 달성을 위한 그간의 성과 및 계획 (500자 이내)\n\n저의 주요 성과로는 42서울에서의 프로젝트 경험과 AWS, ADsP 자격증 취득을 들 수 있습니다. 42서울에서 진행한 Solidity 기반 이더리움 스마트 컨트랙트 설계 및 배포 프로젝트를 통해 블록체인의 핵심 원리와 실제 활용 방안을 학습했습니다. 또한 Vagrant, Kubernetes(K8s), ArgoCD, GitLab helm 배포 프로젝트를 수행하며 온프레미스 환경에서의 인프라 설계와 개발 환경 관리 역량을 키웠고, 이를 통해 클라우드와 온프레미스 환경의 IT 인프라 운영에 대한 실질적인 이해도를 높일 수 있었습니다. 향후 계획으로는 학부 과정에 충실히 임하면서 데이터사이언스 대학원 진학을 위한 준비를 체계적으로 진행하고자 합니다. 대학원에서는 빅데이터 처리, 머신러닝, 딥러닝 등 데이터 분석의 핵심 기술을 심도 있게 학습하고자 합니다. 이와 병행하여 온라인 강좌 수강과 실전 프로젝트 수행을 통해 IT 인프라 및 블록체인 분야의 역량을 지속적으로 강화하고, 각종 공모전 참여를 통해 실력을 검증받고자 합니다. 궁극적으로는 이러한 기술들을 융합하여 데이터의 신뢰성과 보안을 보장하고, 효율적인 시스템을 설계하는 전문가로 성장하고 싶습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "자기 소개서"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "href": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "title": "봉사",
    "section": "봉사 계획서 2",
    "text": "봉사 계획서 2",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-실습일지",
    "href": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-실습일지",
    "title": "봉사",
    "section": "봉사 실습일지",
    "text": "봉사 실습일지",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "href": "posts/04_archives/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "title": "봉사",
    "section": "봉사 결과보고서",
    "text": "봉사 결과보고서",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/06.html#setup과-생산주기",
    "href": "posts/04_archives/bs_3_1/notes/product/06.html#setup과-생산주기",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "setup과 생산주기",
    "text": "setup과 생산주기\n\nsetup: 기계를 준비하는데 필요한 것\n\n정확히 하나의 제품을 만드는 경우에도 setup이 필요함\n생산하는 양에 관계없이 setup 시간이 일정함\nsequence dependent setup: 순서에 따라 setup 시간이 달라짐\n\n생산주기(production cycle): setup + 생산의 과정을 반복\n\nsetup은 아무것도 못하고 시간을 버림",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/06.html#배치-생산과정",
    "href": "posts/04_archives/bs_3_1/notes/product/06.html#배치-생산과정",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "배치 생산과정",
    "text": "배치 생산과정\n\nbatch1: 부품 집합을 흐름 단위로 사용\n생산 주기: batch size만큼 생산하는 주기\n처리능력: \\(\\frac{batch size}{setup time + (batch size * processing time per unit)}\\)\n\nbatch size가 무한히 커질수록 \\(\\frac{1}{p}\\)로 수렴\nsetuptime이 0이여도 \\(\\frac{1}{p}\\)\n\n\n\n\nbatch는 클 수록 좋은가?\n\nbatch size가 커질수록 처리능력이 증가하지만 재고가 많아짐\n→ 처리능력 제약적 상황에서 bottleneck의 batch size를 늘리고, 수요 제약적 상황에서 non-bottleneck의 batch size를 줄이는게 좋음\n→ \\(\\frac{B}{S + Bp} = R → B = \\frac{SR}{1 - Rp}\\)\nR보다 크면 쓸데없이 제고가 쌓이고, 작으면 capacity가 낮아짐\n\nS가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨\np가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "href": "posts/04_archives/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "경제적 주문량 모형",
    "text": "경제적 주문량 모형\n\n외부 공급자에게 부품을 주문하여 생산 및 배송이 이루어지는 경우\n단위시간당 발생하는 비용이 적을수록 좋다\n\n\n\n\n재고량 패턴(재고가 0이 되었을 때 정확히 도착하도록 주문할 수 있다고 가정)\n\n\n\nQ: 한 번에 주문하는 양\nR: 수요(기울기)\n주문 주기: \\(\\frac{Q}{R}\\)\n평균 재고량: \\(\\frac{Q}{2}\\)\n\n\n구매비용(purchase cost / variable cost): 단위 시간 당 구매비용은 Q에 영향을 받지 않음\n단위 재고 비용(h)\n\n단위 시간 당 발생하는 재고 비용: \\(h\\frac{Q}{2}\\)\n\n셋업(주문) 비용 (Fixed cost) (k): 주문량과 무관\n\n단위 시간 당 발생하는 셋업 비용: \\(\\frac{k}{\\frac{Q}{R}}\\)\n\n\n\n목적 함수: \\(C(Q) = \\frac{KR}{Q} + \\frac{hQ}{2}\\)\n경제적 주문량(EOQ): \\(Q^* = \\sqrt{\\frac{2KR}{h}}\\)\n\nK: 주문비용\nR: 수요량\nh: 단위 재고비용\n\nEOQ만큼 주문할 때 단위 시간당 비용\n\n\\(C(Q^*) = \\sqrt{2KhR}\\)\n\n단위당 비용 = \\(\\frac{C(Q^*)}{R} = \\sqrt{\\frac{2Kh}{R}}\\)\n수요가 증가함에 따라 EOQ는 늘어나는데 단위 당 비용은 감소\n\\(\\frac{C(Q)}{C(Q^*)} = \\frac{1}{2}(\\frac{Q^*}{Q} + \\frac{Q}{Q^*})\\)\n\\(\\frac{1}{주문 주기} ≠ 재고 회전율\\)\n\n주문 주기는 Q개가 다 없어지는 시간\n회전율은 Q/2개의 재고가 다 없어지는 시간",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "href": "posts/04_archives/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "buffer or suffer",
    "text": "buffer or suffer\n\nbuffer 제고가 없으면 처리능력이 떨어질 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/06.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/product/06.html#footnotes",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "각주",
    "text": "각주\n\n\nbatch 1개는 부품 집합 1 단위 의미↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "href": "posts/04_archives/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "프로세스 흐름 분석",
    "text": "프로세스 흐름 분석\n\n간트 차트\n\n\n\n방사선조영실 작업 칸트 차트\n\n\n\n작업시간을 표현\n프로세스 상의 작업 순서와 소요시간, 상호관계를 볼 수 있음\n대기: 수요와 공급의 불일치, 작업들에 존재하는 불확실성으로 생기는 것\n\n\n\n프로세스 평가를 위한 3가지 요소\n\n흐름률(flow rate / throughput): 실제 흐름 단위가 프로세스에 진입, 떠나는 비율\n\n\\(\\frac{흐름단위 수}{단위 시간}\\)\n흐름률이 오르면 생산 능력이 오른다.\n유량: 특정한 시간동안 관찰하는 양\n매출 원가(들어온 가격 기준)를 흐름률로 바라볼 수 있다.\n\n흐름시간(flow time): 하나의 흐름단위가 프로세스상에 머무는 시간\n\n흐름시간이 줄어들면 수요-공급 사이의 시간도 줄어든다.\n\n재고(inventory): 프로세스 상에 존재하는 흐름단위 수\n\n매출 원가 기준\n가공중인 제품 WIP(work-in-process)도 재고에 포함\n저량(stock): 특정 시점에서 관찰하는 양\n\n\n\n\n\n\n\n\n\n\n\n프로세스\n\n\n\n흐름(flow): 작업이 진행되는 것을 tracking\n\n단위: 일반적으로 산출물의 단위로 정의\n\n위의 그림에서 흐름 단위는 상품 1개, 서비스 받은 고객 1명",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/02.html#리틀의-법칙",
    "href": "posts/04_archives/bs_3_1/notes/product/02.html#리틀의-법칙",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "리틀의 법칙",
    "text": "리틀의 법칙\n위의 세개와 수요 공급간의 관계가 있다.\n\n\n\n기울기는 흐름률\n\n\n\nI = R * T (항상 성립)1\n\nI: 평균 재고 (flow time 동안 들어온 input)\nR: 평균 흐름률\nT: 평균 흐름시간",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "href": "posts/04_archives/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고 관리 메커니즘",
    "text": "재고 관리 메커니즘\n\n재고를 카운트 하는 방법\n\ninput이 여러개일 경우 단순히 흐름단위만으로 재고를 표현하기 어려울 수 있다.\n\n\nIn terms of $s: I. 원가 기준\nIn terms of days-of-supply(DOS, 공급일수): \\(\\frac{I}{R} = T\\)\nIn terms of inventory turns(재고 회전율): \\(\\frac{R}{I} = \\frac{1}{T}\\)\n\n\n\nTurns and DOS at Kohl’s and Walmart\n\n\n\n두 회사의 재무재표\n\n\n\n위의 사례에서 두 기업의 전략을 볼 수 있음.\nKohl’s: 회전률이 낮은 대신 마진을 높임\nWalmart: 마진이 낮은 대신 회전률을 높임\n공급 일수와 회전율은 반비례 관계에 있다.\n\n\n\n재고가 부담이 되는 이유\n\n이자비용\n유지비용\n\n재고가 구식으로 변함\n물리적으로 부식됨\n사라질 수 있음\n저장공간과 추가적인 간접비 유발\n품질의 저하에 따르는 추가적인 비용 존재\n\n제품 당 재고 비용: \\(\\frac{단위 시간 당 재고 유지 비용}{단위 시간 당 재고 회전율}\\)\n\n\n\n재고 유지의 다섯 가지 이유2\n\n재고 유지는 기업 입장에서 부담이 되지만 그럼에도 불구하고 유지하는 이유가 있다.\n\n\n수송중재고(pipeline): 프로세스에 존재하는 재고\n계절재고(seasonal): 공급 능력은 고정되어 있는데 수요는 변동하는 경우(예측 가능한 수요), 미리 만들어둠\n\n계절 재고 vs 주기 재고\n계절 재고 vs 안전 재고\n\n계절 재고: 수요가 예측 가능할 때\n안전 재고: 수요가 예측 불가능할 때\n\n\n주기재고(cycle): 한 번에 많이 사는게 싸다. 규모의 경제 이용한 비용 절감\n완충재고(buffer, decoupling): 프로세스상의 작업 사이의 지속적 공급을 가능하게 해줌. 단 제고가 계속 쌓이지 않게 line balancing(각 프로세스에서 진행하는 일의 양의 밸런스)을 해줘야 함\n안전재고(safety): 불확실성에 대비해 예측된 수요보다 더 많이 재고를 유지함",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/02.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/product/02.html#footnotes",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "각주",
    "text": "각주\n\n\n시험문제에 더 복잡하게 낸다고 하시긴 함↩︎\n이것들의 차이와 의미하는 바, 사례를 보고 어떤걸 의미하는지 알아야 한다.↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/10.html#예측",
    "href": "posts/04_archives/bs_3_1/notes/product/10.html#예측",
    "title": "예측",
    "section": "예측",
    "text": "예측\n예측: 과거의 데이터를 사용하여 현재 불확실하고 미래에 실현될 결과에 대해 판단하는 과정\n\n\n단기로 갈 수록 디테일한 예측을 하고, 기법이 달라짐.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/10.html#예측-기법",
    "href": "posts/04_archives/bs_3_1/notes/product/10.html#예측-기법",
    "title": "예측",
    "section": "예측 기법",
    "text": "예측 기법\n\n판단적 기법\n\n전문가의 경험과 직관에 의존하여 예측하는 기법\n비정량적 / 주관적 데이터로 정량적인 예측치를 구함\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석:치전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n시장조사법: 설문지, 인터뷰를 바탕으로 신제품의 생산량 결정이나 기존제품의 수요변화 예측\n전문가 의견 종합법: 여러 전문가로 예측치 수집 후 단순평균 or 가중평균\n사례유추법: 비슷한 제품이랑 비교\n델파이 기법: 여러 전문가들로 패널을 구성하고, 반복적인 질문과 결과 피드백을 통하여 합의된 예측치를 도출\n\n\n\n\n\n델파이 기법\n\n\n\n\n시계열 기법\n\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여\n지수평활법: 과거의 모든 데이터를 가중 평균\n\n지수평활계수(α): 최근의 값을 더 높은 가중치가 부여되도록 추정\n\\(\\hat{y_{t+1}} = αy_t+ (1-α)\\hat{y_t} = \\hat{y_t} + α(y_t - \\hat{y_t}\\)\n예측치와 관측치 중 어디에 중점을 둘 지에 따라서 α 결정\n오차를 어느정도 반영할지에 따라서 α 결정\nα == 1: 최근 자료에 비중을 둠. α == 0: 기존 예측을 따름\n\n\n\n\n\n\n상관관계 기법\n\n회귀분석\n\n\n\n\n선행 지수법",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/10.html#계절성-수요예측",
    "href": "posts/04_archives/bs_3_1/notes/product/10.html#계절성-수요예측",
    "title": "예측",
    "section": "계절성 수요예측",
    "text": "계절성 수요예측\n\n추세: 시간의 흐름에 따라 일정한 방향성을 가지고 수요가 변화\n계절성\n\n\n\n\n\n단순 변동, 추세, 계절성이 모두 있는 경우\n\n\n\n승법적 모델: (일정수준 + 추세) * 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 나눈다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 곱해서 예측치를 구한다.\n\n합산적 모델: (일정수준 + 추세) + 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 뺀다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 더해서 예측치를 구한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "href": "posts/04_archives/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "title": "예측",
    "section": "예측의 품질 평가",
    "text": "예측의 품질 평가\n\n비편향 예측: 평균 예측오차가 0\n\n\nMSE: 편차가 클 수록 불이익\nMAE: 각 편차가 동일하게 나쁜 것으로 간주\nMAPE\n\n\n질문: 예측치가 음수일 수 있나?",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/13.html#방법",
    "href": "posts/04_archives/bs_3_1/notes/product/13.html#방법",
    "title": "일정 계획",
    "section": "방법",
    "text": "방법\n\nFCFS\nLCFS\nSPT (Shortest Processing Time)\nLPT (Longest Processing Time)\nEDD (Earliest Due Date)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/13.html#평가-척도",
    "href": "posts/04_archives/bs_3_1/notes/product/13.html#평가-척도",
    "title": "일정 계획",
    "section": "평가 척도",
    "text": "평가 척도\n\nmakespan: 모든 작업이 완료되는 시간\n\n전체 소요시간은 달라지지 않음.\nsequential dependent한 set up이 있는 경우는 달라짐\n\nlateness: \\(C_i - d_i\\) (작업이 완료된 시간 - 작업의 마감 시간)\ntardiness: 지연된 시간\nearliness: 일찍 완료된 시간\nflow time: \\(C_i - r_i\\) (작업이 완료된 시간 - 작업이 시작된 시간)\n흐름률: \\(\\frac{작업량}{makespan}\\)\n\n제고와 flow time을 제일 많이 줄이는 것은 SPT rule\n\n정확한 processing time을 알아야 하고\nprocessing time의 예측은 편향될 수 있고\n공정성 문제가 있을 수 있다.\n\nweighted SPT Rule: \\(\\frac{weight}{processing time}\\)이 높은 작업을 우선적으로 처리",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#intro",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#intro",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Intro",
    "text": "Intro\n\n지금까지는 변동성을 고려하지 않았지만 프로세스 성과 평가에 중요한 영향을 미친다.\n변동성이 대기시간에 미치는 영향을 살펴본다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#example",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#example",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Example",
    "text": "Example\n\n\n변동성\n\n불규칙한 도착 간격\n서비스 시간의 변동성\n영향: 재고, 대기시간, 산출 손실\n\nIU가 100 이하여도 대기가 발생할 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#변동성의-원인",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#변동성의-원인",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "변동성의 원인",
    "text": "변동성의 원인\n\n흐름단위의 input (\\(CV_a\\))\n\nrandom arrival\nincoming quality\nproduct mix\n\nprocessing time의 변동성 (\\(CV_p\\))\n\n그냥 내재적인 변동성\n숙련도 (일을 못해서 오래걸림)\n품질 (재작업)\n\n자원의 무작위적 가용성\n\n자원 고장\n작업자 출근 안함\nsetup time\n\n복수의 흐름단위가 무작위적 경로결정\n\n경로의 변동성\n\n\n\n변동성의 측정: \\(\\frac{표준편차}{평균}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (단일 자원)",
    "text": "대기시간 예측 (단일 자원)\n\n가정\n\n내재활용률은 100% 미만\n\nif D &gt; C, 대기 - 처리능력 부족 (+ 변동성)\nif D &lt; C, 대기 - 변동성\n\n안정적 도착: 평균 고객 수가 시점에 의존하지 않고, 길이에만 의존함\n\n만약 프로세스가 안정적이지 않다면 더 짧은 시간간격으로 나누어 접근\n\n지수분포를 따르는 도착간격\n\n\\(CV_a = 1\\)\n비기억 특성\n\n\n\n\n변수\n\na: 평균 도착 간격 (줄 기준)\np: 평균 서비스 시간\n\\(CV_a\\): 도착간격의 변동계수\n\\(CV_p\\): 서비스 시간의 변동계수\n\\(T_q\\): 대기 시간\n\\(I_q\\): 대기열의 재고\n\\(I_p\\): 서비스 중 재고\n\n\n\n\n공식\n\ncapacity: \\(\\frac{1}{p}\\)\nflow rate = demand(수요 제약적 상황을 가정하니까): \\(\\frac{1}{a}\\)\nutilization: \\(\\frac{p}{a}\\)\nT: \\(T_q\\) + p\n\\(I_p\\): (1 - u) * 0 + u * 1 = u\nI = \\(I_q\\) + \\(I_p\\) = \\(I_q\\) + utilization\n\\(T_q = p * \\frac{u}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n도착 간격이 지수분포를 따르지 않는 경우 근사치만을 제공\n\n\\(I_q = \\frac{1}{a} * T_q = \\frac{T_q}{a}\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (복수 자원)",
    "text": "대기시간 예측 (복수 자원)\n\ncapacity: \\(\\frac{m}{p}\\)\nflow rate: \\(\\frac{p}{am}\\)\n\\(I = I_q + I_p = I_q + mu\\)\n\\(T_q = \\frac{p}{m} * \\frac{u^{\\sqrt{2(m+1)} - 1}}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n근사치만을 제공\n\nservice level: \\(P(T_q ≤ TWT)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/07.html#풀링",
    "href": "posts/04_archives/bs_3_1/notes/product/07.html#풀링",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "풀링",
    "text": "풀링\n\n대기할 수 있는 방법은 여러가지가 있다\n대기시간을 줄일 수 있는 방법에 사람을 많이 뽑는것 외에 다른 고려 요소\n\n\n\n풀링의 효과\n\n풀링되는 시스템이 서로 완전히 독립\n다양한 input을 처리할 수 있어야 함.\n→ 대기시간, 대기 인원 감소",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "href": "posts/04_archives/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "title": "프로젝트 관리",
    "section": "프로젝트 vs 프로세스",
    "text": "프로젝트 vs 프로세스\n\n프로젝트: 일회성, 하나의 흐름단위, 제한된 시간 내에 완료해야 됨\n\n프로젝트 관리: 프로젝트를 구성하는 여러 활동들의 배치와 계획\n\n프로세스: 지속적, 여러 흐름 단위",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "href": "posts/04_archives/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "title": "프로젝트 관리",
    "section": "주경로 기법(Critical Path Method, CPM)",
    "text": "주경로 기법(Critical Path Method, CPM)\n\n프로젝트 완료 시간 계산\n종속성 path 중 가장 긴 path가 주경로. 즉 프로젝트를 완료할 수 있는 가장 짧은 시간.\n\n이 path의 활동이 지연되면 프로젝트 전체가 지연됨\n프로세스 흐름도에서 개별작업의 처리능력이 중요했던것과 달리 프로젝트 종료 시간이 중요.\n\n\n\n주경로 찾기\n\nEST: 각 활동이 가장 빨리 시작할 수 있는 시간. max(선행 activity의 ECT)\nECT: 각 활동이 가장 빨리 끝날 수 있는 시간. EST + cost\nLCT: 각 활동이 가장 늦게 끝날 수 있는 시간. min(후행 activity의 LST)\nLST: 각 활동이 가장 늦게 시작할 수 있는 시간. LCT - cost\nSlack: LST - EST. 0이면 주 활동",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "href": "posts/04_archives/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "title": "프로젝트 관리",
    "section": "불확실성 하에서의 프로젝트 관리",
    "text": "불확실성 하에서의 프로젝트 관리\n\nCPM(Critical Path Method): 소요 시간이 정확하게 알려져 있다고 가정\nPERT(Program Evaluation and Review Technique): 소요 시간이 불확실하다고 가정\n\n\n가정\n\n주 경로는 각 활동의 평균 소요시간으로 구함\n주 경로는 바뀌지 않음\n각 활동의 소요시간은 독립적임\n각 활동의 소요시간이 정규분포를 따름\n\n\n\n특징\n\n\\(d_1, d_2, ..., d_n\\)이 확률변수이면 프로젝트 완성시간 (\\(X\\))도 확률변수\n\\(d_1, d_2, ..., d_n\\)이 정규분포를 따른다면 \\(X\\)도 정규분포를 따름",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/08.html#example-food-truck",
    "href": "posts/04_archives/bs_3_1/notes/product/08.html#example-food-truck",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: Food Truck",
    "text": "example: Food Truck\n\n변동(동일한 확률 가정)\n\n수요\n공급할 수 있는 양\n\n수요와 공급이 동시에 발생하지 않는 경우로 인해 평균 흐름률이 실제랑 다름.\n\n변동성이 흐름률에 영향을 미침\nbuffer가 있으면 흐름률 높일 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/08.html#buffer의-역할",
    "href": "posts/04_archives/bs_3_1/notes/product/08.html#buffer의-역할",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer의 역할",
    "text": "buffer의 역할\n\n\n\n변동성으로 인해 capacity가 낮아지는 이유\n\n\n\n\n변동성이 없다면 cycle time은 1/capacity\n변동성이 있다면 cycle time은 늘어남. (시뮬레이션으로 계산)\n버퍼가 있으면 용량이 커질수록 1/capacity로 점점 줄어듦.\ncell layout을 사용하면 cycle time을 제일 많이 줄일 수 있음.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "href": "posts/04_archives/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer를 둘 수 없는 상황: 병원외상센터",
    "text": "buffer를 둘 수 없는 상황: 병원외상센터\n\n\n대기해야 할 상황이 있으면 다른 병원으로 이동\n\ndiversion 상태, loss(service를 못 받음)\n\n\n\nDiversion 상태 확률\n\nD &lt; C 가정하지 않음\n도착 간격은 지수분포 가정 (processing time 분포는 가정 안함)\n대기하지 않고 바로 이탈한다고 가정\n\\(P_m\\): 내재활용률과 자원의 수에 의해 결정됨\n\\(r = um = \\frac{p}{a}\\), 해야하는 일의 양을 의미\n\n단위: Erlang\n\n\n\n\n\nErlang Loss Table\n\n\n\n들어온 인원(흐름률): \\(\\frac{1}{a}(1 - P_m(r))\\)\n안 들어온 인원: \\(\\frac{1}{a}P_m(r)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/08.html#erlang-loss-table",
    "href": "posts/04_archives/bs_3_1/notes/product/08.html#erlang-loss-table",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "Erlang Loss Table",
    "text": "Erlang Loss Table\n\n\n\n얼랑 솔실 공식1",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/08.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/product/08.html#footnotes",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "각주",
    "text": "각주\n\n\ndiversion 확률, 꽉 차있을 확률, 도착한 환자가 서비스 받을 확률, 다른 병원으로 갈 확률 시험에 나온다.↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/computer/02.html",
    "href": "posts/04_archives/bs_3_1/notes/computer/02.html",
    "title": "컴퓨팅적사고 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "컴퓨팅적사고 발표 ppt"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "href": "posts/04_archives/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "title": "Linear Programming Algorithm",
    "section": "선형 계획을 푸는 알고리즘",
    "text": "선형 계획을 푸는 알고리즘\n\n그래프를 사용하는 방법\n변수가 3개 이하인 경우, 그래프를 그려서 해를 찾을 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/03.html#simplex-method",
    "href": "posts/04_archives/bs_3_1/notes/OR/03.html#simplex-method",
    "title": "Linear Programming Algorithm",
    "section": "Simplex Method",
    "text": "Simplex Method\n\n기하학적 이해\n\n\nInitialization: Collect 1 CFP. 일반적으로 원점을 선택\nOptimality test: find better adj\n\nObj ⋅ (adj - cur) &gt; 0: better\nObj ⋅ (adj - cur) = 0: not changed\nObj ⋅ (adj - cur) &lt; 0: worse\n\n\n\n\n대수적 풀이\n\nbasic solution: 제약식의 변수 중 일부를 기저 변수로 선택하고, 나머지를 0으로 설정하여 얻는 해.\n\n만약 기저변수가 0인 경우, 이를 퇴화라고 부른다.\n\nbasic feasible solution: 모든 변수가 0 이상인 basic solution. 즉, 제약식을 모두 만족하는 해.\n비 기저변수(non basic variable): free variable. 변수의 수 - 방정식의 수 만큼 존재.\n기저 변수(bais variable): pivot variable.\n풀이는 생략\n\n\n\nSimplex Tableau\n\n풀이는 생략\n\n\n\n비 표준형 모델에서의 적용\n\n제약식이 = 인 경우: 인공 변수 추가. 목적 함수에 Big-M 방법을 사용하여 표현.\n\nTableau에서 인공변수의 계수를 0으로 만들어서 진행.\n\n제약식이 ≥ 인 경우: slack 변수랑 surplus variable 추가. surplus variable에 대하여 목적 함수에 Big-M 방법을 사용하여 표현.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/01.html#general",
    "href": "posts/04_archives/bs_3_1/notes/OR/01.html#general",
    "title": "Simplex 표 계산",
    "section": "General",
    "text": "General\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\n\n# Convert all elements to Fraction\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 설정\nobj = [5, -5, -13, 0, 0, -10, 0]\nA = [\n    [-1, 1, 3, 1, 0, 3, 20],\n    [12, 4, 10, 0, 1, 5, 90],\n]\nbasic = np.array([4, 5]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3]) - 1  # x1, x2, x3, x4\n\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        min_rc_idx = None\n        min_rc = Fraction(0)\n        for j in range(len(obj) - 1):  # RHS 제외\n            if obj[j] &lt; min_rc:\n                min_rc = obj[j]\n                min_rc_idx = j\n        \n        # 종료 조건: 음수 계수가 없으면 최적\n        if min_rc_idx is None or min_rc &gt;= 0:\n            print(\"Optimal solution reached.\")\n            solution = {f\"x{i+1}\": Fraction(0) for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((float('inf'), i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == float('inf'):\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [x / pivot for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [A[i][j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [obj[j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    5 |   -5 |  -13 |    0 |    0 |  -10 |     0 |\n+----+-----+------+------+------+------+------+------+-------+\n| x4 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   12 |    4 |   10 |    0 |    1 |    5 |    90 |\n+----+-----+------+------+------+------+------+------+-------+\nEntering variable: x3\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+-------+------+------+-------+\n|    |   Z | x1   | x2   |   x3 | x4    |   x5 |   x6 | RHS   |\n+====+=====+======+======+======+=======+======+======+=======+\n| Z  |   1 | 2/3  | -2/3 |    0 | 13/3  |    0 |    3 | 260/3 |\n+----+-----+------+------+------+-------+------+------+-------+\n| x3 |   0 | -1/3 | 1/3  |    1 | 1/3   |    0 |    1 | 20/3  |\n+----+-----+------+------+------+-------+------+------+-------+\n| x5 |   0 | 46/3 | 2/3  |    0 | -10/3 |    1 |   -5 | 70/3  |\n+----+-----+------+------+------+-------+------+------+-------+\nEntering variable: x2\nLeaving variable: x3\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    0 |    0 |    2 |    5 |    0 |    5 |   100 |\n+----+-----+------+------+------+------+------+------+-------+\n| x2 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   16 |    0 |   -2 |   -4 |    1 |   -7 |    10 |\n+----+-----+------+------+------+------+------+------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 0\nx2 = 20\nx3 = 0\nx4 = 0\nx5 = 10\nx6 = 0\nObjective Value = 100",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/01.html#big-m-method",
    "href": "posts/04_archives/bs_3_1/notes/OR/01.html#big-m-method",
    "title": "Simplex 표 계산",
    "section": "Big M Method",
    "text": "Big M Method\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\nfrom sympy import symbols, simplify, oo\n\nM = symbols('M')\n\n# 초기 설정\nobj = [20, 10, 0, M, 0, M, 0]\nA = [\n    [5, 1, -1, 1, 0, 0, 6],\n    [2, 2, 0, 0, -1, 1, 8],\n]\nbasic = np.array([4, 6]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3, 5]) - 1  # x1, x2, x3, x4\n\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef adjust_obj_for_big_m():\n    global obj\n    print(\"\\nAdjusting Objective Function for Big M Method\")\n    for i in range(len(basic)):\n        basic_var_idx = basic[i]\n        if obj[basic_var_idx] != 0:  # 인공변수일 경우(M이 포함된 경우)\n            factor = obj[basic_var_idx]\n            obj = [simplify(obj[j] - factor * A[i][j]) for j in range(len(obj))]\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    adjust_obj_for_big_m()\n    print_table()\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        eval_obj = [x.evalf(subs={M: 1e6}) if x.has(M) else float(x) for x in obj[:-1]]\n        min_rc_idx = min(range(len(obj)-1), key=lambda j: eval_obj[j])\n        \n        if eval_obj[min_rc_idx] &gt;= 0:\n            print(\"Optimal solution reached.\")\n            # 최적 해 출력\n            solution = {f\"x{i+1}\": 0 for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((oo, i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == oo:\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [simplify(x / pivot) for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [simplify(A[i][j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [simplify(obj[j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nAdjusting Objective Function for Big M Method\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\nEntering variable: x1\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n|    |   Z |   x1 | x2        | x3        | x4        | x5   |   x6 | RHS          |\n+====+=====+======+===========+===========+===========+======+======+==============+\n| Z  |   1 |    0 | 6 - 8*M/5 | 4 - 2*M/5 | 7*M/5 - 4 | M    |    0 | -28*M/5 - 24 |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x1 |   0 |    1 | 1/5       | -1/5      | 1/5       | 0    |    0 | 6/5          |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x6 |   0 |    0 | 8/5       | 2/5       | -2/5      | -1   |    1 | 28/5         |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\nEntering variable: x2\nLeaving variable: x6\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+---------+------+----------+-------+\n|    |   Z |   x1 |   x2 | x3   | x4      | x5   | x6       | RHS   |\n+====+=====+======+======+======+=========+======+==========+=======+\n| Z  |   1 |    0 |    0 | 5/2  | M - 5/2 | 15/4 | M - 15/4 | -45   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x1 |   0 |    1 |    0 | -1/4 | 1/4     | 1/8  | -1/8     | 1/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x2 |   0 |    0 |    1 | 1/4  | -1/4    | -5/8 | 5/8      | 7/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 1/2\nx2 = 7/2\nx3 = 0\nx4 = 0\nx5 = 0\nx6 = 0\nObjective Value = -45",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/01.html#grubi",
    "href": "posts/04_archives/bs_3_1/notes/OR/01.html#grubi",
    "title": "Simplex 표 계산",
    "section": "Grubi",
    "text": "Grubi\n\nfrom gurobipy import *\n\nmodel = Model(\"ex4.4-6\")\nmodel.setParam(GRB.Param.OutputFlag, 0)\n\nxab = model.addVar(vtype=GRB.CONTINUOUS, name=\"xab\")\nxac = model.addVar(vtype=GRB.CONTINUOUS, name=\"xac\")\nxbd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbd\")\nxbe = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbe\")\nxcd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xcd\")\nxce = model.addVar(vtype=GRB.CONTINUOUS, name=\"xce\")\nxde = model.addVar(vtype=GRB.CONTINUOUS, name=\"xde\")\nxdf = model.addVar(vtype=GRB.CONTINUOUS, name=\"xdf\")\nxef = model.addVar(vtype=GRB.CONTINUOUS, name=\"xef\")\nxfa = model.addVar(vtype=GRB.CONTINUOUS, name=\"xfa\")\n\nmodel.setObjective(xfa, GRB.MAXIMIZE)\n\nmodel.addConstr(xab + xac - xfa == 0)\nmodel.addConstr(xbd + xbe - xab == 0)\nmodel.addConstr(xcd + xce - xac == 0)\nmodel.addConstr(xde + xdf - xbd - xcd == 0)\nmodel.addConstr(xef - xbe - xce - xde == 0)\nmodel.addConstr(xfa - xdf - xef == 0)\n\nmodel.addConstr(xab &lt;= 9)\nmodel.addConstr(xac &lt;= 7)\nmodel.addConstr(xbd &lt;= 7)\nmodel.addConstr(xbe &lt;= 2)\nmodel.addConstr(xcd &lt;= 4)\nmodel.addConstr(xce &lt;= 6)\nmodel.addConstr(xde &lt;= 3)\nmodel.addConstr(xdf &lt;= 6)\nmodel.addConstr(xef &lt;= 9)\n\nmodel.optimize()\n\nfor var in model.getVars():\n    print(f\"{var.varName}: {var.x}\")\nprint(\"Obj: \", model.objVal)\n\nRestricted license - for non-production use only - expires 2026-11-23\nxab: 8.0\nxac: 7.0\nxbd: 6.0\nxbe: 2.0\nxcd: 1.0\nxce: 6.0\nxde: 1.0\nxdf: 6.0\nxef: 9.0\nxfa: 15.0\nObj:  15.0",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/04.html#overview",
    "href": "posts/04_archives/bs_3_1/notes/OR/04.html#overview",
    "title": "Simplex Method (part 5)",
    "section": "Overview",
    "text": "Overview\n\n심플렉스 방법의 기하학적·대수적 원리, 행렬형 알고리즘, 그리고 그 실용적 응용(민감도 분석 등)을 체계적으로 설명\n심플렉스 방법은 선형계획 문제에서 최적해를 꼭짓점 가능해(CPF)에서 찾으며, 행렬 연산을 통해 컴퓨터로 효율적으로 구현할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "href": "posts/04_archives/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "title": "Simplex Method (part 5)",
    "section": "Simplex 방법의 기초",
    "text": "Simplex 방법의 기초\n\n꼭짓점 가능해와 제약식 경계\n\n선형계획 문제의 해는 가능해 영역(feasible region)의 경계에 존재한다.\n이 제약식들을 등호(=)로 바꾼 제약식 경계식을 만들 수 있다.\n\n함수 제약식\n\n≤: slack variable\n=: artificial variable\n≥: surplus variable, artificial variable\n\n비음 제약식\n\nnon-restricted: \\(x_i = x_i^{+} - x_i^{-}\\)\n≤ 0: \\(x_i = -x_i^{+}\\)\n\n\n이 경계식들은 2차원에서는 선, 3차원에서는 평면, n차원에서는 초평면(hyperplane)을 형성한다.\n\n\n\n꼭짓점 가능해의 세 가지 주요 속성\n\n최적해의 위치\n\n최적해가 유일하면, 그것은 꼭짓점 가능해이다.\n최적해가 여러 개라면, 그 중 적어도 두 개는 인접 꼭짓점 가능해이다.\n즉, 최적해는 항상 꼭짓점 가능해(혹은 그 선분)에 존재한다.\n\n유한성\n\n꼭짓점 가능해의 개수는 유한합니다.\nm+n개의 제약식 중 n개를 선택하는 조합의 수는 유한하므로, 이론적으로 모든 꼭짓점 가능해를 열거해 비교할 수도 있습니다. 하지만 실제로는 심플렉스 방법이 훨씬 적은 수만 탐색한다.\n\n최적성의 충분조건\n\n인접 꼭짓점 중 더 좋은 해가 없으면, 현재 해가 최적해임이 보장된다.\n\n\n\n\n심플렉스 방법의 핵심 알고리즘 구조\n심플렉스 방법은 다음과 같은 반복 구조를 가진다\n\n초기 꼭짓점 가능해(기저해) 선택\n인접 꼭짓점으로 이동(목적함수 값이 개선되는 방향)\n더 이상 개선이 불가능하면 종료, 그 해가 최적해임을 보장",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "href": "posts/04_archives/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "title": "Simplex Method (part 5)",
    "section": "행렬형의 Simplex",
    "text": "행렬형의 Simplex\n\n행렬형 심플렉스 방법의 기본 구조\n표준형 선형계획 문제를 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\text{Maximize} \\quad &Z=c^Tx \\\\\n\\text{Subject to} \\quad &Ax=b, x≥0 \\\\\n\\end{aligned}\n\\]\n\n여기서 A는 m×n 행렬, x는 n차원 변수 벡터, b는 m차원 상수 벡터, c는 n차원 계수 벡터이다.\n여유변수(slack variable)등을 도입해 모든 제약식을 등식으로 바꾼다.\n\n\n\n행렬 연산을 활용한 반복 과정\n\n(제일 처음 단계의 경우 3, 4단계 먼저 진행)\n\n\n진입기저변수(Entering Variable) 선택\n탈락기저변수(Leaving Variable) 선택\n\n최소비율법(minimum ratio test) 사용\n\n새로운 기저 가능해 결정\n\n기저변수 식별\n기저행렬(Basic Matrix, B): m개의 기저변수에 대해 m×m 행렬 \\(B\\)와 \\(B^{-1}\\)를 만든다. 1\n기저해(Basic Solution) 계산: \\(x_B=B^{-1}b\\)\n목적함수 값 계산: \\(Z=c_B^TB^{−1}b\\)\n\n최적화 검사\n\n비기저변수의 계수(감소계수, reduced cost)를 계산\n\n계산식: \\(c_B^TB^{−1}a_n - c_n\\)\nslack 변수: \\(c_B^TB^{-1}\\)\n\n최적일 경우 종료",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/04.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/OR/04.html#footnotes",
    "title": "Simplex Method (part 5)",
    "section": "각주",
    "text": "각주\n\n\n역행렬 구하는 법. 2차원 말고는 그냥 그 방식으로 풀자.↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "href": "posts/04_archives/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "쌍대 심플렉스 방법",
    "text": "쌍대 심플렉스 방법\n\n쌍대 문제 제약식에 -1 하고 slack 변수 추가해서 반대로 품.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/10.html#상한-기법",
    "href": "posts/04_archives/bs_3_1/notes/OR/10.html#상한-기법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "상한 기법",
    "text": "상한 기법\n\n일단은 대수적으로 푸는 방법만 배움.\n변수가 0일 때 뿐만 아니라 upper bound일 때도 non-basic variable로 취급.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/15.html#overview",
    "href": "posts/04_archives/bs_3_1/notes/OR/15.html#overview",
    "title": "수송문제와 할당 문제들",
    "section": "Overview",
    "text": "Overview\n\n수송문제: 여러 공급지로부터 여러 수요지까지 상품을 운송하는 최적의 방법을 결정하는 문제를 다루지만, 그 적용 범위는 생산 일정 계획과 같이 실제 수송과 직접적인 관련이 없는 경우까지 확장된다.\n할당문제: 주로 인력이나 자원을 특정 과업(tasks)에 배정하는 문제를 다룬다",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "href": "posts/04_archives/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "title": "수송문제와 할당 문제들",
    "section": "2. 수송문제를 위한 능률적인 심플렉스 방법",
    "text": "2. 수송문제를 위한 능률적인 심플렉스 방법\n\n가정\n\n공급량과 수요량은 일치\n\n일치하지 않으면 dummy 공급지를 추가, 비용은 0으로 설정\n불가능한 연결은 무한대 비용으로 설정\n수요가 정수가 아닌 범위일 경우\n\n비용은 분배되는 상품 양(\\(x_{ij}\\))에 비례\n\n\n\n수식\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n창고 (트럭당 운송비용, $)\n\n생산량 (트럭분)\n\n\n\n\n\n\n새크라멘토\n솔트레이크시티\n래피드시티\n앨버커키\n\n\n\n공장 1 (벨링햄)\n\n464\n513\n654\n867\n75\n\n\n공장 2 (유진)\n\n352\n416\n690\n791\n125\n\n\n공장 3 (앨버트 리)\n\n995\n682\n388\n685\n100\n\n\n창고 배정량 (트럭분)\n\n80\n65\n70\n85\n(총 300)\n\n\n\n\n\n\n수송문제 example\n\n\n\n\\(Z= \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} x_{ij}\\)\n공급 제약식: \\(\\sum_{j=1}^{n} x_{ij} = s_i \\quad \\text{for } i=1,2,\\ldots,m\\)\n수요 제약식: \\(\\sum_{i=1}^{m} x_{ij} = d_j \\quad \\text{for } j=1,2,\\ldots,n\\)\n비음 제약식: \\(x_{ij} \\geq 0 \\quad \\text{for } i=1,2,\\ldots,m, j=1,2,\\ldots,n\\)\n\n\n\n수송문제를 위한 매개변수 표\n\n\n일반적인 심플렉스 방법도 수송문제를 푸는 데 적용할 수 있지만, 수송문제의 규모가 크고 제약식 행렬이 대부분 0과 일부 1로 구성된 희소 행렬(sparse matrix)이라는 특성 때문에 비효율적이다. 일반 심플렉스 방법은 초기화 단계에서 많은 인공 변수를 필요로 할 수 있다.\n수송문제에서 기저가능해는 항상 m+n−1개의 기저 변수를 가진다.\n\n\n\n\n초기 bfs를 만들기 위한 절차\n\n\n\n문제 예시\n\n\n\n여전히 고려중인 행들과 열들로부터, 어떤 기준에 따라 다음 기저변수(할당)를 선택한다.\n\n북서모서리법\n\n할당을 충분히 크게 하여 그 행에 있는 남은 공급이나 그 열에 있는 남은 수요를 다 써버리게 한다(둘 중 더 작은 것).\n더 이상의 고려에서 그 행이나 열(둘 중에서 더 작게 남은 공급 혹은 수요를 가진 것)을 제거한다(만약 행과 열이 같은 남은 공급과 수요를 가지면, 임의로 행을 선택하여 제거한다. 열은 후에 퇴화기저변수, 즉 할당 0으로 표시됨을 제공하는 것으로 사용될 것이다).\n만약 단지 하나의 행 혹은 하나의 열이 고려 대상으로 남아 있으면, 가능한 할당과 함께 기저가 되는 그 행이나 열과 연관된 모든 남아 있는 변수(즉 전에 기저로 선택되지 않았고 행 과 열을 제거함으로써 고려 대상에서 제외되지 않은 변수들)를 선택함으로써 절차는 종결된다.\n\n\n\n\n최적화 검사 절차\n\n가장 많은 할당이 일어난 행의 변수 하나를 0으로 설정\n기저인 \\(x_{ij}\\)의 \\({i, j}\\)에 대해 \\(c_{ij} = u_i + v_j\\)를 만족한다는 성질로 \\(u_i\\)와 \\(v_j\\)를 계산한다.\n비기저 변수들의 \\(c_{ij} - u_i - v_j\\)를 계산한다.\n모두 양수이면 최적.\n\n\n\n반복\n\n진입기저변수를 결정하라: 가장 큰(절댓값으로) 음의 값 \\(C_{jj} - u_i - v_j\\)를 가지는 비기저변수 \\(x_{ij}\\)를 선택하라.\n탈락기저변수를 결정하라: 진입기저변수가 증가할 때 가능을 유지하기 위해 요구되는 연쇄반응을 식별하라. 기증셀들 중에서, 가장 작은 값을 가지는 기저변수를 선택하라.\n새 기저가능해를 결정하라: 탈락변수의 값을 각 수신셀의 할당에 더하라. 그 값을 각 기증 셀의 할당에서 빼어라.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "href": "posts/04_archives/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "title": "네트워크 최적화 모형",
    "section": "3. 최단 경로 문제",
    "text": "3. 최단 경로 문제\n\n그냥 다익스트라",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "href": "posts/04_archives/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "title": "네트워크 최적화 모형",
    "section": "5. 최대 흐름 문제 (augmenting path method)",
    "text": "5. 최대 흐름 문제 (augmenting path method)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "href": "posts/04_archives/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "title": "네트워크 최적화 모형",
    "section": "7. 네트워크 심플렉스 해법",
    "text": "7. 네트워크 심플렉스 해법\n모든 실행가능 해는 n-1개의 기저변수를 가지고, spanning tree를 형성한다.\n\n최대흐름문제 심플렉스 (9.7.2) 이거 어케 품\nmingamdo variable add?",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/02.html#business-analytics",
    "href": "posts/04_archives/bs_3_1/notes/OR/02.html#business-analytics",
    "title": "Intro",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nData Analysis\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\n\nOperations Research\n\nPrescriptive Analytics: What should we do? (Optimization)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/02.html#process-of-or-study",
    "href": "posts/04_archives/bs_3_1/notes/OR/02.html#process-of-or-study",
    "title": "Intro",
    "section": "Process of OR Study",
    "text": "Process of OR Study\n\n\n\n\n\nflowchart LR\n  A(Collect data) --&gt; B(Define the problem)\n  B --&gt; C{Data are sufficient?}\n  C --&gt;|No| A\n  C --&gt;|Yes| D(Formulate a model)\n  D --&gt; E(Solve the model)\n  E --&gt; F{Model is good?}\n  F --&gt;|Yes| G(Interpret results make suggestions)\n  F --&gt;|No| D",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "href": "posts/04_archives/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "title": "Intro",
    "section": "LP Model (표준형)",
    "text": "LP Model (표준형)\n\n제한된 자원을 경쟁하는 활동들에게 가능한 최적으로 분배하거나 이와 비슷한 수학적 구조를 가진 문제를 다루는 방법\n\n\\[\\begin{aligned}\nmax & \\sum_{i=1}^{n} c_i x_i \\\\\ns.t. & \\sum_{i=1}^{n} a_{ij} x_i \\leq b_j, j ≤ m \\\\\n& x_1, x_2, ..., x_n ≥ 0\n\\end{aligned}\\]",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/02.html#lp의-가정",
    "href": "posts/04_archives/bs_3_1/notes/OR/02.html#lp의-가정",
    "title": "Intro",
    "section": "LP의 가정",
    "text": "LP의 가정\n\n선형계획은 현실을 단순화한 모델로, 아래의 네 가지 가정이 완벽히 맞지 않을 수 있음.\n작은 불일치는 허용 가능하며, 민감도 분석으로 보완.\n심각한 위반 시 대안 모델(비선형계획, 정수계획 등)을 사용하나, 선형계획의 강력한 알고리즘이 유리하므로 초기 분석에 활용 후 필요 시 복잡한 모델로 전환.\n\n\n비례성(Proportionality)\n\n정의: 목적함수와 제약식에서 활동 수준(예: xx)에 대한 기여도가 선형(비례적)으로 표현됨.\n위반 사례:\n\n초기 투자비용(고정비용)이 있어 \\(Z=3x_1−1\\)이 되는 경우, 비례성이 깨짐.\n규모의 경제로 한계 이익이 증가하면 비례성이 위반됨.\n한계 이익이 감소(예: 마케팅 비용 증가)하면 역시 비례성이 깨짐.\n\n대안: 비례성이 깨지면 비선형계획(12장)이나 혼합정수계획(11장)을 고려.\n\n가합성(Additivity)\n\n정의: 목적함수와 제약식의 값이 각 활동의 개별 기여도의 합으로 표현됨. 즉, 변수 간 교차곱이 없음.\n위반 사례:\n\n제품 간 보완적 상호작용(예: 공동 광고 효과)으로 \\(Z=3x_1+5x_2+x_1x_2\\)가 됨.\n경쟁적 상호작용(예: 설비 공유로 비효율 발생)으로 \\(Z=3x_1+5x_2−x_1x_2\\)가 됨.\n\n대안: 가합성이 위반되면 비선형계획(12장)으로 전환.\n\n가분성(Divisibility)\n\n정의: 의사결정 변수가 실수 값을 가질 수 있음. 즉, 활동 수준이 정수로 제한되지 않음.\n위반 사례: 변수가 정수로 제한되면(예: 배치 단위가 1, 2, 3만 가능) 가분성이 깨짐.\n대안: 정수계획(11장) 사용.\n\n확실성(Certainty)\n\n정의: 모델의 매개변수(예: \\(c_j, a_{ij}, b_i\\))가 알려진 상수로 고정. 해당 상수는 미래 예측에 기반하므로 불확실성이 존재.\n대응: 불확실성이 크면 민감도 분석(6.7절)으로 최적해의 변화를 확인하거나, 확률변수를 도입한 모델(23장) 사용.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/03.html#장점",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/03.html#장점",
    "title": "random forest",
    "section": "장점",
    "text": "장점\n\nClassification, Regression문제 모두 해결 가능\nAccuracy, out-of-bag error에 우수한 결과\nValidation을 위한 별도의 data set이 필요하지 않음\nBuilt-in validation set\nOverfitting이 없다\nOutlier에 강함\nMissing data를 잘 처리\n선처리 작업을 최소화\nFeature의 선택을 자동처리\n변수 삭제 없이 수천 개의 입력 변수를 처리",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/03.html#단점",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/03.html#단점",
    "title": "random forest",
    "section": "단점",
    "text": "단점\n\n속도가 느림\n해석이 어렵다",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "title": "association rule mining",
    "section": "Pattern minig",
    "text": "Pattern minig\n\nBasic Concepts\n\npattern: dataset 안에서 함께 자주 발생하는 subsequences, substructures, set of items\n\n이 pattern은 인과관계를 의미하진 않는다.\n\nAssociation rule minig: 최소 지지도나 신뢰도를 넘는 모든 항목에 대해 pattern을 찾는다.\n\n\n\nApplications\n\nassociation rule, correlation, classification, clustering data mining의 기반이 될 수 있다.\n장바구니 분석\n연속 구매 분석\n\n\n\nTerminologies\n\n지지도(Support): 전체 거래 중 특정 항목 집합이 포함된 거래의 비율.\n신뢰도(Confidence): 항목 X를 포함하는 거래 중에서 항목 Y도 함께 포함하는 거래의 비율.\n빈발 패턴(frequent): 최소 지지도를 넘는 pattern\n\n빈발 항목 집합(frequent itemset): 단순한 묶음\n빈발 시퀀스\n\n\n\n\nclosed pattern\n\nx가 빈발이고, 지지도가 상위 집합들과 다른 집합 (지지도는 상위로 갈 수록 떨어짐)\n지지도 정보를 유지할 수 있다.\n\n신뢰도 계산할 때 사용할 수 있음\n\n\n\n\nmax patterns\n\nx가 빈발이고, 상위 집합들 모두가 빈발 집합이 아닌 집합\n지지도 정보는 유지되지 않음.\n\n신뢰도 계산할 때 사용할 수 없어서 사실 상 결과 요약 외의 용도는 없음\n\nDownward closure property: 어떤 itemset이 빈발하지 않으면, 그 모든 superset은 무조건 빈발하지 않는다. 대우도 성립\n\n교수님은 anti-monotone property로 설명하셨지만 이게 더 자주 사용되는 용어",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/05.html#association-rule",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/05.html#association-rule",
    "title": "association rule mining",
    "section": "Association Rule",
    "text": "Association Rule\n\nfind frequent itemsets\n\nApriori (breadth-first search)\nFP-Growth\nEclat (depth-first search)\n\ngenerate association rules\n\n모든 빈발 itemset I에 대해 모든 I의 subset s로 ‘s -&gt; (I - s)’ 규칙을 생성\n최소 신뢰도 조건을 만족하는 규칙만 남김",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/05.html#algorithm",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/05.html#algorithm",
    "title": "association rule mining",
    "section": "Algorithm",
    "text": "Algorithm\n\nApirori\n\nMonotone 성질을 이용하여 빈발하지 않는 집합은 후보에서 제거\n\n\nscan DB once to get 1-itemsets\n반복\n\nk개의 itemset에 대해 k+1-itemset의 후보를 생성\n\nself-join: k-itemset을 두 개 합쳐서 k+1-itemset을 생성\nprune: k+1-itemset을 생성할 때, k-itemset의 subset이 모두 빈발해야 k+1-itemset이 빈발할 수 있다.\n\nk+1-itemset 후보에 대해 DB를 scan하여 빈발한 itemset을 찾는다.\nk += 1\n빈발 itemset이 없으면 종료\n\n\n\nApriori의 단점: DB scan을 여러 번 해야함, 후보가 많아질 수 있음\n\n후보 수를 줄이는 방법: Hashing\n\n\n\n\nDHP (Direct Hashing and Pruning)\n\nHash값이 같은 itemset의 count를 합하고, minimum support를 넘는 itemset만 남김\nHash table을 매번 만드는 번거로움이 있지만, apriori보다 빠름\n반복\n\n빈발 항목 찾기, 후보 해시 테이블 생성\n\n데이터베이스를 scan하여 최소 지지도를 넘는 1-itemset 후보를 찾음\n동시에 조합 가능한 2-itemset을 만들어 mapping된 해시 테이블 bucket에 count += 1\n\n가지치기\n\n1-itemset 후보를 이용해 self-join, prune으로 2-itemset 후보 생성\n완성된 후보를 1단계에서 만든 hash table에 매핑해서 최소 지지도를 넘는 2-itemset bucket이 아닐 경우 배제",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/12.html",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/12.html",
    "title": "Dataminig 1차 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Dataminig 1차 발표 ppt"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/07.html",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/07.html",
    "title": "ensemble",
    "section": "",
    "text": "다수의 모델을 학습시켜 결과를 종합하여 예측 성능을 높이는 방법",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/07.html#techniques",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/07.html#techniques",
    "title": "ensemble",
    "section": "Techniques",
    "text": "Techniques\n\nstacking:\n\n여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.\n메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.\n\nbagging\n\nvs cross validation:\n\ncross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님\nbagging은 분산을 줄이기 위해 사용함\n\nbagging의 voting, averaging은 unsupervised learning\n\nboosting: sequentially 학습\n\n이전 모델의 오차를 보완하는 방식으로 학습한다.\nsupervised learning",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#데이터-load",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#데이터-load",
    "title": "analysis",
    "section": "데이터 load",
    "text": "데이터 load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nRANDOM_STATE = 54321\nnp.random.seed(RANDOM_STATE)\n\nX_train_df = pd.read_csv(\"_data/train_data.csv\")\nX_test_df = pd.read_csv(\"_data/test_data.csv\")\ny_train = X_train_df['y'].astype('category')\ny_test = X_test_df['y'].astype('category')\nweights_train = X_train_df['weights']\nweights_test = X_test_df['weights']\nX_train = X_train_df.drop(columns=['y', 'weights'])\nX_test = X_test_df.drop(columns=['y', 'weights'])",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "title": "analysis",
    "section": "무작위 분류",
    "text": "무작위 분류\n\nclass_proportions_py = y_train.value_counts(normalize=True)\ntest_categories = list(y_test.cat.categories)\nprop_values_ordered = class_proportions_py.reindex(test_categories, fill_value=0).values\nrandom_predictions = np.random.choice(\n    a=test_categories,\n    size=len(y_test),\n    replace=True,\n    p=prop_values_ordered\n)\nrandom_predictions_cat = pd.Categorical(random_predictions, categories=test_categories, ordered=False)\ncm_random = confusion_matrix(y_test, random_predictions_cat, sample_weight=weights_test, labels=test_categories)\n\nprint(\"\\n=== 무작위 분류기 혼동 행렬 ===\\n\")\ncm_df_random = pd.DataFrame(cm_random, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_random.round(2))\n\naccuracy_random = accuracy_score(y_test, random_predictions_cat, sample_weight=weights_test)\nprecision_random_sk = precision_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_random_sk = recall_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_random_sk = f1_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== 무작위 분류기 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_random:.4f}\")\n\nprint(\"\\n무작위 분류기 범주별 정밀도\")\nprint(pd.Series(precision_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 재현율\")\nprint(pd.Series(recall_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 F1-score\")\nprint(pd.Series(f1_score_random_sk, index=test_categories).round(4))\n\n\n=== 무작위 분류기 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37372.17           44867.14\nActual: stable                     41616.52           49155.97\n\n=== 무작위 분류기 성능 지표 ===\n\n정확도: 0.5001\n\n무작위 분류기 범주별 정밀도\nexplorative    0.4731\nstable         0.5228\ndtype: float64\n\n무작위 분류기 범주별 재현율\nexplorative    0.4544\nstable         0.5415\ndtype: float64\n\n무작위 분류기 범주별 F1-score\nexplorative    0.4636\nstable         0.5320\ndtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#random-forest",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#random-forest",
    "title": "analysis",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nbase_rf = RandomForestClassifier(\n    random_state=RANDOM_STATE,\n    oob_score=True,\n    class_weight='balanced_subsample',\n    n_jobs=-1\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'max_features': ['sqrt', 'log2', None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    estimator=base_rf,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nrf_model_py = grid_search.best_estimator_\nprint(f\"OOB Score: {rf_model_py.oob_score_:.4f}\")\n\nimportances = rf_model_py.feature_importances_\nfeature_names = X_train.columns\n\nmax_importance = importances.max()\nscaled_importances = (importances / max_importance) * 100\nforest_importances = pd.Series(scaled_importances, index=feature_names).sort_values(ascending=False)\n\nprint(\"\\n=== Random Forest 중요도 ===\")\ntop_10_importances = forest_importances\ntop_10_df = pd.DataFrame({'Feature': top_10_importances.index, 'Importance': top_10_importances.values})\nprint(top_10_df.round(2))\n\nsns.barplot(x=top_10_importances.values, y=top_10_importances.index)\nplt.title('Feature Importances (Random Forest)', fontsize=10)\nplt.xlabel('Importance Score', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('ran_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_rf = rf_model_py.predict(X_test)\ny_pred_proba_rf = rf_model_py.predict_proba(X_test)\n\ny_pred_rf_cat = pd.Categorical(y_pred_rf, categories=test_categories, ordered=False)\n\ncm_rf = confusion_matrix(y_test, y_pred_rf_cat, sample_weight=weights_test, labels=test_categories)\nprint(\"\\n=== Random Forest 혼동 행렬 ===\\n\")\ncm_df_rf = pd.DataFrame(cm_rf, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_rf.round(2))\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf_cat, sample_weight=weights_test)\nprecision_rf_sk = precision_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_rf_sk = recall_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_rf_sk = f1_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== Random Forest 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_rf:.4f}\")\nprint(\"\\nRandom Forest 범주별 정밀도:\")\nprint(pd.Series(precision_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 재현율:\")\nprint(pd.Series(recall_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 F1-score:\")\nprint(pd.Series(f1_score_rf_sk, index=test_categories).round(4))\n\n\n최적의 하이퍼파라미터: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n최적 교차 검증 점수: 0.6129\nOOB Score: 0.5865\n\n=== Random Forest 중요도 ===\n                        Feature  Importance\n0            self_confidence_w4      100.00\n1              desire_stress_w1       89.19\n2          parent_attachment_w5       83.50\n3          parent_monitoring_w5       83.40\n4              friend_stress_w4       82.51\n5          parent_monitoring_w4       81.66\n6             deviant_esteem_w3       80.23\n7                 neg_esteem_w4       79.71\n8          parent_monitoring_w3       79.27\n9          parent_attachment_w4       78.51\n10  higher_school_dependence_w1       76.60\n11             desire_stress_w4       75.87\n12         parent_attachment_w3       74.15\n13  higher_school_dependence_w2       73.13\n14         parent_monitoring_w1       72.00\n15             friend_stress_w3       71.55\n16           self_confidence_w1       69.76\n17  higher_school_dependence_w4       67.74\n18             desire_stress_w2       67.66\n19            deviant_esteem_w4       66.96\n20           self_confidence_w3       66.83\n21  higher_school_dependence_w5       66.33\n22         parent_monitoring_w2       64.70\n23           self_confidence_w2       64.55\n24             parent_stress_w2       64.32\n25             friend_stress_w2       63.39\n26             parent_stress_w3       63.12\n27             parent_stress_w4       62.80\n28            deviant_esteem_w5       62.53\n29         parent_attachment_w1       62.00\n30             friend_stress_w1       61.78\n31                neg_esteem_w5       61.55\n32           academic_stress_w3       60.54\n33            deviant_esteem_w2       60.54\n34         parent_attachment_w2       60.46\n35  higher_school_dependence_w3       60.34\n36           academic_stress_w2       59.86\n37             parent_stress_w5       59.49\n38            deviant_esteem_w1       59.29\n39                neg_esteem_w3       58.54\n40           academic_stress_w4       58.47\n41           self_confidence_w5       57.98\n42                neg_esteem_w2       57.81\n43           academic_stress_w5       55.73\n44             desire_stress_w5       55.68\n45           academic_stress_w1       55.06\n46             friend_stress_w5       54.55\n47                neg_esteem_w1       52.13\n48             parent_stress_w1       52.07\n49             desire_stress_w3       51.97\n\n\n\n\n\n\n\n\n\n\n=== Random Forest 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37334.03           44905.28\nActual: stable                     27479.79           63292.70\n\n=== Random Forest 성능 지표 ===\n\n정확도: 0.5816\n\nRandom Forest 범주별 정밀도:\nexplorative    0.576\nstable         0.585\ndtype: float64\n\nRandom Forest 범주별 재현율:\nexplorative    0.4540\nstable         0.6973\ndtype: float64\n\nRandom Forest 범주별 F1-score:\nexplorative    0.5078\nstable         0.6362\ndtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#xgboost",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#xgboost",
    "title": "analysis",
    "section": "XGBoost",
    "text": "XGBoost\n\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_numeric = le.fit_transform(y_train)\ny_test_numeric = le.transform(y_test)\n\nclass_labels_ordered = list(le.classes_) \nnum_classes = len(class_labels_ordered)\n\nprint(f\"Label Encoder 클래스: {class_labels_ordered} -&gt; {list(range(num_classes))}\")\n\nxgb_model = xgb.XGBClassifier(\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    num_class=num_classes,\n    seed=RANDOM_STATE,\n    use_label_encoder=False,\n    verbosity=0\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'learning_rate': [0.05, 0.1],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train_numeric, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nxgb_model_py = grid_search.best_estimator_\n\nimportance_scores = xgb_model_py.feature_importances_\nxgb_importance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': importance_scores\n})\n\nmax_importance = xgb_importance_df['Importance'].max()\nxgb_importance_df['Importance'] = (xgb_importance_df['Importance'] / max_importance) * 100\nxgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nXGBoost 변수 중요도\")\nprint(xgb_importance_df)\n\nsns.barplot(x='Importance', y='Feature', data=xgb_importance_df)\nplt.title('Feature Importances (XGBoost)', fontsize=10)\nplt.xlabel('Importance', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('xg_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_proba_xgb = xgb_model_py.predict_proba(X_test)\ny_pred_xgb_numeric = np.argmax(y_pred_proba_xgb, axis=1)\ny_pred_xgb = le.inverse_transform(y_pred_xgb_numeric)\ny_pred_xgb_cat = pd.Categorical(y_pred_xgb, categories=class_labels_ordered, ordered=False)\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb_cat, sample_weight=weights_test, labels=class_labels_ordered)\nprint(\"\\n=== XGBoost 모델 혼동 행렬 ===\\n\")\ncm_df_xgb = pd.DataFrame(cm_xgb, index=[f\"Actual: {cat}\" for cat in class_labels_ordered], columns=[f\"Predicted: {cat}\" for cat in class_labels_ordered])\nprint(cm_df_xgb.round(2))\n\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb_cat, sample_weight=weights_test)\nprecision_xgb_sk = precision_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nrecall_xgb_sk = recall_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nf1_score_xgb_sk = f1_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\n\nprint(\"\\n=== XGBoost 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_xgb:.4f}\")\nprint(\"\\nXGBoost 모델 범주별 정밀도:\")\nprint(pd.Series(precision_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 재현율:\")\nprint(pd.Series(recall_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_xgb_sk, index=class_labels_ordered).round(4))\n\nLabel Encoder 클래스: ['explorative', 'stable'] -&gt; [0, 1]\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n최적의 하이퍼파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n최적 교차 검증 점수: nan\n\nXGBoost 변수 중요도\n                        Feature  Importance\n40         parent_attachment_w5  100.000000\n38                neg_esteem_w4   89.684937\n36           self_confidence_w4   87.710533\n33         parent_monitoring_w4   84.155891\n30         parent_attachment_w4   83.655487\n35             friend_stress_w4   83.593765\n23         parent_monitoring_w3   80.530685\n15             friend_stress_w2   80.180794\n12             parent_stress_w2   80.104950\n6            self_confidence_w1   78.258446\n17  higher_school_dependence_w2   77.525246\n21            deviant_esteem_w3   75.736404\n22             parent_stress_w3   74.421280\n4              desire_stress_w1   74.244553\n43         parent_monitoring_w5   74.036301\n14             desire_stress_w2   73.580856\n7   higher_school_dependence_w1   73.197128\n25             friend_stress_w3   72.865379\n47  higher_school_dependence_w5   72.309631\n31            deviant_esteem_w4   71.178528\n39           academic_stress_w4   69.950974\n48                neg_esteem_w5   69.804626\n32             parent_stress_w4   69.397591\n3          parent_monitoring_w1   68.932785\n46           self_confidence_w5   68.488029\n45             friend_stress_w5   68.380692\n26           self_confidence_w3   67.708191\n27  higher_school_dependence_w3   66.273537\n19           academic_stress_w2   65.726303\n41            deviant_esteem_w5   65.552612\n20         parent_attachment_w3   65.515610\n11            deviant_esteem_w2   65.489426\n9            academic_stress_w1   64.981789\n34             desire_stress_w4   63.373531\n49           academic_stress_w5   63.285591\n42             parent_stress_w5   62.527847\n13         parent_monitoring_w2   62.383938\n24             desire_stress_w3   62.092834\n18                neg_esteem_w2   61.555267\n29           academic_stress_w3   60.970982\n37  higher_school_dependence_w4   60.602821\n44             desire_stress_w5   59.860622\n5              friend_stress_w1   59.424625\n0          parent_attachment_w1   55.924370\n10         parent_attachment_w2   55.653919\n8                 neg_esteem_w1   52.370869\n28                neg_esteem_w3   50.224579\n2              parent_stress_w1   50.090385\n1             deviant_esteem_w1   49.659233\n16           self_confidence_w2   49.203815\n\n\n\n\n\n\n\n\n\n\n=== XGBoost 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                40510.53           41728.78\nActual: stable                     25198.53           65573.96\n\n=== XGBoost 모델 성능 지표 ===\n\n정확도: 0.6132\n\nXGBoost 모델 범주별 정밀도:\nexplorative    0.6165\nstable         0.6111\ndtype: float64\n\nXGBoost 모델 범주별 재현율:\nexplorative    0.4926\nstable         0.7224\ndtype: float64\n\nXGBoost 모델 범주별 F1-score:\nexplorative    0.5476\nstable         0.6621\ndtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "title": "analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\n\nbase_log_reg = LogisticRegression(\n    solver='lbfgs',\n    max_iter=5000,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nparam_grid = [\n    {\n        'penalty': ['l1'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['l2'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['lbfgs', 'liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['elasticnet'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['saga'],\n        'l1_ratio': [0.2, 0.5, 0.8],\n        'class_weight': ['balanced', None]\n    }\n]\n\ngrid_search = GridSearchCV(\n    estimator=base_log_reg,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nlog_reg_model_py = grid_search.best_estimator_\nclass_labels_logreg = log_reg_model_py.classes_\ncoef_series = pd.Series(log_reg_model_py.coef_[0], index=X_train.columns)\nsorted_coefs = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\ny_train_binary = (y_train == class_labels_logreg[1]).astype(int)\n\nlogit_model = sm.Logit(y_train_binary, X_train_sm)\nlogit_result = logit_model.fit(disp=0)\n\ncoef_summary = logit_result.summary2().tables[1]\ncoef_summary_df = pd.DataFrame(coef_summary)\ncoef_summary_sorted = coef_summary_df.sort_values('Coef.', key=abs, ascending=False)\nprint(\"\\n계수 및 p-value (절댓값이 큰 순서):\")\nprint(coef_summary_sorted[['Coef.', 'P&gt;|z|']])\n\ny_pred_logistic_py = log_reg_model_py.predict(X_test)\n\ntest_categories_logreg = list(y_test.cat.categories) if hasattr(y_test, 'cat') else sorted(list(y_test.unique()))\ny_pred_logistic_cat = pd.Categorical(y_pred_logistic_py, categories=test_categories_logreg, ordered=False)\n\ncm_logistic = confusion_matrix(y_test, y_pred_logistic_cat, sample_weight=weights_test, labels=test_categories_logreg)\nprint(\"\\n=== 로지스틱 회귀 모델 혼동 행렬 ===\\n\")\ncm_df_logistic = pd.DataFrame(cm_logistic, index=[f\"Actual: {cat}\" for cat in test_categories_logreg], columns=[f\"Predicted: {cat}\" for cat in test_categories_logreg])\nprint(cm_df_logistic.round(2))\n\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic_cat, sample_weight=weights_test)\nprecision_logistic_sk = precision_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nrecall_logistic_sk = recall_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nf1_score_logistic_sk = f1_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\n\nprint(\"\\n=== 로지스틱 회귀 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_logistic:.4f}\")\nprint(\"\\n로지스틱 회귀 모델 범주별 정밀도:\")\nprint(pd.Series(precision_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 재현율:\")\nprint(pd.Series(recall_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_logistic_sk, index=test_categories_logreg).round(4))\n\n\n최적의 하이퍼파라미터: {'C': 0.1, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n최적 교차 검증 점수: 0.6006\n\n계수 및 p-value (절댓값이 큰 순서):\n                                Coef.     P&gt;|z|\ndesire_stress_w1             0.499608  0.000015\nself_confidence_w4          -0.332916  0.010294\nparent_stress_w3             0.271418  0.057369\nneg_esteem_w4                0.263964  0.031803\ndesire_stress_w2             0.255400  0.018759\nself_confidence_w3          -0.254064  0.030035\nfriend_stress_w4            -0.247249  0.024386\nconst                        0.245302  0.000002\ndeviant_esteem_w3           -0.237610  0.068700\nparent_monitoring_w3         0.211999  0.050412\nparent_attachment_w3         0.184476  0.274346\nparent_monitoring_w4         0.180603  0.106346\nhigher_school_dependence_w5 -0.164889  0.024659\nacademic_stress_w2          -0.156225  0.223868\nparent_attachment_w5         0.151586  0.345721\ndesire_stress_w3            -0.149660  0.181854\nself_confidence_w2          -0.140414  0.185298\ndeviant_esteem_w1           -0.138070  0.263940\nparent_stress_w4            -0.137391  0.383677\nfriend_stress_w1            -0.136979  0.134193\nacademic_stress_w5           0.118084  0.270881\nparent_monitoring_w1         0.113677  0.264741\nacademic_stress_w1          -0.112715  0.410824\ndesire_stress_w4             0.110917  0.295629\nself_confidence_w1           0.109299  0.356000\nparent_stress_w1            -0.104439  0.436255\nparent_stress_w2            -0.101390  0.460526\nparent_monitoring_w2        -0.098915  0.363357\ndesire_stress_w5             0.096226  0.359037\nneg_esteem_w1                0.096029  0.462255\nacademic_stress_w4           0.091028  0.451824\nhigher_school_dependence_w2 -0.086012  0.175999\nhigher_school_dependence_w1 -0.082377  0.182057\nparent_attachment_w4        -0.078615  0.648722\ndeviant_esteem_w4           -0.077995  0.525561\nneg_esteem_w5               -0.077245  0.532354\nparent_monitoring_w5         0.075278  0.495239\nfriend_stress_w5            -0.073577  0.475849\nfriend_stress_w2            -0.069986  0.466378\nhigher_school_dependence_w4 -0.059155  0.445485\nparent_stress_w5            -0.053443  0.710032\nneg_esteem_w3               -0.045219  0.687428\nparent_attachment_w1        -0.035101  0.822711\nhigher_school_dependence_w3  0.033887  0.623319\ndeviant_esteem_w5           -0.024602  0.843547\nneg_esteem_w2                0.021549  0.824973\nacademic_stress_w3          -0.017071  0.879205\nself_confidence_w5           0.015841  0.893445\nfriend_stress_w3            -0.010240  0.924892\nparent_attachment_w2         0.006187  0.967933\ndeviant_esteem_w2            0.002349  0.982333\n\n=== 로지스틱 회귀 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                42190.16           40049.15\nActual: stable                     26537.42           64235.07\n\n=== 로지스틱 회귀 모델 성능 지표 ===\n\n정확도: 0.6151\n\n로지스틱 회귀 모델 범주별 정밀도:\nexplorative    0.6139\nstable         0.6160\ndtype: float64\n\n로지스틱 회귀 모델 범주별 재현율:\nexplorative    0.5130\nstable         0.7076\ndtype: float64\n\n로지스틱 회귀 모델 범주별 F1-score:\nexplorative    0.5589\nstable         0.6586\ndtype: float64",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "title": "analysis",
    "section": "모델 성능 비교",
    "text": "모델 성능 비교\n\nimport matplotlib.cm as cm\n\nmodel_metrics_list = []\nclass_labels_ordered = list(y_test.cat.categories)\n\ndef compile_metrics(model_name, accuracy, precision_arr, recall_arr, f1_arr):\n    metrics = {'Model': model_name, 'Accuracy': accuracy}\n    for i, label in enumerate(class_labels_ordered):\n        metrics[f'Precision ({label})'] = precision_arr[i]\n        metrics[f'Recall ({label})'] = recall_arr[i]\n        metrics[f'F1-score ({label})'] = f1_arr[i]\n    return metrics\n\nmodel_metrics_list.append(compile_metrics(\n    \"Random Classifier\",\n    accuracy_random,\n    precision_random_sk,\n    recall_random_sk,\n    f1_score_random_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Random Forest\",\n    accuracy_rf,\n    precision_rf_sk,\n    recall_rf_sk,\n    f1_score_rf_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"XGBoost\",\n    accuracy_xgb,\n    precision_xgb_sk,\n    recall_xgb_sk,\n    f1_score_xgb_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Logistic Regression\",\n    accuracy_logistic,\n    precision_logistic_sk,\n    recall_logistic_sk,\n    f1_score_logistic_sk\n))\n\ncomparison_df = pd.DataFrame(model_metrics_list).set_index('Model')\nmodels = comparison_df.index.tolist()\nall_categories = ['Accuracy']\nfor label in class_labels_ordered:\n    all_categories.extend([\n        f'Precision ({label})',\n        f'Recall ({label})',\n        f'F1-score ({label})'\n    ])\nangles = np.linspace(0, 2*np.pi, len(all_categories), endpoint=False).tolist()\nangles += angles[:1]\nax = plt.subplot(111, polar=True)\ncolors = cm.tab10(np.linspace(0, 1, len(models)))\nfor i, model in enumerate(models):\n    values = comparison_df.loc[model, all_categories].values.flatten().tolist()\n    values += values[:1]\n    ax.plot(angles, values, 'o-', linewidth=2, color=colors[i], label=model, alpha=0.8)\n    ax.fill(angles, values, color=colors[i], alpha=0.1)\nax.set_xticks(angles[:-1])\nax.set_xticklabels(all_categories, fontsize=10)\n\nax.set_ylim(0, 1)\nax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\nax.grid(True, linestyle='-', alpha=0.3)\nplt.title('모델 성능 비교', size=15, y=1.1)\nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\nplt.savefig('model_met.png', dpi=300, bbox_inches='tight')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/15.html#roc-커브",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/15.html#roc-커브",
    "title": "analysis",
    "section": "ROC 커브",
    "text": "ROC 커브\n\nrandom_pred_proba = np.zeros((len(y_test), len(class_labels_ordered)))\nfor i, cls in enumerate(class_labels_ordered):\n    random_pred_proba[:, i] = prop_values_ordered[i]\n\npred_probas = {\n    \"Random Classifier\": random_pred_proba,\n    \"Random Forest\": y_pred_proba_rf,\n    \"XGBoost\": y_pred_proba_xgb,\n    \"Logistic Regression\": log_reg_model_py.predict_proba(X_test)\n}\n\ncolors = {\n    \"Random Classifier\": \"grey\",\n    \"Random Forest\": \"forestgreen\",\n    \"XGBoost\": \"darkorange\",\n    \"Logistic Regression\": \"navy\"\n}\n\ny_test_numeric = y_test.cat.codes if hasattr(y_test, 'cat') else y_test\n\nfor model_name, proba in pred_probas.items():\n    if proba.shape[1] &gt; 1:\n        y_score = proba[:, 1]\n    else:\n        y_score = proba.ravel()\n    fpr, tpr, _ = roc_curve(y_test_numeric, y_score, sample_weight=weights_test)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})', color=colors[model_name])\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC 커브', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.gcf().set_size_inches(7, 7)\nplt.tight_layout()\nplt.savefig('roc_curve_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/14.html#data-load",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/14.html#data-load",
    "title": "preprocessing",
    "section": "Data Load",
    "text": "Data Load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager, rc\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nID_VAR = \"id\"\nWEIGHT_VAR = \"wt2\"\nOUTCOME_VAR = \"status_category\"\nRANDOM_STATE = 54321\ntarget_pred_var = [ID_VAR, \"wt1\", \"q33a01\", \"q33a02\", \"q33a03\", \"q33a04\", \"q33a05\", \"q33a06\",\n                    \"q48a07\", \"q48a08\", \"q48a09\", \"q48a10\",\n                    \"q49a01\", \"q49a02\", \"q49a03\", \"q49a04\",\n                    \"q33a07\", \"q33a08\", \"q33a09\",\n                    \"q49a15\", \"q49a16\", \"q49a17\",\n                    \"q49a09\", \"q49a10\", \"q49a11\",\n                    \"q48b1\", \"q48b2\", \"q48b3\",\n                    \"q12a01\", \"q12a02\", \"q12a03\",\n                    \"q48a04\", \"q48a05\", \"q48a06\",\n                    \"q49a05\", \"q49a06\", \"q49a08\"]\ncfa_model = \"\"\"\n  # 1. 부모애착\n  parent_attachment =~ q33a01 + q33a02 + q33a03 + q33a04 + q33a05 + q33a06\n  # 2. 일탈적 자아 낙인\n  deviant_esteem =~ q48a07 + q48a08 + q48a09 + q48a10\n  # 3. 부모에 의한 스트레스\n  parent_stress =~ q49a01 + q49a02 + q49a03 + q49a04\n  # 4. 부모감독\n  parent_monitoring =~ q33a07 + q33a08 + q33a09\n  # 5. 물질적 요인으로 인한 스트레스\n  desire_stress =~ q49a15 + q49a16 + q49a17\n  # 6. 친구로 인한 스트레스\n  friend_stress =~ q49a09 + q49a10 + q49a11\n  # 7. 자기신뢰감\n  self_confidence =~ q48b1 + q48b2 + q48b3\n  # 8. 상급학교 의존도\n  higher_school_dependence =~ q12a01 + q12a02 + q12a03\n  # 9. 부정적 자아존중감\n  neg_esteem =~ q48a04 + q48a05 + q48a06\n  # 10. 학업으로 인한 스트레스\n  academic_stress =~ q49a05 + q49a06 + q49a08\n\"\"\"\ndf1_origin = pd.read_csv('_data/student_1.csv')\ndf2_origin = pd.read_csv('_data/student_2.csv')\ndf3_origin = pd.read_csv('_data/student_3.csv')\ndf4_origin = pd.read_csv('_data/student_4.csv')\ndf5_origin = pd.read_csv('_data/student_5.csv')\ndf6_origin = pd.read_csv('_data/student_6.csv')\ndf_origin = [df1_origin, df2_origin, df3_origin, df4_origin, df5_origin]",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "title": "preprocessing",
    "section": "CFA (Confirmatory Factor Analysis)",
    "text": "CFA (Confirmatory Factor Analysis)\n\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.conversion import localconverter\nfrom rpy2.robjects.packages import importr\n\npandas2ri.activate()\nlavaan = importr('lavaan')\nbase_r = importr('base')\n\nmerged_df_pd = pd.DataFrame()\nfor i, df in enumerate(df_origin):\n    wave = i + 1\n    df_analysis = df[target_pred_var].copy()\n    df_clean = df_analysis.dropna()\n    with localconverter(ro.default_converter + pandas2ri.converter):\n        df_clean_r = ro.conversion.py2rpy(df_clean)\n    cfa_fit_r = lavaan.cfa(\n        model=cfa_model,\n        data=df_clean_r,\n        sampling_weights=\"wt1\",\n        estimator=\"MLR\",\n        warn=False,\n        verbose=False\n    )\n    print(f\"\\n--- Wave {wave} CFA 적합도 지수 ---\")\n    desired_fit_measures = ro.StrVector([\n        'cfi.scaled', 'tli.scaled',\n        'rmsea.scaled', 'rmsea.ci.lower.scaled', 'rmsea.ci.upper.scaled',\n        'srmr_bentler'\n    ])\n    fit_measures_values_r = lavaan.fitMeasures(cfa_fit_r, fit_measures=desired_fit_measures)\n    fit_values_py = [val if val is not ro.NA_Real else float('nan') for val in list(fit_measures_values_r)]\n    fit_measures_s = pd.Series(fit_values_py, index=list(desired_fit_measures))\n    print(fit_measures_s.to_string())\n    print(\"------------------------------------\")\n    factor_scores_r = lavaan.lavPredict(cfa_fit_r, type=\"lv\")\n    factor_scores_pd = pd.DataFrame(factor_scores_r, columns=[\"parent_attachment\", \"deviant_esteem\", \"parent_stress\", \"parent_monitoring\",\n                                                             \"desire_stress\", \"friend_stress\", \"self_confidence\",\n                                                             \"higher_school_dependence\", \"neg_esteem\", \"academic_stress\"])\n    ids_for_scores = df_clean[ID_VAR].reset_index(drop=True)\n    factor_scores_pd[ID_VAR] = ids_for_scores\n    new_colnames = {col: f\"{col}_w{wave}\" for col in factor_scores_pd.columns if col != ID_VAR}\n    factor_scores_pd = factor_scores_pd.rename(columns=new_colnames)\n    if wave == 1:\n        merged_df_pd = factor_scores_pd\n    else:\n        merged_df_pd = pd.merge(merged_df_pd, factor_scores_pd, on=ID_VAR, how='outer')\n\ndef classify_status(q11_value):\n    if q11_value in [1, 7, 8, 9, 71, 81, 91, 10, 101, 11, 111]:\n        return \"stable\"\n    elif q11_value in [2, 3, 4, 5, 6, 12, 13, 14]:\n        return \"explorative\"\n    else:\n        return None\n\npandas2ri.deactivate()\n\n\n--- Wave 1 CFA 적합도 지수 ---\ncfi.scaled               0.927641\ntli.scaled               0.916400\nrmsea.scaled             0.041374\nrmsea.ci.lower.scaled    0.040245\nrmsea.ci.upper.scaled    0.042511\nsrmr_bentler             0.042360\n------------------------------------\n\n--- Wave 2 CFA 적합도 지수 ---\ncfi.scaled               0.937524\ntli.scaled               0.927819\nrmsea.scaled             0.039718\nrmsea.ci.lower.scaled    0.038552\nrmsea.ci.upper.scaled    0.040894\nsrmr_bentler             0.037124\n------------------------------------\n\n--- Wave 3 CFA 적합도 지수 ---\ncfi.scaled               0.930504\ntli.scaled               0.919708\nrmsea.scaled             0.039424\nrmsea.ci.lower.scaled    0.038270\nrmsea.ci.upper.scaled    0.040586\nsrmr_bentler             0.043117\n------------------------------------\n\n--- Wave 4 CFA 적합도 지수 ---\ncfi.scaled               0.925239\ntli.scaled               0.913625\nrmsea.scaled             0.045518\nrmsea.ci.lower.scaled    0.044364\nrmsea.ci.upper.scaled    0.046681\nsrmr_bentler             0.046338\n------------------------------------\n\n--- Wave 5 CFA 적합도 지수 ---\ncfi.scaled               0.934303\ntli.scaled               0.924098\nrmsea.scaled             0.042005\nrmsea.ci.lower.scaled    0.040850\nrmsea.ci.upper.scaled    0.043169\nsrmr_bentler             0.045346\n------------------------------------\n\n\n\n상관행렬\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nplt.figure(figsize=(25, 22))\ncorr_matrix = merged_df_pd.drop(columns=[ID_VAR]).corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(\n    corr_matrix,\n    annot=False,\n    vmax=1.0, \n    vmin=-1.0,\n    center=0,\n    linewidths=.5,\n    cmap=cmap,\n    cbar_kws={\"shrink\": .5, \"label\": \"Correlation Coefficient\"}\n)\n\nplt.title('Correlation Matrix', fontsize=16, pad=20)\nplt.tight_layout()\nplt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ncorr_pairs = corr_matrix.unstack().reset_index()\ncorr_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n\ncorr_pairs = corr_pairs[corr_pairs['Variable 1'] != corr_pairs['Variable 2']]\ncorr_pairs['Pair'] = corr_pairs.apply(lambda x: tuple(sorted([x['Variable 1'], x['Variable 2']])), axis=1)\ncorr_pairs = corr_pairs.drop_duplicates('Pair')\n\ncorr_pairs = corr_pairs.sort_values(by='Correlation', key=abs, ascending=False)\ntop_corr = corr_pairs[['Variable 1', 'Variable 2', 'Correlation']]\n\nprint(\"Highest Absolute Correlations:\")\nprint(top_corr.head(40))\n\n\n\n\n\n\n\n\nHighest Absolute Correlations:\n                Variable 1            Variable 2  Correlation\n619       parent_stress_w2    academic_stress_w2     0.757157\n109       parent_stress_w1    academic_stress_w1     0.737240\n2043  parent_attachment_w5  parent_monitoring_w5     0.713755\n1533  parent_attachment_w4  parent_monitoring_w4     0.692874\n513   parent_attachment_w2  parent_monitoring_w2     0.689347\n2149      parent_stress_w5    academic_stress_w5     0.663941\n3     parent_attachment_w1  parent_monitoring_w1     0.658326\n1023  parent_attachment_w3  parent_monitoring_w3     0.655997\n1129      parent_stress_w3    academic_stress_w3     0.608676\n1540  parent_attachment_w4  parent_attachment_w5     0.604272\n1030  parent_attachment_w3  parent_attachment_w4     0.601522\n10    parent_attachment_w1  parent_attachment_w2     0.582573\n520   parent_attachment_w2  parent_attachment_w3     0.579379\n719       desire_stress_w2    academic_stress_w2     0.574989\n1639      parent_stress_w4    academic_stress_w4     0.566123\n614       parent_stress_w2      desire_stress_w2     0.559798\n409          neg_esteem_w1    academic_stress_w1     0.559204\n209       desire_stress_w1    academic_stress_w1     0.556277\n1040  parent_attachment_w3  parent_attachment_w5     0.539659\n58       deviant_esteem_w1         neg_esteem_w1     0.534314\n104       parent_stress_w1      desire_stress_w1     0.530970\n715       desire_stress_w2      friend_stress_w2     0.530456\n2144      parent_stress_w5      desire_stress_w5     0.519393\n1693  parent_monitoring_w4  parent_monitoring_w5     0.518281\n1132      parent_stress_w3      parent_stress_w4     0.513876\n20    parent_attachment_w1  parent_attachment_w3     0.512048\n1124      parent_stress_w3      desire_stress_w3     0.507338\n1489    academic_stress_w3    academic_stress_w4     0.507213\n163   parent_monitoring_w1  parent_monitoring_w2     0.507147\n1234      desire_stress_w3      desire_stress_w4     0.505383\n530   parent_attachment_w2  parent_attachment_w4     0.504675\n2     parent_attachment_w1      parent_stress_w1    -0.501466\n673   parent_monitoring_w2  parent_monitoring_w3     0.497952\n540   parent_attachment_w2  parent_attachment_w5     0.492200\n1183  parent_monitoring_w3  parent_monitoring_w4     0.489749\n1642      parent_stress_w4      parent_stress_w5     0.489084\n1744      desire_stress_w4      desire_stress_w5     0.486436\n112       parent_stress_w1      parent_stress_w2     0.481296\n1588     deviant_esteem_w4         neg_esteem_w4     0.480133\n1846    self_confidence_w4    self_confidence_w5     0.478809\n\n\n\nextra_df_pd = df6_origin[[ID_VAR, \"q11\", WEIGHT_VAR, \"sex\", \"yy\", \"area\"]].copy()\nextra_df_pd['status_category'] = extra_df_pd['q11'].apply(classify_status)\nmerged_df = pd.merge(merged_df_pd, extra_df_pd, on=ID_VAR, how='left').dropna()",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "인구통계학적 분포 분석",
    "text": "인구통계학적 분포 분석\n\n분포 테이블\n\nsex_dist_py = merged_df['sex'].value_counts().rename_axis('sex').reset_index(name='n')\nsex_dist_py['percentage'] = (sex_dist_py['n'] / sex_dist_py['n'].sum()) * 100\n\nbirth_year_dist_py = merged_df['yy'].value_counts().rename_axis('yy').reset_index(name='n')\nbirth_year_dist_py['percentage'] = (birth_year_dist_py['n'] / birth_year_dist_py['n'].sum()) * 100\nbirth_year_dist_py = birth_year_dist_py.sort_values(by='yy').reset_index(drop=True)\n\narea_dist_py = merged_df['area'].value_counts().rename_axis('area').reset_index(name='n')\narea_dist_py['percentage'] = (area_dist_py['n'] / area_dist_py['n'].sum()) * 100\narea_dist_py = area_dist_py.sort_values(by='area').reset_index(drop=True) # 지역 순으로 정렬\n\nstatus_dist_py = merged_df['status_category'].value_counts(dropna=False).rename_axis('status_category').reset_index(name='n')\nstatus_dist_py['percentage'] = (status_dist_py['n'] / status_dist_py['n'].sum()) * 100\n\nprint(sex_dist_py)\nprint(birth_year_dist_py)\nprint(status_dist_py)\n\n   sex     n  percentage\n0  2.0  1280   52.523595\n1  1.0  1157   47.476405\n     yy     n  percentage\n0  88.0     2    0.082068\n1  89.0  1884   77.308166\n2  90.0   551   22.609766\n  status_category     n  percentage\n0          stable  1369   56.175626\n1     explorative  1068   43.824374\n\n\n\n\n분포 시각화\n\narea_mapping_py = {\n    100: \"서울\", 110: \"서울\", 120: \"서울\", 121: \"서울\", 122: \"서울\",\n    130: \"서울\", 131: \"서울\", 132: \"서울\", 133: \"서울\", 134: \"서울\",\n    135: \"서울\", 136: \"서울\", 137: \"서울\", 138: \"서울\", 139: \"서울\",\n    140: \"서울\", 142: \"서울\", 143: \"서울\", 150: \"서울\", 151: \"서울\",\n    152: \"서울\", 153: \"서울\", 156: \"서울\", 157: \"서울\", 158: \"서울\",\n    200: \"강원\", 209: \"강원\", 210: \"강원\", 215: \"강원\", 217: \"강원\",\n    219: \"강원\", 220: \"강원\", 225: \"강원\", 230: \"강원\", 232: \"강원\",\n    233: \"강원\", 235: \"강원\", 240: \"강원\", 245: \"강원\", 250: \"강원\",\n    252: \"강원\", 255: \"강원\", 269: \"강원\",\n    300: \"대전\", 301: \"대전\", 302: \"대전\", 305: \"대전\", 306: \"대전\",\n    312: \"충남\", 314: \"충남\", 320: \"충남\", 321: \"충남\", 323: \"충남\",\n    325: \"충남\", 330: \"충남\", 336: \"충남\", 339: \"충남\", 340: \"충남\",\n    343: \"충남\", 345: \"충남\", 350: \"충남\", 355: \"충남\", 356: \"충남\",\n    357: \"충남\",\n    360: \"충북\", 361: \"충북\", 363: \"충북\", 365: \"충북\", 367: \"충북\",\n    368: \"충북\", 369: \"충북\", 370: \"충북\", 373: \"충북\", 376: \"충북\",\n    380: \"충북\", 390: \"충북\", 395: \"충북\",\n    400: \"인천\", 401: \"인천\", 402: \"인천\", 403: \"인천\", 404: \"인천\",\n    405: \"인천\", 406: \"인천\", 407: \"인천\", 409: \"인천\", 417: \"인천\",\n    411: \"경기\", 412: \"경기\", 413: \"경기\", 415: \"경기\",\n    420: \"경기\", 421: \"경기\", 422: \"경기\", 423: \"경기\", 425: \"경기\",\n    426: \"경기\", 427: \"경기\", 429: \"경기\", 430: \"경기\", 431: \"경기\",\n    435: \"경기\", 437: \"경기\", 440: \"경기\", 441: \"경기\", 442: \"경기\",\n    443: \"경기\", 445: \"경기\", 447: \"경기\", 449: \"경기\", 456: \"경기\",\n    459: \"경기\", 461: \"경기\", 462: \"경기\", 463: \"경기\", 464: \"경기\",\n    465: \"경기\", 467: \"경기\", 469: \"경기\", 471: \"경기\", 472: \"경기\",\n    476: \"경기\", 477: \"경기\", 480: \"경기\", 481: \"경기\", 482: \"경기\",\n    483: \"경기\", 487: \"경기\",\n    500: \"광주\", 501: \"광주\", 502: \"광주\", 503: \"광주\", 506: \"광주\",\n    513: \"전남\", 515: \"전남\", 516: \"전남\", 517: \"전남\", 519: \"전남\",\n    520: \"전남\", 525: \"전남\", 526: \"전남\", 527: \"전남\", 529: \"전남\",\n    530: \"전남\", 534: \"전남\", 535: \"전남\", 536: \"전남\", 537: \"전남\",\n    539: \"전남\", 540: \"전남\", 542: \"전남\", 545: \"전남\", 546: \"전남\",\n    548: \"전남\", 550: \"전남\",\n    560: \"전북\", 561: \"전북\", 565: \"전북\", 566: \"전북\", 567: \"전북\",\n    568: \"전북\", 570: \"전북\", 573: \"전북\", 576: \"전북\", 579: \"전북\",\n    580: \"전북\", 585: \"전북\", 590: \"전북\", 595: \"전북\", 597: \"전북\",\n    600: \"부산\", 601: \"부산\", 602: \"부산\", 604: \"부산\", 606: \"부산\",\n    607: \"부산\", 608: \"부산\", 609: \"부산\", 611: \"부산\", 612: \"부산\",\n    613: \"부산\", 614: \"부산\", 616: \"부산\", 617: \"부산\", 618: \"부산\",\n    619: \"부산\",\n    621: \"경남\", 626: \"경남\", 627: \"경남\", 631: \"경남\", 635: \"경남\",\n    636: \"경남\", 637: \"경남\", 638: \"경남\", 641: \"경남\", 645: \"경남\",\n    650: \"경남\", 656: \"경남\", 660: \"경남\", 664: \"경남\", 666: \"경남\",\n    667: \"경남\", 668: \"경남\", 670: \"경남\", 676: \"경남\", 678: \"경남\",\n    680: \"울산\", 681: \"울산\", 682: \"울산\", 683: \"울산\", 689: \"울산\",\n    690: \"제주\", 695: \"제주\", 697: \"제주\", 699: \"제주\",\n    700: \"대구\", 701: \"대구\", 702: \"대구\", 703: \"대구\", 704: \"대구\",\n    705: \"대구\", 706: \"대구\", 711: \"대구\",\n    712: \"경북\", 714: \"경북\", 716: \"경북\", 717: \"경북\", 718: \"경북\",\n    719: \"경북\", 730: \"경북\", 740: \"경북\", 742: \"경북\", 745: \"경북\",\n    750: \"경북\", 755: \"경북\", 757: \"경북\", 760: \"경북\", 763: \"경북\",\n    764: \"경북\", 766: \"경북\", 767: \"경북\", 769: \"경북\", 770: \"경북\",\n    780: \"경북\", 790: \"경북\", 791: \"경북\", 799: \"경북\",\n    999: \"국외\"\n}\n\nsns.barplot(x='sex', y='percentage', data=sex_dist_py, palette='pastel')\nplt.title('Gender Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(ticks=[0, 1], labels=['Male', 'Female'])\nplt.tight_layout()\nplt.savefig('sex_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='yy', y='percentage', data=birth_year_dist_py, palette='viridis')\nplt.title('Birth Year Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('birth_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nmerged_df['area_name'] = merged_df['area'].map(area_mapping_py)\narea_name_dist_py = merged_df['area_name'].value_counts(dropna=False).rename_axis('area_name').reset_index(name='n')\narea_name_dist_py['percentage'] = (area_name_dist_py['n'] / area_name_dist_py['n'].sum()) * 100\narea_name_dist_py = area_name_dist_py.sort_values(by='percentage', ascending=False).reset_index(drop=True)\nsns.barplot(x='area_name', y='percentage', data=area_name_dist_py, palette='colorblind')\nplt.title('Regional Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('area_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='status_category', y='percentage', data=status_dist_py, palette='Set2')\nplt.title('Dependent Variable Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.tight_layout()\nplt.savefig('status_dis.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "title": "preprocessing",
    "section": "데이터 전처리 및 train test split",
    "text": "데이터 전처리 및 train test split\n\nimport re\nfrom sklearn.model_selection import train_test_split\n\npred_vars = [col for col in merged_df.columns if re.search(r\"_w[1-5]$\", col)]\nmerged_df[OUTCOME_VAR] = merged_df[OUTCOME_VAR].astype('category')\n\nX = merged_df[pred_vars]\ny = merged_df[OUTCOME_VAR]\ncomposite_stratify_key = y.astype(str) + '_' + \\\n                         merged_df['area_name'].astype(str) + '_' + \\\n                         merged_df['sex'].astype(str)\nweights = merged_df[WEIGHT_VAR]\nX_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, test_size=0.3, random_state=RANDOM_STATE, stratify=composite_stratify_key)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "title": "preprocessing",
    "section": "데이터 저장",
    "text": "데이터 저장\n\ntrain_data_to_save = X_train.copy()\ntrain_data_to_save.insert(0, 'y', y_train)\ntrain_data_to_save.insert(1, 'weights', weights_train)\ntrain_data_to_save.to_csv('_data/train_data.csv', index=False)\ntest_data_to_save = X_test.copy()\ntest_data_to_save.insert(0, 'y', y_test)\ntest_data_to_save.insert(1, 'weights', weights_test)\ntest_data_to_save.to_csv('_data/test_data.csv', index=False)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html",
    "title": "classification with trees",
    "section": "",
    "text": "gini 지수가 낮을수록, misclassification error가 낮을수록, entropy가 낮을수록 Information gain이 높을수록, gain ratio가 클수록 순도가 높고 좋다. 최댓값은 0.5. entropy는 최댓값 1.\n과적합 pruning 할 때 misclassification error를 기준으로 한다.\n의사결정 트리는 데이터 마이닝에서 가장 널리 사용되는 분류 기법 중 하나로, 데이터의 패턴을 트리 구조로 표현하여 예측 모델을 구축한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "title": "classification with trees",
    "section": "What is Data minig",
    "text": "What is Data minig\n\n데이터 마이닝은 대량의 데이터에서 암시적이고 이전에 알려지지 않았던 잠재적으로 유용한 지식이나 패턴을 추출하는 과정입니다.\n\n\n종류\n\n지도 학습1: 주어진 학습 데이터를 이용하여 목표 속성의 값을 예측하는 모델을 생성하는 과정으로, 입력(속성)과 출력(정답)이 모두 주어진 데이터를 바탕으로 학습\n비지도 학습",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html#분류",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html#분류",
    "title": "classification with trees",
    "section": "분류",
    "text": "분류\n\n목표\n\n새로운 데이터에 대해서도 정확한 예측이 가능한 일반화된 모델을 만드는 것\n이를 위해 과거 데이터를 학습용과 테스트용으로 나누어 모델의 성능을 검증",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "title": "classification with trees",
    "section": "의사결정 트리의 구조와 원리",
    "text": "의사결정 트리의 구조와 원리\n\n의사결정 트리는 노드와 가지로 구성된 계층적 구조로, 각 노드는 특성(attribute)을 나타내며 가지는 테스트 결과를 표현.\n잎 노드는 클래스 레이블이나 클래스 분포를 나타냄\n의사결정 트리 구축은 주로 탐욕적 전략(Greedy strategy)을 사용하며, 각 단계에서 가장 좋은 분할 기준을 선택함.\n대표적인 의사결정 트리 알고리즘으로는 CART, ID3, C4.5, SLIQ, SPRINT 등이 있다.\n\n\n노드 불순도 측정 방법\n의사결정 트리에서 최적의 분할을 결정하기 위해 다양한 불순도 측정 방법이 사용됩니다:\n\nGini Index: 노드의 불순도를 측정하는 방법으로, 1-∑[p(j|t)]²로 계산됩니다. 값이 0에 가까울수록 순수한 노드를 의미합니다.\nEntropy(엔트로피): 노드의 동질성을 측정하는 방법으로, -∑p(j|t)log₂p(j|t)로 계산됩니다. 0일 때 완전히 동질적인 노드를 의미합니다.\nInformation Gain(정보 이득): 분할 전후의 엔트로피 차이로, 분할로 인해 얻어지는 불확실성 감소량을 의미합니다. 높은 정보 이득은 해당 속성이 데이터를 잘 나누는 것을 의미합니다.\nGain Ratio(이득 비율): 정보 이득을 분할의 내재 정보량(Split Information)으로 나눈 값으로, 분기가 많은 속성에 대한 편향을 줄이기 위해 고안되었습니다.\n\n\n\n트리 분할 기준\n트리 분할 시 고려해야 할 주요 이슈는 다음과 같습니다: - 데이터 분할 방법 선택 - 속성의 테스트 조건 명시 - 최고의 분할 정의 - 트리 분기 종료 시점 결정\n최적의 분할은 불순도를 최소화하는 방향으로 이루어지며, CART는 Gini 기반 분할을, ID3와 C4.5는 Information Gain 기반 분할을 주로 사용합니다.\n\n\n모델 평가 기준\n의사결정 트리 모델의 평가는 다음과 같은 기준으로 이루어집니다: - 테스트 세트에서의 정확도(%) - 오류율 - 혼동 행렬(Confusion Matrix) - 속도와 확장성 - 노이즈와 결측값 처리 능력",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html#결론",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html#결론",
    "title": "classification with trees",
    "section": "결론",
    "text": "결론\n의사결정 트리는 직관적이고 이해하기 쉬운 분류 모델을 제공하지만, 과적합(overfitting)이나 데이터 단편화와 같은 문제가 발생할 수 있습니다. 이를 해결하기 위해 C4.5와 같은 알고리즘은 Gain Ratio를 도입하여 분기가 많은 속성에 대한 편향을 줄이는 방법을 제시했습니다.\n의사결정 트리의 성공적인 구축을 위해서는 적절한 불순도 측정 방법 선택, 가지치기(pruning), 그리고 다양한 속성 선택 기준의 이해가 필요합니다. 이러한 방법들을 통해 보다 정확하고 일반화된 모델을 구축할 수 있습니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/13.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/13.html#footnotes",
    "title": "classification with trees",
    "section": "각주",
    "text": "각주\n\n\n규칙 기반 시스템 != 연관 규칙 학습↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/01.html#scaling",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/01.html#scaling",
    "title": "데이터 전처리",
    "section": "scaling",
    "text": "scaling\n\nmin-max scaling: \\(x' = \\frac{x - min(x)}{max(x) - min(x)}\\)\n\n0과 1 사이의 값으로 변환\n\nstandardization: \\(x' = \\frac{x - \\mu}{\\sigma}\\), \\(\\mu\\): 평균, \\(\\sigma\\): 표준편차",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "title": "데이터 전처리",
    "section": "Train Test Resampling",
    "text": "Train Test Resampling\n\nresampling\n\nresampling: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\ntrain set: 모델을 학습하는 데이터\nvalidation data set: 하이퍼파라미터를 튜닝\ntest set: 모델의 성능 평가\n층화추출법: stratified sampling\n\n\n\nmethods\n\nhold out method: data를 train set과 test set으로 나누는 방법\n\n어떤 데이터가 train, test set에 포함되는지에 따라 결과가 달라질 수 있다.\n→ train set과 test set을 여러 번 나누어 모델을 학습하고 평가하는 방법이 필요하다.\n\ncross validation: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\n\nk-fold cross validation: 데이터를 k개의 fold로 나누고, 각 fold를 test set으로 사용하여 모델을 학습하고 평가한다.\n\nrandom하게 sampling할 수 있고, 안할 수도 있음\n\nleave-one-out cross validation: 데이터의 개수가 n개일 때, n-1개의 데이터를 train set으로 사용하고 1개의 데이터를 test set으로 사용하는 방법\nerror는 각 train set, fold 에서 계산된 error의 평균으로 구한다.\n\nbootstrap: 데이터를 중복을 허용하여 샘플링하는 방법\n\n원본 데이터에서 n개의 데이터를 랜덤하게 선택하여 train set을 만들고, 나머지 데이터를 test set으로 사용한다.\n여러 번 반복하여 모델을 학습하고 평가한다.\n실제 오류 추정치의 편향과 분산 모두에 대한 정확한 측정값을 얻을 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/04.html",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/04.html",
    "title": "Support vector machine",
    "section": "",
    "text": "bias variance tradeoff\n\nmargin ↑, bias ↑, variance ↓\nmargin ↓, bias ↓, variance ↑\n\nsuppot vector: 임계값에 가까운 데이터 포인트\nlinear classification fomulation:\n\n\n\n\nif not linear solvable → kernel functions\nsoft margin: 이상치를 포함할 수 있는 마진\n\ncross validation으로 최적의 soft margin을 찾는다.\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Support vector machine"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/02.html#k-nn",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/02.html#k-nn",
    "title": "분류",
    "section": "K-NN",
    "text": "K-NN\n\n새로운 데이터 포인트에 대해 k개의 가장 가까운 이웃을 찾고, 그 이웃들의 클래스를 투표하여 다수결로 분류한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "title": "분류",
    "section": "의사결정 트리",
    "text": "의사결정 트리\n불순도가 가장 낮은(한쪽의 class가 더 많은) leaves를 root에 두고, 그 다음 불순도가 낮은 leaf를 그 아래에 두는 방식으로 트리를 구성한다. leaf 노드의 과반수가 같은 클래스를 가지면 그 클래스를 리턴한다. overfit을 방지하기 위해 pruning을 하거나 max depth를 설정한다.\n\ngood split: 불순도가 낮고, 분할된 각 leave의 비율이 비슷한 경우\n\n\n과정\n\n루트 노드에서 시작전체 데이터셋을 기준으로 시작하여 가장 좋은 분할속성(feature)을 선택\n분할 기준 평가각 속성에 대해 데이터를 분할했을 때의 분할 평가함수 적용\n\ngini index\n\ngini: \\(1 - \\sum_{i=1}^{c} p_i^2\\).\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nmisclassification error\n\nerror: \\(1 - \\max(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nentropy: 유용하지 못한 정보를 포함하고 있는 정도.\n\nentropy: \\(-\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 1\n\ninformation gain: 어떤 속성을 기준으로 분할했을 때, 얻을 수 있는 불확실성의 감소량\n\n부모의 엔트로피 - 자식의 엔트로피 가중평균\n목표: 최대화\n단점: 고유한 값을 많이 갖는 변수를 선호하는 경향이 있음\n알고리즘: ID3, C4.5\n\ngain ratio: information gain의 단점을 보완한 방법\n\ngain ratio: \\(\\frac{information\\ gain}{entropy}\\)\n분기의 갯수와 각 분기의 크기를 함께 고려\n목표: 최대화\n단점: entropy가 낮은 것을 선택하는 경향이 있음\n\n→ 평균이 넘는 entorpy만 선택해서 gain ratio를 계산\n\n알고리즘: C4.5\n\n\n최적의 분할 선택 평가된 기준 중 가장 순도가 높은 점수를 갖는 속성을 선택 (greedy)\n재귀적으로 하위 노드 분할. 분할된 하위 데이터에 대해 위의 과정을 반복\n종료 조건 만족 시 정지\n가지치기 수행\n\n\n\nAlgorithm\n\nCART\n\n분할 기준\n\n분류: gini index\n회귀: MSE\n\n모든 분할에서 이진 트리로 분할\n사후 가지치기\n\n비용 복잡도 가지치기: \\(Total SSR + α(leaf size)\\)이 제일 작은 트리 선택\nα는 cross validation으로 결정",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/02.html#평가",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/02.html#평가",
    "title": "분류",
    "section": "평가",
    "text": "평가\n\nconfusion matrix\n\nTP, TN, FP, FN\n\naccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nprecision(정밀도): true로 예측한 것 중 실제 true인 것의 비율\n\n\\(\\frac{TP}{TP + FP}\\)\n\nrecall(민감도, 재현율): 실제 true인 것 중 true로 예측한 것의 비율\n\n\\(\\frac{TP}{TP + FN}\\)\n\nF1 score: \\(2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\)\nmacro 평균: 그냥 각 클래스의 score를 평균\nweighted 평균: 가중 평균\nmicro 평균: 전체 TP, TN, FP, FN을 합쳐서 계산\nROC curve: y축: TPR(민감도), x축: FPR(1 - 특이도)\n\nAUC: ROC curve 아래 면적\n\n1에 가까울수록 좋은 모델\n0.5는 랜덤 추측과 같음\n\n\n(1,1): 무작위 추측과 동일. 모든 샘플을 무조건 양성으로 예측\n(0,0): 모든 샘플을 무조건 음성으로 예측\n(1,0): 완벽한 모델\n(0,1): 반대로 예측하는 모델",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/02.html#learning-curve",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/02.html#learning-curve",
    "title": "분류",
    "section": "learning curve",
    "text": "learning curve\n\n\nsample 수가 적으면 정확도와 신뢰 구간에 부정적 영향을 미침\ntest set에 대한 오류가 증가하는 구간(cross 되는 구간)은 과적합 구간. 여기서 stop",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/06.html#전제-조건",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/06.html#전제-조건",
    "title": "clustering",
    "section": "전제 조건",
    "text": "전제 조건\n\nscalability\n다양한 타입의 속성을 처리해야 함\n\nk-means는 수치형만 처리 가능\n\n인위적인 형상의 군집도 발견할 수 있어야 함\n\nk-means는 non-convex 형태는 잘 못찾음\n\n파라미터 설정에 전문지식을 요하지 않아야함\nnoise와 outliers를 처리해야 함\n데이터가 입력되는 순서에 민감하면 안됨\n차원 수가 높아도 잘 처리할 수 있어야함\n사용자 정의 제약조건도 수용할 수 있어야함\n해석과 사용이 용이해야함",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/06.html#전처리",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/06.html#전처리",
    "title": "clustering",
    "section": "전처리",
    "text": "전처리\n\nscaling 필요\none hot encoding",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/06.html#model",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/06.html#model",
    "title": "clustering",
    "section": "model",
    "text": "model\n\nDistance-based methods\n\nPartitioning methods\n\nk-means1:\n\npolinominal 시간 안에 해결 가능\nnoise, outlier에 민감함\n수치형만 처리 가능\nnon-convex 형태는 잘 못찾음\n\nk-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\nk-prototype: 범주형, 수치형 섞인거 처리 가능\nk-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n\nPAM: Partitioning Around Medoids\n\nscalability 문제 있음\n\nCLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n\n샘플링 과정에서 biased될 수 있음\n\nCLARANS: medoid 후보를 랜덤하게 선택함\n\nk-means++: 초기 centroids를 더 잘 잡음\n\nHierarchical methods\n\ntop-down: divisive, dia\nbottom-up: agglomerative\n\nward’s distance: 군집 간의 거리 계산을 군집 내의 분산을 최소화하는 방식으로 계산\n\nESS: 각 군집의 중심으로 부터의 거리 제곱합\n\n\n\n\nDensity-based methods\n\n다양한 모양의 군집을 찾을 수 있음\nnoise, outlier에 강함\nDBSCAN: 잡음 포인트는 군집에서 제외\n\ncore point를 찾음(eps 이내에 minPts 이상 있는 점)\ncore point를 중심으로 군집을 확장\n\ncore point가 아닌 경우 확장 종료\n\n\n\n고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n군집간 계층관계를 인식하기 어렵다\n\nOPTICS: DBSCAN의 단점을 보완\n\n군집의 밀도가 다를 때도 잘 처리함\n군집의 계층 구조를 인식할 수 있음\neps, minPts 파라미터가 필요함\n\n\nGrid-based methods: 대표만(각 grid를 대표) 가지고 군집분석 하는거\n\n속도와 메모리 측면에서 효율적\n\nModel-based clustering methods\n\n\n거리기반 군집의 단점:\n\n군집의 모양이 구형이 아닐 경우 찾기 어려움\n군집의 갯수 결정하기 어려움\n군집의 밀도가 높아야함",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/06.html#평가",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/06.html#평가",
    "title": "clustering",
    "section": "평가",
    "text": "평가\n\nsilhuette score: \\(\\frac{\\sum_{i=1}^{n} s(i)}{n}\\)\n\ns(i): \\(\\frac{b(i) - a(i)}{max((a(i), b(i)))}\\)\n\na(i): 군집 내 노드간의 평균 거리\nb(i): 가장 가까운 군집과의 노드 간 평균 거리\n\n1에 가까울 수록 좋음",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/data_mining/06.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/data_mining/06.html#footnotes",
    "title": "clustering",
    "section": "각주",
    "text": "각주\n\n\n장단점 기말고사 언급하심↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/11.html",
    "href": "posts/04_archives/bs_3_1/notes/OR/11.html",
    "title": "수송문제와 할당 문제들",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "href": "posts/04_archives/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "title": "시험 범위",
    "section": "6 - 쌍대이론과 민감도 분석",
    "text": "6 - 쌍대이론과 민감도 분석\n\n쌍대이론의 본질\n원-쌍대 관계들\n다른 원형태들에 적용\n민감도 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "href": "posts/04_archives/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "title": "시험 범위",
    "section": "7 - 선형계획을 위한 다른 알고리즘들",
    "text": "7 - 선형계획을 위한 다른 알고리즘들\n\n쌍대 심플렉스 방법\n상한 기법",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "href": "posts/04_archives/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "title": "시험 범위",
    "section": "8 - 수송문제와 할당 문제들",
    "text": "8 - 수송문제와 할당 문제들\n\n수송문제의 기초\n수송문제를 위한 능률적인 심플렉스 방법\n할당 문제\n할당문제를 위한 특별한 알고리즘",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "href": "posts/04_archives/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "title": "시험 범위",
    "section": "9 - 네트워크 최적화 모형",
    "text": "9 - 네트워크 최적화 모형\n\n네트워크 용어들\n최단 경로 문제\n최대 흐름 문제\n네트워크 심플렉스 해법",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "href": "posts/04_archives/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "쌍대이론의 본질",
    "text": "쌍대이론의 본질\n\n모든 선형계획 문제는 쌍대문제를 가진다:\n\n원문제(Primal): 예를 들어 이익 최대화.\n쌍대문제(Dual): 자원비용 최소화.\n\n\n\n\n\n원 문제와 쌍대 문제의 관계\n\n\n\n원-쌍대 관계의 성질\n\n원문제의 최적해가 존재하면 쌍대문제의 최적해도 존재하며, 두 목적함수값은 같다.\n원문제의 해로부터 쌍대해를 읽을 수 있고, 그 역도 성립한다.\n쌍대해는 자원의 경제적 가치(잠재가격, shadow price)를 의미한다",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "href": "posts/04_archives/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "원-쌍대 관계와 상보기저해",
    "text": "원-쌍대 관계와 상보기저해\n\n상보해(Complementary Solutions)\n\n원문제의 기저해와 쌍대문제의 기저해는 서로 직접적으로 대응한다.\n최적해에서는 원문제와 쌍대문제의 목적함수값이 같다.\n\n\n\n상보여유성\n\n원문제의 기저변수가 0이 아니면, 대응 쌍대변수는 0이고, 그 반대도 성립한다.\n이 속성은 심플렉스 방법의 반복과정에서 두 문제의 해가 어떻게 연동되는지 설명한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "href": "posts/04_archives/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "다른 원문제 형태의 쌍대문제",
    "text": "다른 원문제 형태의 쌍대문제\n\n비표준형(등식제약식, 변수의 음수 허용 등)에서도 쌍대문제는 항상 존재\n\n등식제약식은 쌍대에서 해당 쌍대변수의 부호제약을 제거(음수 허용)한다.\n변수의 음수 허용은 쌍대에서 등식제약식으로 나타난다.\n\n\n\nSOB(Sensible-Odd-Bizarre) 법칙\n\n원문제의 제약식 및 변수의 형태(≤, =, ≥, 비음, 무제약 등)에 따라 쌍대문제의 대응 형태를 쉽게 결정하는 규칙.\n대칭성: 쌍대문제의 쌍대는 원문제이므로, 두 문제의 관계는 완전히 대칭적이다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/05.html#민감도-분석",
    "href": "posts/04_archives/bs_3_1/notes/OR/05.html#민감도-분석",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "민감도 분석",
    "text": "민감도 분석",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "href": "posts/04_archives/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "6.7 민감도 분석 적용 – 요약 및 주요 내용",
    "text": "6.7 민감도 분석 적용 – 요약 및 주요 내용\n6.7절 “민감도 분석 적용”은 선형계획(Linear Programming) 문제에서 민감도 분석(Sensitivity Analysis)을 실제로 어떻게 적용하는지, 그리고 다양한 매개변수 변화가 최적해에 어떤 영향을 미치는지 구체적으로 설명하는 부분입니다.\n\n주요 내용 요약\n\n민감도 분석의 출발점\n민감도 분석은 보통 자원(b₁, b₂, …, bₘ)의 공급량 변화가 해에 미치는 영향을 분석하는 것으로 시작합니다. 이는 실제 모델에서 자원의 양을 조정할 수 있는 융통성이 크기 때문입니다.\n우변(b) 변화의 영향\n자원(b)의 값이 변하면, 최종 심플렉스 표의 우변만 바뀌고 나머지(행 0의 비기저변수 계수 등)는 변하지 않을 수 있습니다. 이때는 우변만 수정해서 해가 여전히 가능(feasible)한지(기저변수 값이 모두 음이 아닌지) 확인하면 됩니다. 만약 불가능해지면 쌍대심플렉스법 등으로 재최적화가 필요합니다.\n증분 분석\n자원의 값이 변화할 때, 변화분만큼의 영향(증분)을 계산해서 새로운 해와 목적함수 값을 빠르게 구할 수 있습니다.\n허용범위(Allowable Range)\n각 자원(b)의 변화가 해의 가능성과 최적성을 유지할 수 있는 범위를 계산합니다. 이 범위 내에서는 잠재가격(dual price, shadow price)이 유효하게 적용됩니다.\n동시 변화와 100% 규칙\n여러 자원의 값이 동시에 변할 때, 각 변화가 허용범위 내에서 차지하는 비율의 합이 100%를 넘지 않으면 잠재가격을 이용한 해석이 유효합니다.\n목적함수 계수 변화\n비기저변수나 기저변수의 목적함수 계수(c)가 변할 때 해가 어떻게 변하는지, 허용범위를 어떻게 계산하는지 설명합니다.\n새로운 제약식 추가\n모델에 새로운 제약식이 추가되면, 기존 최적해가 여전히 가능해인지 확인하고, 아니라면 심플렉스 표에 새로운 행을 추가해 재최적화를 진행합니다.\n파라메트릭 분석\n하나 또는 여러 매개변수를 연속적으로 변화시키면서 최적해가 어떻게 달라지는지 체계적으로 분석합니다.\n\n\n\n예시: Wyndor Glass Co. 모델\n\nb₂(자원 2의 공급량)가 12에서 24로 증가하면, 기저해가 더 이상 가능하지 않게 되고, 쌍대심플렉스법을 통해 새로운 최적해를 구해야 함을 보여줍니다.\n허용범위 내에서만 자원의 변화에 대해 잠재가격이 유효하며, 이를 벗어나면 해가 바뀌고 잠재가격도 달라집니다.\n여러 자원이 동시에 변할 때 100% 규칙을 적용해, 변화의 합이 100%를 넘지 않으면 기존 해석이 유효함을 설명합니다.\n\n\n\n실무적 의의\n\n실제 기업(예: Pacific Lumber Company)의 대규모 산림관리 최적화 문제에 민감도 분석이 어떻게 적용되어, 불확실성 하에서 더 나은 의사결정과 수익 증대에 기여했는지 사례로 제시합니다.\n\n\n요약:\n6.7절은 선형계획의 해가 자원, 목적함수 계수, 제약식 등 모델의 매개변수 변화에 얼마나 민감한지, 그리고 이런 변화가 있을 때 해를 신속하게 갱신하거나 재최적화하는 절차를 구체적으로 다룹니다. 이를 통해 실제 의사결정에서 불확실성을 관리하고, 최적화 모델의 실용성을 높일 수 있음을 보여줍니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/11.html#총괄생산계획",
    "href": "posts/04_archives/bs_3_1/notes/product/11.html#총괄생산계획",
    "title": "총괄생산계획",
    "section": "총괄생산계획",
    "text": "총괄생산계획\n\n\n\n중기 범위(6-18개월) 기간에 대한 유사한 제품 묶음 수준에서 수요-공급 균형을 맞추기 위한 러프한 생산 계획\n계획 수립 후 주기적으로 업데이트\n\n\n의사결정\n\n목적: 비용, 인력변동의 최소화, 이윤의 최대화, 바람직한 고객 서비스 수준 유지\n\n보통 trade-off 관계\n\n결정 사항: 고용 수준, 시간당 생산량, 재고수준, 외주 생산량 등\n\n\n\n\n전략\n\n수요대안\n\n가격책정\n판촉\n백오더(납기지연)\n신규수요 창출\n\n공급대안\n\n수요추종전략\n\n생산량을 수요에 일치되도록 조정하는 전략. 재고가 안쌓임\n재고 유지비용이 높고, 생산용량 변경에 따른 비용이 적을 때 효과적\n\n생산평준화전략\n\n생산량을 평균으로 일정하게 유지하는 전략\n안정적 산출량\n재고비용 증가, 납기 지연이나 품절에 따른 고객 서비스 저하\n\n혼합전략",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "href": "posts/04_archives/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "title": "총괄생산계획",
    "section": "최적화 기법을 통한 총괄생산계획 수립",
    "text": "최적화 기법을 통한 총괄생산계획 수립",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/00.html",
    "href": "posts/04_archives/bs_3_1/notes/product/00.html",
    "title": "Intro",
    "section": "",
    "text": "과목 목표: 전통적인 생산 시스템 관리 방법론을 학습\n\n생산: 유, 무형의 제품을 만드는 것\n\n제조업, 서비스업 등\n\n시스템: 투입물을 산출물로 만드는 구성 요소와 프로세스의 총체적 집합\n관리: 목표를 달성하기 위한 체계적인 의사결정\n\n목표: 비용, 품질, 납품, 유연성\n\n네 가지 모두 고려해야하고, 이 사이에는 trade-off가 존재.\n\n\n\n중간 시험범위: lec2 ~ lec9\n\n질문:\n\n\n재고 유지의 다섯 가지 이유 - 수송중 vs 안전\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Intro"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "href": "posts/04_archives/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "title": "기준생산계획 및 자재소요계획",
    "section": "기준생산계획(Master Production Schedule, MPS)",
    "text": "기준생산계획(Master Production Schedule, MPS)\n\n총괄생산계획을 분해한 것\n보통 개별 제품에 대한 계획\n기간은 총괄생산계획에 상대적으로 정함.(분기 - 월, 월 - 주, 주 - 일 등)\n\n부품 주문부터 제품의 최종 조립이 완료될 떼까지의 총 기간은 포함해야함\n\n이어지는 부품의 제조활동이나 자재 조달 활동의 기준이 되는 계획\nRCCP를 통해 초안 생산계획을 수정\n가까운 시기의 계획은 동결하는게 바람직하다. (동결 시작 시기는 agility에 따라 다름)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "href": "posts/04_archives/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "title": "기준생산계획 및 자재소요계획",
    "section": "자재소요계획(Material Requirements Planning, MRP)",
    "text": "자재소요계획(Material Requirements Planning, MRP)\n\n부품에 대한 수요가 발생하는 양상이 완제품인 MPS랑 다름. 종속수요(다른 품목의 수요에 따라 수요가 발생)\n기준생산계획을 지키기 위한 자재로서의 품목 종류, 수량, 시점을 결정\n부품의 공급이 100% 확실하지 않음 (lead time 지연, 품질 등)\n투입물\n\n자재명세서(BOM): 한 단위의 품목 생산에 필요한 자재종류, 품목과 자재의 상하관계, 자재 사용량을 기록한 명세서\n기준생산계획\n재고 기록: 보유량, 주문량, 공급자, 리드타임, 로트사이즈 결정 방침 등 포함\n\n\n\n고려사항\n\n공통부품: LLC(Low Level Coding) 기준으로 MRP를 작성\n로트크기 규칙결정\n\nFixed Order Quantity\nPeriodic Order Quantity: 주문시 향후 T period만큼 필요한 양을 주문\nLot-for-Lot: 주문시 수요량만큼 주문\n\n안전재고: 순소요량에 안전재고 만큼 더함.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "title": "Matching Supply with Demand",
    "section": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지",
    "text": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지\n\n운영하는 관점에서 수요와 공급을 바라볼 예정\n기업을 바라보는 관점: 유/무형의 제품을 생산해서 수요(양, timing, 품질, …)에 맞게 공급하기 위해 노력하는 집단",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급 법칙",
    "text": "수요와 공급 법칙\n\n\n\n파란색 - 수요, 빨간색 - 공급\n\n\n\n경제학 기본적인 법칙. 가격 조정은 건강한 시스템의 증거라고 봄\n운영 관리자(OM)인 우리는 이거랑 다르게 바라봄.\n\nExcess demand = lost revenue\nExcess supply = wasted resources\n가격 조정만으로 수요와 공급 맞추기 어려움\n\n끊임없이 변하는 수요와 비 탄력적인 공급으로 인해 수요와 공급을 맞추기 어렵다.\n과학적 도구로 최대한 수요를 예측하고, 탄력적인 공급을 하는 방법을 찾아야 한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급이 안 맞는 사례",
    "text": "수요와 공급이 안 맞는 사례\n\n푸바오를 보기 위해 사람들이 몰림\n\n수요 공급의 불균형은 안 좋은 효과를 가져옴\n\n마스크, 먹태깡: 수요는 빠르게 변하는데, 공급은 느리게 변함\n\n수요를 예측하고, 설비를 미리 준비하는 과학적 도구가 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소매업\n철광석 공장\n응급실\n심박조율기\n항공 여행\n\n\n\n\n공급\n소비자 전자제품\n철광석\n의료 서비스\n의료 장비\n특정 항공편 좌석\n\n\n수요\n새로운 비디오 시스템을 구매하는 소비자\n제철소\n긴급한 의료 서비스 수요\n심박조율기가 특정 시간과 장소에서 필요한 심장외과 의사\n특정 시간과 목적지로의 여행\n\n\n공급이 수요를 초과\n재고 비용이 높고, 재고 회전율이 낮음\n가격 하락\n의사, 간호사 및 인프라가 충분히 활용되지 않음\n심박조율기가 재고로 남아 있음\n빈 좌석 발생\n\n\n수요가 공급을 초과\n포기한 이익 기회; 소비자 불만족\n가격 상승\n응급실 혼잡 및 지연; 구급차 우회 가능성\n포기된 이익 (일반적으로 의료적 위험과는 관련 없음)\n초과 예약으로 인해 고객이 다른 항공편을 이용해야 함 (이익 손실)\n\n\n공급과 수요를 맞추기 위한 조치\n수요 예측; 신속한 대응\n가격이 지나치게 하락하면 생산 시설이 폐쇄됨\n예측된 수요에 맞춘 인력 배치; 우선순위 설정\n여러 장소에서 심박조율기를 보관하는 유통 시스템\n동적 가격 책정; 예약 정책\n\n\n관리적 중요성\n소비자 전자제품 소매업의 단위당 재고 비용이 종종 순이익을 초과함\n가격 경쟁이 치열하여 주요 초점은 공급 비용 절감에 맞춰짐\n치료 또는 이송 지연이 사망과 연관된 사례 있음\n대부분의 제품(가치 2만 달러)이 사용되기 전에 영업 사원의 차량 트렁크에서 4~5개월 동안 대기함1\n전체 좌석의 약 30%가 빈 채로 운항되며, 좌석 이용률이 1~2%만 증가해도 이익과 손실이 갈림2",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템의 performance",
    "text": "생산 시스템의 performance\n\n서로 상충됨. business 목표에 맞게 balance를 잘 맞춰야함\n\ncost\nquality: 품질이 얼마나 좋고 일관되냐\nvariety: 다양한 사용자의 니즈를 얼마나 잘 맞추냐\ntime",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템 관리를 배우면 할 수 있는 것",
    "text": "생산 시스템 관리를 배우면 할 수 있는 것\n\n비효율성 분석\n상충관계에 대한 의사결정\n신기술 등에 대한 평가",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/01.html#footnotes",
    "href": "posts/04_archives/bs_3_1/notes/product/01.html#footnotes",
    "title": "Matching Supply with Demand",
    "section": "각주",
    "text": "각주\n\n\n뭔소리지↩︎\n뭔소리지↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "href": "posts/04_archives/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "제품 설계 및 개발",
    "text": "제품 설계 및 개발\n\n설계의 중요성\n\n총 제품 비용 중 설계가 치지하는 비중은 적지만, 설계의 영향을 받는 비중이 높다.\n\n\n\n제품 설계 프로세스\n\n\n\n제품 설계 프로세스\n\n\n\n아이디어 선정: 소비자의 니즈, 경쟁사 제품 등 벤치마킹(reverse engineering)\n제품 선정: 시장 분석, 경제성 분석, 기술 분석\n\n\n\n제조 고려 설계\n\n제조 과정 단순화 및 비용 절감을 고려해서 설계해야 한다.\n\n부품 개수 최소화\n모듈화 표준화\n조립, 재활용, 분해 고려\n\nsubtract manufacturing보단 additive manufacturing(적층제조, DFAM)을 고려\n\n\n\n표준화 모듈화\n\n표준화: 부품 호환성 및 운영 효율성\n모듈화: 표준화된 부품의 집합 &lt;-&gt; integral\n\n\n\n지연전략\n\n차별화 지연 전략: 수요를 알기 전까지 많은 종류 생산은 지연",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "href": "posts/04_archives/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "생산 프로세스의 유형",
    "text": "생산 프로세스의 유형\n\n주문충족 방식에 따른 분류\n\nmake to stock: 수요가 발생하기 전에 생산. 수요가 예측이 쉬울 경우 적합 (push)\nassemble / configure / build to order: 제품 구성요소를 재고로 보유. 고객의 요구에 따라 조립하여 생산. 지연 전략에 맞닿아 있다 (pull)\nmake to order: 주문에 따른 생산. 설계가 완료된 걸 다른 옵션으로 제공. 옵션이 많거나 고가 제품에 적합 (push-pull)\nengineer to order: 고객의 요구에 따라 설계 및 생산. 일회성 프로젝트에 적합 (pull)\n\n위로 갈 수록 제공 시간은 짧아지고, 아래로 갈 수록 유연성이 높아진다.\n\n\n생산 흐름에 따른 분류\n\n프로젝트 프로세스: 일회성 생산. 흐름이라고 할 수는 없다.\n개별작업 프로세스(job shop): 공정별 배치. 높은 유연성, 낮은 규모\n배치 프로세스: job shop과 라인 프로세스의 중간 형태. batch 수가 맞춰지기 전까지 대기 / 유휴시간 존재.\n라인 프로세스: 제품별 배치. 낮은 유연성, 대규모\n연속 흐름 프로세스: 멈춤, 수정, 변경 최소화\n\n\n\n다양한 유형의 프로세스와 설비배치를 혼합 적용하는 것이 일반적\n\n\n\n다품종소량생산, 개인맞춤생산 시대 도래\n\n\nreconfigurable manufacturing system: 다양한 제품을 생산할 수 있는 유연한 생산 시스템\n\n\n\ncell manufacturing\n\nline process랑 job shop이 혼합된거\n비슷한 작업이 필요한 부품들을 하나의 그룹으로 묶어서 전용 셀에서 생산",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/05.html#조립공정의-분석",
    "href": "posts/04_archives/bs_3_1/notes/product/05.html#조립공정의-분석",
    "title": "인건비 추정과 감축",
    "section": "조립공정의 분석",
    "text": "조립공정의 분석\n\n처리 능력: \\(\\frac{자원의 수}{처리시간}\\)\nbottleneck은 처리능력이 제일 낮은 자원\nX개를 생산하는데 걸리는 시간\n\n가동중인 생산 시스템: \\(\\frac{X}{R}\\)\n비어있는 생산 시스템: 비어있는 시스템을 흘러 가는데 걸리는 시간 + \\(\\frac{X - 1}{R}\\)\n\n\n\n비어있는 시스템을 흘러가는데 걸리는 시간\n\nWorker-paced process: 모든 작업의 처리시간의 합\nMachine-paced process(컨베이어 벨트): 프로세스상의 단계 수 * 병목공정의 처리시간",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "href": "posts/04_archives/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "title": "인건비 추정과 감축",
    "section": "노동량과 유휴시간",
    "text": "노동량과 유휴시간\n\n이상적인 노동비: 작업자의 처리시간의 합 * 시간당 평균 임금\n\n유휴시간을 고려하지 않았을 때\n\n직접 노동 인건비= \\(\\frac{단위시간당 총 임금}{단위시간당 흐름률}\\)\n\n실제 노동시간 + 유휴시간(idle time)\n흐름률이 높아지면 유휴시간이 줄어들면서 직접 노동 인건비가 줄어든다.\n\n\n\n유휴시간 종류\n\nbottleneck에 맞추기 위한 유휴시간\n수요에 맞추기 위해 발생하는 유휴시간\n\n이런 경우 작업시간을 줄이는 방법을 생각할 수 있다.\n하지만 flexible하게 맞추기는 어려울 것이다.\n\n\n\n\nCycle time(주기 시간)\n\n프로세스에서 산출되는 연속된 두 제품 간의 시간간격\n프로세스가 얼마나 빨리 생산하는지를 나타내는 지표\nflow rate의 역수\n1인 작업자의 유휴시간 = cycle time - 1인 작업자의 작업시간\n\n\n\n평균 노동 활용률\n\n제품 생산에만 들어가는 노동의 양과 실제 인건비 지불의 기준이 되는 노동의 양(노동량 + 유휴시간)을 비교\n\\(\\frac{노동량}{노동량 + 모든 작업자의 유휴시간 총합}\\)\n\\(\\frac{1}{노동자 수}(활용률의 합)\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/05.html#line-balancing",
    "href": "posts/04_archives/bs_3_1/notes/product/05.html#line-balancing",
    "title": "인건비 추정과 감축",
    "section": "Line Balancing",
    "text": "Line Balancing\n\nbottleneck에 맞추기 위한 유휴시간으로 이상적인 노동비를 산출할 수 없음. → line balancing을 맞춰줌\n프로세스 내부적인 수요(요구되는 노동량)와 공급(작업자의 처리 능력)을 맞추는 것\n\n\n대량생산으로의 확장\n\n라인의 병렬적 배치\n프로세스 단계별 작업자 추가\n과업의 분화 및 전문화\n\n전문화될수록 라인 밸런싱이 어려워지고 평균 노동 활용률이 낮아짐(작업이 평탄하지 않음)\n→ 전문화 정도를 감소시켜 라인 밸런싱을 쉬워지게 한다.\n\n작업 셀: 한 명이 모든 과업을 수행함. 한 명의 작업시간이 노동량에 해당하고, 노동 활용률은 100%",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "href": "posts/04_archives/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "프로세스 처리 능력 및 활용률",
    "text": "프로세스 처리 능력 및 활용률\n\nprocess capacity: 흐름률의 upper bound(유량).\n병목(bottleneck): 제일 낮은 처리능력의 자원\n전체 프로세스의 처리 능력 = 병목의 처리능력 (단 작업이 일렬로 수행될 때)\nproduct mix:\n\n다양한 제품이 input으로 들어와 처리능력이 달라짐\nbottleneck을 계산하기는 어려움\n\n비율이 매번 달라질 수도 있어서\n작업이 일렬로만 수행되지 않아서\n\n\n실제 생산한 양(흐름률)은 capacity에서만 결정되지 않는다.\n\n수요(market + 계절 / 안전 재고 같은 내부적 수요)\n원자재 투입량\n\n흐름률 = min(시간 당 수요, 프로세스 처리 능력)\n공급능력: 투입량, 처리 능력\n\n\n수요 / 공급 제약적 상황\n\n\n수요(가) 제약적: 수요 &lt; 공급\n\nbottleneck 활용률 &lt; 100%\nflow rate == Demand rate\n\n공급(이) 제약적: 수요 &gt; 공급\n\n투입 제약적\n처리능력 제약적\n\nbottleneck 활용률 == 100%\nflow rate = capacity\n\n\n\n\n\n활용률\n\n\n실제 생산하는 양을 capacity로 나눈 것\n\n\\(\\frac{흐름률}{처리능력}\\)\n활용률을 100% 달성하려면 쉬지 않고 프로세스가 돌아가야하지만 현실적으로 쉽지 않다.\n\n수요가 공급보다 적을 수 있다.\n투입물이 충분하지 않다.\n몇몇 공정의 사용이 공장이나 수리로 제한될 수 있다.\n불확실성\n\n\n병목을 제외한 다른 하위 작업은 활용률이 떨어질 수 있다.\n\n모든 프로세스가 병목인 것이 가장 이상적인 상황 (과도하게 높은 공급능력)\n\n공급 제약적 상황에서 수요가 얼마나 많은지 알 수 없음.\n→ implied utilization\n\n\n\nimplied utilization\n\n\\(U = \\frac{R}{Capacity}\\)\n\\(IU = \\frac{Demand or workload}{Capacity} (≤100% or &gt; 100%)\\)\nif min(demmand, capacity, input) = demand then U = IU\n이 외에도 잠재적 수요 못따라가는 작업도 알 수 있음\n\nIU 100 넘는거 개선 필요\n\n또, 작업이 sequential하게 진행되지 않을 때 병목현상을 확인할 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "href": "posts/04_archives/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "여러 종류의 흐름 단위",
    "text": "여러 종류의 흐름 단위\ninput 당 뭐가 다르면 다른 단위로 치환",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/others/2.html",
    "href": "posts/04_archives/bs_3_1/notes/others/2.html",
    "title": "성적 장학금",
    "section": "",
    "text": "오~예~ (남은 등록금 300만원을 대출 받으며)\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "성적 장학금"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html",
    "href": "posts/04_archives/bs_3_1/index.html",
    "title": "학부 3학년 1학기",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-06-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html#details",
    "href": "posts/04_archives/bs_3_1/index.html#details",
    "title": "학부 3학년 1학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 1학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html#tasks",
    "href": "posts/04_archives/bs_3_1/index.html#tasks",
    "title": "학부 3학년 1학기",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    푸른등대 기부장학금 - 두나무UDC 신청 (~2025-01-20 18:00)\n                \n                불합격\n            \n            \n            \n                \n                \n                    2025 DB 드림리더 장학생 신청 (~2025-01-10)\n                \n                잘할 자신이 없다\n            \n            \n            \n                \n                \n                    경기도 학자금대출 이자 지원 신청 (~2025.02.14 18:00)\n                \n                신청 완료\n            \n            \n            \n                \n                    \n                    학과 근로 신청\n                \n                신청 완료\n            \n\n            \n            \n                \n                    \n                    KMOOC 학점 인정 신청 (2025-03-10~)\n                \n                done\n            \n\n            \n            \n                \n                \n                    봉사활동 계획서 작성 (2025-03-11~)\n                \n                작성 완료\n            \n            \n            \n                \n                \n                    OR 과제 1 (~2025-03-16 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 2 (~2025-03-23 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                    \n                    교수님과 커피\n                \n                나 커피 못 마시는데\n            \n\n            \n            \n                \n                \n                    데이터마이닝 팀과제 script\n                \n                일단 완성\n            \n            \n            \n                \n                \n                    OR 과제 3 (~2025-04-06 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 4 (~2025-04-13 23:59)\n                \n                그만...\n            \n            \n            \n                \n                \n                    data mining 1차 과제 ppt 완성\n                \n                done\n            \n            \n            \n                \n                    \n                    진로 지도 상담 받기\n                \n                \n            \n\n            \n            \n                \n                \n                    컴퓨팅적 사고 발표 ppt 만들기\n                \n                \n            \n            \n            \n                \n                    \n                    데이터마이닝 2차 과제 준비\n                \n                노션에 정리 중\n            \n\n            \n            \n                \n                    \n                    교통비 지원금 신청\n                \n                \n            \n\n            \n            \n                \n                \n                    OR 과제 6 (~2025-05-18 23:59)\n                \n                힘들다",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html#필요한-자료",
    "href": "posts/04_archives/bs_3_1/index.html#필요한-자료",
    "title": "학부 3학년 1학기",
    "section": "필요한 자료",
    "text": "필요한 자료\n\n자기소개서 작성 1\n교육 이수 증빙자료\nPortfolio: 큰일 났다. 진짜 못만들었다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html#참고-자료",
    "href": "posts/04_archives/bs_3_1/index.html#참고-자료",
    "title": "학부 3학년 1학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/index.html#related-posts",
    "href": "posts/04_archives/bs_3_1/index.html#related-posts",
    "title": "학부 3학년 1학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/index.html",
    "href": "posts/02_areas/hadoop/index.html",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/index.html#details",
    "href": "posts/02_areas/hadoop/index.html#details",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/index.html#tasks",
    "href": "posts/02_areas/hadoop/index.html#tasks",
    "title": "Hadoop",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/index.html#참고-자료",
    "href": "posts/02_areas/hadoop/index.html#참고-자료",
    "title": "Hadoop",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/index.html#related-posts",
    "href": "posts/02_areas/hadoop/index.html#related-posts",
    "title": "Hadoop",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/02_areas/hadoop/notes/00.html",
    "href": "posts/02_areas/hadoop/notes/00.html",
    "title": "Hadoop Ecosystem",
    "section": "",
    "text": "HDFS: Hadoop Distributed File System\n\n클러스터의 하드 드라이브를 하나의 거대한 파일 시스템으로 통합\n자동 복제 및 장애 조치 기능 제공\n\nYARN: Yet Another Resource Negotiator\n\n클러스터의 리소스를 관리하고 작업을 스케줄링\n\nMapReduce: 데이터 처리 모델\n\n데이터를 분할하고 병렬로 처리하는 프레임워크\nMap 단계에서 데이터를 필터링하고 정렬, Reduce 단계에서 집계 및 요약\n\nPig: 데이터 흐름 언어\n\nSQL과 유사한 스크립트 언어로 MapReduce / TEZ를 위한 데이터 처리 작업을 작성\n\nHive: SQL과 유사한 쿼리 언어\n\n대규모 데이터 세트에 대한 쿼리 및 분석을 위한 SQL 인터페이스 제공\n\nAmbari: 데이터 시각화 도구\n\nHadoop 클러스터에서 데이터를 시각화하고 대시보드를 생성\n\nHBase: NoSQL 데이터베이스\n\n대규모 데이터 세트를 실시간으로 읽고 쓸 수 있는 분산형 데이터베이스\n\nstorm: 실시간 데이터 처리\n\n스트림 데이터를 실시간으로 처리하고 분석하는 프레임워크\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Hadoop",
      "Notes",
      "Hadoop Ecosystem"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/00.html#시계열-소개",
    "href": "posts/01_projects/adp_실기/notes/time_series/00.html#시계열-소개",
    "title": "Overview",
    "section": "시계열 소개",
    "text": "시계열 소개\n\n시계열은 단순히 시간에 따라 정렬된 데이터 요소들의 집합이다.\n분해: 시계열을 서로 다른 여러 구성요소로 분리하는 통계적 작업\n\n\n구성 요소\n\n추세(level)\n\n순환?\n\n계절성: 추세에서 벗어나는 변화의 정도\n잔차(white noise)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "Overview"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/00.html#시계열-vs-회귀",
    "href": "posts/01_projects/adp_실기/notes/time_series/00.html#시계열-vs-회귀",
    "title": "Overview",
    "section": "시계열 vs 회귀",
    "text": "시계열 vs 회귀\n\n시계열은 순서가 존재\n다른 특징 없이 시계열 정보만 있는 경우도 존재\n\n이동평균 모델\n자기회귀 모델",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "Overview"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/01.html#베이스라인-모델-정의",
    "href": "posts/01_projects/adp_실기/notes/time_series/01.html#베이스라인-모델-정의",
    "title": "단순 미래 예측",
    "section": "베이스라인 모델 정의",
    "text": "베이스라인 모델 정의\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여\n\n\nall 평균 (추세)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf = pd.read_csv('_data/jj.csv')\n\ntrain = df[:-4]\ntest = df[-4:]\n\nhistorical_mean = np.mean(train['data'])\nhistorical_mean\n\n4.308499987499999\n\n\n\ntest['pred_mean'] = historical_mean\n\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred) / y_true) * 100\n\nmape_hist_mean = mape(test['data'], test['pred_mean'])\nmape_hist_mean\n\n70.00752579965119\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_mean', label='단순 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x7d6386b6cc50&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6386b6c3e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63871faa50&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384124b90&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63841262a0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384125610&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384126b40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384127fb0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63841245f0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384124f80&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384127170&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n최근만 평균 (추세)\n\nlast_year_mean = np.mean(train.iloc[-4:]['data'])\ntest['pred_last_yr_mean'] = last_year_mean\nmape_last_year_mean = mape(test['data'], test['pred_last_yr_mean'])\nmape_last_year_mean\n\n15.5963680725103\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_last_yr_mean', label='최근 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x7d6384149a60&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6386b6ee10&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6384147d40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6386ba21e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec6930&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec7b00&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec4620&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec45f0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec7770&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec7380&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d63d5ec59d0&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n단순 예측법\n\nlast = train.iloc[-1]['data']\ntest['pred_last'] = last\nmape_last = mape(test['data'], test['pred_last'])\nmape_last\n\n30.457277908606535\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_last', label='단순 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x7d6385e82030&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d638413a810&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385e8f140&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6386ba1ca0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ed0500&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ed2ff0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ed0ec0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ed1c10&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ceca40&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ed3710&gt;,\n  &lt;matplotlib.axis.XTick at 0x7d6385ced160&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n계절적 예측\n\ntest['pred_last_season'] = train.iloc[-4:]['data'].values\nmape_naive_seasonal = mape(test['data'], test['pred_last_season'])\nmape_naive_seasonal\n\n11.561658552433654",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "단순 미래 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/bayse/19.html",
    "href": "posts/01_projects/adp_실기/notes/bayse/19.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "19"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html",
    "title": "확률보행 따라가보기",
    "section": "",
    "text": "확률보행: 무작위로 상승 또는 하락이 발생할 확률이 동일한 프로세스\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nsteps = np.random.standard_normal(1000)\nsteps[0] = 0\nrandom_walk = np.cumsum(steps)\nsns.lineplot(x=np.arange(len(random_walk)), y=random_walk)\nplt.xlabel('시간')\nplt.ylabel('값')\n\nText(0, 0.5, '값')\n\n\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행 따라가보기"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html#what-is-확률보행",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html#what-is-확률보행",
    "title": "확률보행",
    "section": "what is 확률보행",
    "text": "what is 확률보행\n\n확률보행: 무작위로 상승 또는 하락이 발생할 확률이 동일한 프로세스\n\n\\(y_t = C + y_{t-1} + ϵ_t\\)\nC가 0이 아닌 경우 표류가 있는 확률보행\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nsteps = np.random.standard_normal(1000)\nsteps[0] = 0\nrandom_walk = np.cumsum(steps)\nsns.lineplot(x=np.arange(len(random_walk)), y=random_walk)\nplt.xlabel('시간')\nplt.ylabel('값')\n\nText(0, 0.5, '값')",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html#확률보행-식별",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html#확률보행-식별",
    "title": "확률보행",
    "section": "확률보행 식별",
    "text": "확률보행 식별\n\n확률보행은 정상적이고 자기상관관계가 없는 시계열로 나타난다.\n\n\n정상성\n\n시간이 지나도 통계적 특성이 변하지 않는 시계열\n\n평균과 분산이 상수이고 자기상관관계가 있으며, 이러한 특성들이 시간에 따라 변하지 않는다.\n\n정상화:\n\n평균: 차분\n분산: 로그 변환, Box-Cox 변환 등\n\n정상성 검정:\n\nADF (Augmented Dickey-Fuller) 테스트\n\n\\(H_0\\): 시계열에 단위근1이 존재하여 비정상적이다.\n\\(H_1\\): 시계열에 단위근이 존재하지 않아 정상적이다.\n\n\n\n\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\n\ndef simulate_process(alpha: float) -&gt; np.array:\n    process = np.empty(401)\n    process[0] = 0\n    for i in range(400):\n        process[i+1] = alpha * process[i] + np.random.standard_normal()\n    return process\n\nstationary = simulate_process(alpha=0.5)\nnon_stationary = simulate_process(alpha=1)\n\nsns.lineplot(x=np.arange(len(stationary)), y=stationary, label='정상성 프로세스')\nsns.lineplot(x=np.arange(len(non_stationary)), y=non_stationary, label='비정상성 프로세스')\n\n\n\n\n\n\n\n\n\ndef mean_var_over_time(process: np.array) -&gt; np.array:\n    means = []\n    vars = []\n    for i in range(len(process)):\n        means.append(np.mean(process[:i]))\n        vars.append(np.var(process[:i]))\n    return means, vars\n\nmeans_stationary, vars_stationary = mean_var_over_time(stationary)\nmeans_non_stationary, vars_non_stationary = mean_var_over_time(non_stationary)\n\nsns.lineplot(x=np.arange(len(means_stationary)), y=means_stationary, label='정상성 평균')\nsns.lineplot(x=np.arange(len(means_non_stationary)), y=means_non_stationary, label='비정상성 평균')\n\n\n\n\n\n\n\n\n\nsns.lineplot(x=np.arange(len(vars_stationary)), y=vars_stationary, label='정상성 분산')\nsns.lineplot(x=np.arange(len(vars_non_stationary)), y=vars_non_stationary, label='비정상성 분산')\n\n\n\n\n\n\n\n\n\nresult1 = adfuller(stationary)\nresult2 = adfuller(non_stationary)\nprint(f'ADF Statistic: 정상성: {result1[0]}, 비정상성: {result2[0]}')\nprint(f'p-value: 정상성: {result1[1]}, 비정상성: {result2[1]}')\n\nADF Statistic: 정상성: -10.182654946345801, 비정상성: -2.2791893442991604\np-value: 정상성: 6.632269182398948e-18, 비정상성: 0.17876768439598817\n\n\n\n\n자기상관관계\n\n자기 상관관계: 시계열의 선행값과 후행값 아이의 선형관계\n\nx: 지연 (\\(y_t, y_{t-2}\\)의 경우 지연 2)\ny: 계수\n\n추세가 있는 경우: 짧은 지연에서 계수가 높고, 지연이 커질수록 계수가 낮아지는 경향\n계절성이 있는 경우: 주기적인 패턴이 나타남.\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(non_stationary, lags=20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n비정상적 시계열에서 추세가 보인다. 차분을 진행해보자.\n\n\ndiff = np.diff(non_stationary, n=1)\n\nresult = adfuller(diff)\nresult[0], result[1]\n\n(-19.871934314399926, 0.0)\n\n\n\nplot_acf(diff, lags=20)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html#footnotes",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html#footnotes",
    "title": "확률보행",
    "section": "각주",
    "text": "각주\n\n\n\\(y_t = C + αy_{t-1} + ϵ_t\\) 형태의 시계열로, α가 1보다 작은 경우, 과거의 값이 현재 값에 미치는 영향이 작아져 시계열이 정상적이다.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html#예시",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html#예시",
    "title": "확률보행",
    "section": "예시",
    "text": "예시\n\nimport pandas as pd\n\ndf = pd.read_csv('_data/googl.csv')\nsns.lineplot(data=df, x='Date', y='Close')\n\nplt.xlabel('날짜')\nplt.ylabel('종가')\n\nplt.xticks([4, 24, 46, 68, 89, 110, 132, 152, 174, 193, 212, 235],\n           ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr'],\n           rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nresult = adfuller(df['Close'])\n\nresult[0], result[1]\n\n(0.16025048664771407, 0.9699419435913058)\n\n\n\ndiff = np.diff(df['Close'], n=1)\n\nresult_diff = adfuller(diff)\nresult_diff[0], result_diff[1]\n\n(-5.303439704295227, 5.386530961454778e-06)\n\n\n\nplot_acf(diff, lags=20)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/02.html#확류보행-예측",
    "href": "posts/01_projects/adp_실기/notes/time_series/02.html#확류보행-예측",
    "title": "확률보행",
    "section": "확류보행 예측",
    "text": "확류보행 예측\n\ndf = pd.DataFrame({'value': random_walk})\ntrain = df.iloc[:800]\ntest = df.iloc[800:]\n\nmean = np.mean(train['value'])\ntest['pred_mean'] = mean\n\nlast_value = train.iloc[-1]['value']\ntest['pred_last'] = last_value\n\n\n표류 기법\n\ndrift = (train.iloc[-1]['value'] - train.iloc[0]['value']) / (len(train) - 1)\ndrift\n\n-0.0073114182124709975\n\n\n\nx_vals = np.arange(801, 1001)\npred_drift = drift * x_vals\ntest['pred_drift'] = pred_drift\nsns.lineplot(data=df, x=df.index, y='value')\nsns.lineplot(data=test, x=test.index, y='pred_mean', label='평균 예측')\nsns.lineplot(data=test, x=test.index, y='pred_last', label='마지막 값 예측')\nsns.lineplot(data=test, x=test.index, y='pred_drift', label='표류 예측')\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\nmse_mean = mean_squared_error(test['value'], test['pred_mean'])\nmse_last = mean_squared_error(test['value'], test['pred_last'])\nmse_drift = mean_squared_error(test['value'], test['pred_drift'])\n\nsns.barplot(x=['평균', '마지막 값', '표류'], y=[mse_mean, mse_last, mse_drift])\n\n\n\n\n\n\n\n\n\n\n단순 예측법\n\ndf_shift = df.shift(periods=1)\nmse_one_step = mean_squared_error(test['value'], df_shift['value'].iloc[800:])\nmse_one_step\n\n1.0373151143278658",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/03.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/03.html",
    "title": "이동평균과정 모델링",
    "section": "",
    "text": "이동편균 모델: 현잿 값이 현재와 과거 오차에 선형적으로 비례한다.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/widget.csv')\nsns.lineplot(data=df, x=df.index, y='widget_sales')\nplt.xticks(\n    [0, 30, 57, 87, 116, 145, 175, 204, 234, 264, 293, 323, 352, 382, 409, 439, 468, 498], \n    ['Jan 2019', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan 2020', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(df['widget_sales'])\n\nADF_result[0], ADF_result[1]\n\n(-1.5121662069359054, 0.5274845352272601)\n\n\n\n정상 시계열이 아님. 차분 진행\n\n\ndiff_df = np.diff(df['widget_sales'], n=1)\n\nADF_result = adfuller(diff_df)\nADF_result[0], ADF_result[1]\n\n(-10.576657780341959, 7.076922818587193e-19)\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(diff_df, lags=30)\nplt.show()\n\n\n\n\n\n\n\n\n\n지연 2 이후 유의하지 않음.\nMA(2) 진행\n\n\nfrom sklearn.model_selection import train_test_split\n\ndiff_df = pd.DataFrame({'widget_sales_diff': diff_df})\ntrain, test = train_test_split(diff_df, test_size=0.1)\n\n\nMA(q)는 q 크기까지만 예측 가능.\n회귀적으로 예측을 진행해야함\n\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    if method == 'mean':\n        pred_mean = []\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            pred_mean.extend(mean for _ in range(window))\n        return pred_mean\n    if method == 'last':\n        pred_last_value = []\n        for i in range(train_len, total_len, window):\n            last_value = df.iloc[i-1].values[0]\n            pred_last_value.extend(last_value for _ in range(window))\n        return pred_last_value\n    if method == 'MA':\n        pred_MA = []\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(0,0,2))\n            res = model.fit(disp=False)\n            predictions = res.get_prediction(0, i + window - 1)\n            oos_pred = predictions.predicted_mean.iloc[-window:]\n            pred_MA.extend(oos_pred)\n        return pred_MA\n\n\npred_df = test.copy()\nTRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\npred_mean = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'mean')\npred_last = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'last')\npred_MA = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'MA')\n\npred_df['pred_mean'] = pred_mean\npred_df['pred_last'] = pred_last\npred_df['pred_MA'] = pred_MA\n\n\ndf['pred_widget_sales'] = pd.Series()\ndf['pred_widget_sales'].iloc[450:] = df['widget_sales'].iloc[450] + pred_df['pred_MA'].cumsum()\n\n\nsns.lineplot(data=df, x=df.index, y='widget_sales', label='실제 값')\nsns.lineplot(data=df, x=df.index, y='pred_widget_sales', label='MA(2)')\nplt.xticks(\n    [409, 439, 468, 498], \n    ['Mar', 'Apr', 'May', 'Jun'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "이동평균과정 모델링"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/04.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/04.html",
    "title": "자기귀모형",
    "section": "",
    "text": "자기회귀과정: 예측값이 이전 값에만 선형적으로 의존한다고 가정\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/foot.csv')\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(df['foot_traffic'])\nADF_result[0], ADF_result[1]\n\n(-1.1758885999240625, 0.6838808917896241)\n\n\n\n비 정상성 발견. 차분 진행\n\n\nfoot_diff = np.diff(df['foot_traffic'], n=1)\n\nADF_result = adfuller(foot_diff)\nADF_result[0], ADF_result[1]\n\n(-5.268231347422049, 6.369317654781143e-06)\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(foot_diff, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\nma2 = np.array([1, 0, 0])\nar2 = np.array([1, -0.33, -0.50])\nAR2_process = ArmaProcess(ar2, ma2).generate_sample(nsample=1000)\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplot_pacf(AR2_process, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nplot_pacf(foot_diff, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "자기귀모형"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/05.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/05.html",
    "title": "복잡한 시계열 모델",
    "section": "",
    "text": "from statsmodels.tsa.arima_process import ArmaProcess\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nar1 = np.array([1, -0.33])\nma1 = np.array([1, 0.9])\n\nARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000)\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(ARMA_1_1)\n\nADF_result[0], ADF_result[1]\n\n(-7.656346674014989, 1.7349289952389427e-11)\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(ARMA_1_1, lags=20)\nplt.show()\nplot_pacf(ARMA_1_1, lags=20)\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/05.html#일반적-모델링-절차",
    "href": "posts/01_projects/adp_실기/notes/time_series/05.html#일반적-모델링-절차",
    "title": "복잡한 시계열 모델",
    "section": "일반적 모델링 절차",
    "text": "일반적 모델링 절차\n\n\\(AIC = 2k - 2ln(\\hat{L})\\)\nk = p + q\nL = max(likelihood)\n\n\nfrom itertools import product\n\nps = range(0, 4, 1)\nqs = range(0, 4, 1)\n\norder_list = list(product(ps, qs))\n\n\nfrom typing import Union\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef optimize_ARMA(endog: Union[pd.Series, list], order_list: list) -&gt; pd.DataFrame:\n    results = []\n    for order in order_list:\n        try:\n            model = SARIMAX(endog, order=(order[0], 0, order[1]), simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, aic])\n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p, q)', 'AIC']\n    result_df = result_df.sort_values(by=\"AIC\").reset_index(drop=True)\n\n    return result_df\n\nresult_df = optimize_ARMA(ARMA_1_1, order_list)\nresult_df\n\n/home/cryscham123/.local/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\n\n\n\n\n\n\n(p, q)\nAIC\n\n\n\n\n0\n(1, 2)\n2880.745326\n\n\n1\n(2, 2)\n2881.184850\n\n\n2\n(2, 1)\n2881.215973\n\n\n3\n(0, 3)\n2881.552804\n\n\n4\n(0, 2)\n2881.934005\n\n\n5\n(1, 1)\n2882.021239\n\n\n6\n(1, 3)\n2882.411069\n\n\n7\n(3, 1)\n2882.880564\n\n\n8\n(2, 3)\n2883.178911\n\n\n9\n(3, 3)\n2884.999545\n\n\n10\n(3, 2)\n2885.215581\n\n\n11\n(0, 1)\n2998.204843\n\n\n12\n(3, 0)\n3059.234951\n\n\n13\n(2, 0)\n3135.429791\n\n\n14\n(1, 0)\n3326.498642\n\n\n15\n(0, 0)\n3894.252408\n\n\n\n\n\n\n\n\n잔차 분석\n\nmodel = SARIMAX(ARMA_1_1, order=(1, 0, 1), simple_differencing=False)\nmodel_fit = model.fit(disp=False)\n\nmodel_fit.plot_diagnostics(figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\ntr = acorr_ljungbox(model_fit.resid, np.arange(1, 11))\nprint(tr)\n\n     lb_stat  lb_pvalue\n1   0.289153   0.590764\n2   2.289623   0.318284\n3   2.291207   0.514207\n4   2.963817   0.563899\n5   3.571527   0.612593\n6   3.938499   0.684999\n7   5.248137   0.629711\n8   7.642936   0.469103\n9   8.139031   0.520198\n10  8.329570   0.596679",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/05.html#예시---대역폭-사용량-예측",
    "href": "posts/01_projects/adp_실기/notes/time_series/05.html#예시---대역폭-사용량-예측",
    "title": "복잡한 시계열 모델",
    "section": "예시 - 대역폭 사용량 예측",
    "text": "예시 - 대역폭 사용량 예측\n\ndf = pd.read_csv('_data/bandwidth.csv')\nsns.lineplot(data=df, x=df.index, y='hourly_bandwidth')\n\nplt.xlabel('시간')\nplt.ylabel('시간당 대역폭 사용량(MBps)')\nplt.xticks(\n    np.arange(0, 10000, 730), \n    ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', '2020', 'Feb'])\nplt.show()\n\n\n\n\n\n\n\n\n\nADF_result = adfuller(df['hourly_bandwidth'])\n\nADF_result[0], ADF_result[1]\n\n(-0.8714653199452845, 0.7972240255014515)\n\n\n\nbandwidth_diff = np.diff(df['hourly_bandwidth'], n=1)\n\nADF_result = adfuller(bandwidth_diff)\n\nADF_result[0], ADF_result[1]\n\n(-20.694853863789028, 0.0)\n\n\n\ndf_diff = pd.DataFrame({'bandwidth_diff': bandwidth_diff})\ntrain = df_diff.iloc[:-168]\ntest = df_diff.iloc[-168:]\n\n\nps = range(0, 4, 1)\nqs = range(0, 4, 1)\norder_list = list(product(ps, qs))\nresult_df = optimize_ARMA(train['bandwidth_diff'], order_list)\nresult_df\n\n\n\n\n\n\n\n\n(p, q)\nAIC\n\n\n\n\n0\n(3, 2)\n27991.063879\n\n\n1\n(2, 3)\n27991.287509\n\n\n2\n(2, 2)\n27991.603598\n\n\n3\n(3, 3)\n27993.416924\n\n\n4\n(1, 3)\n28003.349550\n\n\n5\n(1, 2)\n28051.351401\n\n\n6\n(3, 1)\n28071.155496\n\n\n7\n(3, 0)\n28095.618186\n\n\n8\n(2, 1)\n28097.250766\n\n\n9\n(2, 0)\n28098.407664\n\n\n10\n(1, 1)\n28172.510044\n\n\n11\n(1, 0)\n28941.056983\n\n\n12\n(0, 3)\n31355.802141\n\n\n13\n(0, 2)\n33531.179284\n\n\n14\n(0, 1)\n39402.269523\n\n\n15\n(0, 0)\n49035.184224\n\n\n\n\n\n\n\n\nmodel = SARIMAX(train['bandwidth_diff'], order=(2, 0, 2), simple_differencing=False)\nmodel_fit = model.fit(disp=False)\nmodel_fit.plot_diagnostics(figsize=(12, 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacorr_ljungbox(model_fit.resid, np.arange(1, 11))\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n1\n0.042190\n0.837257\n\n\n2\n0.418364\n0.811247\n\n\n3\n0.520271\n0.914416\n\n\n4\n0.850554\n0.931545\n\n\n5\n0.850841\n0.973678\n\n\n6\n1.111754\n0.981019\n\n\n7\n2.124864\n0.952607\n\n\n8\n3.230558\n0.919067\n\n\n9\n3.248662\n0.953615\n\n\n10\n3.588289\n0.964015\n\n\n\n\n\n\n\n\ndef rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    if method == 'mean':\n        pred_mean = []\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            pred_mean.extend(mean for _ in range(window))\n        return pred_mean\n    if method == 'last':\n        pred_last_value = []\n        for i in range(train_len, total_len, window):\n            last_value = df.iloc[i-1].values[0]\n            pred_last_value.extend(last_value for _ in range(window))\n        return pred_last_value\n    if method == 'ARMA':\n        pred_MA = []\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(2,0,2))\n            res = model.fit(disp=False)\n            predictions = res.get_prediction(0, i + window - 1)\n            oos_pred = predictions.predicted_mean.iloc[-window:]\n            pred_MA.extend(oos_pred)\n        return pred_MA\n\npred_df = test.copy()\nTRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\npred_mean = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'mean')\npred_last = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'last')\npred_ARMA = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'ARMA')\n\npred_df['pred_mean'] = pred_mean\npred_df['pred_last'] = pred_last\npred_df['pred_ARMA'] = pred_ARMA\n\nsns.lineplot(data=pred_df, x=pred_df.index, y='bandwidth_diff', label='실제값')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_mean', label='평균 예측')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_last', label='마지막 값 예측')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_ARMA', label='ARMA(2, 2) 예측')\nplt.xlabel('시간')\nplt.ylabel('시간당 대역폭 사용량(MBps)')\nplt.xticks(\n    [9802, 9850, 9898, 9946, 9994],\n    ['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])\nplt.xlim(9800, 9999)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse_mean = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_mean'])\nmse_last = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_last'])\nmse_ARMA = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_ARMA'])\nmse_mean, mse_last, mse_ARMA\n\n(6.306526957989325, 2.2297582947733656, 1.7690462113874967)\n\n\n\n역변환\n\ndf['pred_bandwidth'] = pd.Series()\ndf['pred_bandwidth'].iloc[9832:] = df['hourly_bandwidth'].iloc[9832] + pred_df['pred_ARMA'].cumsum()\n\nsns.lineplot(data=df, x=df.index, y='hourly_bandwidth', label='실제 값')\nsns.lineplot(data=df, x=df.index, y='pred_bandwidth', label='ARMA(2, 2) 예측')\nplt.xticks(\n    [9802, 9850, 9898, 9946, 9994],\n    ['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])\nplt.xlim(9800, 9999)\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/06.html#sarima-모델",
    "href": "posts/01_projects/adp_실기/notes/time_series/06.html#sarima-모델",
    "title": "계절성 고려",
    "section": "SARIMA 모델",
    "text": "SARIMA 모델\n\n\\(SARIMA(p, d, q)(P, D, Q)_m\\) 형태로 표현\n\\(m\\): 계절성 주기\n\\(P, D, Q\\): 계절성 AR, 차분, MA 차수\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/air.csv')\n\n\nfrom statsmodels.tsa.seasonal import STL\n\ndecomposition = STL(df['Passengers'], period=12).fit()\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndf_diff = np.diff(df['Passengers'], n=1)\n\nADF_result = adfuller(df_diff)\nADF_result[0], ADF_result[1]\n\n(-2.8292668241700025, 0.05421329028382508)\n\n\n\ndf_diff = np.diff(df_diff, n=12)\n\nADF_result = adfuller(df_diff)\nADF_result[0], ADF_result[1]\n\n(-17.624862360208343, 3.823046855816035e-30)\n\n\n\nfrom typing import Union\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef optimize_SARIMA(endog: Union[pd.Series, list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:\n    results = []\n    for order in order_list:\n        try:\n            model = SARIMAX(endog, \n                            order=(order[0], d, order[1]), \n                            seasonal_order=(order[2], D, order[3], s),\n                            simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, aic])\n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p, q, P, Q)', 'AIC']\n    result_df = result_df.sort_values(by=\"AIC\").reset_index(drop=True)\n\n    return result_df\n\n\nfrom itertools import product\n\ntrain = df.iloc[:-12]['Passengers']\ntest = df.iloc[-12:]\n#  ps = range(0, 4, 1)\n#  qs = range(0, 4, 1)\n#  Ps = range(0, 4, 1)\n#  Qs = range(0, 4, 1)\n# \n#  SARIMA_order_list = list(product(ps, qs, Ps, Qs))\n# \n# \n#  d = 1\n#  D = 1\n#  s = 12\n#  SARIMA_result_df = optimize_SARIMA(train, SARIMA_order_list, d, D, s)\n#  SARIMA_result_df\n\n\nSARIMA_model = SARIMAX(train, order=(2, 1, 1), seasonal_order=(1, 1, 2, 12), simple_differencing=False)\nSARIMA_model_fit = SARIMA_model.fit(disp=False)\n\nSARIMA_model_fit.plot_diagnostics(figsize=(10, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\ntr = acorr_ljungbox(SARIMA_model_fit.resid, np.arange(1, 11))\ntr\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n1\n0.004785\n0.944850\n\n\n2\n0.745422\n0.688864\n\n\n3\n1.021040\n0.796161\n\n\n4\n1.226086\n0.873785\n\n\n5\n1.436408\n0.920290\n\n\n6\n1.711782\n0.944208\n\n\n7\n2.307234\n0.940900\n\n\n8\n2.717276\n0.950829\n\n\n9\n2.733486\n0.973931\n\n\n10\n4.969176\n0.893228\n\n\n\n\n\n\n\n\nSARIMA_pred = SARIMA_model_fit.get_prediction(132, 143).predicted_mean\n\ntest['SARIMA_pred'] = SARIMA_pred\n\nsns.lineplot(data=df, x=df.index, y='Passengers', label='실제 값')\nsns.lineplot(data=test, x=test.index, y='SARIMA_pred', label='SARIMA 예측값')\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\nplt.xlim(120, 143)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "계절성 고려"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/07.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/07.html",
    "title": "외생 변수 추가하기",
    "section": "",
    "text": "import statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nmacro_econ_data = sm.datasets.macrodata.load_pandas().data\nmacro_econ_data\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.980\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.150\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.350\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.370\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.540\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198\n2008.0\n3.0\n13324.600\n9267.7\n1990.693\n991.551\n9838.3\n216.889\n1474.7\n1.17\n6.0\n305.270\n-3.16\n4.33\n\n\n199\n2008.0\n4.0\n13141.920\n9195.3\n1857.661\n1007.273\n9920.4\n212.174\n1576.5\n0.12\n6.9\n305.952\n-8.79\n8.91\n\n\n200\n2009.0\n1.0\n12925.410\n9209.2\n1558.494\n996.287\n9926.4\n212.671\n1592.8\n0.22\n8.1\n306.547\n0.94\n-0.71\n\n\n201\n2009.0\n2.0\n12901.504\n9189.0\n1456.678\n1023.528\n10077.5\n214.469\n1653.6\n0.18\n9.2\n307.226\n3.37\n-3.19\n\n\n202\n2009.0\n3.0\n12990.341\n9256.0\n1486.398\n1044.088\n10040.6\n216.385\n1673.9\n0.12\n9.6\n308.013\n3.56\n-3.44\n\n\n\n\n203 rows × 14 columns\n\n\n\n\ntarget = macro_econ_data[['realgdp', 'realcons', 'realinv', 'realgovt', 'realdpi', 'cpi']]\ntarget.plot(subplots=True, figsize=(10, 8), layout=(3, 2))\nplt.xticks(np.arange(0, 208, 16), np.arange(1959, 2010, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\n이 다음 부분은 오류인거 같음. 다른 책 보자.\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "외생 변수 추가하기"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#다차원-척도법mds이란",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#다차원-척도법mds이란",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "다차원 척도법(MDS)이란?",
    "text": "다차원 척도법(MDS)이란?\n다차원 척도법(Multidimensional Scaling, MDS)은 고차원 데이터를 저차원 공간에 시각화하는 차원 축소 기법입니다. 객체들 간의 거리나 유사도를 보존하면서 2차원 또는 3차원 공간에 데이터를 투영합니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#필요한-라이브러리-설치-및-임포트",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#필요한-라이브러리-설치-및-임포트",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "필요한 라이브러리 설치 및 임포트",
    "text": "필요한 라이브러리 설치 및 임포트\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.datasets import make_classification\nfrom scipy.spatial.distance import pdist, squareform\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#샘플-데이터-생성",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#샘플-데이터-생성",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "샘플 데이터 생성",
    "text": "샘플 데이터 생성\n다양한 케이스를 위한 샘플 데이터를 생성합니다.\n\n# 1. 연속변수만 포함하는 데이터\nnp.random.seed(42)\ncontinuous_data = make_classification(\n    n_samples=100, \n    n_features=5, \n    n_classes=3, \n    n_redundant=0, \n    n_informative=5,\n    random_state=42\n)[0]\n\ncontinuous_df = pd.DataFrame(\n    continuous_data, \n    columns=[f'feature_{i+1}' for i in range(5)]\n)\n\nprint(\"연속변수 데이터셋 shape:\", continuous_df.shape)\nprint(continuous_df.head())\n\n연속변수 데이터셋 shape: (100, 5)\n   feature_1  feature_2  feature_3  feature_4  feature_5\n0   0.051936  -1.797511  -1.855638  -1.396449  -1.196204\n1   0.403789   0.921306   3.200886   1.984403   0.106783\n2   0.300321  -0.930015   0.162936  -0.576956   2.232421\n3  -0.199444  -0.496488  -1.928236   0.929103  -1.480070\n4   1.144153  -1.221289  -0.581620  -0.475414   1.675759\n\n\n\n# 2. 명목변수를 포함하는 혼합 데이터\nnp.random.seed(42)\n\n# 연속변수\nage = np.random.normal(35, 10, 100)\nincome = np.random.normal(50000, 15000, 100)\nexperience = np.random.normal(5, 3, 100)\n\n# 명목변수\neducation = np.random.choice(['고등학교', '대학교', '대학원'], 100)\ndepartment = np.random.choice(['영업', '마케팅', '개발', 'HR'], 100)\nlocation = np.random.choice(['서울', '부산', '대구', '광주'], 100)\n\nmixed_df = pd.DataFrame({\n    'age': age,\n    'income': income,\n    'experience': experience,\n    'education': education,\n    'department': department,\n    'location': location\n})\n\nprint(\"\\n혼합 데이터셋 shape:\", mixed_df.shape)\nprint(mixed_df.head())\n\n\n혼합 데이터셋 shape: (100, 6)\n         age        income  experience education department location\n0  39.967142  28769.438869    6.073362      고등학교         개발       서울\n1  33.617357  43690.320159    6.682354       대학교        마케팅       서울\n2  41.476885  44859.282252    8.249154      고등학교        마케팅       광주\n3  50.230299  37965.840962    8.161406      고등학교         개발       서울\n4  32.658466  47580.714325    0.866992       대학원         개발       광주\n\n\n\n# 3. 거리 행렬 데이터 (도시간 거리 예시)\ncities = ['서울', '부산', '대구', '인천', '광주', '대전', '울산']\n# 실제 도시간 거리 (km)\ndistance_matrix = np.array([\n    [0, 325, 237, 28, 267, 140, 340],      # 서울\n    [325, 0, 88, 353, 158, 185, 45],       # 부산\n    [237, 88, 0, 265, 215, 97, 85],        # 대구\n    [28, 353, 265, 0, 295, 168, 368],      # 인천\n    [267, 158, 215, 295, 0, 168, 200],     # 광주\n    [140, 185, 97, 168, 168, 0, 230],      # 대전\n    [340, 45, 85, 368, 200, 230, 0]       # 울산\n])\n\ndistance_df = pd.DataFrame(distance_matrix, \n                          index=cities, \n                          columns=cities)\n\nprint(\"\\n도시간 거리 행렬:\")\nprint(distance_df)\n\n\n도시간 거리 행렬:\n     서울   부산   대구   인천   광주   대전   울산\n서울    0  325  237   28  267  140  340\n부산  325    0   88  353  158  185   45\n대구  237   88    0  265  215   97   85\n인천   28  353  265    0  295  168  368\n광주  267  158  215  295    0  168  200\n대전  140  185   97  168  168    0  230\n울산  340   45   85  368  200  230    0",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#연속변수만-포함하는-기본-mds-분석",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#연속변수만-포함하는-기본-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "1. 연속변수만 포함하는 기본 MDS 분석",
    "text": "1. 연속변수만 포함하는 기본 MDS 분석\n연속변수로만 구성된 데이터에 MDS를 적용하는 예시입니다.\n\n# 데이터 표준화\nscaler = StandardScaler()\ncontinuous_scaled = scaler.fit_transform(continuous_df)\n\n# 기본 MDS 적용 (2차원)\nmds = MDS(n_components=2, random_state=42)\nmds_result = mds.fit_transform(continuous_scaled)\n\n# 결과를 DataFrame으로 변환\nmds_df = pd.DataFrame(mds_result, columns=['MDS1', 'MDS2'])\n\nprint(\"MDS 결과:\")\nprint(mds_df.head())\nprint(f\"\\nStress 값: {mds.stress_:.4f}\")\n\nMDS 결과:\n       MDS1      MDS2\n0  0.329495 -1.755421\n1  0.049463  3.238964\n2 -0.946161  0.369966\n3  0.357709 -0.007320\n4 -0.769899 -0.435848\n\nStress 값: 3208.7256\n\n\n\n# 시각화\nplt.figure(figsize=(10, 8))\nplt.scatter(mds_df['MDS1'], mds_df['MDS2'], alpha=0.7, s=50)\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 연속변수 데이터')\nplt.grid(True, alpha=0.3)\n\n# 각 점에 인덱스 번호 표시\nfor i, (x, y) in enumerate(zip(mds_df['MDS1'], mds_df['MDS2'])):\n    if i % 10 == 0:  # 10개마다 번호 표시\n        plt.annotate(str(i), (x, y), xytext=(5, 5), \n                    textcoords='offset points', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 클래스별로 색상을 다르게 하여 시각화 (원본 클래스 정보 사용)\n_, y_true = make_classification(\n    n_samples=100, \n    n_features=5, \n    n_classes=3, \n    n_redundant=0, \n    n_informative=5,\n    random_state=42\n)\n\nplt.figure(figsize=(10, 8))\ncolors = ['red', 'blue', 'green']\nfor i in range(3):\n    mask = y_true == i\n    plt.scatter(mds_df.loc[mask, 'MDS1'], \n                mds_df.loc[mask, 'MDS2'], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=50)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 클래스별 색상 구분')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 거리 보존 정도 확인\nfrom sklearn.metrics import pairwise_distances\n\n# 원본 데이터의 거리 행렬\noriginal_distances = pairwise_distances(continuous_scaled)\n# MDS 결과의 거리 행렬\nmds_distances = pairwise_distances(mds_result)\n\n# 거리 상관계수 계산\ndistance_correlation = np.corrcoef(\n    original_distances.flatten(), \n    mds_distances.flatten()\n)[0, 1]\n\nprint(f\"원본 거리와 MDS 거리의 상관계수: {distance_correlation:.4f}\")\n\n# Shepard diagram 그리기\nplt.figure(figsize=(8, 6))\nplt.scatter(original_distances.flatten(), \n            mds_distances.flatten(), \n            alpha=0.3, s=1)\nplt.xlabel('Original Distances')\nplt.ylabel('MDS Distances')\nplt.title('Shepard Diagram - 거리 보존 정도')\nplt.plot([0, original_distances.max()], \n         [0, original_distances.max()], \n         'r--', alpha=0.8)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n원본 거리와 MDS 거리의 상관계수: 0.8435",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#명목변수를-포함하는-혼합-데이터-mds-분석",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#명목변수를-포함하는-혼합-데이터-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "2. 명목변수를 포함하는 혼합 데이터 MDS 분석",
    "text": "2. 명목변수를 포함하는 혼합 데이터 MDS 분석\n명목변수와 연속변수가 혼합된 데이터에 MDS를 적용하는 예시입니다.\n\n# 혼합 데이터 전처리\ndef preprocess_mixed_data(df):\n    \"\"\"혼합 데이터를 MDS에 적합하도록 전처리\"\"\"\n    processed_df = df.copy()\n    \n    # 연속변수 표준화\n    continuous_cols = ['age', 'income', 'experience']\n    scaler = StandardScaler()\n    processed_df[continuous_cols] = scaler.fit_transform(processed_df[continuous_cols])\n    \n    # 명목변수 원핫 인코딩\n    categorical_cols = ['education', 'department', 'location']\n    for col in categorical_cols:\n        dummies = pd.get_dummies(processed_df[col], prefix=col)\n        processed_df = pd.concat([processed_df, dummies], axis=1)\n        processed_df.drop(col, axis=1, inplace=True)\n    \n    return processed_df\n\n# 데이터 전처리\nmixed_processed = preprocess_mixed_data(mixed_df)\nprint(\"전처리된 혼합 데이터 shape:\", mixed_processed.shape)\nprint(\"\\n컬럼 목록:\")\nprint(mixed_processed.columns.tolist())\n\n전처리된 혼합 데이터 shape: (100, 14)\n\n컬럼 목록:\n['age', 'income', 'experience', 'education_고등학교', 'education_대학교', 'education_대학원', 'department_HR', 'department_개발', 'department_마케팅', 'department_영업', 'location_광주', 'location_대구', 'location_부산', 'location_서울']\n\n\n\n# 혼합 데이터에 MDS 적용\nmds_mixed = MDS(n_components=2, random_state=42)\nmds_mixed_result = mds_mixed.fit_transform(mixed_processed)\n\n# 결과를 DataFrame으로 변환\nmds_mixed_df = pd.DataFrame(mds_mixed_result, columns=['MDS1', 'MDS2'])\n\nprint(\"혼합 데이터 MDS 결과:\")\nprint(mds_mixed_df.head())\nprint(f\"\\nStress 값: {mds_mixed.stress_:.4f}\")\n\n혼합 데이터 MDS 결과:\n       MDS1      MDS2\n0  1.112396 -1.853207\n1  0.199845 -1.172801\n2 -0.073573 -1.903613\n3 -0.293398 -2.665801\n4  1.287388  1.284487\n\nStress 값: 3843.1912\n\n\n\n# 부서별로 색상을 다르게 하여 시각화\nplt.figure(figsize=(12, 8))\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    plt.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 부서별 색상 구분 (혼합 데이터)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 학력별로 마커를 다르게 하여 시각화\nplt.figure(figsize=(12, 8))\neducations = mixed_df['education'].unique()\nmarkers = ['o', 's', '^']\n\nfor i, edu in enumerate(educations):\n    mask = mixed_df['education'] == edu\n    plt.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                marker=markers[i], label=edu, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 학력별 마커 구분 (혼합 데이터)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 부서와 학력을 함께 시각화\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n# 부서별 시각화\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    ax1.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nax1.set_xlabel('MDS Dimension 1')\nax1.set_ylabel('MDS Dimension 2')\nax1.set_title('부서별 구분')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 학력별 시각화\neducations = mixed_df['education'].unique()\ncolors2 = ['purple', 'brown', 'pink']\n\nfor i, edu in enumerate(educations):\n    mask = mixed_df['education'] == edu\n    ax2.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors2[i], label=edu, alpha=0.7, s=60)\n\nax2.set_xlabel('MDS Dimension 1')\nax2.set_ylabel('MDS Dimension 2')\nax2.set_title('학력별 구분')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Gower 거리를 사용한 혼합 데이터 MDS\ndef gower_distance(X):\n    \"\"\"Gower 거리 계산 (연속변수와 명목변수 혼합용)\"\"\"\n    n_samples, n_features = X.shape\n    distances = np.zeros((n_samples, n_samples))\n    \n    # 각 피처가 연속변수인지 이진변수인지 판단\n    is_continuous = []\n    for j in range(n_features):\n        unique_vals = np.unique(X[:, j])\n        is_continuous.append(len(unique_vals) &gt; 2)\n    \n    for i in range(n_samples):\n        for j in range(i+1, n_samples):\n            distance = 0\n            for k in range(n_features):\n                if is_continuous[k]:\n                    # 연속변수: 절대차이를 범위로 나눔\n                    range_k = np.max(X[:, k]) - np.min(X[:, k])\n                    if range_k &gt; 0:\n                        distance += abs(X[i, k] - X[j, k]) / range_k\n                else:\n                    # 명목변수: 같으면 0, 다르면 1\n                    distance += 0 if X[i, k] == X[j, k] else 1\n            \n            distances[i, j] = distances[j, i] = distance / n_features\n    \n    return distances\n\n# 원본 혼합 데이터로 Gower 거리 계산\nmixed_array = mixed_processed.values\ngower_dist = gower_distance(mixed_array)\n\n# Gower 거리 기반 MDS\nmds_gower = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\nmds_gower_result = mds_gower.fit_transform(gower_dist)\n\n# 결과 시각화\nplt.figure(figsize=(10, 8))\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    plt.scatter(mds_gower_result[mask, 0], \n                mds_gower_result[mask, 1], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('Gower 거리 기반 MDS 결과')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Gower 거리 기반 MDS Stress 값: {mds_gower.stress_:.4f}\")\n\n\n\n\n\n\n\n\nGower 거리 기반 MDS Stress 값: 70.5540",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#거리-행렬-기반-mds-분석",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#거리-행렬-기반-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "3. 거리 행렬 기반 MDS 분석",
    "text": "3. 거리 행렬 기반 MDS 분석\n미리 계산된 거리 행렬을 사용하여 MDS를 적용하는 예시입니다.\n\n# 도시간 거리 행렬로 MDS 수행\nmds_distance = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\ncity_mds_result = mds_distance.fit_transform(distance_matrix)\n\n# 결과를 DataFrame으로 변환\ncity_mds_df = pd.DataFrame(city_mds_result, \n                          columns=['MDS1', 'MDS2'], \n                          index=cities)\n\nprint(\"도시 MDS 결과:\")\nprint(city_mds_df)\nprint(f\"\\nStress 값: {mds_distance.stress_:.4f}\")\n\n도시 MDS 결과:\n          MDS1        MDS2\n서울  -85.433007 -154.346568\n부산   59.327174  130.031955\n대구  -27.112189   81.647524\n인천  -99.393194 -179.287057\n광주  145.126739  -10.586077\n대전  -24.860674  -32.769460\n울산   32.345151  165.309682\n\nStress 값: 2023.2616\n\n\n\n# 도시 위치 시각화\nplt.figure(figsize=(12, 10))\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n\nfor i, city in enumerate(cities):\n    plt.scatter(city_mds_df.loc[city, 'MDS1'], \n                city_mds_df.loc[city, 'MDS2'], \n                c=colors[i], s=200, alpha=0.7, \n                label=city, edgecolors='black', linewidth=1)\n    \n    # 도시 이름 표시\n    plt.annotate(city, \n                (city_mds_df.loc[city, 'MDS1'], city_mds_df.loc[city, 'MDS2']),\n                xytext=(10, 10), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('도시간 거리 기반 MDS 결과')\nplt.grid(True, alpha=0.3)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 실제 거리와 MDS 거리 비교\nmds_city_distances = pairwise_distances(city_mds_result)\n\n# 거리 상관계수 계산\ncity_distance_correlation = np.corrcoef(\n    distance_matrix.flatten(), \n    mds_city_distances.flatten()\n)[0, 1]\n\nprint(f\"실제 거리와 MDS 거리의 상관계수: {city_distance_correlation:.4f}\")\n\n# Shepard diagram for city distances\nplt.figure(figsize=(8, 6))\nplt.scatter(distance_matrix.flatten(), \n            mds_city_distances.flatten(), \n            alpha=0.6, s=30)\nplt.xlabel('Original Distances (km)')\nplt.ylabel('MDS Distances')\nplt.title('Shepard Diagram - 도시간 거리 보존 정도')\nplt.plot([0, distance_matrix.max()], \n         [0, distance_matrix.max()], \n         'r--', alpha=0.8)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n실제 거리와 MDS 거리의 상관계수: 0.9970\n\n\n\n\n\n\n\n\n\n\n# 3차원 MDS 수행\nmds_3d = MDS(n_components=3, dissimilarity='precomputed', random_state=42)\ncity_mds_3d_result = mds_3d.fit_transform(distance_matrix)\n\n# 3D 시각화\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\n\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n\nfor i, city in enumerate(cities):\n    ax.scatter(city_mds_3d_result[i, 0], \n               city_mds_3d_result[i, 1], \n               city_mds_3d_result[i, 2],\n               c=colors[i], s=200, alpha=0.7, \n               label=city, edgecolors='black', linewidth=1)\n    \n    # 도시 이름 표시\n    ax.text(city_mds_3d_result[i, 0], \n            city_mds_3d_result[i, 1], \n            city_mds_3d_result[i, 2], \n            city, fontsize=10, fontweight='bold')\n\nax.set_xlabel('MDS Dimension 1')\nax.set_ylabel('MDS Dimension 2')\nax.set_zlabel('MDS Dimension 3')\nax.set_title('3차원 MDS 결과 - 도시간 거리')\nax.legend(bbox_to_anchor=(1.1, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\nprint(f\"3차원 MDS Stress 값: {mds_3d.stress_:.4f}\")\n\n\n\n\n\n\n\n\n3차원 MDS Stress 값: 1974.2860\n\n\n\n# 차원별 Stress 값 비교\ndimensions = range(1, 6)\nstress_values = []\n\nfor dim in dimensions:\n    mds_temp = MDS(n_components=dim, dissimilarity='precomputed', random_state=42)\n    mds_temp.fit(distance_matrix)\n    stress_values.append(mds_temp.stress_)\n\n# Stress plot (Scree plot)\nplt.figure(figsize=(10, 6))\nplt.plot(dimensions, stress_values, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('차원 수')\nplt.ylabel('Stress 값')\nplt.title('차원 수에 따른 Stress 값 변화')\nplt.grid(True, alpha=0.3)\nplt.xticks(dimensions)\n\n# 각 점에 stress 값 표시\nfor i, stress in enumerate(stress_values):\n    plt.annotate(f'{stress:.3f}', \n                (dimensions[i], stress), \n                xytext=(0, 10), textcoords='offset points', \n                ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"차원별 Stress 값:\")\nfor dim, stress in zip(dimensions, stress_values):\n    print(f\"{dim}차원: {stress:.4f}\")\n\n\n\n\n\n\n\n\n차원별 Stress 값:\n1차원: 340937.8571\n2차원: 2023.2616\n3차원: 1974.2860\n4차원: 1980.2935\n5차원: 1982.6011",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#다양한-mds-변형-기법",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#다양한-mds-변형-기법",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "4. 다양한 MDS 변형 기법",
    "text": "4. 다양한 MDS 변형 기법\n다양한 MDS 알고리즘을 비교해보는 예시입니다.\n\n# 다양한 MDS 알고리즘 비교\nfrom sklearn.manifold import MDS\n\n# 연속 데이터를 사용하여 다양한 MDS 알고리즘 비교\nalgorithms = {\n    'Classical MDS': MDS(metric=True, random_state=42),\n    'Non-metric MDS': MDS(metric=False, random_state=42),\n    'MDS with different init': MDS(metric=True, n_init=10, random_state=42)\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor idx, (name, mds_algo) in enumerate(algorithms.items()):\n    # MDS 수행\n    result = mds_algo.fit_transform(continuous_scaled)\n    \n    # 클래스별 색상으로 시각화\n    colors = ['red', 'blue', 'green']\n    for i in range(3):\n        mask = y_true == i\n        axes[idx].scatter(result[mask, 0], result[mask, 1], \n                         c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n    \n    axes[idx].set_xlabel('MDS Dimension 1')\n    axes[idx].set_ylabel('MDS Dimension 2')\n    axes[idx].set_title(f'{name}\\nStress: {mds_algo.stress_:.4f}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 거리 메트릭 비교\nfrom sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n\n# 다양한 거리 메트릭으로 MDS 수행\ndistance_metrics = {\n    'Euclidean': euclidean_distances(continuous_scaled),\n    'Manhattan': manhattan_distances(continuous_scaled),\n    'Cosine': cosine_distances(continuous_scaled)\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor idx, (metric_name, dist_matrix) in enumerate(distance_metrics.items()):\n    # 거리 행렬을 사용한 MDS\n    mds_metric = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n    result = mds_metric.fit_transform(dist_matrix)\n    \n    # 클래스별 색상으로 시각화\n    colors = ['red', 'blue', 'green']\n    for i in range(3):\n        mask = y_true == i\n        axes[idx].scatter(result[mask, 0], result[mask, 1], \n                         c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n    \n    axes[idx].set_xlabel('MDS Dimension 1')\n    axes[idx].set_ylabel('MDS Dimension 2')\n    axes[idx].set_title(f'{metric_name} Distance MDS\\nStress: {mds_metric.stress_:.4f}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#mds-결과-해석-및-평가",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#mds-결과-해석-및-평가",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "5. MDS 결과 해석 및 평가",
    "text": "5. MDS 결과 해석 및 평가\nMDS 결과를 해석하고 평가하는 방법들을 소개합니다.\n\n# Stress 값 해석 기준\ndef interpret_stress(stress_value):\n    \"\"\"Stress 값을 해석하는 함수\"\"\"\n    if stress_value &lt; 0.05:\n        return \"매우 좋음 (Excellent)\"\n    elif stress_value &lt; 0.1:\n        return \"좋음 (Good)\"\n    elif stress_value &lt; 0.2:\n        return \"보통 (Fair)\"\n    else:\n        return \"나쁨 (Poor)\"\n\n# 각 MDS 결과의 Stress 값 해석\nprint(\"=== MDS 결과 평가 ===\")\nprint(f\"연속변수 MDS Stress: {mds.stress_:.4f} - {interpret_stress(mds.stress_)}\")\nprint(f\"혼합데이터 MDS Stress: {mds_mixed.stress_:.4f} - {interpret_stress(mds_mixed.stress_)}\")\nprint(f\"Gower 거리 MDS Stress: {mds_gower.stress_:.4f} - {interpret_stress(mds_gower.stress_)}\")\nprint(f\"도시 거리 MDS Stress: {mds_distance.stress_:.4f} - {interpret_stress(mds_distance.stress_)}\")\n\n=== MDS 결과 평가 ===\n연속변수 MDS Stress: 3208.7256 - 나쁨 (Poor)\n혼합데이터 MDS Stress: 3843.1912 - 나쁨 (Poor)\nGower 거리 MDS Stress: 70.5540 - 나쁨 (Poor)\n도시 거리 MDS Stress: 2023.2616 - 나쁨 (Poor)\n\n\n\n# 차원 축소 효과 비교\ndef calculate_explained_variance_ratio(original_data, mds_result):\n    \"\"\"MDS로 설명되는 분산 비율 계산\"\"\"\n    original_var = np.var(original_data, axis=0).sum()\n    mds_var = np.var(mds_result, axis=0).sum()\n    return mds_var / original_var\n\n# 연속변수 데이터의 분산 설명 비율\nexplained_ratio = calculate_explained_variance_ratio(continuous_scaled, mds_result)\nprint(f\"\\n연속변수 MDS 분산 설명 비율: {explained_ratio:.4f} ({explained_ratio*100:.2f}%)\")\n\n# 원본 데이터 차원과 MDS 결과 비교\nprint(f\"원본 데이터 차원: {continuous_scaled.shape[1]}차원\")\nprint(f\"MDS 결과 차원: {mds_result.shape[1]}차원\")\nprint(f\"차원 축소율: {(1 - mds_result.shape[1]/continuous_scaled.shape[1])*100:.1f}%\")\n\n\n연속변수 MDS 분산 설명 비율: 0.9358 (93.58%)\n원본 데이터 차원: 5차원\nMDS 결과 차원: 2차원\n차원 축소율: 60.0%\n\n\n\n# MDS vs PCA 비교\nfrom sklearn.decomposition import PCA\n\n# PCA 수행\npca = PCA(n_components=2, random_state=42)\npca_result = pca.fit_transform(continuous_scaled)\n\n# 비교 시각화\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# MDS 결과\ncolors = ['red', 'blue', 'green']\nfor i in range(3):\n    mask = y_true == i\n    ax1.scatter(mds_result[mask, 0], mds_result[mask, 1], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n\nax1.set_xlabel('MDS Dimension 1')\nax1.set_ylabel('MDS Dimension 2')\nax1.set_title(f'MDS 결과\\nStress: {mds.stress_:.4f}')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# PCA 결과\nfor i in range(3):\n    mask = y_true == i\n    ax2.scatter(pca_result[mask, 0], pca_result[mask, 1], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n\nax2.set_xlabel('PC1')\nax2.set_ylabel('PC2')\nax2.set_title(f'PCA 결과\\n설명분산비율: {pca.explained_variance_ratio_.sum():.4f}')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"PCA 설명 분산 비율: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")\nprint(f\"각 주성분별 설명 분산 비율: PC1={pca.explained_variance_ratio_[0]:.4f}, PC2={pca.explained_variance_ratio_[1]:.4f}\")\n\n\n\n\n\n\n\n\nPCA 설명 분산 비율: 0.5028 (50.28%)\n각 주성분별 설명 분산 비율: PC1=0.2727, PC2=0.2301",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/etc/00.html#결론",
    "href": "posts/01_projects/adp_실기/notes/etc/00.html#결론",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "결론",
    "text": "결론\n다차원 척도법(MDS)은 다양한 유형의 데이터에 적용할 수 있는 강력한 차원 축소 기법입니다:\n\n주요 특징:\n\n거리 보존: 원본 데이터의 거리 관계를 저차원에서 최대한 보존\n유연성: 연속변수, 명목변수, 거리 행렬 등 다양한 데이터 타입 지원\n직관적 해석: 2D/3D 시각화를 통한 직관적인 데이터 이해\n\n\n\n적용 사례:\n\n연속변수: 표준화 후 직접 적용\n혼합 데이터: 원핫 인코딩 또는 Gower 거리 사용\n거리 행렬: 미리 계산된 거리 정보 활용\n\n\n\n평가 지표:\n\nStress 값: 낮을수록 좋음 (&lt; 0.1이 권장)\n거리 상관계수: 원본과 MDS 거리의 상관관계\nShepard diagram: 거리 보존 정도 시각화",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/time_series/08.html",
    "href": "posts/01_projects/adp_실기/notes/time_series/08.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "08"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/02.html#smart-전략",
    "href": "posts/01_projects/opic/notes/02.html#smart-전략",
    "title": "Tips",
    "section": "SMART 전략",
    "text": "SMART 전략\n\nS(start by stating your main point/idea): 주제나 요점 먼저 말하기 (main point1)\nM(mention a few example): 연관된 사례나 예시 제시하여 주제 설명 (body)\nA(address why your MP is important by returning to it): MP가 중요한 이유를 설명하며 주제로 돌아와 그 중요성 강조\nR(Reflect on what you’ve learned or realized): 배운 점이나 깨달은 점 반영. conclusion 가기 전에 언급\nT(tie everything back yo your main point): 모든 내용을 MP에 연결시켜 마무리\n\n(억지로 붙여넣네)",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/02.html#role-play",
    "href": "posts/01_projects/opic/notes/02.html#role-play",
    "title": "Tips",
    "section": "Role play",
    "text": "Role play\n\n11\n\n대충 내용 설명\n첫 번째 질문\n상대방 말 보여주기\nreally?\n나를 포함시키기\n두번째 질문 하기\n상대방 말 보여주기\nreally?\n나를 포함시키기\n세번째 질문 하기\n상대방 말 보여주기\n나를 포함시키기",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/02.html#그-외",
    "href": "posts/01_projects/opic/notes/02.html#그-외",
    "title": "Tips",
    "section": "그 외",
    "text": "그 외\n\n한국 고유말 한국말로 쓰면 감점이 있다고 함. 걍 한국말 쓰지 말자.\nthere’s this []: 어떤 []가 있다.\n어려운 표현 쓸거면 비슷한 쉬운 표현도 뒤에 같이 써서 의미를 명확히 전달할 것\nand 조심해서 써라\n15번은 그냥 스킵해라\nyou expression 많이 써라\n과거 이야기 할 때 main point 먼저 말하고 develope 해라\n생각하고 대답하는 척 하기\n보통 다른 사람 and i 순. i and 다른 사람은 어색\nmoreover, furthermore, in addition은 대화에는 어색함\nwhich because really since 로 길게 말할 수 있음",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/02.html#footnotes",
    "href": "posts/01_projects/opic/notes/02.html#footnotes",
    "title": "Tips",
    "section": "각주",
    "text": "각주\n\n\nWhat, Feeling, Why Hibit: whenever _↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/03.html",
    "href": "posts/01_projects/opic/notes/03.html",
    "title": "일정 정리",
    "section": "",
    "text": "7/16: 오픽노잼 AL 시리즈 마무리\n~7/18: 예상 질문 정리\n\n부단용 오픽 정리\n강지완 오픽 정리\n\n~7/19:\n\n말하기 연습\n\n7/20:\n\n롤플레이 연습\n여우 강사 최나영 오픽 모의고사\n\n7/26: 시험\n\n대학연합 오픽\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "일정 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/opic/notes/01.html#자기소개",
    "href": "posts/01_projects/opic/notes/01.html#자기소개",
    "title": "설문 script 정리",
    "section": "자기소개",
    "text": "자기소개\nHello Eva. It’s great to see you.\nMy name is hyunghoon, and I will do my best to answer your questions clearly\nSo, let’s have a great conversation. I’m ready!”",
    "crumbs": [
      "PARA",
      "Projects",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/index.html",
    "href": "posts/01_projects/toeic_speaking/index.html",
    "title": "토익 스피킹 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: 2025-07-19\n        종료일: 2025-08-02\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/index.html#details",
    "href": "posts/01_projects/toeic_speaking/index.html#details",
    "title": "토익 스피킹 준비",
    "section": "Details",
    "text": "Details\n아 하기 싫다.",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/index.html#tasks",
    "href": "posts/01_projects/toeic_speaking/index.html#tasks",
    "title": "토익 스피킹 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/index.html#참고-자료",
    "href": "posts/01_projects/toeic_speaking/index.html#참고-자료",
    "title": "토익 스피킹 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/index.html#related-posts",
    "href": "posts/01_projects/toeic_speaking/index.html#related-posts",
    "title": "토익 스피킹 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#결정-트리",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#결정-트리",
    "title": "분류",
    "section": "결정 트리",
    "text": "결정 트리\n\n결정 노드가 많아지면 과적합이 발생할 수 있음\n가능한 한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 규칙을 정해야 함\n균일하게 데이터 세트를 구성할 수 있도록 분할하는 것이 필요\n균일도만 신경쓰면 되기 때문에 전처리 작업이 필요 없음\n\n\n파라미터\n\nmin_samples_split: 노드를 분할하기 위한 최소한의 샘플 수.\nmin_samples_leaf: 말단 노드가 되기 위한 최소한의 샘플 수. 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요\nmax_features: 분할을 고려할 feature의 수. default는 None으로 모든 feature를 고려함\nmax_depth\nmax_leaf_nodes\n\n\n\n시각화\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport graphviz\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\ndt_clf = DecisionTreeClassifier()\n\niris_data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2)\n\ndt_clf.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\nexport_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport numpy as np\n\nfor name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n    print(f'{name}: {value:.3f}')\nsns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)\n\nsepal length (cm): 0.017\nsepal width (cm): 0.000\npetal length (cm): 0.532\npetal width (cm): 0.452\n\n\n\n\n\n\n\n\n\n\n\nexamples\n\nimport pandas as pd\n\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] &gt; 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7352 entries, 0 to 7351\nColumns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)\ndtypes: float64(561)\nmemory usage: 31.5 MB\n\n\n\ny_train['action'].value_counts()\n\naction\n6    1407\n5    1374\n4    1286\n1    1226\n2    1073\n3     986\nName: count, dtype: int64\n\n\n\nfrom sklearn.metrics import accuracy_score\n\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\npred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.8561248727519511",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#앙상블",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#앙상블",
    "title": "분류",
    "section": "앙상블",
    "text": "앙상블\n\n서로 다른 알고리즘을 결합할 수 있지만 대부분 결정 트리 기반의 같은 알고리즘을 결합함",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#부스팅",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#부스팅",
    "title": "분류",
    "section": "부스팅",
    "text": "부스팅",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#배깅",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#배깅",
    "title": "분류",
    "section": "배깅",
    "text": "배깅",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/03.html",
    "href": "posts/04_archives/opic/notes/03.html",
    "title": "일정 정리",
    "section": "",
    "text": "7/16: 오픽노잼 AL 시리즈 마무리\n~7/18: 예상 질문 정리\n\n부단용 오픽 정리\n강지완 오픽 정리\n\n~7/19:\n\n말하기 연습\n\n7/20:\n\n롤플레이 연습\n여우 강사 최나영 오픽 모의고사\n\n7/26: 시험\n\n대학연합 오픽\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "일정 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/04.html#돌발",
    "href": "posts/04_archives/opic/notes/04.html#돌발",
    "title": "돌발 script 정리",
    "section": "돌발",
    "text": "돌발\n\n재활용\n\n집에서 하는 재활용 과정\n\nhow do you recycle at home? when and how often do you recycle? describe each step of the process from beginning to end\n\n재활용을 하며 기억에 남는 경험\n\ntell me about a memorable experience you had while recycling. what happened? what made it so memorable? tell me about it in as much detail as possible.\n\n우리나라의 재활용\n\ntell me about recycling in your country. what kind of items do people usually recycle? please describe the recycling system in your country in detail.\n\n\n\n\n약속\n\n약속을 잡는 경향\n\npeople set up appointments for various reasons. what sort of appointments do you usually make, social or otherwise? who do you usually meet with? where do you usually meet them? give me as many details as possible\n\n기억에 남는 약속\n\ntell me about the most memorable appointment you have ever had. what kind of appointment was it? who did you meet? what did you do? did anything unexpected or interesting occur? why was it so memorable?\n\n약속 접기 전 연락 과정\n\nwhen you want to meet up with someone, how do you get in touch with him or her? do you make phone calls, send an e-mail, or do something else? how does the exchange usually go? tell me about the process from beginning to end\n\n\n\n\n은행\n\n우리나라의 은행\n\ndescribe the banks in your country. where are they usually located, and what are they like? what hours are banks typically open there? give me as many details as possible\n\n과거와 현재의 은행 비교\n\nhave there been any changes to the banks in your country since you were a child? how were they in the past? how are they now? please describe the changes in detail\n\n은행에서 겪은 문제\n\nhave you ever experienced any problems at a bank? for instance, sometimes ATMs malfunction. what sort of problem did you face, and how did you deal with it?\n\n\n\n\n휴대폰\n\n\n지형 야외활동\n\n우리나라의 지형\n\ni would like to know about the geographic features of your country. what makes them different from other countries? please describe them in as much detail as possible.\n\n기억에 남는 야회 활동 경험\n\ndescribe the most memorable experience you have had outdoors. what is a beautiful place you have been to? please provide s many details as possible\n\n우리나라 사람들이 즐겨하는 야외 활동\n\nwhat kind of outdoor activities do people in your country do? do they enjoy things like jogging, cycling, or hiking? why do the like to do those activities?\n\n\n\n\n모임, 기념일\n\n\n외식 음식\n\n자주가는 식당\n\ni would like to know about a restaurant you often visit. what kind of dishes does it serve? what do you like about the restaurant? what does it look like?\n\n식당에서 겪은 경험\n\nhave you ever had a special experience at a restaurant? who were you with? what happened? tell me about the experience in detail, and explain what made it memorable.\n\n유명한 한국 요리\n\ntell me about the most famous dish of your country. what are the main ingredients in it? do you know how yo make it? what is special about the dish?\n\n\n\n\n기술\n\n과거와 현재의 기술 비교\n\ntechnology is advancing more rapidly than ever. can you tell me about the way technology has been changing? what changes have occurred since you were child? provide me with as many details as possible.\n\n기술과 관련되어 생긴 문제\n\nhave you ever experienced a problem related to technology? for instance, sometimes a device does not work properly or is difficult to use. describe your technological problem in detail. how did you handle it?\n\n우리나라에서 인기 있는 기술\n\ntell me about the technologies that are popular in your country. which technology do people use the most there? what is it used for? can you tell me why they like to use it?\n\n\n\n\n날씨 계절\n\n우리나라의 계절\n\ni would like to know about the seasons in your country. how many seasons are there? how are they different? what is the weather like in each season?\n\n과거와 현재의 날씨 비교\n\ndo you think that the weather has changed over the past few years? what was it like in the past? how has it changed? please describe the differences in detail.\n\n이상 기후로 인해 겪은 경험\n\nhave you ever had a memorable or unexpected experience because of unusual weather conditions? when was it? what happened? tell me about it in as much detail as possible\n\n\n\n\n휴일",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/04.html#roleplaying",
    "href": "posts/04_archives/opic/notes/04.html#roleplaying",
    "title": "돌발 script 정리",
    "section": "roleplaying",
    "text": "roleplaying\n\n면접관에게 질문하기\n\ni live in an apartment. please ask me three or four questions about the place i live in.\ni enjoy traveling too. please ask me three or four questions about traveling.\ni live in Canada. ask me three or four questions about the geographic features of my country.\n\n\n\n주어진 상황에서 직접 질문하기\n\ni’m going yo give you a situation to act out. pretend you are overseas on vacation and you need a car to get around, so you have gone to a car rental agency. ask the agent three or four questions about renting a car.\ni am going to give you a situation to act out. imagine that you have gone to a store to buy new furniture. ask the salesperson three or four questions about the furniture you are looking for\n\n\n\n주어진 상황에서 전화로 질문하기\n\ni would like to give you a situation to act out. imagine you are planning a vacation. call a travel agency, and ask three or four questions about potential destinations and itineraries.\n\n\n\n상황 설명하고 대안 제시하기\n\ni have a problem for you to solve. you found out you purchased the wrong tickets at the movie theater. talk to the person at the ticket window about your situation and offer suggestions to solve the problem.\nthere is a problem i’d like you to solve. you’ve ordered some furniture and its just been delivered. however, the furniture thats arrived is not what you ordered. call the furniture store to explain the situation and suggest some alternatives to the problem.\nthere is a problem i need you to solve. pretend that you’ve gone to the library to look for a book, but the one you want has been checked out. explain the situation to a librarian and offer two or theree alternatives to the problem.",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/00.html#시험-구조",
    "href": "posts/04_archives/opic/notes/00.html#시험-구조",
    "title": "오픽 구조 파악",
    "section": "시험 구조",
    "text": "시험 구조\n\n시험 시간은 총 40분. 답변 시간은 자유\n2분 정도 지나면 다음 문제로 넘어갈 수 있는 버튼이 활성화됨.\n후반 문제는 조금 늦게 나옴",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/00.html#질문-유형",
    "href": "posts/04_archives/opic/notes/00.html#질문-유형",
    "title": "오픽 구조 파악",
    "section": "질문 유형",
    "text": "질문 유형\n\n자기소개\nbackground survey\n돌발\n\n1번 문제가 자기소개. 점수엔 딱히 영향 없고, 그냥 목소리 푸는 용도\n2/3/4, 5/6/7, 8/9/10, 11/12/13, 14/15 는 background survey 및 돌발 질문\n각각의 묶음은 combo 문제\n\ncombo 별 10개 유형\n\n묘사\n\n일반적으로 이렇다는걸 대답. 현재형으로 대답\n가장 쉽고 배점이 낮기 때문에 적당히 대답\n\n루틴, 단계, 활동\n\n1번과 마찬가지로 현재형으로 대답\n\n처음 / 최근 경험\n\n육하원칙, 과거형\n최초 경험은 시간에 따른 취향 변화도 물어봄\n\n가장 인상적인 경험\n\n구체적인 설명 필요. 문제 경험을 물어보기도 함\n배점이 높으니까 신경써서 대답\n\n에바에게 질문하기\n\n안나옴\n\n주어진 상황에 3-4가지 정보 요청하기\n6번에서의 문제 상황 설명 및 대책 2-3가지 세우기\n이와 같은 본인 경험\n\n7번과 유사한 나의 경험\n\n비교 / 대조\n\n자세한 예를 들어 설명\n마무리는 선호\n\n주제 관련 이슈, 문제점, 걱정거리, 뉴스\n\n예시, 상황, 생각, 의견",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/00.html#답변-구조",
    "href": "posts/04_archives/opic/notes/00.html#답변-구조",
    "title": "오픽 구조 파악",
    "section": "답변 구조",
    "text": "답변 구조\n\nmain idea\n\n첫인상, 요지, hook\n\nbody\n\n\n\n\n강조 구간 (2-3 문장 속도감 있게)\n강조 구간\n\nconclusion\n\n역질문 what about you?\n제안하기 why dont we 동사, lets 동사\n급 마무리 thats it for this question\n1 + 3",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/00.html#공부-방법",
    "href": "posts/04_archives/opic/notes/00.html#공부-방법",
    "title": "오픽 구조 파악",
    "section": "공부 방법",
    "text": "공부 방법\n주제별로 공부\n묘사 -&gt; 루틴 -&gt; 비교 -&gt; 과거 경험 -&gt; 롤플레이 -&gt; 어드밴스",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html",
    "href": "posts/00_inboxes/notes/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html#서울대",
    "href": "posts/00_inboxes/notes/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html#카이스트",
    "href": "posts/00_inboxes/notes/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html#포항공대",
    "href": "posts/00_inboxes/notes/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html#연세대",
    "href": "posts/00_inboxes/notes/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/02.html#고려대",
    "href": "posts/00_inboxes/notes/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/01.html#자기소개",
    "href": "posts/04_archives/opic/notes/01.html#자기소개",
    "title": "설문 script 정리",
    "section": "자기소개",
    "text": "자기소개\nHello Eva. It’s great to see you.\nMy name is hyunghoon, and I will do my best to answer your questions clearly\nSo, let’s have a great conversation. I’m ready!”",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/01.html#주제",
    "href": "posts/04_archives/opic/notes/01.html#주제",
    "title": "설문 script 정리",
    "section": "주제",
    "text": "주제\n일 경험 없음 학생: 아니요 수강 후 5년 이상 지남\n\n개인주택이나 아파트에 홀로 거주\nQ. I would like to know where you live. Describe your house in detail. What does it look like? How many rooms do you have?\n\nMP: 나는 지금 부모님과 함께 아파트에 살고 있다.\n아파트 주변에는 산과 공원이 있어서 산책하기 좋다.\n침실 3개, 화장실 2개가 있다.\n이 중에서 내 방에는 안락한 소파와 조명이 있는데, 혼자 있는걸 좋아해서 내 방에서 시간을 많이 보낸다.\n\n\nI live in a pretty cozy studio apartment, and I love this place because it feels like my own personal space. It’s not that big, but it’s just the right size for me. I have a small kitchen area, a living room that doubles as my bedroom, and a bathroom. living room에는 안락한 소파와 조명이 있는데, 이곳에 앉아 있으면 편안함을 느낍니다. 이것들이 없었다면 제 집은 정말 단조로웠을 거예요. so, even though it’s small, it has everything I need to feel at home.\n\n\nQ. Can you tell me about the household appliances in your house? What is your favorite one among them? Why do you like it most?\n\nMP: 로봇 청소기가 자동으로 청소해줘서 좋아한다.\n\n\nQ. Tell me about the house or apartment you lived in when you were a child. How was it different from the one you live in now? What are the similarities and differences?\n\n옛날: 시골 / 부모님과 살았음 / 이웃들과 가까웠음\n지금: 도시 / 부모님과 살고 있음 / 이웃들과 친하게 안 지냄\n\n\n10. Now, tell me about the problems that happen at your home. What are those problems? Why do they occur? How do people deal with those problems? How do you personally deal with those problems?\n\n쓰레기를 제때 버리지 않아서 벌레가 생겼다.\n벌레들이 짝찟기를 하며 날라다니는 모습이 역겨웠다.\n쓰레기를 버리고 버려도 몇 주 동안 벌레가 계속 나왔다.\n요즘에는 벌레가 생기지 않도록 미리미리 쓰레기를 버리고 있다.\n\n\n\n영화/공연/콘서트/음악\nQ1. What is your favorite genre of movies? Why do you like those types of movies? (당신이 좋아하는 영화 장르는 무엇인가요? 왜 그 장르를 좋아하나요?)\n\nMP: 요즘 가장 좋아하는 장르는 뮤지컬 이다. 뮤지컬 영화는 사람의 감정을 깊게 느낄 수 있어서 좋다.\n틱틱붐, 라라랜드, 위대한 쇼맨 같은 뮤지컬 영화가 좋다.\n\n\nQ2. I’d like you to tell me about one of the most memorable movies you’ve seen. (당신이 본 영화 중 가장 기억에 남는 영화는 무엇인가요?)\n\nMP: 가장 기억에 남는 영화는 ’라라랜드’이다.\n이 영화는 뮤지컬 영화로, 사랑과 꿈을 쫓는 두 주인공의 이야기를 다룬다.\n영화의 음악과 춤, 그리고 감정 표현이 정말 인상적이었다.\n\n\nQ3. Tell me about a time when you went to listen to some live music. (라이브 음악을 들으러 갔을 때에 대해 말해주세요.)\n\nQ4. Could you compare the movies made today to movies you saw while you were growing up? (요즘 영화와 당신이 자라날 때 보았던 영화를 비교해주세요.)\n\nQ5. How have the performances in your country changed or developed over the last several years? (지난 몇 년간 당신의 나라의 공연들은 어떻게 변해왔나요?)\n\n\n공원가기 캠핑하기\nQ1. You indicated that you like to go to parks. Tell me about one of the parks that you often visit. What does it look like? (자주 가는 공원에 대해 말해주세요. 어떻게 생겼나요?)\n\n집앞 공원에 자주 가고, 조명이 잘 되어 있고, 나무가 많다.\n\n\nQ3. Do you have any memorable experience when you went camping? When was it? Who did you go with? What happened? (캠핑을 갔을 때 기억에 남는 경험이 있나요?)\n\n\n\n\nQ4. I would like to know things you usually bring when you go camping. Why do you take those? (캠핑갈 때 주로 가져가는 것들에 대해 말해주세요.)\n\n최소한의 짐만 챙긴다.\n세면도구와 갈아입을 옷 정도만 챙긴다.\n나머지는 빌린다.\n\n\nQ5. Tell me about your most memorable experience that have happened at the park. (공원에서 발생했던 당신의 가장 기억에 남는 경험에 대해 말해주세요.)\n\n공원에서 조깅을 하고 있었는데 갑자기 모르는 외국인이 다가와서 말을 걸었다.\n그 사람은 나에게 길을 물어봤고, 나는 영어를 잘 못해서 대답을 못했다.\n\n\n\n조깅 걷기 운동을 전혀 하지 않음\nQ1. Where do you often go to for jogging? Describe your favorite place for jogging in detail. (주로 어디로 조깅을 하러 가나요? 그곳을 자세히 묘사해주세요.)\n\n집 앞 공원에서 조깅을 한다.\n공원은 나무가 많고, 조깅하기에 좋은 길이 있다.\n밤에는 조명이 켜져서 안전하게 조깅할 수 있다.\n\n\nQ2. How did you get into jogging initially? Tell me about your motivation to try and continue. (처음 조깅을 시작하게 된 계기는 무엇인가요? 동기에 대해 말해주세요.)\n\n군 시절에 체력 단련을 위해 조깅을 시작했다.\n서울 공항에서 복역했고, 그 곳에는 조깅을 하기 좋은 장소가 있었다.\n매일 저녁 자유 체력 단련 시간에 혼자 조깅을 하거나, 가끔 동기나 후임들을 끌고 나가서 조깅을 했다.\n전역을 한 지금도 계속 조깅을 하고 있다.\n\n\nQ3. Please describe a memorable experience you had while walking. Explain what happened and why it was so memorable. (걷는 동안 있었던 기억에 남는 경험을 묘사해주세요.)\n\n군 시절 조깅 / 걷기 중에 미군 부대 근처를 지나고 있었는데, 갑자기 미군들이 내쪽으로 인사를 했다.\n그날은 휴일이였고, 그들은 자식들과 함께 산책을 하는 것처럼 보였다.\n그때 나는 영어를 할줄 몰라서 그냥 경례를 하고 빠르게 지나갔다.\n\n\nQ4. What do you have to consider when you go jogging? What can you do to avoid an injury? (조깅하러 갈 때 어떤 걸 고려하나요? 부상을 피하기 위해 무엇을 할 수 있나요?)\n\n먼저 나는 주로 야간에 조깅을 하기 때문에, 주변이 잘 보이는지 확인한다.\n또한 운동화는 반드시 편한 것을 입고, 몸에 맞는 옷을 입는다.\n그날의 날씨와 미세먼지도 확인한다.\n\n\n\n집에서 보내는 휴가\nQ1. Most people want to travel during vacations. Tell me why you prefer staying at home. What makes your vacation at home enjoyable? (대부분 여행을 원하는데, 당신이 집에 머무는 걸 더 좋아하는 이유는 무엇인가요?)\n\n나는 사람이 많은 곳에 오래 있으면 어지럽다.\n집에서는 할 수 있는게 꽤 많다.\n주로 집에서 영화나 드라마를 본다.\n\n\nQ3. I would like to know about the memorable vacation you have spent at home. What did you do and who were you with? Why was it so memorable? (집에서 보낸 휴가 중 가장 기억에 남는 휴가에 대해 말해주세요.)\n\n지난 크리스마스에 친한 사람들과 집에 모여서 축구 경기를 관람했다. (물론 이건 구라다)\n\n\nQ4. Why do you think people need vacations? What purposes do you think vacations have for different people? (사람들에게 왜 휴가가 필요하다고 생각하나요? 휴가의 목적은 무엇일까요?)\n\n새로운 환경과 경험은 더 많은 집중력과 창의력을 가져다 준다고 생각한다.\n휴가는 일상에서 벗어나서 재충전할 수 있는 기회라고 생각한다.\n사람마다 휴가의 목적은 다르지만, 대부분은 스트레스를 해소하고 새로운 경험을 찾기 위해 휴가를 떠나는 것 같다.\n\n\n\n국내여행 해외여행\nQ1. What are some things that you do to prepare for trips? (여행을 준비할 때 당신이 하는 일들은 무엇인가요?)\n\n짐은 최소한으로 싸는 편이라 세면도구와 갈아입을 정도만 챙긴다.\n여행의 묘미는 예측하지 못한 일 속에서 즐거움을 찾는 것이라고 생각한다.\n위생을 중요하게 생각해서, ~는 꼭 챙긴다.\n\n\nQ2. You indicated that you enjoy domestic travel. Where do you like to visit? Describe the place and why you love to go there. (국내 여행지 중 어디를 좋아하나요? 그곳을 묘사하고 왜 좋아하는지 설명해주세요.)\n\n바다를 좋아하기 때문에 섬으로 여행 가는 것을 좋아한다.\n경치가 좋고, 주변에 좋은 음식점이 많아서 좋다.\n\n\nQ4. Unexpected things can happen while you are traveling. Talk about an unforgettable happening on a trip. (여행 중 겪은 잊지 못할 일에 대해 말해주세요.)\n\n1 시간이나 기다렸는데 버스가 안왔다.\n주변에 아무도 없었고, 핸드폰 배터리도 없었고, 날씨는 추웠다.\n무작정 기다리다가 결국, 사람이 많은 곳을 찾아서 무작정 걸었다.",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/02.html#smart-전략",
    "href": "posts/04_archives/opic/notes/02.html#smart-전략",
    "title": "Tips",
    "section": "SMART 전략",
    "text": "SMART 전략\n\nS(start by stating your main point/idea): 주제나 요점 먼저 말하기 (main point1)\nM(mention a few example): 연관된 사례나 예시 제시하여 주제 설명 (body)\nA(address why your MP is important by returning to it): MP가 중요한 이유를 설명하며 주제로 돌아와 그 중요성 강조\nR(Reflect on what you’ve learned or realized): 배운 점이나 깨달은 점 반영. conclusion 가기 전에 언급\nT(tie everything back yo your main point): 모든 내용을 MP에 연결시켜 마무리\n\n(억지로 붙여넣네)",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/02.html#main-point",
    "href": "posts/04_archives/opic/notes/02.html#main-point",
    "title": "Tips",
    "section": "Main point",
    "text": "Main point\n\n설명/묘사\n\n메인포인트는 간결하고 깔끔하게 해야 한다. 말하고 싶은 내용을 메인 포인트로 말한 뒤, 그것에 대해 내가 어떻게 느끼는지, 왜 그렇게 느끼는지 간단한 질문이 많기 때문에 너무 장황하게 설명할 필요 없이 짧고 간결하게 답변하는 것도 좋다.\n\n습관\n\n행동적인 메인 포인트가 있어야 한다.\n질문 : 당신은 은행에 갈 때 보통 무엇을 하나요? 대답 : 전 은행에 갈 때마다 지갑을 꼭 챙겨요.\nHibit: whenever _\n\n과거 경험\n\n시간의 순서로 줄줄이 말하는 것 보다 경험의 클라이맥스 부분을 메인 포인트로 시작하는 게 좋다. 예를 들어 며칠전 카페에 가서 커피를 마시다가 화장실을 가려고 하는데 어떤 여자와 부딪쳐서 음료를 쏟았고 매우 당황했다는 이야기는 할 때, 사실과 느낀 점을 몇 마디로 정리해서 먼저 말하고 그다음 주절주절 자세하게 말하는 게 좋다.\n\n비교- 시간 비교\n\n현재에만 집중해서 메인 포인트를 말해야 한다. 질문 : 당신의 음악 선호도가 과거부터 지금까지 어떻게 변했나요? 답변 : 전 요즘 KPOP을 듣는게 좋아요. 왜냐면 엄청 신나거든요. 이후 예전에는 어떤 걸 좋아했고 왜 좋아했는데 어떤 계기로 요즘에는 바뀌었는지 등등을 설명하면 된다.\n\n비교- 일반 비교\n\n특별한 포인트가 있다기 보다는 비교하는 대상에 대해서 일반적인 메인 포인트를 딱 이야기하면 된다. 질문 : 당신이 좋아하는 가수 2명을 비교해보세요 답변 : 저는 A와 B를 좋아해요. 둘을 각각 다른 이유로 좋아하는데 왜 그런지 말해 줄게요.",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/02.html#role-play",
    "href": "posts/04_archives/opic/notes/02.html#role-play",
    "title": "Tips",
    "section": "Role play",
    "text": "Role play\n\n11\n\n대충 내용 설명\n첫 번째 질문\n상대방 말 보여주기\nreally?\n나를 포함시키기\n두번째 질문 하기\n상대방 말 보여주기\nreally?\n나를 포함시키기\n세번째 질문 하기\n상대방 말 보여주기\n나를 포함시키기",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/02.html#그-외",
    "href": "posts/04_archives/opic/notes/02.html#그-외",
    "title": "Tips",
    "section": "그 외",
    "text": "그 외\n\n한국 고유말 한국말로 쓰면 감점이 있다고 함. 걍 한국말 쓰지 말자.\nthere’s this []: 어떤 []가 있다.\n어려운 표현 쓸거면 비슷한 쉬운 표현도 뒤에 같이 써서 의미를 명확히 전달할 것\nand 조심해서 써라\n15번은 그냥 스킵해라\nyou expression 많이 써라\n과거 이야기 할 때 main point 먼저 말하고 develope 해라\n생각하고 대답하는 척 하기\n보통 다른 사람 and i 순. i and 다른 사람은 어색\nmoreover, furthermore, in addition은 대화에는 어색함\nwhich because really since 로 길게 말할 수 있음",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/notes/02.html#footnotes",
    "href": "posts/04_archives/opic/notes/02.html#footnotes",
    "title": "Tips",
    "section": "각주",
    "text": "각주\n\n\nWhat, Feeling, Why↩︎",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html",
    "href": "posts/04_archives/opic/index.html",
    "title": "OPIc 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: 2025-06-16\n        종료일: 2025-07-07\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html#details",
    "href": "posts/04_archives/opic/index.html#details",
    "title": "OPIc 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html#tasks",
    "href": "posts/04_archives/opic/index.html#tasks",
    "title": "OPIc 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html#참고-자료",
    "href": "posts/04_archives/opic/index.html#참고-자료",
    "title": "OPIc 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html#why-failed",
    "href": "posts/04_archives/opic/index.html#why-failed",
    "title": "OPIc 준비",
    "section": "Why failed?",
    "text": "Why failed?\n나는 암기식 공부가 더 편하다.\n토익 스피킹을 준비하자.",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/opic/index.html#related-posts",
    "href": "posts/04_archives/opic/index.html#related-posts",
    "title": "OPIc 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/notes/01.html#돌발",
    "href": "posts/01_projects/toeic_speaking/notes/01.html#돌발",
    "title": "돌발 script 정리",
    "section": "돌발",
    "text": "돌발\n\n재활용\n\n집에서 하는 재활용 과정\n\nhow do you recycle at home? when and how often do you recycle? describe each step of the process from beginning to end\n\n재활용을 하며 기억에 남는 경험\n\ntell me about a memorable experience you had while recycling. what happened? what made it so memorable? tell me about it in as much detail as possible.\n\n우리나라의 재활용\n\ntell me about recycling in your country. what kind of items do people usually recycle? please describe the recycling system in your country in detail.\n\n\n\n\n약속\n\n약속을 잡는 경향\n\npeople set up appointments for various reasons. what sort of appointments do you usually make, social or otherwise? who do you usually meet with? where do you usually meet them? give me as many details as possible\n\n기억에 남는 약속\n\ntell me about the most memorable appointment you have ever had. what kind of appointment was it? who did you meet? what did you do? did anything unexpected or interesting occur? why was it so memorable?\n\n약속 접기 전 연락 과정\n\nwhen you want to meet up with someone, how do you get in touch with him or her? do you make phone calls, send an e-mail, or do something else? how does the exchange usually go? tell me about the process from beginning to end\n\n\n\n\n은행\n\n우리나라의 은행\n\ndescribe the banks in your country. where are they usually located, and what are they like? what hours are banks typically open there? give me as many details as possible\n\n과거와 현재의 은행 비교\n\nhave there been any changes to the banks in your country since you were a child? how were they in the past? how are they now? please describe the changes in detail\n\n은행에서 겪은 문제\n\nhave you ever experienced any problems at a bank? for instance, sometimes ATMs malfunction. what sort of problem did you face, and how did you deal with it?\n\n\n\n\n휴대폰\n\n\n지형 야외활동\n\n우리나라의 지형\n\ni would like to know about the geographic features of your country. what makes them different from other countries? please describe them in as much detail as possible.\n\n기억에 남는 야회 활동 경험\n\ndescribe the most memorable experience you have had outdoors. what is a beautiful place you have been to? please provide s many details as possible\n\n우리나라 사람들이 즐겨하는 야외 활동\n\nwhat kind of outdoor activities do people in your country do? do they enjoy things like jogging, cycling, or hiking? why do the like to do those activities?\n\n\n\n\n모임, 기념일\n\n\n외식 음식\n\n자주가는 식당\n\ni would like to know about a restaurant you often visit. what kind of dishes does it serve? what do you like about the restaurant? what does it look like?\n\n식당에서 겪은 경험\n\nhave you ever had a special experience at a restaurant? who were you with? what happened? tell me about the experience in detail, and explain what made it memorable.\n\n유명한 한국 요리\n\ntell me about the most famous dish of your country. what are the main ingredients in it? do you know how yo make it? what is special about the dish?\n\n\n\n\n기술\n\n과거와 현재의 기술 비교\n\ntechnology is advancing more rapidly than ever. can you tell me about the way technology has been changing? what changes have occurred since you were child? provide me with as many details as possible.\n\n기술과 관련되어 생긴 문제\n\nhave you ever experienced a problem related to technology? for instance, sometimes a device does not work properly or is difficult to use. describe your technological problem in detail. how did you handle it?\n\n우리나라에서 인기 있는 기술\n\ntell me about the technologies that are popular in your country. which technology do people use the most there? what is it used for? can you tell me why they like to use it?\n\n\n\n\n날씨 계절\n\n우리나라의 계절\n\ni would like to know about the seasons in your country. how many seasons are there? how are they different? what is the weather like in each season?\n\n과거와 현재의 날씨 비교\n\ndo you think that the weather has changed over the past few years? what was it like in the past? how has it changed? please describe the differences in detail.\n\n이상 기후로 인해 겪은 경험\n\nhave you ever had a memorable or unexpected experience because of unusual weather conditions? when was it? what happened? tell me about it in as much detail as possible\n\n\n\n\n휴일",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic_speaking/notes/01.html#roleplaying",
    "href": "posts/01_projects/toeic_speaking/notes/01.html#roleplaying",
    "title": "돌발 script 정리",
    "section": "roleplaying",
    "text": "roleplaying\n\n면접관에게 질문하기\n\ni live in an apartment. please ask me three or four questions about the place i live in.\ni enjoy traveling too. please ask me three or four questions about traveling.\ni live in Canada. ask me three or four questions about the geographic features of my country.\n\n\n\n주어진 상황에서 직접 질문하기\n\ni’m going yo give you a situation to act out. pretend you are overseas on vacation and you need a car to get around, so you have gone to a car rental agency. ask the agent three or four questions about renting a car.\ni am going to give you a situation to act out. imagine that you have gone to a store to buy new furniture. ask the salesperson three or four questions about the furniture you are looking for\n\n\n\n주어진 상황에서 전화로 질문하기\n\ni would like to give you a situation to act out. imagine you are planning a vacation. call a travel agency, and ask three or four questions about potential destinations and itineraries.\n\n\n\n상황 설명하고 대안 제시하기\n\ni have a problem for you to solve. you found out you purchased the wrong tickets at the movie theater. talk to the person at the ticket window about your situation and offer suggestions to solve the problem.\nthere is a problem i’d like you to solve. you’ve ordered some furniture and its just been delivered. however, the furniture thats arrived is not what you ordered. call the furniture store to explain the situation and suggest some alternatives to the problem.\nthere is a problem i need you to solve. pretend that you’ve gone to the library to look for a book, but the one you want has been checked out. explain the situation to a librarian and offer two or theree alternatives to the problem.",
    "crumbs": [
      "PARA",
      "Projects",
      "토익 스피킹 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#개요",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#개요",
    "title": "분류 - 결정 트리",
    "section": "개요",
    "text": "개요\n\n결정 노드가 많아지면 과적합이 발생할 수 있음\n가능한 한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 규칙을 정해야 함\n균일하게 데이터 세트를 구성할 수 있도록 분할하는 것이 필요\n균일도만 신경쓰면 되기 때문에 전처리 작업이 필요 없음",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#파라미터",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#파라미터",
    "title": "분류 - 결정 트리",
    "section": "파라미터",
    "text": "파라미터\n\nmin_samples_split: 노드를 분할하기 위한 최소한의 샘플 수.\nmin_samples_leaf: 말단 노드가 되기 위한 최소한의 샘플 수. 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요\nmax_features: 분할을 고려할 feature의 수. default는 None으로 모든 feature를 고려함\nmax_depth\nmax_leaf_nodes",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#시각화",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#시각화",
    "title": "분류 - 결정 트리",
    "section": "시각화",
    "text": "시각화\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport graphviz\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\ndt_clf = DecisionTreeClassifier()\n\niris_data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2)\n\ndt_clf.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\nexport_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport numpy as np\n\nfor name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n    print(f'{name}: {value:.3f}')\nsns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)\n\nsepal length (cm): 0.000\nsepal width (cm): 0.017\npetal length (cm): 0.544\npetal width (cm): 0.439",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/01.html#examples",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/01.html#examples",
    "title": "분류 - 결정 트리",
    "section": "examples",
    "text": "examples\n\nimport pandas as pd\n\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] &gt; 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7352 entries, 0 to 7351\nColumns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)\ndtypes: float64(561)\nmemory usage: 31.5 MB\n\n\n\ny_train['action'].value_counts()\n\naction\n6    1407\n5    1374\n4    1286\n1    1226\n2    1073\n3     986\nName: count, dtype: int64\n\n\n\ndefault 파라미터 예측\n\nfrom sklearn.metrics import accuracy_score\n\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\npred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.8608754665761792\n\n\n\n\n하이퍼파라미터 최적화\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'max_depth': [6, 8, 10, 12, 16, 20, 24],\n    'min_samples_split': [16]\n}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy',cv=5, verbose=1)\ngrid_cv.fit(X_train, y_train)\n\ncv_results_df = pd.DataFrame(grid_cv.cv_results_)\ncv_results_df[['param_max_depth', 'mean_test_score']]\ncv_results_df\n\nFitting 5 folds for each of 7 candidates, totalling 35 fits\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_min_samples_split\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n1.730651\n0.071066\n0.004747\n0.000612\n6\n16\n{'max_depth': 6, 'min_samples_split': 16}\n0.812373\n0.870156\n0.834694\n0.865306\n0.869388\n0.850383\n0.023090\n2\n\n\n1\n2.199078\n0.058100\n0.004974\n0.000507\n8\n16\n{'max_depth': 8, 'min_samples_split': 16}\n0.812373\n0.831407\n0.842177\n0.870068\n0.890476\n0.849300\n0.027790\n3\n\n\n2\n2.591271\n0.101800\n0.004741\n0.000625\n10\n16\n{'max_depth': 10, 'min_samples_split': 16}\n0.813732\n0.819850\n0.837415\n0.888435\n0.896599\n0.851206\n0.034711\n1\n\n\n3\n2.873773\n0.100605\n0.005099\n0.001333\n12\n16\n{'max_depth': 12, 'min_samples_split': 16}\n0.783821\n0.817811\n0.853741\n0.885714\n0.882313\n0.844680\n0.039008\n5\n\n\n4\n3.446341\n0.244669\n0.004722\n0.000739\n16\n16\n{'max_depth': 16, 'min_samples_split': 16}\n0.803535\n0.810333\n0.842177\n0.878231\n0.887075\n0.844270\n0.034062\n6\n\n\n5\n3.307252\n0.254648\n0.004365\n0.000410\n20\n16\n{'max_depth': 20, 'min_samples_split': 16}\n0.794018\n0.811693\n0.831293\n0.887075\n0.891837\n0.843183\n0.039608\n7\n\n\n6\n3.243996\n0.227535\n0.004434\n0.000333\n24\n16\n{'max_depth': 24, 'min_samples_split': 16}\n0.796737\n0.809653\n0.846259\n0.882313\n0.895238\n0.846040\n0.038707\n4\n\n\n\n\n\n\n\n\nbest_df_clf = grid_cv.best_estimator_\n\nftr_importances_values = best_df_clf.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\nsns.barplot(x=ftr_top20, y=ftr_top20.index)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#voting",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#voting",
    "title": "분류 - 앙상블",
    "section": "voting",
    "text": "voting\n\n서로 다른 알고리즘이 결합. 분류에서는 voting1으로 결정\n\n\nExample\n\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\n\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.9210526315789473\n\n\n\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n\nLogisticRegression 정확도: 0.9298\nKNeighborsClassifier 정확도: 0.9211\n\n\n\n반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#bagging",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#bagging",
    "title": "분류 - 앙상블",
    "section": "bagging",
    "text": "bagging\n\n같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting2으로 결정\n\n\nRandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] &gt; 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n\n\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.9178825924669155",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#footnotes",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#footnotes",
    "title": "분류 - 앙상블",
    "section": "각주",
    "text": "각주\n\n\nhard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.↩︎\nhard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#boosting",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#boosting",
    "title": "분류 - 앙상블",
    "section": "boosting",
    "text": "boosting\n\nGBM\n\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n\n0.939, 701.6343066692352초\n\n아주 오래 걸림.\n\n\n\nXGBoost\n\n결손값을 자체 처리할 수 있다.\n조기 종료 기능이 있다.\n자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\npython xgboost\n\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\n\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n\n\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n\n\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n\n[0] train-logloss:0.61277   eval-logloss:0.58601\n[1] train-logloss:0.57664   eval-logloss:0.55582\n[2] train-logloss:0.54304   eval-logloss:0.52806\n[3] train-logloss:0.51255   eval-logloss:0.50298\n[4] train-logloss:0.48471   eval-logloss:0.47989\n[5] train-logloss:0.45884   eval-logloss:0.45674\n[6] train-logloss:0.43517   eval-logloss:0.43736\n[7] train-logloss:0.41363   eval-logloss:0.42013\n[8] train-logloss:0.39341   eval-logloss:0.40354\n[9] train-logloss:0.37494   eval-logloss:0.38841\n[10]    train-logloss:0.35744   eval-logloss:0.37480\n[11]    train-logloss:0.34054   eval-logloss:0.35955\n[12]    train-logloss:0.32442   eval-logloss:0.34527\n[13]    train-logloss:0.30963   eval-logloss:0.32976\n[14]    train-logloss:0.29638   eval-logloss:0.32018\n[15]    train-logloss:0.28306   eval-logloss:0.30855\n[16]    train-logloss:0.27060   eval-logloss:0.29773\n[17]    train-logloss:0.25894   eval-logloss:0.28763\n[18]    train-logloss:0.24842   eval-logloss:0.27760\n[19]    train-logloss:0.23861   eval-logloss:0.27116\n[20]    train-logloss:0.22890   eval-logloss:0.26281\n[21]    train-logloss:0.21995   eval-logloss:0.25627\n[22]    train-logloss:0.21183   eval-logloss:0.25092\n[23]    train-logloss:0.20363   eval-logloss:0.24393\n[24]    train-logloss:0.19606   eval-logloss:0.23852\n[25]    train-logloss:0.18881   eval-logloss:0.23108\n[26]    train-logloss:0.18165   eval-logloss:0.22460\n[27]    train-logloss:0.17502   eval-logloss:0.21895\n[28]    train-logloss:0.16889   eval-logloss:0.21248\n[29]    train-logloss:0.16323   eval-logloss:0.20904\n[30]    train-logloss:0.15760   eval-logloss:0.20456\n[31]    train-logloss:0.15228   eval-logloss:0.20042\n[32]    train-logloss:0.14728   eval-logloss:0.19496\n[33]    train-logloss:0.14254   eval-logloss:0.19157\n[34]    train-logloss:0.13775   eval-logloss:0.18653\n[35]    train-logloss:0.13340   eval-logloss:0.18317\n[36]    train-logloss:0.12934   eval-logloss:0.17926\n[37]    train-logloss:0.12562   eval-logloss:0.17500\n[38]    train-logloss:0.12162   eval-logloss:0.17143\n[39]    train-logloss:0.11812   eval-logloss:0.16801\n[40]    train-logloss:0.11493   eval-logloss:0.16522\n[41]    train-logloss:0.11171   eval-logloss:0.16259\n[42]    train-logloss:0.10874   eval-logloss:0.16035\n[43]    train-logloss:0.10593   eval-logloss:0.15740\n[44]    train-logloss:0.10316   eval-logloss:0.15462\n[45]    train-logloss:0.10017   eval-logloss:0.15140\n[46]    train-logloss:0.09748   eval-logloss:0.14985\n[47]    train-logloss:0.09515   eval-logloss:0.14732\n[48]    train-logloss:0.09280   eval-logloss:0.14613\n[49]    train-logloss:0.09039   eval-logloss:0.14424\n[50]    train-logloss:0.08814   eval-logloss:0.14301\n[51]    train-logloss:0.08583   eval-logloss:0.14055\n[52]    train-logloss:0.08369   eval-logloss:0.13767\n[53]    train-logloss:0.08167   eval-logloss:0.13494\n[54]    train-logloss:0.07971   eval-logloss:0.13295\n[55]    train-logloss:0.07782   eval-logloss:0.13025\n[56]    train-logloss:0.07603   eval-logloss:0.12777\n[57]    train-logloss:0.07431   eval-logloss:0.12528\n[58]    train-logloss:0.07265   eval-logloss:0.12285\n[59]    train-logloss:0.07107   eval-logloss:0.12062\n[60]    train-logloss:0.06952   eval-logloss:0.11986\n[61]    train-logloss:0.06804   eval-logloss:0.11877\n[62]    train-logloss:0.06626   eval-logloss:0.11728\n[63]    train-logloss:0.06490   eval-logloss:0.11527\n[64]    train-logloss:0.06361   eval-logloss:0.11325\n[65]    train-logloss:0.06205   eval-logloss:0.11093\n[66]    train-logloss:0.06085   eval-logloss:0.10911\n[67]    train-logloss:0.05957   eval-logloss:0.10839\n[68]    train-logloss:0.05846   eval-logloss:0.10659\n[69]    train-logloss:0.05701   eval-logloss:0.10541\n[70]    train-logloss:0.05598   eval-logloss:0.10382\n[71]    train-logloss:0.05487   eval-logloss:0.10325\n[72]    train-logloss:0.05390   eval-logloss:0.10238\n[73]    train-logloss:0.05262   eval-logloss:0.10137\n[74]    train-logloss:0.05173   eval-logloss:0.09989\n[75]    train-logloss:0.05078   eval-logloss:0.09905\n[76]    train-logloss:0.04998   eval-logloss:0.09774\n[77]    train-logloss:0.04902   eval-logloss:0.09725\n[78]    train-logloss:0.04813   eval-logloss:0.09723\n[79]    train-logloss:0.04728   eval-logloss:0.09558\n[80]    train-logloss:0.04655   eval-logloss:0.09499\n[81]    train-logloss:0.04558   eval-logloss:0.09360\n[82]    train-logloss:0.04481   eval-logloss:0.09289\n[83]    train-logloss:0.04411   eval-logloss:0.09233\n[84]    train-logloss:0.04323   eval-logloss:0.09104\n[85]    train-logloss:0.04244   eval-logloss:0.09051\n[86]    train-logloss:0.04163   eval-logloss:0.08929\n[87]    train-logloss:0.04105   eval-logloss:0.08824\n[88]    train-logloss:0.04029   eval-logloss:0.08709\n[89]    train-logloss:0.03970   eval-logloss:0.08667\n[90]    train-logloss:0.03908   eval-logloss:0.08651\n[91]    train-logloss:0.03840   eval-logloss:0.08554\n[92]    train-logloss:0.03790   eval-logloss:0.08459\n[93]    train-logloss:0.03717   eval-logloss:0.08382\n[94]    train-logloss:0.03655   eval-logloss:0.08279\n[95]    train-logloss:0.03609   eval-logloss:0.08246\n[96]    train-logloss:0.03551   eval-logloss:0.08162\n[97]    train-logloss:0.03503   eval-logloss:0.08062\n[98]    train-logloss:0.03438   eval-logloss:0.07993\n[99]    train-logloss:0.03390   eval-logloss:0.07963\n[100]   train-logloss:0.03329   eval-logloss:0.07899\n[101]   train-logloss:0.03284   eval-logloss:0.07873\n[102]   train-logloss:0.03245   eval-logloss:0.07871\n[103]   train-logloss:0.03202   eval-logloss:0.07846\n[104]   train-logloss:0.03158   eval-logloss:0.07822\n[105]   train-logloss:0.03122   eval-logloss:0.07799\n[106]   train-logloss:0.03076   eval-logloss:0.07690\n[107]   train-logloss:0.03032   eval-logloss:0.07710\n[108]   train-logloss:0.02993   eval-logloss:0.07759\n[109]   train-logloss:0.02950   eval-logloss:0.07750\n[110]   train-logloss:0.02908   eval-logloss:0.07647\n[111]   train-logloss:0.02867   eval-logloss:0.07550\n[112]   train-logloss:0.02831   eval-logloss:0.07529\n[113]   train-logloss:0.02787   eval-logloss:0.07401\n[114]   train-logloss:0.02750   eval-logloss:0.07395\n[115]   train-logloss:0.02712   eval-logloss:0.07300\n[116]   train-logloss:0.02674   eval-logloss:0.07235\n[117]   train-logloss:0.02635   eval-logloss:0.07196\n[118]   train-logloss:0.02599   eval-logloss:0.07107\n[119]   train-logloss:0.02565   eval-logloss:0.07043\n[120]   train-logloss:0.02536   eval-logloss:0.07095\n[121]   train-logloss:0.02505   eval-logloss:0.07092\n[122]   train-logloss:0.02473   eval-logloss:0.07007\n[123]   train-logloss:0.02444   eval-logloss:0.07007\n[124]   train-logloss:0.02418   eval-logloss:0.07058\n[125]   train-logloss:0.02393   eval-logloss:0.07069\n[126]   train-logloss:0.02363   eval-logloss:0.07066\n[127]   train-logloss:0.02333   eval-logloss:0.06986\n[128]   train-logloss:0.02305   eval-logloss:0.06984\n[129]   train-logloss:0.02277   eval-logloss:0.06906\n[130]   train-logloss:0.02252   eval-logloss:0.06911\n[131]   train-logloss:0.02224   eval-logloss:0.06825\n[132]   train-logloss:0.02198   eval-logloss:0.06751\n[133]   train-logloss:0.02175   eval-logloss:0.06699\n[134]   train-logloss:0.02155   eval-logloss:0.06748\n[135]   train-logloss:0.02138   eval-logloss:0.06752\n[136]   train-logloss:0.02114   eval-logloss:0.06747\n[137]   train-logloss:0.02096   eval-logloss:0.06682\n[138]   train-logloss:0.02075   eval-logloss:0.06686\n[139]   train-logloss:0.02057   eval-logloss:0.06663\n[140]   train-logloss:0.02032   eval-logloss:0.06654\n[141]   train-logloss:0.02013   eval-logloss:0.06599\n[142]   train-logloss:0.01995   eval-logloss:0.06647\n[143]   train-logloss:0.01972   eval-logloss:0.06640\n[144]   train-logloss:0.01950   eval-logloss:0.06636\n[145]   train-logloss:0.01925   eval-logloss:0.06568\n[146]   train-logloss:0.01910   eval-logloss:0.06597\n[147]   train-logloss:0.01891   eval-logloss:0.06518\n[148]   train-logloss:0.01876   eval-logloss:0.06547\n[149]   train-logloss:0.01854   eval-logloss:0.06481\n[150]   train-logloss:0.01838   eval-logloss:0.06530\n[151]   train-logloss:0.01824   eval-logloss:0.06490\n[152]   train-logloss:0.01806   eval-logloss:0.06506\n[153]   train-logloss:0.01789   eval-logloss:0.06519\n[154]   train-logloss:0.01771   eval-logloss:0.06496\n[155]   train-logloss:0.01762   eval-logloss:0.06516\n[156]   train-logloss:0.01742   eval-logloss:0.06457\n[157]   train-logloss:0.01729   eval-logloss:0.06484\n[158]   train-logloss:0.01716   eval-logloss:0.06408\n[159]   train-logloss:0.01698   eval-logloss:0.06389\n[160]   train-logloss:0.01679   eval-logloss:0.06333\n[161]   train-logloss:0.01671   eval-logloss:0.06355\n[162]   train-logloss:0.01657   eval-logloss:0.06357\n[163]   train-logloss:0.01645   eval-logloss:0.06321\n[164]   train-logloss:0.01631   eval-logloss:0.06317\n[165]   train-logloss:0.01621   eval-logloss:0.06322\n[166]   train-logloss:0.01604   eval-logloss:0.06270\n[167]   train-logloss:0.01594   eval-logloss:0.06232\n[168]   train-logloss:0.01587   eval-logloss:0.06253\n[169]   train-logloss:0.01572   eval-logloss:0.06206\n[170]   train-logloss:0.01564   eval-logloss:0.06167\n[171]   train-logloss:0.01554   eval-logloss:0.06097\n[172]   train-logloss:0.01547   eval-logloss:0.06117\n[173]   train-logloss:0.01534   eval-logloss:0.06110\n[174]   train-logloss:0.01526   eval-logloss:0.06115\n[175]   train-logloss:0.01516   eval-logloss:0.06047\n[176]   train-logloss:0.01502   eval-logloss:0.06018\n[177]   train-logloss:0.01493   eval-logloss:0.06022\n[178]   train-logloss:0.01482   eval-logloss:0.06012\n[179]   train-logloss:0.01475   eval-logloss:0.05975\n[180]   train-logloss:0.01468   eval-logloss:0.05968\n[181]   train-logloss:0.01461   eval-logloss:0.05988\n[182]   train-logloss:0.01454   eval-logloss:0.05952\n[183]   train-logloss:0.01447   eval-logloss:0.05945\n[184]   train-logloss:0.01437   eval-logloss:0.05952\n[185]   train-logloss:0.01428   eval-logloss:0.05933\n[186]   train-logloss:0.01420   eval-logloss:0.05926\n[187]   train-logloss:0.01410   eval-logloss:0.05917\n[188]   train-logloss:0.01404   eval-logloss:0.05883\n[189]   train-logloss:0.01397   eval-logloss:0.05843\n[190]   train-logloss:0.01389   eval-logloss:0.05825\n[191]   train-logloss:0.01382   eval-logloss:0.05821\n[192]   train-logloss:0.01372   eval-logloss:0.05829\n[193]   train-logloss:0.01364   eval-logloss:0.05811\n[194]   train-logloss:0.01358   eval-logloss:0.05808\n[195]   train-logloss:0.01352   eval-logloss:0.05823\n[196]   train-logloss:0.01346   eval-logloss:0.05829\n[197]   train-logloss:0.01340   eval-logloss:0.05823\n[198]   train-logloss:0.01331   eval-logloss:0.05832\n[199]   train-logloss:0.01324   eval-logloss:0.05813\n[200]   train-logloss:0.01317   eval-logloss:0.05811\n[201]   train-logloss:0.01312   eval-logloss:0.05769\n[202]   train-logloss:0.01305   eval-logloss:0.05754\n[203]   train-logloss:0.01296   eval-logloss:0.05764\n[204]   train-logloss:0.01289   eval-logloss:0.05749\n[205]   train-logloss:0.01284   eval-logloss:0.05756\n[206]   train-logloss:0.01279   eval-logloss:0.05772\n[207]   train-logloss:0.01273   eval-logloss:0.05768\n[208]   train-logloss:0.01268   eval-logloss:0.05783\n[209]   train-logloss:0.01263   eval-logloss:0.05752\n[210]   train-logloss:0.01258   eval-logloss:0.05711\n[211]   train-logloss:0.01251   eval-logloss:0.05697\n[212]   train-logloss:0.01243   eval-logloss:0.05662\n[213]   train-logloss:0.01237   eval-logloss:0.05671\n[214]   train-logloss:0.01231   eval-logloss:0.05659\n[215]   train-logloss:0.01226   eval-logloss:0.05629\n[216]   train-logloss:0.01219   eval-logloss:0.05595\n[217]   train-logloss:0.01214   eval-logloss:0.05591\n[218]   train-logloss:0.01208   eval-logloss:0.05580\n[219]   train-logloss:0.01201   eval-logloss:0.05606\n[220]   train-logloss:0.01196   eval-logloss:0.05592\n[221]   train-logloss:0.01190   eval-logloss:0.05601\n[222]   train-logloss:0.01185   eval-logloss:0.05608\n[223]   train-logloss:0.01181   eval-logloss:0.05569\n[224]   train-logloss:0.01176   eval-logloss:0.05559\n[225]   train-logloss:0.01169   eval-logloss:0.05585\n[226]   train-logloss:0.01164   eval-logloss:0.05576\n[227]   train-logloss:0.01158   eval-logloss:0.05549\n[228]   train-logloss:0.01153   eval-logloss:0.05521\n[229]   train-logloss:0.01149   eval-logloss:0.05509\n[230]   train-logloss:0.01145   eval-logloss:0.05493\n[231]   train-logloss:0.01140   eval-logloss:0.05507\n[232]   train-logloss:0.01136   eval-logloss:0.05469\n[233]   train-logloss:0.01132   eval-logloss:0.05500\n[234]   train-logloss:0.01128   eval-logloss:0.05474\n[235]   train-logloss:0.01124   eval-logloss:0.05472\n[236]   train-logloss:0.01120   eval-logloss:0.05490\n[237]   train-logloss:0.01115   eval-logloss:0.05503\n[238]   train-logloss:0.01111   eval-logloss:0.05516\n[239]   train-logloss:0.01107   eval-logloss:0.05524\n[240]   train-logloss:0.01103   eval-logloss:0.05537\n[241]   train-logloss:0.01099   eval-logloss:0.05536\n[242]   train-logloss:0.01096   eval-logloss:0.05568\n[243]   train-logloss:0.01090   eval-logloss:0.05543\n[244]   train-logloss:0.01087   eval-logloss:0.05556\n[245]   train-logloss:0.01083   eval-logloss:0.05519\n[246]   train-logloss:0.01081   eval-logloss:0.05537\n[247]   train-logloss:0.01077   eval-logloss:0.05536\n[248]   train-logloss:0.01072   eval-logloss:0.05549\n[249]   train-logloss:0.01068   eval-logloss:0.05562\n[250]   train-logloss:0.01064   eval-logloss:0.05537\n[251]   train-logloss:0.01061   eval-logloss:0.05555\n[252]   train-logloss:0.01058   eval-logloss:0.05568\n[253]   train-logloss:0.01054   eval-logloss:0.05532\n[254]   train-logloss:0.01051   eval-logloss:0.05508\n[255]   train-logloss:0.01049   eval-logloss:0.05525\n[256]   train-logloss:0.01045   eval-logloss:0.05524\n[257]   train-logloss:0.01042   eval-logloss:0.05537\n[258]   train-logloss:0.01037   eval-logloss:0.05554\n[259]   train-logloss:0.01033   eval-logloss:0.05563\n[260]   train-logloss:0.01030   eval-logloss:0.05528\n[261]   train-logloss:0.01028   eval-logloss:0.05546\n[262]   train-logloss:0.01025   eval-logloss:0.05559\n[263]   train-logloss:0.01022   eval-logloss:0.05558\n[264]   train-logloss:0.01017   eval-logloss:0.05576\n[265]   train-logloss:0.01014   eval-logloss:0.05593\n[266]   train-logloss:0.01011   eval-logloss:0.05558\n[267]   train-logloss:0.01008   eval-logloss:0.05571\n[268]   train-logloss:0.01005   eval-logloss:0.05584\n[269]   train-logloss:0.01002   eval-logloss:0.05561\n[270]   train-logloss:0.00998   eval-logloss:0.05560\n[271]   train-logloss:0.00997   eval-logloss:0.05577\n[272]   train-logloss:0.00992   eval-logloss:0.05574\n[273]   train-logloss:0.00989   eval-logloss:0.05540\n[274]   train-logloss:0.00987   eval-logloss:0.05557\n[275]   train-logloss:0.00984   eval-logloss:0.05570\n[276]   train-logloss:0.00981   eval-logloss:0.05547\n[277]   train-logloss:0.00978   eval-logloss:0.05546\n[278]   train-logloss:0.00973   eval-logloss:0.05543\n[279]   train-logloss:0.00970   eval-logloss:0.05556\n[280]   train-logloss:0.00969   eval-logloss:0.05562\n[281]   train-logloss:0.00966   eval-logloss:0.05528\n\n\n\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x &gt; 0.5 else 0 for x in pred_probs]\n\n\nsklearn xgboost\n\n\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n\n[0] validation_0-logloss:0.61277    validation_1-logloss:0.58601\n[1] validation_0-logloss:0.57664    validation_1-logloss:0.55582\n[2] validation_0-logloss:0.54304    validation_1-logloss:0.52806\n[3] validation_0-logloss:0.51255    validation_1-logloss:0.50298\n[4] validation_0-logloss:0.48471    validation_1-logloss:0.47989\n[5] validation_0-logloss:0.45884    validation_1-logloss:0.45674\n[6] validation_0-logloss:0.43517    validation_1-logloss:0.43736\n[7] validation_0-logloss:0.41363    validation_1-logloss:0.42013\n[8] validation_0-logloss:0.39341    validation_1-logloss:0.40354\n[9] validation_0-logloss:0.37494    validation_1-logloss:0.38841\n[10]    validation_0-logloss:0.35744    validation_1-logloss:0.37480\n[11]    validation_0-logloss:0.34054    validation_1-logloss:0.35955\n[12]    validation_0-logloss:0.32442    validation_1-logloss:0.34527\n[13]    validation_0-logloss:0.30963    validation_1-logloss:0.32976\n[14]    validation_0-logloss:0.29638    validation_1-logloss:0.32018\n[15]    validation_0-logloss:0.28306    validation_1-logloss:0.30855\n[16]    validation_0-logloss:0.27060    validation_1-logloss:0.29773\n[17]    validation_0-logloss:0.25894    validation_1-logloss:0.28763\n[18]    validation_0-logloss:0.24842    validation_1-logloss:0.27760\n[19]    validation_0-logloss:0.23861    validation_1-logloss:0.27116\n[20]    validation_0-logloss:0.22890    validation_1-logloss:0.26281\n[21]    validation_0-logloss:0.21995    validation_1-logloss:0.25627\n[22]    validation_0-logloss:0.21183    validation_1-logloss:0.25092\n[23]    validation_0-logloss:0.20363    validation_1-logloss:0.24393\n[24]    validation_0-logloss:0.19606    validation_1-logloss:0.23852\n[25]    validation_0-logloss:0.18881    validation_1-logloss:0.23108\n[26]    validation_0-logloss:0.18165    validation_1-logloss:0.22460\n[27]    validation_0-logloss:0.17502    validation_1-logloss:0.21895\n[28]    validation_0-logloss:0.16889    validation_1-logloss:0.21248\n[29]    validation_0-logloss:0.16323    validation_1-logloss:0.20904\n[30]    validation_0-logloss:0.15760    validation_1-logloss:0.20456\n[31]    validation_0-logloss:0.15228    validation_1-logloss:0.20042\n[32]    validation_0-logloss:0.14728    validation_1-logloss:0.19496\n[33]    validation_0-logloss:0.14254    validation_1-logloss:0.19157\n[34]    validation_0-logloss:0.13775    validation_1-logloss:0.18653\n[35]    validation_0-logloss:0.13340    validation_1-logloss:0.18317\n[36]    validation_0-logloss:0.12934    validation_1-logloss:0.17926\n[37]    validation_0-logloss:0.12562    validation_1-logloss:0.17500\n[38]    validation_0-logloss:0.12162    validation_1-logloss:0.17143\n[39]    validation_0-logloss:0.11812    validation_1-logloss:0.16801\n[40]    validation_0-logloss:0.11493    validation_1-logloss:0.16522\n[41]    validation_0-logloss:0.11171    validation_1-logloss:0.16259\n[42]    validation_0-logloss:0.10874    validation_1-logloss:0.16035\n[43]    validation_0-logloss:0.10593    validation_1-logloss:0.15740\n[44]    validation_0-logloss:0.10316    validation_1-logloss:0.15462\n[45]    validation_0-logloss:0.10017    validation_1-logloss:0.15140\n[46]    validation_0-logloss:0.09748    validation_1-logloss:0.14985\n[47]    validation_0-logloss:0.09515    validation_1-logloss:0.14732\n[48]    validation_0-logloss:0.09280    validation_1-logloss:0.14613\n[49]    validation_0-logloss:0.09039    validation_1-logloss:0.14424\n[50]    validation_0-logloss:0.08814    validation_1-logloss:0.14301\n[51]    validation_0-logloss:0.08583    validation_1-logloss:0.14055\n[52]    validation_0-logloss:0.08369    validation_1-logloss:0.13767\n[53]    validation_0-logloss:0.08167    validation_1-logloss:0.13494\n[54]    validation_0-logloss:0.07971    validation_1-logloss:0.13295\n[55]    validation_0-logloss:0.07782    validation_1-logloss:0.13025\n[56]    validation_0-logloss:0.07603    validation_1-logloss:0.12777\n[57]    validation_0-logloss:0.07431    validation_1-logloss:0.12528\n[58]    validation_0-logloss:0.07265    validation_1-logloss:0.12285\n[59]    validation_0-logloss:0.07107    validation_1-logloss:0.12062\n[60]    validation_0-logloss:0.06952    validation_1-logloss:0.11986\n[61]    validation_0-logloss:0.06804    validation_1-logloss:0.11877\n[62]    validation_0-logloss:0.06626    validation_1-logloss:0.11728\n[63]    validation_0-logloss:0.06490    validation_1-logloss:0.11527\n[64]    validation_0-logloss:0.06361    validation_1-logloss:0.11325\n[65]    validation_0-logloss:0.06205    validation_1-logloss:0.11093\n[66]    validation_0-logloss:0.06085    validation_1-logloss:0.10911\n[67]    validation_0-logloss:0.05957    validation_1-logloss:0.10839\n[68]    validation_0-logloss:0.05846    validation_1-logloss:0.10659\n[69]    validation_0-logloss:0.05701    validation_1-logloss:0.10541\n[70]    validation_0-logloss:0.05598    validation_1-logloss:0.10382\n[71]    validation_0-logloss:0.05487    validation_1-logloss:0.10325\n[72]    validation_0-logloss:0.05390    validation_1-logloss:0.10238\n[73]    validation_0-logloss:0.05262    validation_1-logloss:0.10137\n[74]    validation_0-logloss:0.05173    validation_1-logloss:0.09989\n[75]    validation_0-logloss:0.05078    validation_1-logloss:0.09905\n[76]    validation_0-logloss:0.04998    validation_1-logloss:0.09774\n[77]    validation_0-logloss:0.04902    validation_1-logloss:0.09725\n[78]    validation_0-logloss:0.04813    validation_1-logloss:0.09723\n[79]    validation_0-logloss:0.04728    validation_1-logloss:0.09558\n[80]    validation_0-logloss:0.04655    validation_1-logloss:0.09499\n[81]    validation_0-logloss:0.04558    validation_1-logloss:0.09360\n[82]    validation_0-logloss:0.04481    validation_1-logloss:0.09289\n[83]    validation_0-logloss:0.04411    validation_1-logloss:0.09233\n[84]    validation_0-logloss:0.04323    validation_1-logloss:0.09104\n[85]    validation_0-logloss:0.04244    validation_1-logloss:0.09051\n[86]    validation_0-logloss:0.04163    validation_1-logloss:0.08929\n[87]    validation_0-logloss:0.04105    validation_1-logloss:0.08824\n[88]    validation_0-logloss:0.04029    validation_1-logloss:0.08709\n[89]    validation_0-logloss:0.03970    validation_1-logloss:0.08667\n[90]    validation_0-logloss:0.03908    validation_1-logloss:0.08651\n[91]    validation_0-logloss:0.03840    validation_1-logloss:0.08554\n[92]    validation_0-logloss:0.03790    validation_1-logloss:0.08459\n[93]    validation_0-logloss:0.03717    validation_1-logloss:0.08382\n[94]    validation_0-logloss:0.03655    validation_1-logloss:0.08279\n[95]    validation_0-logloss:0.03609    validation_1-logloss:0.08246\n[96]    validation_0-logloss:0.03551    validation_1-logloss:0.08162\n[97]    validation_0-logloss:0.03503    validation_1-logloss:0.08062\n[98]    validation_0-logloss:0.03438    validation_1-logloss:0.07993\n[99]    validation_0-logloss:0.03390    validation_1-logloss:0.07963\n[100]   validation_0-logloss:0.03329    validation_1-logloss:0.07899\n[101]   validation_0-logloss:0.03284    validation_1-logloss:0.07873\n[102]   validation_0-logloss:0.03245    validation_1-logloss:0.07871\n[103]   validation_0-logloss:0.03202    validation_1-logloss:0.07846\n[104]   validation_0-logloss:0.03158    validation_1-logloss:0.07822\n[105]   validation_0-logloss:0.03122    validation_1-logloss:0.07799\n[106]   validation_0-logloss:0.03076    validation_1-logloss:0.07690\n[107]   validation_0-logloss:0.03032    validation_1-logloss:0.07710\n[108]   validation_0-logloss:0.02993    validation_1-logloss:0.07759\n[109]   validation_0-logloss:0.02950    validation_1-logloss:0.07750\n[110]   validation_0-logloss:0.02908    validation_1-logloss:0.07647\n[111]   validation_0-logloss:0.02867    validation_1-logloss:0.07550\n[112]   validation_0-logloss:0.02831    validation_1-logloss:0.07529\n[113]   validation_0-logloss:0.02787    validation_1-logloss:0.07401\n[114]   validation_0-logloss:0.02750    validation_1-logloss:0.07395\n[115]   validation_0-logloss:0.02712    validation_1-logloss:0.07300\n[116]   validation_0-logloss:0.02674    validation_1-logloss:0.07235\n[117]   validation_0-logloss:0.02635    validation_1-logloss:0.07196\n[118]   validation_0-logloss:0.02599    validation_1-logloss:0.07107\n[119]   validation_0-logloss:0.02565    validation_1-logloss:0.07043\n[120]   validation_0-logloss:0.02536    validation_1-logloss:0.07095\n[121]   validation_0-logloss:0.02505    validation_1-logloss:0.07092\n[122]   validation_0-logloss:0.02473    validation_1-logloss:0.07007\n[123]   validation_0-logloss:0.02444    validation_1-logloss:0.07007\n[124]   validation_0-logloss:0.02418    validation_1-logloss:0.07058\n[125]   validation_0-logloss:0.02393    validation_1-logloss:0.07069\n[126]   validation_0-logloss:0.02363    validation_1-logloss:0.07066\n[127]   validation_0-logloss:0.02333    validation_1-logloss:0.06986\n[128]   validation_0-logloss:0.02305    validation_1-logloss:0.06984\n[129]   validation_0-logloss:0.02277    validation_1-logloss:0.06906\n[130]   validation_0-logloss:0.02252    validation_1-logloss:0.06911\n[131]   validation_0-logloss:0.02224    validation_1-logloss:0.06825\n[132]   validation_0-logloss:0.02198    validation_1-logloss:0.06751\n[133]   validation_0-logloss:0.02175    validation_1-logloss:0.06699\n[134]   validation_0-logloss:0.02155    validation_1-logloss:0.06748\n[135]   validation_0-logloss:0.02138    validation_1-logloss:0.06752\n[136]   validation_0-logloss:0.02114    validation_1-logloss:0.06747\n[137]   validation_0-logloss:0.02096    validation_1-logloss:0.06682\n[138]   validation_0-logloss:0.02075    validation_1-logloss:0.06686\n[139]   validation_0-logloss:0.02057    validation_1-logloss:0.06663\n[140]   validation_0-logloss:0.02032    validation_1-logloss:0.06654\n[141]   validation_0-logloss:0.02013    validation_1-logloss:0.06599\n[142]   validation_0-logloss:0.01995    validation_1-logloss:0.06647\n[143]   validation_0-logloss:0.01972    validation_1-logloss:0.06640\n[144]   validation_0-logloss:0.01950    validation_1-logloss:0.06636\n[145]   validation_0-logloss:0.01925    validation_1-logloss:0.06568\n[146]   validation_0-logloss:0.01910    validation_1-logloss:0.06597\n[147]   validation_0-logloss:0.01891    validation_1-logloss:0.06518\n[148]   validation_0-logloss:0.01876    validation_1-logloss:0.06547\n[149]   validation_0-logloss:0.01854    validation_1-logloss:0.06481\n[150]   validation_0-logloss:0.01838    validation_1-logloss:0.06530\n[151]   validation_0-logloss:0.01824    validation_1-logloss:0.06490\n[152]   validation_0-logloss:0.01806    validation_1-logloss:0.06506\n[153]   validation_0-logloss:0.01789    validation_1-logloss:0.06519\n[154]   validation_0-logloss:0.01771    validation_1-logloss:0.06496\n[155]   validation_0-logloss:0.01762    validation_1-logloss:0.06516\n[156]   validation_0-logloss:0.01742    validation_1-logloss:0.06457\n[157]   validation_0-logloss:0.01729    validation_1-logloss:0.06484\n[158]   validation_0-logloss:0.01716    validation_1-logloss:0.06408\n[159]   validation_0-logloss:0.01698    validation_1-logloss:0.06389\n[160]   validation_0-logloss:0.01679    validation_1-logloss:0.06333\n[161]   validation_0-logloss:0.01671    validation_1-logloss:0.06355\n[162]   validation_0-logloss:0.01657    validation_1-logloss:0.06357\n[163]   validation_0-logloss:0.01645    validation_1-logloss:0.06321\n[164]   validation_0-logloss:0.01631    validation_1-logloss:0.06317\n[165]   validation_0-logloss:0.01621    validation_1-logloss:0.06322\n[166]   validation_0-logloss:0.01604    validation_1-logloss:0.06270\n[167]   validation_0-logloss:0.01594    validation_1-logloss:0.06232\n[168]   validation_0-logloss:0.01587    validation_1-logloss:0.06253\n[169]   validation_0-logloss:0.01572    validation_1-logloss:0.06206\n[170]   validation_0-logloss:0.01564    validation_1-logloss:0.06167\n[171]   validation_0-logloss:0.01554    validation_1-logloss:0.06097\n[172]   validation_0-logloss:0.01547    validation_1-logloss:0.06117\n[173]   validation_0-logloss:0.01534    validation_1-logloss:0.06110\n[174]   validation_0-logloss:0.01526    validation_1-logloss:0.06115\n[175]   validation_0-logloss:0.01516    validation_1-logloss:0.06047\n[176]   validation_0-logloss:0.01502    validation_1-logloss:0.06018\n[177]   validation_0-logloss:0.01493    validation_1-logloss:0.06022\n[178]   validation_0-logloss:0.01482    validation_1-logloss:0.06012\n[179]   validation_0-logloss:0.01475    validation_1-logloss:0.05975\n[180]   validation_0-logloss:0.01468    validation_1-logloss:0.05968\n[181]   validation_0-logloss:0.01461    validation_1-logloss:0.05988\n[182]   validation_0-logloss:0.01454    validation_1-logloss:0.05952\n[183]   validation_0-logloss:0.01447    validation_1-logloss:0.05945\n[184]   validation_0-logloss:0.01437    validation_1-logloss:0.05952\n[185]   validation_0-logloss:0.01428    validation_1-logloss:0.05933\n[186]   validation_0-logloss:0.01420    validation_1-logloss:0.05926\n[187]   validation_0-logloss:0.01410    validation_1-logloss:0.05917\n[188]   validation_0-logloss:0.01404    validation_1-logloss:0.05883\n[189]   validation_0-logloss:0.01397    validation_1-logloss:0.05843\n[190]   validation_0-logloss:0.01389    validation_1-logloss:0.05825\n[191]   validation_0-logloss:0.01382    validation_1-logloss:0.05821\n[192]   validation_0-logloss:0.01372    validation_1-logloss:0.05829\n[193]   validation_0-logloss:0.01364    validation_1-logloss:0.05811\n[194]   validation_0-logloss:0.01358    validation_1-logloss:0.05808\n[195]   validation_0-logloss:0.01352    validation_1-logloss:0.05823\n[196]   validation_0-logloss:0.01346    validation_1-logloss:0.05829\n[197]   validation_0-logloss:0.01340    validation_1-logloss:0.05823\n[198]   validation_0-logloss:0.01331    validation_1-logloss:0.05832\n[199]   validation_0-logloss:0.01324    validation_1-logloss:0.05813\n[200]   validation_0-logloss:0.01317    validation_1-logloss:0.05811\n[201]   validation_0-logloss:0.01312    validation_1-logloss:0.05769\n[202]   validation_0-logloss:0.01305    validation_1-logloss:0.05754\n[203]   validation_0-logloss:0.01296    validation_1-logloss:0.05764\n[204]   validation_0-logloss:0.01289    validation_1-logloss:0.05749\n[205]   validation_0-logloss:0.01284    validation_1-logloss:0.05756\n[206]   validation_0-logloss:0.01279    validation_1-logloss:0.05772\n[207]   validation_0-logloss:0.01273    validation_1-logloss:0.05768\n[208]   validation_0-logloss:0.01268    validation_1-logloss:0.05783\n[209]   validation_0-logloss:0.01263    validation_1-logloss:0.05752\n[210]   validation_0-logloss:0.01258    validation_1-logloss:0.05711\n[211]   validation_0-logloss:0.01251    validation_1-logloss:0.05697\n[212]   validation_0-logloss:0.01243    validation_1-logloss:0.05662\n[213]   validation_0-logloss:0.01237    validation_1-logloss:0.05671\n[214]   validation_0-logloss:0.01231    validation_1-logloss:0.05659\n[215]   validation_0-logloss:0.01226    validation_1-logloss:0.05629\n[216]   validation_0-logloss:0.01219    validation_1-logloss:0.05595\n[217]   validation_0-logloss:0.01214    validation_1-logloss:0.05591\n[218]   validation_0-logloss:0.01208    validation_1-logloss:0.05580\n[219]   validation_0-logloss:0.01201    validation_1-logloss:0.05606\n[220]   validation_0-logloss:0.01196    validation_1-logloss:0.05592\n[221]   validation_0-logloss:0.01190    validation_1-logloss:0.05601\n[222]   validation_0-logloss:0.01185    validation_1-logloss:0.05608\n[223]   validation_0-logloss:0.01181    validation_1-logloss:0.05569\n[224]   validation_0-logloss:0.01176    validation_1-logloss:0.05559\n[225]   validation_0-logloss:0.01169    validation_1-logloss:0.05585\n[226]   validation_0-logloss:0.01164    validation_1-logloss:0.05576\n[227]   validation_0-logloss:0.01158    validation_1-logloss:0.05549\n[228]   validation_0-logloss:0.01153    validation_1-logloss:0.05521\n[229]   validation_0-logloss:0.01149    validation_1-logloss:0.05509\n[230]   validation_0-logloss:0.01145    validation_1-logloss:0.05493\n[231]   validation_0-logloss:0.01140    validation_1-logloss:0.05507\n[232]   validation_0-logloss:0.01136    validation_1-logloss:0.05469\n[233]   validation_0-logloss:0.01132    validation_1-logloss:0.05500\n[234]   validation_0-logloss:0.01128    validation_1-logloss:0.05474\n[235]   validation_0-logloss:0.01124    validation_1-logloss:0.05472\n[236]   validation_0-logloss:0.01120    validation_1-logloss:0.05490\n[237]   validation_0-logloss:0.01115    validation_1-logloss:0.05503\n[238]   validation_0-logloss:0.01111    validation_1-logloss:0.05516\n[239]   validation_0-logloss:0.01107    validation_1-logloss:0.05524\n[240]   validation_0-logloss:0.01103    validation_1-logloss:0.05537\n[241]   validation_0-logloss:0.01099    validation_1-logloss:0.05536\n[242]   validation_0-logloss:0.01096    validation_1-logloss:0.05568\n[243]   validation_0-logloss:0.01090    validation_1-logloss:0.05543\n[244]   validation_0-logloss:0.01087    validation_1-logloss:0.05556\n[245]   validation_0-logloss:0.01083    validation_1-logloss:0.05519\n[246]   validation_0-logloss:0.01081    validation_1-logloss:0.05537\n[247]   validation_0-logloss:0.01077    validation_1-logloss:0.05536\n[248]   validation_0-logloss:0.01072    validation_1-logloss:0.05549\n[249]   validation_0-logloss:0.01068    validation_1-logloss:0.05562\n[250]   validation_0-logloss:0.01064    validation_1-logloss:0.05537\n[251]   validation_0-logloss:0.01061    validation_1-logloss:0.05555\n[252]   validation_0-logloss:0.01058    validation_1-logloss:0.05568\n[253]   validation_0-logloss:0.01054    validation_1-logloss:0.05532\n[254]   validation_0-logloss:0.01051    validation_1-logloss:0.05508\n[255]   validation_0-logloss:0.01049    validation_1-logloss:0.05525\n[256]   validation_0-logloss:0.01045    validation_1-logloss:0.05524\n[257]   validation_0-logloss:0.01042    validation_1-logloss:0.05537\n[258]   validation_0-logloss:0.01037    validation_1-logloss:0.05554\n[259]   validation_0-logloss:0.01033    validation_1-logloss:0.05563\n[260]   validation_0-logloss:0.01030    validation_1-logloss:0.05528\n[261]   validation_0-logloss:0.01028    validation_1-logloss:0.05546\n[262]   validation_0-logloss:0.01025    validation_1-logloss:0.05559\n[263]   validation_0-logloss:0.01022    validation_1-logloss:0.05558\n[264]   validation_0-logloss:0.01017    validation_1-logloss:0.05576\n[265]   validation_0-logloss:0.01014    validation_1-logloss:0.05593\n[266]   validation_0-logloss:0.01011    validation_1-logloss:0.05558\n[267]   validation_0-logloss:0.01008    validation_1-logloss:0.05571\n[268]   validation_0-logloss:0.01005    validation_1-logloss:0.05584\n[269]   validation_0-logloss:0.01002    validation_1-logloss:0.05561\n[270]   validation_0-logloss:0.00998    validation_1-logloss:0.05560\n[271]   validation_0-logloss:0.00997    validation_1-logloss:0.05577\n[272]   validation_0-logloss:0.00992    validation_1-logloss:0.05574\n[273]   validation_0-logloss:0.00989    validation_1-logloss:0.05540\n[274]   validation_0-logloss:0.00987    validation_1-logloss:0.05557\n[275]   validation_0-logloss:0.00984    validation_1-logloss:0.05570\n[276]   validation_0-logloss:0.00981    validation_1-logloss:0.05547\n[277]   validation_0-logloss:0.00978    validation_1-logloss:0.05546\n[278]   validation_0-logloss:0.00973    validation_1-logloss:0.05543\n[279]   validation_0-logloss:0.00970    validation_1-logloss:0.05556\n[280]   validation_0-logloss:0.00969    validation_1-logloss:0.05562\n[281]   validation_0-logloss:0.00966    validation_1-logloss:0.05528\n\n\n\n\nLightGBM\n\n성능은 xgboost랑 별로 차이가 없음.\n1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\none hot 인코딩 필요 없음\npython lightgbm\n\n\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n\n[LightGBM] [Info] Number of positive: 262, number of negative: 147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4092\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.640587 -&gt; initscore=0.577912\n[LightGBM] [Info] Start training from score 0.577912\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[159]   training's binary_logloss: 0.00195741   valid_1's binary_logloss: 0.0442418",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#stacking",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#stacking",
    "title": "분류 - 앙상블",
    "section": "stacking",
    "text": "stacking\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n\n\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n\n\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n\n0.991\n\n\n\ntest 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n\nCV 세트 기반 stacking\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n\n\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n\n\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n\n0.982",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/02.html#baysian-optimization",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/02.html#baysian-optimization",
    "title": "분류 - 앙상블",
    "section": "Baysian Optimization",
    "text": "Baysian Optimization\n\nGrid search로는 시간이 너무 오래 걸리는 경우\n목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\nSurrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\nacquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n\n  0%|          | 0/20 [00:00&lt;?, ?trial/s, best loss=?]100%|██████████| 20/20 [00:00&lt;00:00, 1705.00trial/s, best loss: -284.0]\n\n\n{'x': 4.0, 'y': 15.0}\n\n\n\nXGBoost 하이퍼파라미터 최적화\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n\n\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:00&lt;00:15,  3.22trial/s, best loss: -0.9582752410828395]  4%|▍         | 2/50 [00:00&lt;00:14,  3.38trial/s, best loss: -0.9714186127570582]  6%|▌         | 3/50 [00:00&lt;00:15,  3.13trial/s, best loss: -0.9714186127570582]  8%|▊         | 4/50 [00:01&lt;00:16,  2.82trial/s, best loss: -0.9714186127570582] 10%|█         | 5/50 [00:01&lt;00:13,  3.43trial/s, best loss: -0.9714186127570582] 12%|█▏        | 6/50 [00:01&lt;00:10,  4.12trial/s, best loss: -0.9714186127570582] 14%|█▍        | 7/50 [00:01&lt;00:08,  4.92trial/s, best loss: -0.9714186127570582] 16%|█▌        | 8/50 [00:01&lt;00:07,  5.39trial/s, best loss: -0.9714186127570582] 18%|█▊        | 9/50 [00:02&lt;00:07,  5.79trial/s, best loss: -0.9714186127570582] 20%|██        | 10/50 [00:02&lt;00:06,  5.88trial/s, best loss: -0.9714186127570582] 22%|██▏       | 11/50 [00:02&lt;00:05,  6.59trial/s, best loss: -0.9714186127570582] 24%|██▍       | 12/50 [00:02&lt;00:05,  6.58trial/s, best loss: -0.9714186127570582] 26%|██▌       | 13/50 [00:02&lt;00:05,  7.31trial/s, best loss: -0.9714186127570582] 28%|██▊       | 14/50 [00:02&lt;00:04,  7.28trial/s, best loss: -0.9714186127570582] 30%|███       | 15/50 [00:02&lt;00:04,  7.17trial/s, best loss: -0.9714186127570582] 38%|███▊      | 19/50 [00:03&lt;00:02, 14.61trial/s, best loss: -0.9714186127570582] 42%|████▏     | 21/50 [00:03&lt;00:02, 10.53trial/s, best loss: -0.9714186127570582] 46%|████▌     | 23/50 [00:03&lt;00:02,  9.43trial/s, best loss: -0.9714186127570582] 50%|█████     | 25/50 [00:03&lt;00:02,  8.95trial/s, best loss: -0.9736261182758219] 54%|█████▍    | 27/50 [00:04&lt;00:02,  8.87trial/s, best loss: -0.9736261182758219] 56%|█████▌    | 28/50 [00:04&lt;00:02,  8.90trial/s, best loss: -0.9736261182758219] 60%|██████    | 30/50 [00:04&lt;00:02,  8.95trial/s, best loss: -0.9736261182758219] 62%|██████▏   | 31/50 [00:04&lt;00:02,  8.95trial/s, best loss: -0.9736261182758219] 64%|██████▍   | 32/50 [00:04&lt;00:02,  7.08trial/s, best loss: -0.9736261182758219] 66%|██████▌   | 33/50 [00:05&lt;00:02,  5.92trial/s, best loss: -0.9736261182758219] 68%|██████▊   | 34/50 [00:05&lt;00:03,  5.20trial/s, best loss: -0.9736261182758219] 70%|███████   | 35/50 [00:05&lt;00:03,  3.97trial/s, best loss: -0.9736261182758219] 72%|███████▏  | 36/50 [00:05&lt;00:03,  3.84trial/s, best loss: -0.9736261182758219] 74%|███████▍  | 37/50 [00:06&lt;00:03,  3.79trial/s, best loss: -0.9736261182758219] 76%|███████▌  | 38/50 [00:06&lt;00:02,  4.05trial/s, best loss: -0.9736261182758219] 78%|███████▊  | 39/50 [00:06&lt;00:02,  4.46trial/s, best loss: -0.9736261182758219] 80%|████████  | 40/50 [00:06&lt;00:01,  5.31trial/s, best loss: -0.9736261182758219] 82%|████████▏ | 41/50 [00:06&lt;00:01,  6.03trial/s, best loss: -0.9736261182758219] 84%|████████▍ | 42/50 [00:06&lt;00:01,  6.79trial/s, best loss: -0.9736261182758219] 86%|████████▌ | 43/50 [00:07&lt;00:01,  6.73trial/s, best loss: -0.9736261182758219] 88%|████████▊ | 44/50 [00:07&lt;00:00,  7.04trial/s, best loss: -0.9736261182758219] 90%|█████████ | 45/50 [00:07&lt;00:00,  7.43trial/s, best loss: -0.9736261182758219] 92%|█████████▏| 46/50 [00:07&lt;00:00,  7.02trial/s, best loss: -0.9736261182758219] 94%|█████████▍| 47/50 [00:07&lt;00:00,  7.14trial/s, best loss: -0.9736261182758219] 96%|█████████▌| 48/50 [00:07&lt;00:00,  6.67trial/s, best loss: -0.9736261182758219] 98%|█████████▊| 49/50 [00:07&lt;00:00,  7.19trial/s, best loss: -0.9736261182758219]100%|██████████| 50/50 [00:08&lt;00:00,  7.52trial/s, best loss: -0.9736261182758219]100%|██████████| 50/50 [00:08&lt;00:00,  6.22trial/s, best loss: -0.9736261182758219]\n\n\n{'colsample_bytree': 0.8065561529248224,\n 'learning_rate': 0.1128018538935688,\n 'max_depth': 18.0,\n 'min_child_weight': 2.0}",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/03.html#preprocessing",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/03.html#preprocessing",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('_data/santander/train.csv', encoding='latin-1')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 76020 entries, 0 to 76019\nColumns: 371 entries, ID to TARGET\ndtypes: float64(111), int64(260)\nmemory usage: 215.2 MB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nID\nvar3\nvar15\nimp_ent_var16_ult1\nimp_op_var39_comer_ult1\nimp_op_var39_comer_ult3\nimp_op_var40_comer_ult1\nimp_op_var40_comer_ult3\nimp_op_var40_efect_ult1\nimp_op_var40_efect_ult3\n...\nsaldo_medio_var33_hace2\nsaldo_medio_var33_hace3\nsaldo_medio_var33_ult1\nsaldo_medio_var33_ult3\nsaldo_medio_var44_hace2\nsaldo_medio_var44_hace3\nsaldo_medio_var44_ult1\nsaldo_medio_var44_ult3\nvar38\nTARGET\n\n\n\n\ncount\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n...\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n7.602000e+04\n76020.000000\n\n\nmean\n75964.050723\n-1523.199277\n33.212865\n86.208265\n72.363067\n119.529632\n3.559130\n6.472698\n0.412946\n0.567352\n...\n7.935824\n1.365146\n12.215580\n8.784074\n31.505324\n1.858575\n76.026165\n56.614351\n1.172358e+05\n0.039569\n\n\nstd\n43781.947379\n39033.462364\n12.956486\n1614.757313\n339.315831\n546.266294\n93.155749\n153.737066\n30.604864\n36.513513\n...\n455.887218\n113.959637\n783.207399\n538.439211\n2013.125393\n147.786584\n4040.337842\n2852.579397\n1.826646e+05\n0.194945\n\n\nmin\n1.000000\n-999999.000000\n5.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n5.163750e+03\n0.000000\n\n\n25%\n38104.750000\n2.000000\n23.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.787061e+04\n0.000000\n\n\n50%\n76043.000000\n2.000000\n28.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.064092e+05\n0.000000\n\n\n75%\n113748.750000\n2.000000\n40.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.187563e+05\n0.000000\n\n\nmax\n151838.000000\n238.000000\n105.000000\n210000.000000\n12888.030000\n21024.810000\n8237.820000\n11073.570000\n6600.000000\n6600.000000\n...\n50003.880000\n20385.720000\n138831.630000\n91778.730000\n438329.220000\n24650.010000\n681462.900000\n397884.300000\n2.203474e+07\n1.000000\n\n\n\n\n8 rows × 371 columns\n\n\n\n\ndf['var3'].replace(-999999, 2, inplace=True)\ndf.drop('ID', axis=1, inplace=True)\n\nX_features = df.iloc[:, :-1]\nlabels = df.iloc[:, -1]\n\n\ntest_df = pd.read_csv('_data/santander/test.csv', encoding='latin-1')\ntest_df['var3'].replace(-999999, 2, inplace=True)\ntest_df.drop('ID', axis=1, inplace=True)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, labels, test_size=0.2)\n\n\ntrain, test의 label의 비율이 동일한게 좋은걸까",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/03.html#xgboost",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/03.html#xgboost",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "XGBoost",
    "text": "XGBoost\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3)\n\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb_clf = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    early_stopping_rounds=100,\n                    eval_metric=['auc'])\nxgb_clf.fit(X_tr, y_tr, eval_set=evals, verbose=False)\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\nprint(f'{xgb_roc_score:.3f}')\n\n\n베이지안 최적화\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            early_stopping_rounds=30,\n                            eval_metric='auc',\n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            colsample_bytree=search_space['colsample_bytree'],\n                            learning_rate=search_space['learning_rate'])\n    roc_auc_list = []\n    kf = KFold(n_splits=3)\n    for tr_index, val_index in kf.split(X_train):\n        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n        X_val, y_val =  X_train.iloc[val_index], y_train.iloc[val_index]\n\n        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])\n        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n        roc_auc_list.append(score)\n\n    return -1 * np.mean(roc_auc_list)\n\n\nfrom hyperopt import hp, fmin, tpe, Trials\n\nxgb_search_space = {\n  'max_depth': hp.quniform('max_depth', 5, 15, 1),\n  'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n  'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n  'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n}\n\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trials)\nprint(best)\n\n\n\n재 학습\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb_clf = XGBClassifier(n_estimators=500, \n                    learning_rate=round(best['learning_rate'], 5),\n                    max_depth=int(best['max_depth']),\n                    min_child_weight=int(best['min_child_weight']),\n                    colsample_bytree=round(best['colsample_bytree'], 5),\n                    early_stopping_rounds=100,\n                    eval_metric=['auc'])\nxgb_clf.fit(X_tr, y_tr, eval_set=evals, verbose=False)\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\nprint(f'{xgb_roc_score:.3f}')\n\n\n\nplot importance\n\nfrom xgboost import plot_importance\n\nplot_importance(xgb_clf, max_num_features=20, height=0.4)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#경사하강법",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#경사하강법",
    "title": "회귀",
    "section": "경사하강법",
    "text": "경사하강법\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = 2 * np.random.rand(100, 1)\ny = 6 + 4 * X + np.random.randn(100, 1)\nplt.scatter(X, y)\n\n\n\n\n\n\n\n\n\ndef get_cost(y, y_pred):\n    N = len(y)\n    cost = np.sum(np.square(y - y_pred)) / N\n    return cost\n\ndef get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n    N = len(y)\n    w1_update = np.zeros_like(w1)\n    w0_update = np.zeros_like(w0)\n    y_pred = np.dot(X, w1.T) + w0\n    diff = y - y_pred\n\n    w1_update = -(2/N) * learning_rate * np.dot(X.T, diff)\n    w0_update = -(2/N) * learning_rate * np.sum(diff)\n\n    return w1_update, w0_update\n\ndef gradient_descent_steps(X, y, iters=10000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for _ in range(iters):\n        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n\n\nw1, w0 = gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n\nw0: 5.955 w1: 3.940, total cost: 1.018\n\n\n\n\n\n\n\n\n\n\n일반 경사하강법은 시간이 오래걸려서 잘 안씀",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#미니-배치-확률적-경사-하강법",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#미니-배치-확률적-경사-하강법",
    "title": "회귀",
    "section": "미니 배치 확률적 경사 하강법",
    "text": "미니 배치 확률적 경사 하강법\n\ndef stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for ind in range(iters):\n        stochastic_random_index = np.random.permutation(X.shape[0])\n        sample_X = X[stochastic_random_index[0:batch_size]]\n        sample_y = y[stochastic_random_index[0:batch_size]]\n\n        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n\n\nw1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n\nw0: 5.931 w1: 3.978, total cost: 1.021",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#선형-회귀",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#선형-회귀",
    "title": "회귀",
    "section": "선형 회귀",
    "text": "선형 회귀\n\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_boston\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nboston = load_boston()\n\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\ndf.head()\n\n\nlm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRAIO', 'LSTAT', 'RAD']\n\nfig, axs = plt.subplots(figsize=(16, 8), ncols=len(lm_features) // 2, nrows=2)\n\nfor i, feature in enumerate(lm_features):\n    row = i // 4\n    col = i % 4\n\n    sns.regplot(x=feature, y='price', data=df, ax=axs[row][col])\n\nboston 데이터가 윤리적 문제로 사용 불가능하다고 한다.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ny_target = df['price']\nX_data = df.drop(['price'], axis=1, inplace=False)\nlr = LinearRegression()\n\nneg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n\ncross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#다항-선형-회귀",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#다항-선형-회귀",
    "title": "회귀",
    "section": "다항 선형 회귀",
    "text": "다항 선형 회귀",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#다항-회귀",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#다항-회귀",
    "title": "회귀",
    "section": "다항 회귀",
    "text": "다항 회귀\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\ndef polynominal_func(X):\n    y = 1 + 2 * X[:, 0] + 3 * X[:, 0]**2 + 4 * X[:, 1]**3\n    return y\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=3)),\n                  ('linear', LinearRegression())])\nX = np.arange(4).reshape(2, 2)\ny = polynominal_func(X)\n\nmodel = model.fit(X, y)\n\nnp.round(model.named_steps['linear'].coef_, 2)\n\narray([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#규제",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#규제",
    "title": "회귀",
    "section": "규제",
    "text": "규제\n\nL2 규제(Ridge): \\(min(RSS(W) + \\lambda ||W||^2)\\)\nL1 규제(Lasso): \\(min(RSS(W) + \\lambda ||W||_1)\\)\nλ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐\nL1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음\n\n\n릿지\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha = 10)\nneg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n\n\n\n라쏘 엘라스틱넷\n\nimport pandas as pd\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\ndef get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None):\n    coeff_df = pd.DataFrame()\n    for param in params:\n        if model_name == 'Ridge':\n            model = Ridge(alpha=param)\n        elif model_name == 'Lasso':\n            model = Lasso(alpha=param)\n        elif model_name == 'ElasticNet':\n            model = ElasticNet(alpha=param, l1_ratio=0.7)\n        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=\"neg_mean_squared_error\", cv=5)\n        rmse_scores = np.sqrt(-1 * neg_mse_scores)\n        avg_rmse = np.mean(rmse_scores)\n        print(f'{param}: {avg_rmse:.3f}')\n\n        model.fit(X_data_n, y_target_n)\n        coeff = pd.Series(data=model.coef_, index=X_data_n.columns)\n        colname = 'alpha:' + str(param)\n        coeff_df[colname] = coeff\n\n    return coeff_df",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/05.html#선형-회귀-모델을-위한-데이터-변환",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/05.html#선형-회귀-모델을-위한-데이터-변환",
    "title": "회귀",
    "section": "선형 회귀 모델을 위한 데이터 변환",
    "text": "선형 회귀 모델을 위한 데이터 변환\n\n로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.\n\n\nnp.log1p(data)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/02.html",
    "href": "posts/02_areas/deep_learning/notes/02.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "02"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/06.html#pca",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/06.html#pca",
    "title": "차원 축소",
    "section": "PCA",
    "text": "PCA\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris = load_iris()\ncolumns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\ndf = pd.DataFrame(iris.data, columns=columns)\ndf['target'] = iris.target\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['sepal_length']\n    y_axis_data = df[df['target'] == i]['sepal_width']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('sepal length')\nplt.ylabel('sepal width')\nplt.show()\n\n\n\n\n\n\n\n\n\nPCA는 scaling의 영향을 받음.\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaled_df = StandardScaler().fit_transform(df.iloc[:, :-1])\n\npca = PCA(n_components=2)\ndf = pca.fit_transform(scaled_df)\n\n\npca_columns = ['pca_component_1', 'pca_component_2']\ndf = pd.DataFrame(df, columns=pca_columns)\ndf['target'] = iris.target\n\n\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['pca_component_1']\n    y_axis_data = df[df['target'] == i]['pca_component_2']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('pca_component_1')\nplt.ylabel('pca_component_2')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/06.html#신용카드-고객-데이터",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/06.html#신용카드-고객-데이터",
    "title": "차원 축소",
    "section": "신용카드 고객 데이터",
    "text": "신용카드 고객 데이터\n\ndf = pd.read_excel('_data/creadit_card.xls', header=1, sheet_name='Data').iloc[:, 1:]\ndf.rename(columns={'PAY_0': 'PAY_1', 'default payment next month': 'default'}, inplace=True)\ntarget = df['default']\nfeatures = df.drop('default', axis=1)\n\n\nimport seaborn as sns\n\ncorr = features.corr()\nsns.heatmap(corr, annot=True, fmt='.1g')\n\n\n\n\n\n\n\n\n\nBILL_AMT1~6, PAY_1~6의 상관도가 높다.\n\n\ncols_bill = ['BILL_AMT' + str(i) for i in range(1, 7)]\nscaler = StandardScaler()\ndf_cols_scaled = scaler.fit_transform(features[cols_bill])\npca = PCA(n_components=2)\npca.fit(df_cols_scaled)\npca.explained_variance_ratio_\n\narray([0.90555253, 0.0509867 ])\n\n\n\nPCA 할 때 column 전부 다 안 넣어도 되나?\n다 넣어야 하는 듯",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/06.html#lda",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/06.html#lda",
    "title": "차원 축소",
    "section": "LDA",
    "text": "LDA\n\n클래스 분리를 최대화하는 축을 찾음\nPCA와 다르게 지도 학습임.\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\niris_scaled = StandardScaler().fit_transform(iris.data)\n\n\nlda = LinearDiscriminantAnalysis(n_components=2)\niris_lda = lda.fit_transform(iris_scaled, iris.target)\n\n\nlda_columns = ['lda_components_1', 'lda_components_2']\ndf = pd.DataFrame(iris_lda, columns=lda_columns)\ndf['target'] = iris.target\n\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['lda_components_1']\n    y_axis_data = df[df['target'] == i]['lda_components_2']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('lda_components_1')\nplt.ylabel('lda_components_2')\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/07.html#overview",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/07.html#overview",
    "title": "텍스트 분석",
    "section": "overview",
    "text": "overview\n\nNLP vs 텍스트 분석\n\nNLP(자연어 처리)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술을 의미\n텍스트 분석은 주로 비정형 텍스트 데이터를 머신러닝, 통계 등의 방법으로 예측 분석이나 유용한 정보를 추출하는 데 중점을 둔다.\n\n\n\n종류\n\n텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측 (연예 / 정치 / 스포츠 같은 카테고리 분류 혹은 스팸 메일 검출). 지도 학습\n감성 분석: 텍스트에서 주관적 요소를 분석하는 기법. 지도 혹은 비지도.\n텍스트 요약: 텍스트 내에서 주제나 중심 사상을 추출\n텍스트 군집화: 비슷한 유형의 문서를 군집화 하는 것. 비지도 학습",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/07.html#프로세스",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/07.html#프로세스",
    "title": "텍스트 분석",
    "section": "프로세스",
    "text": "프로세스\n\n텍스트 전처리: 대 / 소문자 변경, 특수 문자 제거, 토큰화, 불용어 제거, 어근 추출 등의 정규화 작업\n피처 벡터화 / 추출: 텍스트에서 피처를 추출하고 벡터 값을 할당. BOW와 Word2Vec이 대표적\nML 모델 수립 및 학습 / 예측 / 평가",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/07.html#전처리",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/07.html#전처리",
    "title": "텍스트 분석",
    "section": "전처리",
    "text": "전처리\n\n클렌징: 문자, 기호 등을 사전에 제거\n토큰화\n\n문장 토큰화: 마침표, 개행문자 등을 기준으로 문장을 분리. 각 문장이 가지는 의미가 중요한 경우 사용.\n단어 토큰화: 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리.\n\nn-gram: 단어의 연속된 n개를 묶어서 하나의 단위로 처리하는 방법. 문장이 가지는 의미를 조금이라도 보존할 수 있다.\n\n\n\n\nfrom nltk import sent_tokenize\nimport nltk\nnltk.download('punkt') # 문장을 분리하는 마침표, 개행문자 등의 데이터 셋 다운로드\nnltk.download('punkt_tab')\n\ntext_sample = \"The Matrix is everywhere its all around us, here even in this room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes.\"\nsentences = sent_tokenize(text_sample)\nprint(sentences)\n\n['The Matrix is everywhere its all around us, here even in this room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\n\nfrom nltk import word_tokenize\n\nsentence = \"The Matrix is everywhere its all around us, here even in this room.\"\nwords = word_tokenize(sentence)\nprint(words)\n\n['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n\n\n\ndef tokenize_text(text):\n    sentences = sent_tokenize(text)\n    words = [word_tokenize(sentence) for sentence in sentences]\n\n    return words\n\nword_tokens = tokenize_text(text_sample)\nword_tokens\n\n[['The',\n  'Matrix',\n  'is',\n  'everywhere',\n  'its',\n  'all',\n  'around',\n  'us',\n  ',',\n  'here',\n  'even',\n  'in',\n  'this',\n  'room',\n  '.'],\n ['You',\n  'can',\n  'see',\n  'it',\n  'when',\n  'you',\n  'look',\n  'out',\n  'your',\n  'window',\n  'or',\n  'when',\n  'you',\n  'turn',\n  'on',\n  'your',\n  'television',\n  '.'],\n ['You',\n  'can',\n  'feel',\n  'it',\n  'when',\n  'you',\n  'go',\n  'to',\n  'work',\n  ',',\n  'when',\n  'you',\n  'go',\n  'to',\n  'church',\n  ',',\n  'when',\n  'you',\n  'pay',\n  'your',\n  'taxes',\n  '.']]\n\n\n\nstopword 제거: 분석에 필요하지 않은 단어를 제거하는 작업. 예) 관사, 전치사, 접속사 등\n\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')  # stopwords 데이터 셋 다운로드\n\nstopwords.words('english')[:20]\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n['a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been']\n\n\n\nsw = stopwords.words('english')\nall_tokens = []\nfor sentence in word_tokens:\n    filtered_words = []\n    for word in sentence:\n        word = word.lower()\n        if word not in sw:\n            filtered_words.append(word)\n    all_tokens.append(filtered_words)\nprint(all_tokens)\n\n[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'look', 'window', 'turn', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', ',', 'pay', 'taxes', '.']]\n\n\n\nstemming, lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것\n\nstemming이 더 단순하고 빠르지만 lemmatization 이 더 저오학함\n\n\n\nfrom nltk.stem import LancasterStemmer\n\nstemmer = LancasterStemmer()\n\nprint(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\nprint(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\nprint(stemmer.stem('happier'), stemmer.stem('happiest'))\nprint(stemmer.stem('fancier'), stemmer.stem('fanciest'))\n\nwork work work\namus amus amus\nhappy happiest\nfant fanciest\n\n\n\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('wordnet')\n\nlemma = WordNetLemmatizer()\n\nprint(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\nprint(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\nprint(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))\n\namuse amuse amuse\nhappy happy\nfancy fancy\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/07.html#bow",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/07.html#bow",
    "title": "텍스트 분석",
    "section": "BOW",
    "text": "BOW\n\n문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 빈도 값을 부여해 피처 값을 추출하는 모델\ncount 기반 벡터화: 빈도가 높을수록 중요한 단어로 인식\nTF-IDF(term frequency - inverse document frequency) 기반 벡터화: 빈도가 높을수록 좋으나, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 패털티를 줌\n\n\\(TF_i * log\\frac{N}{DF_i}\\)\n\n\\(TF_i\\): 개별 문서에서의 단어 i 빈도\n\\(DF_i\\): 단어 i를 가지고 있는 문서 개수\nN: 전체 문서 개수\n\n\n희소행렬 문제: 불필요한 0 값이 많아지는 문제\n\nCOO\nCSR\n혹은 희소행렬을 잘 처리하는 알고리즘: 로지스틱 회귀, 선형 svm, 나이브 베이즈 등\n\n\n\nCOO\n\n0이 아닌 데이터만 별도의 array에 저장.\n\n\nimport numpy as np\nfrom scipy import sparse\n\ndense = np.array([[3, 0, 1], [0, 2, 0]])\ndata = np.array([3, 1, 2])\nrow_pos = np.array([0, 0, 1])\ncol_pos = np.array([0, 2, 1])\nsparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\nsparse_coo\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 3 stored elements and shape (2, 3)&gt;\n\n\n\nsparse_coo.toarray()\n\narray([[3, 0, 1],\n       [0, 2, 0]])\n\n\n\n\nCSR\n\nCOO + 시작위치만 기록하는 방법\n\n\nfrom scipy import sparse\n\ndense2 = np.array([[0, 0, 1, 0, 0, 5],\n                   [1, 4, 0, 3, 2, 5],\n                   [0, 6, 0, 3, 0, 0],\n                   [2, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 7, 0, 8],\n                   [1, 0, 0, 0, 0, 0]])\ndata2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\nrow_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\ncol_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\nrow_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n\nsparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\nsparse_csr.toarray()\n\narray([[0, 0, 1, 0, 0, 5],\n       [1, 4, 0, 3, 2, 5],\n       [0, 6, 0, 3, 0, 0],\n       [2, 0, 0, 0, 0, 0],\n       [0, 0, 0, 7, 0, 8],\n       [1, 0, 0, 0, 0, 0]])\n\n\n\nsparse_csr = sparse.csr_matrix(dense2)\nsparse_csr.toarray()\n\narray([[0, 0, 1, 0, 0, 5],\n       [1, 4, 0, 3, 2, 5],\n       [0, 6, 0, 3, 0, 0],\n       [2, 0, 0, 0, 0, 0],\n       [0, 0, 0, 7, 0, 8],\n       [1, 0, 0, 0, 0, 0]])",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/09.html",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/09.html",
    "title": "텍스트 분석 - 감성 분석",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 감성 분석"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/notes/00.html#서론",
    "href": "posts/04_archives/toeic_speaking/notes/00.html#서론",
    "title": "토익 스피킹 후기",
    "section": "서론",
    "text": "서론\n\n\n\n결과\n\n\n당연히 IM 등급이 나올 줄 알았는데 의외로 점수가 후하게 나온 것 같다.\n말을 7번 정도 절었고, 다른 사람들이 yes라고 대답할 때, 나는 no라고 대답했고, 마지막 문제는 30초 정도 아무 말도 안해서 점수에 대한 큰 기대는 안하고 있었다.\n이렇게 쉽게 점수가 나오는 줄 알았으면 조금 더 열심히 공부해볼걸 하는 생각이 든다.\n물론 내가 대학원을 가지 않는다면, 적어도 근 시일 내에 스피킹 시험을 다시 보는 일은 없겠지만.",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/notes/00.html#공부",
    "href": "posts/04_archives/toeic_speaking/notes/00.html#공부",
    "title": "토익 스피킹 후기",
    "section": "공부",
    "text": "공부\n유튜브에 있는 제이크 토익 스피킹 채널의 모의고사 9개 정도 풀어봤다.\n해설은 4편 정도 보다가 거기서 설명하는대로 안 할것 같아서 그냥 내 방식대로 템플릿 만드는데 더 시간을 썼다.\n원래 모의고사 20개 정도는 풀어보려고 했는데, 시험 날짜를 너무 일찍 잡아버려서 그냥 이대로 봐버렸다.",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/notes/00.html#피드백",
    "href": "posts/04_archives/toeic_speaking/notes/00.html#피드백",
    "title": "토익 스피킹 후기",
    "section": "피드백",
    "text": "피드백\n\n\n\n생각보다 친절한 피드백\n\n\n이런 기능까지 제공해줄 줄이야..\nopic 시험은 추가 결제를 해야 피드백을 줬던걸로 기억하는데.. 정성이 기가막히다.\n\n말을 만들어 내야 할 때 또렷하지 못한 발음과 부적절한 억양 또는 강세: 템플릿이 아닌 문장은 말을 뭉뚱그려서 한다는 점을 지적한것 같다. 내가 그랬나?\n문법 오류: 수능 영어 이후로 영어 문법을 공부해 본 적이 없다. 애초에 100% 완벽한 문법을 구사할 수 있으리라곤 생각조차 안했다.\n한정된 어휘: 이 부분은 템플릿 티가 많이 났다는 지적이 아닐까 생각한다.\n발음, 억양 및 강세: 전체적인 톤과 발음은 괜찮다고 평가해주는 것 같다. 진짜로 내 발음이 괜찮다기 보단 그냥 평가를 좀 후하게 해준거 같다.\n\n피드백을 전반적으로 보면 템플릿 티가 날 때 감점이 조금 있는 듯 하다.\n유튜브에 있는 유명한 템플릿에 지나치게 의존하는 것 보다는 본인만의 템플릿을 만든다던가, 아니면 애드리브로 본인만의 문장을 구사한다면 좋은 점수를 받을 수 있지 않을까?",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/notes/00.html#결론",
    "href": "posts/04_archives/toeic_speaking/notes/00.html#결론",
    "title": "토익 스피킹 후기",
    "section": "결론",
    "text": "결론\n이번 시험 결과에서 IH가 안나오면 대학원을 바로 준비해야지 생각했는데.. 이것 참 럴수 럴수 이럴수가한 상황이 돼버렸다.\n진로에 대한 고민이 참 많아지는 시기인것 같다.",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/00.html#서론",
    "href": "posts/04_archives/bs_3_1/notes/00.html#서론",
    "title": "3학년 1학기 후기",
    "section": "서론",
    "text": "서론\n\n\n\n학기 점수\n\n\n\n\n\n과목 점수\n\n\n수강 학점이 24.5 학점이긴 한데, 사이버 강의로 가득 채워서 높게 나왔다.\n실제로는 전공 과목 5개, 교양 과목 2개를 들었다.\n\n\n\n시간표\n\n\n이 중에서도 배경지식이 전혀 없는 과목은 생산 시스템 관리 1과목 뿐이라 은근히 수월하게 이번학기를 마무리 한 것 같다.\n나의 상대적으로 높은 배경지식은, 그나마 뽑을 수 있는 휴학을 오래한 경험의 장점 중 하나가 아닌가 생각한다.\n물론 다시 과거로 돌아가서 어린 김형훈에게 ‘이 장점 하나만 보고 휴학을 4년동안 하겠습니까?’ 라고 한다면 내 대답은 no.다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/00.html#점수",
    "href": "posts/04_archives/bs_3_1/notes/00.html#점수",
    "title": "3학년 1학기 후기",
    "section": "점수",
    "text": "점수\n컴퓨팅적사고 과목은 코딩 기초 과목인데, 재수강 과목이라 A- 점수가 최대 점수였다.\n그래서 데이터 마이닝 과목만 A+이 나왔다면 이번 학기 받을 수 있는 최대 점수를 얻을 수 있었다.\n점수가 잘 안나온 이유를 몇 가지 생각해 보자면,\n\n이 과목이 발표 중심이라, 상대적으로 발표 능력이 약했던 점에서 마이너스가 됐다.\n기말 분석 과제의 성과가 좋지 않았다.\n마지막 필기 시험에서 검토도 한 번 하지 않고 1등으로 시험지를 제출하고 나왔다.\n\n..정도가 있지 않을까.\n특히 3번은, 나는 시험을 볼 때, 검토를 하면 보통 실수한 문제 1, 2개가 반드시 나오는데, 감히 검토를 하지 않은것은 상당히 큰 문제였다고 할 수 있다.\n사실 이건 이 과목 시험이 마지막 시험이라 빨리 모든걸 끝내고 싶다는 나의 안일하고 연약한 마음가짐이 가장 큰 결점사항이였다고 생각한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/00.html#다른-부족했던-점",
    "href": "posts/04_archives/bs_3_1/notes/00.html#다른-부족했던-점",
    "title": "3학년 1학기 후기",
    "section": "다른 부족했던 점",
    "text": "다른 부족했던 점\n다른 과목은 A+이 나오긴 했지만, 그렇다고 완벽하게 모든걸 잘 처리하지는 않았다.\nOR 과목은 과제를 2번 정도 제출을 못했고, 생산시스템관리는 중간 퀴즈에서 평균보다 못한 점수를 받았다.\n시간이 없어서.. 보다는 시간을 효율적으로 사용하지 못한 원인이 컸다.\n복습도 그날그날 해야 하는데, 이 부분이 잘 이뤄지지 않았다.\n복습을 집에서 하려고 하면 시간이 너무 늦어져서 잘 안하게 되는거 같고, 다음 학기는 학교 안에서 복습까지 하고 집에 가는 방법을 생각해 봐야겠다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/00.html#결론",
    "href": "posts/04_archives/bs_3_1/notes/00.html#결론",
    "title": "3학년 1학기 후기",
    "section": "결론",
    "text": "결론\n그래도 저번 학기에 비하면 시간적으로도 여유가 있었고, 요령도 어느정도 터특을 한것 같다.\n요령은 1학년때 터득을 했어야 하는데, 뭐.. 지난일에 미련을 갖지 말자.\n이정도 추세라면 내가 목표하는 학점 4점은 넘기고 졸업 과업은 무난하게 수행해낼 수 있어 보인다.\n다음학기에는 재수강 과목이 전혀 없기 때문에 드디어 4.5점을 받고, 학기 1등을 해볼 수 있는 기회가 찾아왔다.\n열심히 한 번 해 보자.\n\n\n\n이번에도 어김없이 장학금은 25%",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html",
    "href": "posts/02_areas/deep_learning/notes/03.html",
    "title": "오차역전법",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/02.html#손실함수",
    "href": "posts/02_areas/deep_learning/notes/02.html#손실함수",
    "title": "신경망 학습",
    "section": "손실함수",
    "text": "손실함수\n\n오차 제곱합\n\n\\(E = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - t_i)^2\\)\n\\(y_i\\): 예측값\n\\(t_i\\): 정답값\n\\(\\frac{1}{2}\\)는 미분을 쉽게 하기 위해서 곱해주는 상수\n\n\nimport numpy as np\n\ndef sum_squared_error(y, t):\n    return 0.5 * np.sum((y - t) ** 2)\n\n\n\n교차 엔트로피\n\n\\(E = -\\sum_{i=1}^{n} t_i \\log(y_i)\\)\n일반적으로 정답값인 \\(t_i\\)는 0 또는 1이기 때문에(one hot encoding), \\(t_i = 1\\)인 경우에만 계산된다.\n\\(y_i\\)가 1에 가까울수록 손실이 작아진다.\n\n\ndef cross_entropy_error(y, t):\n    delta = 1e-7  # log(0) 방지\n    return -np.sum(t * np.log(y + delta))",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/index.html",
    "href": "posts/04_archives/toeic_speaking/index.html",
    "title": "토익 스피킹 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-07-19\n        종료일: 2025-07-26\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/index.html#details",
    "href": "posts/04_archives/toeic_speaking/index.html#details",
    "title": "토익 스피킹 준비",
    "section": "Details",
    "text": "Details\n아 하기 싫다.",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/index.html#tasks",
    "href": "posts/04_archives/toeic_speaking/index.html#tasks",
    "title": "토익 스피킹 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/index.html#참고-자료",
    "href": "posts/04_archives/toeic_speaking/index.html#참고-자료",
    "title": "토익 스피킹 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/toeic_speaking/index.html#related-posts",
    "href": "posts/04_archives/toeic_speaking/index.html#related-posts",
    "title": "토익 스피킹 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/08.html#전처리",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/08.html#전처리",
    "title": "텍스트 분석 - 20 뉴스그룹 분류",
    "section": "전처리",
    "text": "전처리\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ntrain_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quates'))\nX_train = train_news.data\ny_train = train_news.target\n\ntest_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quates'))\nX_test = test_news.data\ny_test = test_news.target",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 20 뉴스그룹 분류"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/08.html#학습",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/08.html#학습",
    "title": "텍스트 분석 - 20 뉴스그룹 분류",
    "section": "학습",
    "text": "학습\n\nCount Vector\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncnt_vect = CountVectorizer()\nX_train_cnt_vect = cnt_vect.fit_transform(X_train)\nX_test_cnt_vect = cnt_vect.transform(X_test)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train_cnt_vect, y_train)\npred = lr_clf.predict(X_test_cnt_vect)\nprint(f'{accuracy_score(y_test, pred):.3f} ')\n\n0.731 \n\n\n\n\nTF-IDF\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer()\nX_train_tfidf_vect = tfidf_vect.fit_transform(X_train)\nX_test_tfidf_vect = tfidf_vect.transform(X_test)\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train_tfidf_vect, y_train)\npred = lr_clf.predict(X_test_tfidf_vect)\nprint(f'{accuracy_score(y_test, pred):.3f} ')\n\n0.778",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 20 뉴스그룹 분류"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/02.html#미니배치-학습",
    "href": "posts/02_areas/deep_learning/notes/02.html#미니배치-학습",
    "title": "신경망 학습",
    "section": "미니배치 학습",
    "text": "미니배치 학습\n\n모든 데이터를 한 번에 학습하는 것이 아니라, 일부 데이터만을 사용하여 학습하는 방법\n\n\nimport numpy as np\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\n\ntrain_size = x_train.shape[0]\nbatch_size = 10\nbatch_mask = np.random.choice(train_size, batch_size)\nx_batch = x_train[batch_mask]\nt_batch = t_train[batch_mask]\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + 1e-7)) / batch_size",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/02.html#경사법",
    "href": "posts/02_areas/deep_learning/notes/02.html#경사법",
    "title": "신경망 학습",
    "section": "경사법",
    "text": "경사법\n\n기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향\n기울어진 방향이 반드시 최솟값은 아니지만, 그 방향으로 가야 함수의 값을 줄일 수 있다.\n\n\nimport numpy as np\ndef numerical_gradient(f, x):\n    h = 1e-4\n    grad = np.zeros_like(x)\n\n    for idx in range(x.size):\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x)\n\n        x[idx] = tmp_val - h\n        fxh2 = f(x)\n\n        grad[idx] = (fxh1 - fxh2) / (2 * h)\n        x[idx] = tmp_val\n\n    return grad\n\n\ndef gradient_descent(f, init_x, lr=0.01, step_num=100):\n    x = init_x.copy()\n\n    for _ in range(step_num):\n        grad = numerical_gradient(f, x)\n        x -= lr * grad\n    return x\n\n\nfrom dl_common.functions import softmax, cross_entropy_error\nfrom dl_common.gradient import numerical_gradient\n\nclass simpleNet:\n    def __init__(self):\n        self.W = np.random.randn(2, 3)\n\n    def predict(self, x):\n        return np.dot(x, self.W)\n\n    def loss(self, x, t):\n        z = self.predict(x)\n        y = softmax(z)\n        return cross_entropy_error(y, t)\n\n\nnet = simpleNet()\nx = np.array([0.6, 0.9])\np = net.predict(x)\nnp.argmax(p)\n\n0\n\n\n\nt = np.array([0, 0, 1])\nnet.loss(x, t)\n\n1.5687375483483121\n\n\n\nf = lambda w: net.loss(x, t)\n\ndW = numerical_gradient(f, net.W)\nprint(dW)\n\n[[ 0.39309067  0.08192436 -0.47501503]\n [ 0.58963601  0.12288654 -0.71252255]]",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/02.html#학습-알고리즘-구현",
    "href": "posts/02_areas/deep_learning/notes/02.html#학습-알고리즘-구현",
    "title": "신경망 학습",
    "section": "학습 알고리즘 구현",
    "text": "학습 알고리즘 구현\n\nfrom dl_common.functions import *\nfrom dl_common.gradient import numerical_gradient\n\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, b1 = self.params['W1'], self.params['b1']\n        W2, b2 = self.params['W2'], self.params['b2']\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        return y\n\n    def loss(self, x, t):\n        y = self.predict(x)\n        return cross_entropy_error(y, t)\n\n    def accuracy(self, x, t):\n        y = self.predict(x)\n        if t.ndim != 1:  # one-hot encoding\n            t = np.argmax(t, axis=1)\n        return np.sum(np.argmax(y, axis=1) == t) / float(x.shape[0])\n\n    def numerical_gradient(self, x, t):\n        loss_w = lambda w: self.loss(x, t)\n        grads = {}\n        grads['W1'] = numerical_gradient(loss_w, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_w, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n        return grads\n\n\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\niter_per_epoch = max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    grads = network.numerical_gradient(x_batch, t_batch)\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grads[key]\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/05.html#intro",
    "href": "posts/02_areas/42_seoul/notes/05.html#intro",
    "title": "inception-of-things part 2",
    "section": "Intro",
    "text": "Intro\nPart 1에 이어서 진행해보겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/05.html#개념-설명",
    "href": "posts/02_areas/42_seoul/notes/05.html#개념-설명",
    "title": "inception-of-things part 2",
    "section": "개념 설명",
    "text": "개념 설명\n\n\n\nPart 1 구조\n\n\nPart 1에서 클러스터가 여러 노드들을 논리적으로 묶어주는 기술임을 설명했습니다.\n그렇다면 구체적으로 각각의 노드들이 어떻게 협업을 한다는 것일까요?",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/05.html#코드-설명",
    "href": "posts/02_areas/42_seoul/notes/05.html#코드-설명",
    "title": "inception-of-things part 2",
    "section": "코드 설명",
    "text": "코드 설명\n\nProvision\n\n\nVagrantfile\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"bento/ubuntu-24.04\"\n  config.vm.box_version = \"202404.26.0\"\n\n  config.vm.define \"hyunghkiS\" do |control|\n    control.vm.hostname = \"hyunghkiS\"\n    control.vm.network \"private_network\", ip: \"192.168.56.110\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiS\"]\n      v.memory = \"8192\"\n      v.cpus = \"5\"\n    end\n    config.vm.synced_folder \"confs\", \"/etc/vagrant/confs\"\n    control.vm.provision \"shell\", path: \"scripts/server.sh\"\n  end\nend\n\nVagrant 파일을 이용해서 가상머신을 생성해줍니다. 이번 파트에서는 k3s 클러스터 안에 하나의 노드만 provision하고 있는데, 이 경우 노드 하나가 master, worker 역할을 모두 수행합니다.\n\n\nk3s config file\n\n\ndeploy.yml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-one\n  labels:\n    app: app1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app1\n  template:\n    metadata:\n      labels:\n        app: app1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-two\n  labels:\n    app: app2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-three\n  labels:\n    app: app3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app3\n  template:\n    metadata:\n      labels:\n        app: app3\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\n\n\ning.yml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: \"app1.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: app-one\n            port:\n              number: 80\n  - host: \"app2.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: app-two\n            port:\n              number: 80\n\n\n\nroute.yml\n\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: default-backend\nspec:\n  routes:\n    - match: PathPrefix(`/`)\n      kind: Rule\n      services:\n        - name: app-three\n          port: 80\n\n\n\nsvc.yml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-one\nspec:\n  selector:\n    app: app1\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-two\nspec:\n  selector:\n    app: app2\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-three\nspec:\n  selector:\n    app: app3\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n\n\n\n실행 스크립트\n\n\nscripts/server.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s - server\nuntil kubectl get crd | grep -q 'ingressroutes.traefik.containo.us'; do\n    echo 'waiting for CRD...'\n    sleep 1\ndone\n\n# 작성한 설정 파일을 적용해줍니다.\nkubectl apply -f /etc/vagrant/confs\n\n# pod이 정상적으로 올라올 때 까지 기다려줍니다.\nuntil [ ! -z \"$(kubectl get pods -o jsonpath='{.items[*].metadata.name}')\" ]; do\n    sleep 1\ndone\nfor pod in $(kubectl get pods -o jsonpath='{.items[*].metadata.name}'); do\n    until kubectl get pods $pod | grep -q 'Running'; do\n        sleep 1\n    done\n    app_name=$(kubectl get pods $pod -o jsonpath='{.metadata.labels.app}')\n\n    # 과제에서 제시된 HTML 파일을 생성된 pod에 저장해줍니다.\n    HTML=$(cat &lt;&lt;EOF\n&lt;!DOCTYPE html&gt;\n&lt;html&gt; \n&lt;head&gt; \n    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n    &lt;div class=\"main\"&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;div id=\"message\"&gt; \n    Hello from ${app_name}.\n&lt;/div&gt;\n&lt;div id=\"info\"&gt;\n    &lt;table&gt; \n        &lt;tr&gt;\n            &lt;th&gt;pod:&lt;/th&gt; \n            &lt;td&gt;${pod}&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;th&gt;node:&lt;/th&gt; \n            &lt;td&gt;$(uname -s) ($(uname -r))&lt;/td&gt; \n        &lt;/tr&gt; \n    &lt;/table&gt;\n\n&lt;/div&gt; \n    &lt;/div&gt; \n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n)\n    echo $\"${HTML}\" &gt; /home/vagrant/index.html\n    kubectl cp /home/vagrant/index.html $pod:/usr/share/nginx/html/index.html &gt; /dev/null\ndone\n\nProvision이 완료되고 자동으로 실행되는 script를 작성해줍니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_areas/42_seoul/notes/05.html#outro",
    "href": "posts/02_areas/42_seoul/notes/05.html#outro",
    "title": "inception-of-things part 2",
    "section": "Outro",
    "text": "Outro\nPart 3와 Bonus는 다음 포스팅에서 작성하겠습니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/05.html#합성곱-계층",
    "href": "posts/02_areas/deep_learning/notes/05.html#합성곱-계층",
    "title": "합성곱 신경망",
    "section": "합성곱 계층",
    "text": "합성곱 계층\n\n완전연결 계층의 문제점\n\n데이터의 형상이 무시된다: n차원으로 된 데이터도, 1차원 데이터로 평탄화해서 입력해야 한다.\n\n\n\n연산\n\n입력 데이터 * 필터(커널) + 편향\n\n필터가 완전연결 신경망에서의 가중치 역할을 함.\n단일 곱셈 누산: 이 연산을 윈도우를 일정 간격으로 이동해 가며 입력 데이터에 적용함.\n3차원 이미지(R, G, B)의 경우, 입력과 필터의 채널 크기가 같아야 한다.\n\n패딩: 입력 데이터 주변을 특정 값으로 채움. 크기가 커지면 출력 크기가 커짐.\n스트라이드: 필터를 정용하는 위치의 간격. 크기가 커지면 출력 크기가 작아짐.\n출력 크기\n\n\\(OH = \\frac{H + 2P - FH}{S} + 1\\)\n\\(OW = \\frac{W + 2P - FW}{S} + 1\\)\n\n\\((H, W)\\): 입력 크기, \\((FH, FW)\\): 필터 크기, \\((OH, OW)\\): 출력 크기, \\(P\\): 패딩, \\(S\\): 스트라이드",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/05.html#풀링-계층",
    "href": "posts/02_areas/deep_learning/notes/05.html#풀링-계층",
    "title": "합성곱 신경망",
    "section": "풀링 계층",
    "text": "풀링 계층\n\n세로, 가로 방향의 공간을 줄이는 연산 (차원 축소 계층)\n풀링의 윈도우 크기와 스트라이드 값은 같게 설정하는 것이 일반적\n대상 영역에서 최대 / 평균 값을 산출\n\n\n특징\n\n학습해야할 매개변수가 없음\n채널 수가 변하지 않음\n입력의 변화에 영향을 적게 받는다",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/05.html#전체-구조",
    "href": "posts/02_areas/deep_learning/notes/05.html#전체-구조",
    "title": "합성곱 신경망",
    "section": "전체 구조",
    "text": "전체 구조\n\nfrom dl_common.util import im2col\nimport numpy as np\n\nx1 = np.random.rand(1, 3, 7, 7) # 1개 배치, 3 채널, 7 x 7 크기 이미지\ncol1 = im2col(x1, 5, 5, stride=1, pad=0)\n\nx2 = np.random.rand(10, 3, 7, 7)\ncol2 = im2col(x2, 5, 5, stride=1, pad=0)\n\n\nclass Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n\n    def forward(self, x):\n        FN, C, FH, FW =  self.W.shape\n        N, C, H, W = x.shape\n        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n        \n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col_W = self.W.reshape(FN, -1).T\n        out = np.dot(col, col_W) + self.b\n\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        return out",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/03.html#plot-importance",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/03.html#plot-importance",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "plot importance",
    "text": "plot importance\n\nfrom xgboost import plot_importance\n\nplot_importance(xgb_clf, max_num_features=20, height=0.4)",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/03.html#lightgbm",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/03.html#lightgbm",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "LightGBM",
    "text": "LightGBM\n\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\n\nlgbm_clf = LGBMClassifier(n_estimators=500, early_stopping_rounds=100, eval_metric='auc')\n\neval_set = [(X_tr, y_tr), (X_val, y_val)]\nlgbm_clf.fit(X_tr, y_tr, eval_set=eval_set)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\nprint(f'{lgbm_roc_score:.3f}')\n\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Info] Number of positive: 1653, number of negative: 40918\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 13447\n[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 251\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978\n[LightGBM] [Info] Start training from score -3.208978\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[28]    training's binary_logloss: 0.117279 valid_1's binary_logloss: 0.137813\n[LightGBM] [Warning] Unknown parameter: eval_metric\n0.834\n\n\n\n베이지안 최적화\n\nfrom sklearn.model_selection import KFold\n\ndef objective_func(search_space):\n    lgbm_clf = LGBMClassifier(n_estimators=100, \n                            early_stopping_rounds=30,\n                            eval_metric='auc',\n                            num_leaves=int(search_space['num_leaves']),\n                            max_depth=int(search_space['max_depth']),\n                            min_child_samples=int(search_space['min_child_samples']),\n                            subsample=search_space['subsample'],\n                            learning_rate=search_space['learning_rate'])\n    roc_auc_list = []\n    kf = KFold(n_splits=3)\n    for tr_index, val_index in kf.split(X_train):\n        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n        X_val, y_val =  X_train.iloc[val_index], y_train.iloc[val_index]\n\n        lgbm_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])\n        score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, 1])\n        roc_auc_list.append(score)\n\n    return -1 * np.mean(roc_auc_list)\n\n\nfrom hyperopt import hp, fmin, tpe, Trials\n\nlgbm_search_space = {\n  'num_leaves': hp.quniform('num_leaves', 32, 64, 1),\n  'max_depth': hp.quniform('max_depth', 100, 160, 1),\n  'min_child_samples': hp.quniform('min_child_samples', 60, 100, 1),\n  'subsample': hp.uniform('subsample', 0.7, 1),\n  'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n}\n\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=lgbm_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trials)\nprint(best)\n\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009804 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12809\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.168309\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:\n[36]    training's binary_logloss: 0.121676 valid_1's binary_logloss: 0.127049\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011714 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.194075\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:\n[44]    training's binary_logloss: 0.115084 valid_1's binary_logloss: 0.135595\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12874\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.233233\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.110571 valid_1's binary_logloss: 0.140209\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006776 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12809\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Early stopping, best iteration is:\n[68]    training's binary_logloss: 0.119949 valid_1's binary_logloss: 0.127337\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:02&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008185 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12874\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:\n[74]    training's binary_logloss: 0.114945 valid_1's binary_logloss: 0.135003\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005666 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Total Bins 12865\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:03&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 Did not meet early stopping. Best iteration is:\n[75]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.140191\n  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:04&lt;01:41,  2.08s/trial, best loss: -0.8354243542379886]  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006637 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[21]    training's binary_logloss: 0.121998 valid_1's binary_logloss: 0.127349\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:04&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12970\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.115056 valid_1's binary_logloss: 0.136143\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008614 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 13049\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.110308 valid_1's binary_logloss: 0.140967\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:05&lt;01:41,  2.12s/trial, best loss: -0.8361046999787884]  6%|▌         | 3/50 [00:05&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006389 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.119702 valid_1's binary_logloss: 0.127682\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005965 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[21]    training's binary_logloss: 0.11491  valid_1's binary_logloss: 0.13632\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:06&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006447 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.113764 valid_1's binary_logloss: 0.141398\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:07&lt;01:31,  1.95s/trial, best loss: -0.8361046999787884]  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:07&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:\n[71]    training's binary_logloss: 0.114179 valid_1's binary_logloss: 0.127237\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:08&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[57]    training's binary_logloss: 0.113751 valid_1's binary_logloss: 0.136174\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007084 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[58]    training's binary_logloss: 0.11113  valid_1's binary_logloss: 0.140897\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:09&lt;01:24,  1.83s/trial, best loss: -0.8361046999787884] 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:09&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006562 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[23]    training's binary_logloss: 0.120721 valid_1's binary_logloss: 0.127623\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005976 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[21]    training's binary_logloss: 0.117914 valid_1's binary_logloss: 0.135692\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:10&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.117142 valid_1's binary_logloss: 0.141073\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:11&lt;01:28,  1.98s/trial, best loss: -0.8361046999787884] 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005795 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[23]    training's binary_logloss: 0.122492 valid_1's binary_logloss: 0.127389\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:11&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12882\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.119931 valid_1's binary_logloss: 0.13599\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12883\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[21]    training's binary_logloss: 0.116742 valid_1's binary_logloss: 0.14122\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:12&lt;01:20,  1.83s/trial, best loss: -0.8361046999787884] 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 14%|█▍        | 7/50 [00:12&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[37]    training's binary_logloss: 0.118076 valid_1's binary_logloss: 0.12711\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:13&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[27]    training's binary_logloss: 0.118244 valid_1's binary_logloss: 0.135768\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.11261  valid_1's binary_logloss: 0.140798\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:14&lt;01:12,  1.69s/trial, best loss: -0.8361046999787884] 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007634 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12907\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:14&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:\n[93]    training's binary_logloss: 0.113871 valid_1's binary_logloss: 0.127108\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007168 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12934\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:15&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:\n[75]    training's binary_logloss: 0.113106 valid_1's binary_logloss: 0.135792\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008788 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12989\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:16&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 Did not meet early stopping. Best iteration is:\n[82]    training's binary_logloss: 0.109277 valid_1's binary_logloss: 0.140921\n 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:17&lt;01:14,  1.76s/trial, best loss: -0.8361046999787884] 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007124 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12809\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:17&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.168309\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[57]    training's binary_logloss: 0.120677 valid_1's binary_logloss: 0.127111\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12874\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.194075\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:18&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.118347 valid_1's binary_logloss: 0.135488\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Total Bins 12865\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Info] Start training from score -3.233233\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:19&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 Early stopping, best iteration is:\n[54]    training's binary_logloss: 0.114424 valid_1's binary_logloss: 0.140196\n 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:20&lt;01:27,  2.14s/trial, best loss: -0.8361046999787884] 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006974 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12907\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:20&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[78]    training's binary_logloss: 0.111459 valid_1's binary_logloss: 0.12715\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006741 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12943\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[58]    training's binary_logloss: 0.112371 valid_1's binary_logloss: 0.13579\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:21&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007176 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13017\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:22&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[76]    training's binary_logloss: 0.105423 valid_1's binary_logloss: 0.141018\n 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:25&lt;01:30,  2.26s/trial, best loss: -0.8361046999787884] 22%|██▏       | 11/50 [00:25&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006851 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.123258 valid_1's binary_logloss: 0.127671\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008232 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:26&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.118847 valid_1's binary_logloss: 0.135735\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006765 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.115967 valid_1's binary_logloss: 0.140884\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:27&lt;02:09,  3.33s/trial, best loss: -0.8361046999787884] 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:27&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007459 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[17]    training's binary_logloss: 0.118279 valid_1's binary_logloss: 0.128419\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:28&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[16]    training's binary_logloss: 0.114963 valid_1's binary_logloss: 0.136964\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.111041 valid_1's binary_logloss: 0.141811\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:29&lt;01:49,  2.88s/trial, best loss: -0.8361046999787884] 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:29&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006716 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.117911 valid_1's binary_logloss: 0.127609\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007818 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:30&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.112846 valid_1's binary_logloss: 0.135945\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007760 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[21]    training's binary_logloss: 0.11335  valid_1's binary_logloss: 0.141758\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:31&lt;01:36,  2.60s/trial, best loss: -0.8361046999787884] 28%|██▊       | 14/50 [00:31&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[15]    training's binary_logloss: 0.123793 valid_1's binary_logloss: 0.127794\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:32&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[17]    training's binary_logloss: 0.117509 valid_1's binary_logloss: 0.136341\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[14]    training's binary_logloss: 0.118131 valid_1's binary_logloss: 0.141827\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:33&lt;01:29,  2.48s/trial, best loss: -0.8361046999787884] 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006221 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:33&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.116669 valid_1's binary_logloss: 0.127316\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:34&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.112195 valid_1's binary_logloss: 0.13634\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008114 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.110627 valid_1's binary_logloss: 0.141491\n 30%|███       | 15/50 [00:35&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:36&lt;01:18,  2.23s/trial, best loss: -0.8361046999787884] 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006910 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[44]    training's binary_logloss: 0.11468  valid_1's binary_logloss: 0.127009\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:36&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008679 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.116699 valid_1's binary_logloss: 0.136132\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009082 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:37&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[37]    training's binary_logloss: 0.110851 valid_1's binary_logloss: 0.140914\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:38&lt;01:17,  2.28s/trial, best loss: -0.8361046999787884] 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009004 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:38&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.133099 valid_1's binary_logloss: 0.130118\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:39&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.128591 valid_1's binary_logloss: 0.138373\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006967 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:40&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.125837 valid_1's binary_logloss: 0.144181\n 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:41&lt;01:17,  2.35s/trial, best loss: -0.8361046999787884] 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007196 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:41&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[35]    training's binary_logloss: 0.117838 valid_1's binary_logloss: 0.127509\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009471 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12882\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.114266 valid_1's binary_logloss: 0.136132\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:42&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12935\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[44]    training's binary_logloss: 0.107097 valid_1's binary_logloss: 0.141644\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:43&lt;01:18,  2.47s/trial, best loss: -0.8361046999787884] 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:43&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006872 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12911\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[32]    training's binary_logloss: 0.120651 valid_1's binary_logloss: 0.12748\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12970\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:44&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.117806 valid_1's binary_logloss: 0.135748\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007569 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 13049\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  Early stopping, best iteration is:\n[39]    training's binary_logloss: 0.111183 valid_1's binary_logloss: 0.140593\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:45&lt;01:17,  2.49s/trial, best loss: -0.8361046999787884] 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:45&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010567 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.127621 valid_1's binary_logloss: 0.127975\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:46&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028233 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.123117 valid_1's binary_logloss: 0.136484\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:47&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007220 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.120407 valid_1's binary_logloss: 0.141888\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:48&lt;01:10,  2.36s/trial, best loss: -0.8361046999787884] 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:48&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008396 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12809\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[94]    training's binary_logloss: 0.120321 valid_1's binary_logloss: 0.12706\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007023 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12874\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 42%|████▏     | 21/50 [00:49&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[95]    training's binary_logloss: 0.116042 valid_1's binary_logloss: 0.135298\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007237 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Total Bins 12865\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:50&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.112382 valid_1's binary_logloss: 0.140103\n 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:51&lt;01:13,  2.53s/trial, best loss: -0.8361046999787884] 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007562 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [00:51&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.120691 valid_1's binary_logloss: 0.127296\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009653 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [00:52&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[99]    training's binary_logloss: 0.116561 valid_1's binary_logloss: 0.135521\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007650 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [00:53&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.113883 valid_1's binary_logloss: 0.140482\n 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:54&lt;01:11,  2.56s/trial, best loss: -0.8361206531552328] 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [00:54&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.12506  valid_1's binary_logloss: 0.127517\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007298 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [00:55&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.120784 valid_1's binary_logloss: 0.135909\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [00:56&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.11795  valid_1's binary_logloss: 0.141026\n 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [00:57&lt;01:11,  2.65s/trial, best loss: -0.8361206531552328] 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006418 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [00:57&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[49]    training's binary_logloss: 0.120728 valid_1's binary_logloss: 0.12726\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:00&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007255 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[51]    training's binary_logloss: 0.116085 valid_1's binary_logloss: 0.13534\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:01&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[60]    training's binary_logloss: 0.11095  valid_1's binary_logloss: 0.140369\n 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:02&lt;01:10,  2.71s/trial, best loss: -0.8361206531552328] 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:02&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[97]    training's binary_logloss: 0.118162 valid_1's binary_logloss: 0.127362\n 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:03&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006679 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[93]    training's binary_logloss: 0.114804 valid_1's binary_logloss: 0.135408\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:04&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007134 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[98]    training's binary_logloss: 0.111333 valid_1's binary_logloss: 0.140214\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:05&lt;01:28,  3.53s/trial, best loss: -0.8361206531552328] 52%|█████▏    | 26/50 [01:05&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006927 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[35]    training's binary_logloss: 0.11927  valid_1's binary_logloss: 0.127628\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007967 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:06&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[33]    training's binary_logloss: 0.11608  valid_1's binary_logloss: 0.136215\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007403 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:07&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[37]    training's binary_logloss: 0.111732 valid_1's binary_logloss: 0.141059\n 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:08&lt;01:23,  3.49s/trial, best loss: -0.8361206531552328] 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:08&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[65]    training's binary_logloss: 0.11962  valid_1's binary_logloss: 0.127063\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007475 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:09&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[72]    training's binary_logloss: 0.114002 valid_1's binary_logloss: 0.135602\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008925 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:10&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[71]    training's binary_logloss: 0.111386 valid_1's binary_logloss: 0.140329\n 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:11&lt;01:11,  3.11s/trial, best loss: -0.8361206531552328] 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006519 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.123961 valid_1's binary_logloss: 0.127545\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010127 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:11&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[32]    training's binary_logloss: 0.117126 valid_1's binary_logloss: 0.13534\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007890 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:12&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[41]    training's binary_logloss: 0.111145 valid_1's binary_logloss: 0.140511\n 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:13&lt;01:06,  3.03s/trial, best loss: -0.8361206531552328] 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010351 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:13&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.135256 valid_1's binary_logloss: 0.131344\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007594 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.130846 valid_1's binary_logloss: 0.139185\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:14&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006775 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.127867 valid_1's binary_logloss: 0.14514\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:15&lt;00:57,  2.75s/trial, best loss: -0.8361206531552328] 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006594 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:15&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.127239 valid_1's binary_logloss: 0.127384\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:16&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.12288  valid_1's binary_logloss: 0.135571\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.120284 valid_1's binary_logloss: 0.140863\n 60%|██████    | 30/50 [01:17&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:18&lt;00:54,  2.70s/trial, best loss: -0.8361206531552328] 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008255 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:18&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[76]    training's binary_logloss: 0.119732 valid_1's binary_logloss: 0.127276\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007609 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[70]    training's binary_logloss: 0.116807 valid_1's binary_logloss: 0.135585\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:19&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[80]    training's binary_logloss: 0.112368 valid_1's binary_logloss: 0.14032\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:20&lt;00:49,  2.59s/trial, best loss: -0.8361206531552328] 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:20&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007186 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.120657 valid_1's binary_logloss: 0.126949\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:21&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[42]    training's binary_logloss: 0.118801 valid_1's binary_logloss: 0.135645\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006469 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[51]    training's binary_logloss: 0.113559 valid_1's binary_logloss: 0.140513\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:22&lt;00:47,  2.66s/trial, best loss: -0.8361206531552328] 66%|██████▌   | 33/50 [01:22&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012104 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[36]    training's binary_logloss: 0.121629 valid_1's binary_logloss: 0.127166\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:23&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.117903 valid_1's binary_logloss: 0.13587\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011244 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.115553 valid_1's binary_logloss: 0.140845\n 66%|██████▌   | 33/50 [01:24&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:25&lt;00:42,  2.48s/trial, best loss: -0.8361206531552328] 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007149 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[16]    training's binary_logloss: 0.118882 valid_1's binary_logloss: 0.128522\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008471 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 68%|██████▊   | 34/50 [01:25&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[11]    training's binary_logloss: 0.12009  valid_1's binary_logloss: 0.136809\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012520 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:26&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[14]    training's binary_logloss: 0.114296 valid_1's binary_logloss: 0.141912\n 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:27&lt;00:37,  2.36s/trial, best loss: -0.8361206531552328] 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12870\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[67]    training's binary_logloss: 0.117834 valid_1's binary_logloss: 0.127248\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:27&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008813 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12934\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[55]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.135743\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12939\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 70%|███████   | 35/50 [01:28&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[62]    training's binary_logloss: 0.112207 valid_1's binary_logloss: 0.140686\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:29&lt;00:33,  2.26s/trial, best loss: -0.8361206531552328] 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:29&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[72]    training's binary_logloss: 0.116081 valid_1's binary_logloss: 0.12713\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008642 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:30&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[49]    training's binary_logloss: 0.117446 valid_1's binary_logloss: 0.135845\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006554 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:31&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  Early stopping, best iteration is:\n[54]    training's binary_logloss: 0.113495 valid_1's binary_logloss: 0.140635\n 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:32&lt;00:32,  2.33s/trial, best loss: -0.8361206531552328] 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12809\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:32&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[97]    training's binary_logloss: 0.11803  valid_1's binary_logloss: 0.126746\n 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:35&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008127 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12874\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[85]    training's binary_logloss: 0.115626 valid_1's binary_logloss: 0.135332\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:36&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013095 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Total Bins 12865\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  Did not meet early stopping. Best iteration is:\n[89]    training's binary_logloss: 0.112365 valid_1's binary_logloss: 0.140135\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:37&lt;00:31,  2.40s/trial, best loss: -0.8361206531552328] 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007640 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:37&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[98]    training's binary_logloss: 0.123399 valid_1's binary_logloss: 0.126912\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007213 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:38&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.119208 valid_1's binary_logloss: 0.135189\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011345 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:39&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.116397 valid_1's binary_logloss: 0.140343\n 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:40&lt;00:40,  3.35s/trial, best loss: -0.8368093643173017] 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006904 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:40&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.13496  valid_1's binary_logloss: 0.13089\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006719 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.130487 valid_1's binary_logloss: 0.138913\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:41&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12939\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.12765  valid_1's binary_logloss: 0.144942\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:42&lt;00:34,  3.12s/trial, best loss: -0.8368093643173017] 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:42&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008446 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[91]    training's binary_logloss: 0.119162 valid_1's binary_logloss: 0.126781\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010092 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12943\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:43&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[76]    training's binary_logloss: 0.117526 valid_1's binary_logloss: 0.135504\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018761 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:44&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[84]    training's binary_logloss: 0.113084 valid_1's binary_logloss: 0.140427\n 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:45&lt;00:29,  2.92s/trial, best loss: -0.8368093643173017] 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007948 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:45&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.124667 valid_1's binary_logloss: 0.127059\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006578 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:46&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.120428 valid_1's binary_logloss: 0.135621\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006144 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.117733 valid_1's binary_logloss: 0.140661\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:47&lt;00:25,  2.86s/trial, best loss: -0.8368093643173017] 84%|████████▍ | 42/50 [01:47&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008411 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.120438 valid_1's binary_logloss: 0.127484\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006109 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:48&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.118015 valid_1's binary_logloss: 0.13605\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006744 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.115064 valid_1's binary_logloss: 0.14127\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:49&lt;00:22,  2.76s/trial, best loss: -0.8368093643173017] 86%|████████▌ | 43/50 [01:49&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[16]    training's binary_logloss: 0.117532 valid_1's binary_logloss: 0.128445\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:50&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[13]    training's binary_logloss: 0.116506 valid_1's binary_logloss: 0.136088\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007065 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13049\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.10854  valid_1's binary_logloss: 0.14215\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:51&lt;00:17,  2.51s/trial, best loss: -0.8368093643173017] 88%|████████▊ | 44/50 [01:51&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[31]    training's binary_logloss: 0.120736 valid_1's binary_logloss: 0.127726\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007563 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:52&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.117682 valid_1's binary_logloss: 0.135689\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010431 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[35]    training's binary_logloss: 0.112434 valid_1's binary_logloss: 0.141034\n 88%|████████▊ | 44/50 [01:53&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:54&lt;00:14,  2.37s/trial, best loss: -0.8368093643173017] 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007370 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12907\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.114951 valid_1's binary_logloss: 0.127139\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:54&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12970\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.114337 valid_1's binary_logloss: 0.136683\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007329 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 13017\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [01:55&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[22]    training's binary_logloss: 0.110257 valid_1's binary_logloss: 0.141881\n 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:56&lt;00:11,  2.29s/trial, best loss: -0.8368093643173017] 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[45]    training's binary_logloss: 0.120473 valid_1's binary_logloss: 0.127224\n 92%|█████████▏| 46/50 [01:56&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007092 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[47]    training's binary_logloss: 0.115765 valid_1's binary_logloss: 0.135738\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 92%|█████████▏| 46/50 [01:57&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12865\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[43]    training's binary_logloss: 0.114087 valid_1's binary_logloss: 0.140748\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [01:58&lt;00:09,  2.26s/trial, best loss: -0.8368093643173017] 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007464 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12870\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [01:58&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.117042 valid_1's binary_logloss: 0.127461\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006729 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [01:59&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[58]    training's binary_logloss: 0.110513 valid_1's binary_logloss: 0.135839\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011866 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12989\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [02:00&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[61]    training's binary_logloss: 0.107023 valid_1's binary_logloss: 0.140678\n 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:01&lt;00:06,  2.26s/trial, best loss: -0.8368093643173017] 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12818\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:01&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.116856 valid_1's binary_logloss: 0.127122\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12934\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:02&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[36]    training's binary_logloss: 0.117447 valid_1's binary_logloss: 0.13599\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006844 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12935\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[48]    training's binary_logloss: 0.110674 valid_1's binary_logloss: 0.140834\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:03&lt;00:04,  2.44s/trial, best loss: -0.8368093643173017] 98%|█████████▊| 49/50 [02:03&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1637, number of negative: 38907\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007160 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12809\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040376 -&gt; initscore=-3.168309\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.168309\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.117901 valid_1's binary_logloss: 0.128002\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:04&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1597, number of negative: 38947\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008421 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039389 -&gt; initscore=-3.194075\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.194075\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.11285  valid_1's binary_logloss: 0.135927\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of positive: 1538, number of negative: 39006\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007163 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Total Bins 12874\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037934 -&gt; initscore=-3.233233\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Info] Start training from score -3.233233\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:05&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.110102 valid_1's binary_logloss: 0.141424\n 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:06&lt;00:02,  2.50s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.48s/trial, best loss: -0.8368093643173017]100%|██████████| 50/50 [02:06&lt;00:00,  2.53s/trial, best loss: -0.8368093643173017]\n{'learning_rate': 0.043324531254078945, 'max_depth': 133.0, 'min_child_samples': 85.0, 'num_leaves': 36.0, 'subsample': 0.7305792288105732}\n\n\n\n\n재학습\n\nlgbm_clf = LGBMClassifier(n_estimators=500, \n                          num_leaves=int(best['num_leaves']),\n                          max_depth=int(best['max_depth']),\n                          min_child_samples=int(best['min_child_samples']),\n                          subsample=round(best['subsample'], 5),\n                          learning_rate=round(best['learning_rate'], 5),\n                          early_stopping_rounds=100, \n                          eval_metric='auc')\n\neval_set = [(X_tr, y_tr), (X_val, y_val)]\nlgbm_clf.fit(X_tr, y_tr, eval_set=eval_set)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\nprint(f'{lgbm_roc_score:.3f}')\n\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Info] Number of positive: 1653, number of negative: 40918\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12969\n[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 192\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038829 -&gt; initscore=-3.208978\n[LightGBM] [Info] Start training from score -3.208978\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[62]    training's binary_logloss: 0.119449 valid_1's binary_logloss: 0.137449\n[LightGBM] [Warning] Unknown parameter: eval_metric\n0.838",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/03.html#제출",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/03.html#제출",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "제출",
    "text": "제출\n\ntarget = lgbm_clf.predict(test_df)\n\nsubmit = pd.read_csv('_data/santander/sample_submission.csv', encoding='latin-1')\nsubmit['TARGET'] = target\nsubmit.to_csv('_data/santander/submission.csv', encoding='latin-1', index=False)\n\n[LightGBM] [Warning] Unknown parameter: eval_metric",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/core/00.html#질적-변수",
    "href": "posts/01_projects/adp_실기/notes/core/00.html#질적-변수",
    "title": "EDA",
    "section": "질적 변수",
    "text": "질적 변수\n\n상관분석\n\nfrom scipy.stats import spearmanr, kendalltau\n\ncorr, p = spearmanr(df['var1'], df['var2'])\n\ncorr, p = kendalltau(df['var1'], df['var2'])\n\ndf.corr(method='kendall') # kendal, spearman\n\n\nfrom scipy.stats.contingeny import association\n\nv2 = association(table.values, method=\"tschuprow\") # phi 계수\nv2 = association(table.values, method='cramer') # 크래머 v\nv2\n\n\n상관계수: 공분산을 각 변수의 표준편차로 나눈 것\n스피어만 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.\n켄달의 타우: 서열척도 vs 서열척도.\n\n둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.\n샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.\n두 변수의 크기는 같아야함.\n\nphi 계수: 명목척도 vs 명목척도\n\n두 변인 모두 level이 2개일 때 사용\n두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산\n\n크래머 v: 명목척도 vs 명목척도.\n\n적어도 하나의 변수가 3개 이상의 level을 가지면 사용\n범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.\n\nPoint-biserial correlation: 명목척도 vs 연속형\n\n명목척도의 level이 2개일 때\n\nPolyserial correlation: 명목척도 vs 연속형\n\n명목척도의 level이 3개 이상일 때\n\n명목과 순서의 경우\n\nlevel이 2개: Mann-Whitney U검정\n3개 이상: Kruskal-Wallis H test\n\n\n\n\n시각화\n\nimport pandas as pd\n\ncols = ['your', 'target', 'cols', ...]\nfreq = pd.DataFrame(df[cols].value_counts()) # 도수분포표\nfreq['proportion'] = df[cols].value_counts(normalize=True) # 상대도수분포표\n\n\nfreq['count'].plot.bar(figsize=(15, 10), subplots=True, layout=(3, 3))\nplt.tight_layout()\nplt.show()\n\n\nplt.pie(freq['count'].values, \n        labels=freq.index, \n        autopct='%1.1f%%', \n        colors=sns.color_palette('pastel', n_colors=len(freq)))\nplt.show()",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "EDA"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/core/00.html#양적-변수",
    "href": "posts/01_projects/adp_실기/notes/core/00.html#양적-변수",
    "title": "EDA",
    "section": "양적 변수",
    "text": "양적 변수\n\n기술통계\n\nfrom scipy.stats.mstats import gmean, hmean, tmean\nimport numpy as np\n\nnp.mean(example) # 산술평균\ngmean(example) # 기하평균\nhmean(example) # 조화평균\ntmean(example, (1, 5)) # 절사평균\nnp.sqrt(np.mean(np.array(example) ** 2)) # 평방평균\n\n\n기하평균: 비율의 평균에 주로 사용됨. 한 값이라도 0이면 전체가 0이 됨\n조화평균: 속도, 밀도 등의 평균에 주로 사용됨.\n절사평균: 극단값의 영향을 줄이기 위해 상위, 하위 몇 %를 제외한 평균\n평방평균: 신호, 파동 등에서 자주 사용\n\n\ndf.median()\ndf.mode()[0]\ndf.quantile(q=0.25)\n\n\n상관계수: 피어슨\n\n\n\n시각화\n\n도수분포표\n상대도수분포표\n줄기잎그림\n히스토그램\n상자그림\n산점도",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "EDA"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/04.html#개요",
    "href": "posts/00_inboxes/notes/04.html#개요",
    "title": "다중 공산성",
    "section": "개요",
    "text": "개요\n\n횡단면: 여러 개체를 한 시점에서 관찰\n시계열: 한 개체를 여러 시점에서 관찰\n패널: 여러 개체를 여러 시점에서 관찰\n\nwithing: 객체\nbetween: 시점\n\n\n\n갭이 존재: 특정 시점의 자료가 누락\nbalanced panel: 모든 개체가 같은 양의 시점 자료를 갖는 경우",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/04.html#상관",
    "href": "posts/00_inboxes/notes/04.html#상관",
    "title": "다중 공산성",
    "section": "상관",
    "text": "상관\n\n동시적 상관: 특정 시점에서 개체 간 상관관계\n자기 상관: 특정 개체의 시점 간 상관관계",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/04.html#모형",
    "href": "posts/00_inboxes/notes/04.html#모형",
    "title": "다중 공산성",
    "section": "모형",
    "text": "모형\n\n합동 OLS(POLS): 모든 개체를 하나의 집단으로 보고 분석\n개체 간 효과 모형: 개체별로 분석. 그러나 시계열은 고려하지 않음\n고정 효과 모형: 개체별로 분석. 시계열을 고려. 개체의 이지성이 독립변수와 관련이 있다고 가정\n\nsigma_u: 관찰되지 않은 개체별 이지성\nsigma_e: 시간과 객체에 따라 변화하는 순수한 오차항\n관찰되지 않은 이지성을 제거하는 방법\n관찰되지 않은 이지성을 추정하는 방법",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/03.html#다중-공산성",
    "href": "posts/00_inboxes/notes/03.html#다중-공산성",
    "title": "다중 공산성",
    "section": "다중 공산성",
    "text": "다중 공산성\n\n독립 변수 집합 X 가 본래는 서로 독립적(직교적)이어야 한다는 가정에서 벗어난 정도.",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/03.html#문제",
    "href": "posts/00_inboxes/notes/03.html#문제",
    "title": "다중 공산성",
    "section": "문제",
    "text": "문제\n\n회귀계수 추정치의 분산이 커지고, 결과적으로 추정이 매우 불안정해짐.\n특정 독립변수가 설명하는 효과를 다른 변수와 구분하기 어려워짐.\n예측력이나 설명력이 높게 나올 수 있지만, 모형의 구조적 해석은 신뢰할 수 어려움.\n물론 이런 문제들은 중요한 변수에 영향을 줄 때 생김.",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/00_inboxes/notes/03.html#해결-방법",
    "href": "posts/00_inboxes/notes/03.html#해결-방법",
    "title": "다중 공산성",
    "section": "해결 방법",
    "text": "해결 방법\n\n잘못된 예시\n\n그냥 둔다\n직교화\n\nPCA(주성분 분석)나 요인분석 등을 사용하여 독립변수들을 직교화\n요인이 해석 불가능한 경우가 아니면 좋지 않음\n\n규칙 기반 접근: 상관계수가 0.8이 넘는걸 제거하거나, 종속변수의 상관계수보다 높은 변수 제거\n\n직관에만 의존하고, 잘못된 결론을 낳을 수 있음\n\n\n\n\n좋은 예시\n다중 공산성의 문제를 세부적으로 진단하고 각각에 대해 해결 방법을 다음과 같이 제시한다.\n\n전 변수 집합 대상:\n\n독립변수의 전체 차원이 부족한 경우\n표본을 더 모으거나 새로운 변수를 도입\n\n개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)\n\n특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우\nVIF가 10을 넘을 경우\n덜 중요하다면 제거\n중요하다면 변수에 대한 독립적 정보 보강(세분화, …)\n\n두 변수의 상관계수가 높은 경우\n\n둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)\n둘 중 하나를 제거",
    "crumbs": [
      "PARA",
      "Inboxes",
      "Notes",
      "다중 공산성"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/machine_learning/04.html#under-over-sampling",
    "href": "posts/01_projects/adp_실기/notes/machine_learning/04.html#under-over-sampling",
    "title": "분류 - 신용 카드 사기 검출",
    "section": "under, over sampling",
    "text": "under, over sampling\n\nunder sampling: 많은 비중을 차지하는 레이블을 작은 비중의 레이블에 맞추는것\nover sampling: 반대\n\nsmote: k 최근접 이웃 진행 후, 이웃 간 간격을 맞추는 record를 새로 생성하는 방식\n\n\n\nimport pandas as pd",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 신용 카드 사기 검출"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html#계산-그래프",
    "href": "posts/02_areas/deep_learning/notes/03.html#계산-그래프",
    "title": "오차역전법",
    "section": "계산 그래프",
    "text": "계산 그래프\n\n특징\n\n국소적 계산: 자신과 관계된 정보만 결과로 출력\n중간 결과를 모두 저장할 수 있다.\n역전파를 통해 특정 단계에서의 최정 결과에 대한 미분을 효율적으로 계산할 수 있다.\n\n\n\n예시 - 순전파\n\n철수는 슈퍼에서 사과를 2개 귤을 3개 샀다. 사과는 1개 100원, 귤은 1개 150원이다. 소비세가 10%일 때 지불 금액은?\n\n\n\n\n\n\nflowchart LR\n    apple_num((사과 갯수)) -- 2 --&gt; apple_x\n    apple_price((사과 가격)) -- 100 --&gt; apple_x\n    orange_num((귤 갯수)) -- 3 --&gt; orange_x\n    orange_price((귤 가격)) -- 150 --&gt; orange_x\n    tax((소비세)) -- 1.1 --&gt; total_x\n    apple_x[x] -- 200 --&gt; fruit_plus\n    orange_x[x] -- 450 --&gt; fruit_plus\n    fruit_plus(+) -- 650 --&gt; total_x\n    total_x[x] -- 715 --&gt; output((지불 금액))",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html#오차역전법",
    "href": "posts/02_areas/deep_learning/notes/03.html#오차역전법",
    "title": "오차역전법",
    "section": "오차역전법",
    "text": "오차역전법\n\n연쇄법칙을 생각하면 됨.\n\n\n덧셈 노드 역전파\n\n덧셈 노드의 역전파는 입력값을 그대로 전달한다.\n\n\n\n곱셈 노드 역전파\n\n곱셈 노드의 역전파는 입력값을 그대로 전달하되, 다른 입력값을 곱해준다.\n\n\n\n예시 - 역전파\n\n철수는 슈퍼에서 사과를 2개 귤을 3개 샀다. 사과는 1개 100원, 귤은 1개 150원이다. 소비세가 10%일 때 지불 금액은?\n\n\n\n\n\n\nflowchart LR\n    apple_num((사과 갯수)) -- 2 --&gt; apple_x\n    apple_price((사과 가격)) -- 100 --&gt; apple_x\n    tax((소비세)) -- 1.1 --&gt; total_x\n    apple_x[x] -- 200 --&gt; total_x\n    total_x[x] -- 220 --&gt; output((지불 금액))\n\n    apple_x -. 110 .-&gt; apple_num\n    apple_x -. 2.2 .-&gt; apple_price\n    total_x -. 200 .-&gt; tax\n    total_x -. 1.1 .-&gt; apple_x\n    output -. 1 .-&gt; total_x",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html#python-구현",
    "href": "posts/02_areas/deep_learning/notes/03.html#python-구현",
    "title": "오차역전법",
    "section": "python 구현",
    "text": "python 구현\n\nclass MulLayer:\n\n    def __init__(self):\n        self.x = None\n        self.y = None\n\n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        out = x * y\n\n        return out\n\n    def backward(self, dout):\n        dx = dout * self.y\n        dy = dout * self.x\n\n        return dx, dy\n\n\nclass AddLayer:\n    def __init__(self):\n        pass\n\n    def forward(self, x, y):\n        out = x + y\n        return out\n\n    def backward(self, dout):\n        dx = dout * 1\n        dy = dout * 1\n        return dx, dy",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html#활성화-함수-계층",
    "href": "posts/02_areas/deep_learning/notes/03.html#활성화-함수-계층",
    "title": "오차역전법",
    "section": "활성화 함수 계층",
    "text": "활성화 함수 계층\n\nReLU\n\n입력이 0보다 크면 그대로, 작으면 0\n\n\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x &lt;= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dx = dout.copy()\n        dx[self.mask] = 0\n        return dx\n\n\n\nSigmoid\n\n\\(\\frac{hL}{hy}y^2exp(-x) = \\frac{hL}{hy}y(1-y)\\) (계산 생략)\n\n\nimport numpy as np\n\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = 1 / (1 + np.exp(-x))\n        self.out = out\n\n        return out\n\n    def backward(self, dout):\n        dx = dout * (1 - self.out) * self.out\n        return dx\n\n\n\nAffine\n\nWX + B 계산 node\n\\(\\frac{dL}{dX} = \\frac{dL}{dY} ⋅ W^T\\)\n\\(\\frac{dL}{dW} = X^T ⋅ \\frac{dL}{dY}\\)\n\\(\\frac{dL}{dB} = \\sum \\frac{dL}{dY}\\)\n\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        self.x = x\n        out = np.dot(x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n\n        return dx\n\n\n\nSoftmax\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n      self.t = t\n      self.y = softmax(x)\n      self.loss = cross_entropy_error(self.y, self.t)\n\n      return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        dx = (self.y - self.t) / batch_size\n\n        return dx",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/04.html#매개변수-갱신",
    "href": "posts/02_areas/deep_learning/notes/04.html#매개변수-갱신",
    "title": "학습 관련 기술들",
    "section": "매개변수 갱신",
    "text": "매개변수 갱신\n\n확률적 경사하강법은 매개변수를 찾는 과정이 비효율적이다.\n\n비등방성 함수에서 탐색 경로가 비효율적임\n\n\n\n모멘텀\n\n\\(v = αv - η \\frac{dL}{dW}\\)\n\\(W = W + v\\)\n\n\nclass Momentum:\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n        for key in params.keys():\n            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n            params[key] += self.v[key]\n\n\n\nAdaGrad\n\n각각의 매개변수에 대해 학습률을 점점 낮추는 방법\n\\(h = h + \\frac{dL}{dW} ⊙ \\frac{dL}{dW}\\)\n\\(W = W - η\\frac{1}{\\sqrt{h}}\\frac{dL}{dW}\\)\n\n\nclass AdaGrad:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n        for key, in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n\n\n하지만 시간이 지나면 결국 기울기가 0으로 되버림\n\n이것을 개선한 기법: RMSProp\n\n\n\n\nAdam",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/04.html#가중치의-초깃값",
    "href": "posts/02_areas/deep_learning/notes/04.html#가중치의-초깃값",
    "title": "학습 관련 기술들",
    "section": "가중치의 초깃값",
    "text": "가중치의 초깃값\n\n초깃값은 무작위로 설정되어야 한다.\n\n\nXavier 초깃값\n\n표준편차가 \\(\\frac{1}{\\sqrt{n}}\\)인 초깃값\nsigmoid, tanh에서 사용됨.\n\n\n\nHe 초깃값\n\n표준편차가 \\(\\frac{2}{\\sqrt{n}}\\)인 초깃값\nReLU에 특화됨",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/04.html#배치-정규화",
    "href": "posts/02_areas/deep_learning/notes/04.html#배치-정규화",
    "title": "학습 관련 기술들",
    "section": "배치 정규화",
    "text": "배치 정규화\n\n각 층의 활성화를 적당히 퍼뜨리도록 강제 하는 것\n학습속도 개선, 초깃값 의존도 감소, 과대적합 억제의 장점이 있음\n활성화 함수 앞이나 뒤에서 standardization scaling을 진행\n이후 \\(y_i = \\gamma \\hat{x}_i + β\\)의 수식으로, 두 파라미터를 적합한 값으로 학습해 나감",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/04.html#과대적합-방지",
    "href": "posts/02_areas/deep_learning/notes/04.html#과대적합-방지",
    "title": "학습 관련 기술들",
    "section": "과대적합 방지",
    "text": "과대적합 방지\n\n가중치 감소\n\n손실함수에 l2(\\(\\frac{1}{2}λ W^2\\)) l1 norm을 더함\n\n\n\n드롭아웃\n\n신경망 모델이 복잡해지면 가중치 감소만으로 대응하기 어려움\n훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다.\n시험 때 각 뉴련의 출력에 훈련 때 삭제 안 한 비율을 곱한다.\n\n\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio\n            return x * self.mask\n        return x * (1.0 - self.dropout_ratio)\n\n    def backward(self, dout):\n        return dout * self.mask",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/03.html#오차역전법-신경망",
    "href": "posts/02_areas/deep_learning/notes/03.html#오차역전법-신경망",
    "title": "오차역전법",
    "section": "오차역전법 신경망",
    "text": "오차역전법 신경망\n\nfrom dl_common.layers import *\nfrom dl_common.gradient import numerical_gradient\nfrom collections import OrderedDict\n\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] =  weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b1'] = np.zeros(output_size)\n      \n        self.layers = OrderedDict()\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        self.last_layer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x , t):\n        y = self.predict(x)\n        return self.last_layer.forward(y, t)\n\n    def auuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        if t.ndim != 1:\n            t = np.argmax(t, axis=1)\n        acc = np.mean(y == t)\n\n        return acc\n\n    def gradient(self, x, t):\n        self.loss(x, t)\n\n        dout = 1\n        dout = self.last_layer.backward(dout)\n        \n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n        \n        grads = {}\n\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n\n\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\niter_per_epoch = max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    grads = network.gradient(x_batch, t_batch)\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grads[key]\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/06.html#자연어-처리nlp",
    "href": "posts/02_areas/deep_learning/notes/06.html#자연어-처리nlp",
    "title": "자연어와 단어의 분산 표현",
    "section": "자연어 처리(NLP)",
    "text": "자연어 처리(NLP)\n\n우리 말을 컴퓨터에게 이해시키기 위한 기술\n\n기계 번역, 검색, 등",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/06.html#시소러스",
    "href": "posts/02_areas/deep_learning/notes/06.html#시소러스",
    "title": "자연어와 단어의 분산 표현",
    "section": "시소러스",
    "text": "시소러스\n\n사람이 직접 레이블링 한 유의어 사전\n대표적으로 WordNet이 있다.\n\n\n한계\n\n시대 변화에 따라 단어의 의미가 바뀌는 경우가 있다.\n사람 쓰는 비용이 크다.\n단어의 미묘한 차이를 표현할 수 없다.",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/06.html#통계-기반-기법",
    "href": "posts/02_areas/deep_learning/notes/06.html#통계-기반-기법",
    "title": "자연어와 단어의 분산 표현",
    "section": "통계 기반 기법",
    "text": "통계 기반 기법\n\n말뭉치: 언어의 실제 사용 예시를 모은 텍스트 데이터\n위키백과, 뉴스 기사, 블로그 글 등\n\n\nimport numpy as np\n\ndef preprocess(text):\n    text = text.lower()\n    text = text.replace(\".\", \" .\")\n    words = text.split(' ')\n\n    word_to_id = {}\n    id_to_word = {}\n\n    for word in words:\n        if word not in word_to_id:\n            new_id = len(word_to_id)\n            word_to_id[word] = new_id\n            id_to_word[new_id] = word\n    corpus = np.array([word_to_id[w] for w in words])\n    return corpus, word_to_id, id_to_word\n\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\ncorpus, word_to_id, id_to_word = preprocess(text)\n\n\n단어의 분산 표현\n\n단어의 의미를 벡터로 표현하는 방법\n분포 가설: 단어의 의미는 그 단어의 주변 단어들(맥락)에 의해 결정된다.\n\n\ndef create_co_matrix(corpus, vocab_size, window_size=1):\n    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n    corpus_size = len(corpus)\n\n    for idx, word_id in enumerate(corpus):\n        left = max(0, idx - window_size)\n        right = min(corpus_size, idx + window_size + 1)\n        for i in range(left, right):\n            if i == idx:\n                continue\n            co_matrix[word_id, corpus[i]] += 1\n    return co_matrix\n\n\ndef cos_similarity(x, y, eps=1e-8):\n    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n    return np.dot(nx, ny)\n\n\nvocab_size = len(word_to_id)\nC = create_co_matrix(corpus, vocab_size, window_size=1)\n\nC0 = C[word_to_id['the']]\nC1 = C[word_to_id['dog']]\nsimilarity = cos_similarity(C0, C1)\nsimilarity\n\n0.4082482852200891\n\n\n\n\n유사 단어 랭킹\n\ndef most_similar(query, word_to_id, id_to_word, C, top=5):\n    if query not in word_to_id:\n        print(f\"{query}는 사전에 없습니다.\")\n        return None\n\n    query_id = word_to_id[query]\n    query_vec = C[query_id]\n\n    vocab_size = C.shape[0]\n    similarity = np.zeros(vocab_size)\n\n    for i in range(vocab_size):\n        similarity[i] = cos_similarity(query_vec, C[i])\n\n    # 유사도 순으로 정렬\n    count = 0\n    for i in (-1 * similarity).argsort():\n        if id_to_word[i] == query:\n            continue\n        print(f\"{id_to_word[i]}: {similarity[i]:.4f}\")\n        count += 1\n        if count &gt;= top:\n            return",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/06.html#통계-기반-기법-개선",
    "href": "posts/02_areas/deep_learning/notes/06.html#통계-기반-기법-개선",
    "title": "자연어와 단어의 분산 표현",
    "section": "통계 기반 기법 개선",
    "text": "통계 기반 기법 개선\n\n이전 방법의 한계\n\n단어의 의미를 벡터로 표현하는 방법이 단순히 주변 단어의 빈도수에 의존한다.\n\n점별 상호정보량(PMI)\n\n\n\ndef ppmi(C, eps=1e-8):\n    M = np.zeros_like(C, dtype=np.float32)\n    N = np.sum(C)\n    S = np.sum(C, axis=0)\n\n    for i in range(C.shape[0]):\n        for j in range(C.shape[1]):\n            pmi = np.log2((C[i, j] * N) / (S[i] * S[j]) + eps)\n            M[i, j] = max(0, pmi)\n    return M",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#질문",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#질문",
    "title": "확률과 통계 1 정리",
    "section": "질문",
    "text": "질문\n\n\n\n이 공식은 무조건 t분포에서만 쓰이는건가?\n\n\n\n\n\n이거 어떻게 푸는거야\n\n\n\n모분산을 모르고 표본분산을 쓰면 무조건 t분포?\n쌍체표본은 무조건 t분포?\n적당한 β값은 존재하지 않은건가? 아니면 10%가 너무 큰건가?\n분모에서 분산 어떻게 추정함?\n모집단의 분포와 관계없이 표본분산 \\(S^2\\)은 \\(σ^2\\)의 불편추정량이다",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#통계학",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#통계학",
    "title": "확률과 통계 1 정리",
    "section": "통계학",
    "text": "통계학\n\n불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률",
    "title": "확률과 통계 1 정리",
    "section": "확률",
    "text": "확률\n\n고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n통계적 의미: 특정 사건이 발생하는 상대도수의 극한\n\n각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포 정의 단계",
    "text": "확률 분포 정의 단계\n\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n확률 변수: 확률실험의 결과를 수치로 나타낸 변수",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률-분포",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#확률-분포",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포",
    "text": "확률 분포\n\n이산 확률 분포: 이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n베르누이 분포: 각 시행은 서로 독립적이고, 실패와 성공 두 가지 결과만 존재.\n\n단 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면 비복원 추출에서도 유효\n\n이항 분포: n번의 독립적인 베르누이 시행을 수행하여 성공 횟수를 측정\n기하 분포: 성공 확률이 p인 베르누이 시행에서 첫 성공까지의 시행 횟수\n초기하 분포: 베르누이 시행이 아닌 시행에서 성공하는 횟수\n포아송 분포: 임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수\n\nn이 매우 크고, p가 매우 작을 때, 이항 분포를 포아송 분포로 근사할 수 있다.\n\n\n연속 확률 분포: 연속 표본 공간에서 정의 가능\n\n균일 분포\n정규 분포\n\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)\n\nt 분포\n\n자유도가 커질수록 표준 정규분포에 근사함.\n t(n)\n\nf 분포\n\n\\(F = \\frac{X_1/ν_1}{X_2/ν_2}\\), \\(X_1 \\sim χ^2(ν_1)\\), \\(X_2 \\sim χ^2(ν_2)\\)\n\n감마 분포\n\n카이제곱 분포: α = v/2, θ = 2 인 감마분포\n\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)\n\n지수 분포: 포아송 분포에서 사건 발생 간격의 분포\n\n\\(\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)\\), \\(θ = 1/λ\\)",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/04_archives/bs_3_1/notes/statistics/00.html#표본의-분포",
    "href": "posts/04_archives/bs_3_1/notes/statistics/00.html#표본의-분포",
    "title": "확률과 통계 1 정리",
    "section": "표본의 분포",
    "text": "표본의 분포\n\n샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n통계량: 표본의 특성을 나타내는 값\n추정량: 아래의 조건을 만족하는 통계량\n\n불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n\n\n표본 평균의 분포\n\n모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 \\(σ^2\\)이면, \\(\\bar{X}\\)의 평균은 μ이고, 분산은 \\(σ^2/n\\)인 정규분포를 따른다.\n\n단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리)\n\n만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n\n\n\n표본 분산의 분포\n\n정규 모집단으로 부터 나온 표본의 분산 S에 대하여, \\(\\frac{(n-1)S^2}{σ^2}\\)은 자유도가 n-1인 카이제곱 분포를 따른다.\n\n모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n\n두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.",
    "crumbs": [
      "PARA",
      "Archives",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/13.html",
    "href": "posts/02_areas/선형대수/notes/13.html",
    "title": "전치행렬",
    "section": "",
    "text": "전치행렬의 행렬식은 원래 행렬의 행렬식과 같다.\n\\((ABC)^T = C^T B^T A^T\\)\n\\((A + B)^T = A^T + B^T\\)\n\\((A^{-1})^T = (A^T)^{-1}\\)\n\\(C(A^T) = N(A)^⟂\\)\n\\(N(A^T) = C(A)^⟂\\)\nrank(A) + nullity(Aᵀ) = n\n\\(P=A(A^TA)^{−1}A^T\\): projection matrix\n\\(Px\\)는 x를 A의 column space로 projection한 것이다.\n\\(A^TAx = A^Tb\\): || Ax = b ||의 최소제곱해를 구하는 식\n닮음 변환: \\(D=C^{-1}AC\\) (C는 invertible matrix)\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "전치행렬"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "href": "posts/02_areas/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "title": "linear transformations",
    "section": "PreRequest for linear transformation",
    "text": "PreRequest for linear transformation\n\nT(a + b) = T(a) + T(b)\nT(ca) = cT(a)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/11.html#transformation",
    "href": "posts/02_areas/선형대수/notes/11.html#transformation",
    "title": "linear transformations",
    "section": "Transformation",
    "text": "Transformation\n\n표준 기저벡터가 어디로 이동하는지에 따라 변환을 정의한다.\nθ만큼 회전하는 A in \\(R^2\\)::\n\n\\(A = \\begin{bmatrix} \\cos θ & -\\sin θ \\\\ \\sin θ & \\cos θ \\end{bmatrix}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/11.html#projection",
    "href": "posts/02_areas/선형대수/notes/11.html#projection",
    "title": "linear transformations",
    "section": "Projection",
    "text": "Projection\n\n\\(proj_L(x) = \\frac{x ⋅ v}{u ⋅ v} v\\)\n\nif v is unit vecotr, then \\(proj_L(x) = (x ⋅ v) v\\)\n\\(A = \\begin{bmatrix} μ_1^2 & μ_2μ_1 \\\\ μ_1μ_2 & μ_2^2 \\end{bmatrix}\\)",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/11.html#합성곱",
    "href": "posts/02_areas/선형대수/notes/11.html#합성곱",
    "title": "linear transformations",
    "section": "합성곱",
    "text": "합성곱\n\n선형 변환의 합성곱 역시 선형 변환: 합성곱을 Ax로 표현 가능.\n행렬의 곱은 결합법칙이 성립한다.\n행렬의 곱은 교환법칙이 성립하지 않는다.\n행렬의 곱은 분배법칙이 성립한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html",
    "href": "posts/02_areas/대학원/notes/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html#서울대",
    "href": "posts/02_areas/대학원/notes/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html#카이스트",
    "href": "posts/02_areas/대학원/notes/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html#포항공대",
    "href": "posts/02_areas/대학원/notes/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html#연세대",
    "href": "posts/02_areas/대학원/notes/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/02.html#고려대",
    "href": "posts/02_areas/대학원/notes/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/00.html#인프라-엔지니어",
    "href": "posts/02_areas/대학원/notes/00.html#인프라-엔지니어",
    "title": "관심 분야 JD",
    "section": "인프라 / 엔지니어",
    "text": "인프라 / 엔지니어",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/00.html#computer-vision-품질검사",
    "href": "posts/02_areas/대학원/notes/00.html#computer-vision-품질검사",
    "title": "관심 분야 JD",
    "section": "Computer Vision / 품질검사",
    "text": "Computer Vision / 품질검사\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 현대오토에버\n\n\n\n\n\n\n2025 상반기 현대트랜시스\n\n\n\n\n\n\n2025 상반기 에스엘",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/00.html#자율주행",
    "href": "posts/02_areas/대학원/notes/00.html#자율주행",
    "title": "관심 분야 JD",
    "section": "자율주행",
    "text": "자율주행\n \n\n\n\n\n2025 상반기 KAI\n\n\n\n\n\n\n2025 상반기 현대트랜시스",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/00.html#llm",
    "href": "posts/02_areas/대학원/notes/00.html#llm",
    "title": "관심 분야 JD",
    "section": "LLM",
    "text": "LLM\n\n\n\n2024 LG화학 기반기술 연구소",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/00.html#기타",
    "href": "posts/02_areas/대학원/notes/00.html#기타",
    "title": "관심 분야 JD",
    "section": "기타",
    "text": "기타\n\n\n\n2025 상반기 HD한국조선해양\n\n\n\n \n\n \n\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 삼성물산\n\n\n\n\n\n\n2024 하반기 LG U+\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 네이버\n\n\n\n\n\n\n2025 상반기 네이버",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/index.html",
    "href": "posts/02_areas/대학원/index.html",
    "title": "대학원",
    "section": "",
    "text": "대학원 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/index.html#details",
    "href": "posts/02_areas/대학원/index.html#details",
    "title": "대학원",
    "section": "",
    "text": "대학원 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/index.html#tasks",
    "href": "posts/02_areas/대학원/index.html#tasks",
    "title": "대학원",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/index.html#참고-자료",
    "href": "posts/02_areas/대학원/index.html#참고-자료",
    "title": "대학원",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/index.html#related-posts",
    "href": "posts/02_areas/대학원/index.html#related-posts",
    "title": "대학원",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/12.html#함수의-역",
    "href": "posts/02_areas/선형대수/notes/12.html#함수의-역",
    "title": "역함수와 역변환",
    "section": "함수의 역",
    "text": "함수의 역\n\n\\(f\\) is invertible ⟺ ∀y ∈ Y, ∃!x ∈ X such that \\(f(x) = y\\), 즉 f는 전단사 함수이다.\n\n전사: \\(f(R^n) -&gt; R^m, f(x) = Ax, C(A) = R^M\\), 즉 rank(A) = m\n단사: N(A) = {0}, 즉 Ax = 0의 해는 유일하다. = C(A)가 linearly independent이다.\n즉, A는 정방행렬이다.\n즉, A의 기약행렬은 I이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/12.html#함수의-해집합",
    "href": "posts/02_areas/선형대수/notes/12.html#함수의-해집합",
    "title": "역함수와 역변환",
    "section": "함수의 해집합",
    "text": "함수의 해집합\n\nAx = b의 해집합은 Ax = 0의 해집합(Null space)을 특수해 만큼 평행 이동한 것과 같다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/02_areas/선형대수/notes/12.html#행렬식",
    "href": "posts/02_areas/선형대수/notes/12.html#행렬식",
    "title": "역함수와 역변환",
    "section": "행렬식",
    "text": "행렬식\n\n선형 변환 시 부피의 변화율과 방향을 나타낸다. (기존 부피에 행렬식의 절댓값을 곱한 것이 새로운 부피)\nn x n: row를 정해서, 그 row를 기준으로, \\(a_{row 1}A_{row 1} - a_{row 2}A_{row 2} + ... + a_{row n}A_{row n}\\)의 형태로 나타낼 수 있다.\n부호는 checkerboard pattern을 따른다.\n행렬의 scalar 곱은, 행렬식의 \\(\\text{scalar}^n\\)\n행렬의 특정 row끼리 더한 행렬의 행렬식은 두 행렬식의 합과 같다.\nduplicated row는 행렬식이 0이 된다. 즉, 역행렬이 존재하지 않는다.\n\n따라서 한 행에 상수배를 해서 다른 행에 더해도 행렬식은 변하지 않는다.\n\n상삼각행렬의 행렬식은 대각선 원소의 곱이다.",
    "crumbs": [
      "PARA",
      "Areas",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/02_areas/대학원/notes/01.html",
    "href": "posts/02_areas/대학원/notes/01.html",
    "title": "연구실",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "대학원",
      "Notes",
      "연구실"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/core/01.html#결측치-처리",
    "href": "posts/01_projects/adp_실기/notes/core/01.html#결측치-처리",
    "title": "전처리",
    "section": "결측치 처리",
    "text": "결측치 처리\n\n대푯값으로 대체\n단순확률대치법\n다른 모델로 예측\n보간법: 시계열에서 주로 사용.",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/core/01.html#이상치-처리",
    "href": "posts/01_projects/adp_실기/notes/core/01.html#이상치-처리",
    "title": "전처리",
    "section": "이상치 처리",
    "text": "이상치 처리\n\nESD\nIQR\nDBSCAN",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기/notes/core/01.html#클래스-불균형",
    "href": "posts/01_projects/adp_실기/notes/core/01.html#클래스-불균형",
    "title": "전처리",
    "section": "클래스 불균형",
    "text": "클래스 불균형",
    "crumbs": [
      "PARA",
      "Projects",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#명동형부",
    "href": "posts/01_projects/toeic/notes/00.html#명동형부",
    "title": "Toeic 문법",
    "section": "명동형부",
    "text": "명동형부\n\n명사 / 대명사\n\n기본 명사 생김새\n\n명사 1초 공식 TOP 5\n\n관사(a/an/the) + 명사 + 전치사\n전치사 + 명사 + 전치사\n관사/소유격 + (부사) + (형용사) + 명사\n형용사(지시형용사, 수량형용사) + 명사\n명사 + 명사\n\n명사 자리\n\n주어\n타/목\n전/목\n보어\n\n가산명사 vs. 불가산명사 &gt; 관사 a/an는 가산 단수 명사 앞에만 쓸 수 있다. &gt; the는 가산 단수 복수, 불가산 명사 앞에 쓸 수 있다.\n\n불가산 명사: a/an, -s 안 붙음 information: 정보 advice: 조언 merchandise: 상품 access: 접근 assistance: 지원 equipment: 장비 luggage/ baggage: 수하물 stationery: 문구류 funding: 자금 제공 furniture: 가구 produce: 농산물 news: 소식 \n\n~ing는 대부분 불가산 명사\n\n\n사람명사 vs. 사물명사\n\n사람명사: 가산 명사\n사물명사: 가산/불가산 명사\n\n특이 어미 명사: -al(proposal, arrival, approval), -ive(initiative),\n\n기본 인칭대명사 표\n\n소유격-소유대명사\n\n소유격 뒤에만 명사가 올 수 있다.\nhis: 소유격이면서 소유대명사\n\n목적격-재귀대명사\n\n재귀대명사는 재귀용법, 강조용법(부사 자리)으로 사용 가능\n\n지시대명사\n\nThis, That + 단수동사/ These, Those + 복수동사\nThose who + 복수동사\n\n부정대명사\n\none vs another\nsome vs any\n\nsome: 긍정문(not이 없음), 권유나 제안을 나타내는 의문문\nany: 부정문, 의문문, 조건문, 어떤 ~라도\n\n\n\n\n\n\n동사 (수, 태, 시제)\n\n\n형용사 / 부사",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#전접부",
    "href": "posts/01_projects/toeic/notes/00.html#전접부",
    "title": "Toeic 문법",
    "section": "전접부",
    "text": "전접부\n\n전치사 / 등위접속사 / 부사절 접속사\n\n\n명사절 접속사 / 형용사절 접속사\n\n\n접속부사 / 전치사 접속사 부사",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#준동사",
    "href": "posts/01_projects/toeic/notes/00.html#준동사",
    "title": "Toeic 문법",
    "section": "준동사",
    "text": "준동사\n\nTo부정사 / 동명사 / 분사",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#기타",
    "href": "posts/01_projects/toeic/notes/00.html#기타",
    "title": "Toeic 문법",
    "section": "기타",
    "text": "기타\n\n분사 구문 / 비교 구문",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/notes/01.html#sql-기본",
    "href": "posts/01_projects/sqld/notes/01.html#sql-기본",
    "title": "SQL 기본 및 활용",
    "section": "SQL 기본",
    "text": "SQL 기본\n\n관계형 데이터베이스 개요\n\n\nSELECT 문\n\n\n함수\n\n\nWHERE 절\n\n\nGROUP By, HAVING 절\n\n\nORDER BY 절\n\n\nJOIN\n\n\nSTANDARD JOIN",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/notes/01.html#sql-활용",
    "href": "posts/01_projects/sqld/notes/01.html#sql-활용",
    "title": "SQL 기본 및 활용",
    "section": "SQL 활용",
    "text": "SQL 활용\n\n서브쿼리\n\nScalar subquery\ninline view\nnested subquery\n\n\n\n뷰\n\n\n집합 연산자\n\n\n그룹 함수\n\n\n윈도우 함수\n\n\nTOP-N 쿼리\n\n\nSelf join\n\n\n계층 쿼리",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/notes/01.html#관리-구문",
    "href": "posts/01_projects/sqld/notes/01.html#관리-구문",
    "title": "SQL 기본 및 활용",
    "section": "관리 구문",
    "text": "관리 구문\n\nDML\n\n\nTCL\n\n\nDDL\n\n\nDCL",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/index.html",
    "href": "posts/04_archives/대학원/index.html",
    "title": "대학원",
    "section": "",
    "text": "대학원 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/index.html#details",
    "href": "posts/04_archives/대학원/index.html#details",
    "title": "대학원",
    "section": "",
    "text": "대학원 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/index.html#tasks",
    "href": "posts/04_archives/대학원/index.html#tasks",
    "title": "대학원",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/index.html#why-failed",
    "href": "posts/04_archives/대학원/index.html#why-failed",
    "title": "대학원",
    "section": "why failed?",
    "text": "why failed?\n내가 준비해온 스펙과 준비할 수 있는 스펙을 봤을 때, 높은 곳에 지원하기는 어려울 것 같아서 포기했다.",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/index.html#related-posts",
    "href": "posts/04_archives/대학원/index.html#related-posts",
    "title": "대학원",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html",
    "href": "posts/04_archives/대학원/notes/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html#서울대",
    "href": "posts/04_archives/대학원/notes/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html#카이스트",
    "href": "posts/04_archives/대학원/notes/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html#포항공대",
    "href": "posts/04_archives/대학원/notes/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html#연세대",
    "href": "posts/04_archives/대학원/notes/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/02.html#고려대",
    "href": "posts/04_archives/대학원/notes/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/00.html#인프라-엔지니어",
    "href": "posts/04_archives/대학원/notes/00.html#인프라-엔지니어",
    "title": "관심 분야 JD",
    "section": "인프라 / 엔지니어",
    "text": "인프라 / 엔지니어",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/00.html#computer-vision-품질검사",
    "href": "posts/04_archives/대학원/notes/00.html#computer-vision-품질검사",
    "title": "관심 분야 JD",
    "section": "Computer Vision / 품질검사",
    "text": "Computer Vision / 품질검사\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 현대오토에버\n\n\n\n\n\n\n2025 상반기 현대트랜시스\n\n\n\n\n\n\n2025 상반기 에스엘",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/00.html#자율주행",
    "href": "posts/04_archives/대학원/notes/00.html#자율주행",
    "title": "관심 분야 JD",
    "section": "자율주행",
    "text": "자율주행\n \n\n\n\n\n2025 상반기 KAI\n\n\n\n\n\n\n2025 상반기 현대트랜시스",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/00.html#llm",
    "href": "posts/04_archives/대학원/notes/00.html#llm",
    "title": "관심 분야 JD",
    "section": "LLM",
    "text": "LLM\n\n\n\n2024 LG화학 기반기술 연구소",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/00.html#기타",
    "href": "posts/04_archives/대학원/notes/00.html#기타",
    "title": "관심 분야 JD",
    "section": "기타",
    "text": "기타\n\n\n\n2025 상반기 HD한국조선해양\n\n\n\n \n\n \n\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 삼성물산\n\n\n\n\n\n\n2024 하반기 LG U+\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 네이버\n\n\n\n\n\n\n2025 상반기 네이버",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/notes/00.html",
    "href": "posts/02_areas/ros/notes/00.html",
    "title": "Basic",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS",
      "Notes",
      "Basic"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/index.html",
    "href": "posts/03_resources/air_flow/index.html",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/index.html#details",
    "href": "posts/03_resources/air_flow/index.html#details",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/index.html#tasks",
    "href": "posts/03_resources/air_flow/index.html#tasks",
    "title": "AirFlow",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/index.html#참고-자료",
    "href": "posts/03_resources/air_flow/index.html#참고-자료",
    "title": "AirFlow",
    "section": "참고 자료",
    "text": "참고 자료\n\nUdemy 강의",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/index.html#related-posts",
    "href": "posts/03_resources/air_flow/index.html#related-posts",
    "title": "AirFlow",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/01.html#dag-skeleton",
    "href": "posts/03_resources/air_flow/notes/01.html#dag-skeleton",
    "title": "Coding pipeline",
    "section": "DAG skeleton",
    "text": "DAG skeleton\n\nfrom airflow import DAG\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='example_dag',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    pass\n\n\nDAG는 start_date / last_execution time + schedule_interval에 실행된다.",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/01.html#operator",
    "href": "posts/03_resources/air_flow/notes/01.html#operator",
    "title": "Coding pipeline",
    "section": "Operator",
    "text": "Operator\n\noperator 하나 당 하나의 task만 실행하는게 좋다.\n\n\noperator type\n\nAction operators\n\nBashOperator\nPythonOperator\n\nTransfer operators\nSensor operators",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/01.html#providers",
    "href": "posts/03_resources/air_flow/notes/01.html#providers",
    "title": "Coding pipeline",
    "section": "Providers",
    "text": "Providers\n\nAirflow providers are a set of packages that contain operators, sensors, hooks, and other utilities to interact with external platforms and services.\nProviders are installed separately from Airflow and can be added to your environment as needed.\nIn Airflow core, Bash and Python operators, … are included\n\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nwith DAG(\n    dag_id='example_db',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    create_table = PostgresOperator(\n        task_id='create_table',\n        postgres_conn_id='postgres',\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS example_table (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(50)\n            );\n        \"\"\",\n    )\n\nDB에 접속하기 위해서 connection을 설정해야 한다.",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/01.html#hook",
    "href": "posts/03_resources/air_flow/notes/01.html#hook",
    "title": "Coding pipeline",
    "section": "Hook",
    "text": "Hook",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/notes/00.html",
    "href": "posts/03_resources/helm/notes/00.html",
    "title": "개요",
    "section": "",
    "text": "Chart.yml 파일을 통해 Helm 차트의 메타데이터를 정의할 수 있습니다. 이 파일은 Helm 차트의 이름, 버전, 설명, 라이선스 등의 정보를 포함합니다. 또한, 차트가 의존하는 다른 차트나 리소스에 대한 정보도 포함할 수 있습니다.\ntemplate/ 디렉토리는 Helm 차트의 템플릿 파일을 포함합니다. 이 템플릿 파일은 Kubernetes 리소스를 생성하는 데 사용됩니다. Helm은 이 템플릿 파일을 렌더링하여 실제 Kubernetes 리소스를 생성합니다.\nvalues.yml 파일은 Helm 차트의 기본값을 정의합니다. 이 파일은 Helm 차트를 설치할 때 사용자가 제공할 수 있는 값들을 포함합니다. 사용자는 이 파일을 수정하여 Helm 차트의 동작을 사용자 정의할 수 있습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm",
      "Notes",
      "개요"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/03.html#가우스-조던-소거법",
    "href": "posts/03_resources/선형대수/notes/03.html#가우스-조던-소거법",
    "title": "3-몰라",
    "section": "가우스 조던 소거법",
    "text": "가우스 조던 소거법\n\n선형대수의 목표는 \\(Ax = b\\)에서 x를 찾는 것이다.\n\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n이 수식을 다시 살펴보자. 위의 수식은 아래와 같이 적용할 수 있다.\n\\[\\begin{aligned}\n2x + 4y \\quad  &= 8 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 열립방정식을 풀면 \\(y = 1\\)이라는 결과를 얻는다. 다시 \\(y=1\\)을 대입해서 \\(x=2\\)라는 값을 구할 수 있다.\n이제 이를 matrix와 vector로 풀어보자.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}\n\\]\n이를 확장행렬로 표현하면 다음과 같다\n\\[\n[A|b] = \\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n2 & 5 & | & 9\n\\end{bmatrix}\n\\]\n이제 가우스 조던 소거법을 적용해보자\n적용 순서는 다음과 같다.\n\n양 변에 0이 아닌 상수배를 해준다.\n상수배를 한 행을 다른행에 더하거나 뺀다.\n행끼리 자리 바꾼다.\n\n이에 맞춰서 위의 식을 풀이하면,\n\n두 번째 행에서 첫 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n\n첫 번째 행에서 두 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 0 & | & 2 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n따라서 \\(x = 2\\), \\(y = 1\\)이라는 해를 얻을 수 있다.\n즉 가우스조던 소거법은 왼쪽을 항등행렬로 만들고, 그 오른쪽에 있는 값이 답이되는 소거법이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/03.html#역행렬-구하기",
    "href": "posts/03_resources/선형대수/notes/03.html#역행렬-구하기",
    "title": "3-몰라",
    "section": "역행렬 구하기",
    "text": "역행렬 구하기\n역행렬을 구할 수 있다면 x의 값을 쉽게 구할 수 있다. (\\(x = A^{-1}b\\))\n가우스 조던 소거법을 이용해 역행렬을 구해보자.\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\nc & d & | & 0 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & \\frac{ad-bc}{a} & | & -\\frac{c}{a} & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & 0 & | & \\frac{ad}{ad-bc} & \\frac{-ab}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 0 & | & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n∴ A^{-1} = \\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\n\ninvertible\n역행렬이 존재할 경우 invertible하다고 한다.\n\nnon singular matrix\ndet(A) ≠ 0: ad - bc(determinant) = 0인 경우 역행렬이 존재하지 않는다.\nA가 full rank이다\nN(A) = 0",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/03.html#determinant",
    "href": "posts/03_resources/선형대수/notes/03.html#determinant",
    "title": "3-몰라",
    "section": "determinant",
    "text": "determinant\n정사각행렬의 element로 scalar 값을 만드는 함\n\n3 x 3 행렬의 det\n\\[\nA=\n\\begin{bmatrix}\na & b & c\\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n\\]\n\\(det(A) = a(ei - fh) - b(di-fg)+c(dh-eg)\\)\nLaplace expansion or cofactor expansion\n\n\nproperties\n\ndet(A) = 0 이면 A is singular\nA가 rank-deficient 이면 det(A) = 0\ndiagonal or triangular matrix, det(A) = 대각요소의 곱\n항등행렬의 det=1\ndet(cA) = \\(c^ndet(A)\\) (A = nxn)\n\\(det(A^T) = det(A)\\)\ndet(AB) = det(A)det(B)\n\\(\\color{red}{det(A^{-1}) = \\frac{1}{det(A)}}\\)\n\\(\\color{red}{det(A) = λ_1λ_2,...,λ_n}\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/03.html#trace",
    "href": "posts/03_resources/선형대수/notes/03.html#trace",
    "title": "3-몰라",
    "section": "Trace",
    "text": "Trace\n정사각 행렬에 대해서만 정의되는 것, diagonal 전부 더함\n\\(tr(A) = \\sum_{i=1}^{n}a_{ii}\\)\n\ntr(A + B) = tr(A) + tr(B)\ntr(cA) = ctr(A)\n\\(tr(A^T) = tr(A)\\)\ntr(AB) = tr(BA)\n\\(tr(a^Tb) = tr(ba^T)\\)\ntr(ABCD) = tr(BCDA) = tr(CDAB) = tr(DABC) (cyclic property)\n\\(tr(A) = \\sum_{i=1}^{n}\\lambda_i\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/03.html#최소자승법",
    "href": "posts/03_resources/선형대수/notes/03.html#최소자승법",
    "title": "3-몰라",
    "section": "최소자승법",
    "text": "최소자승법",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/01.html#vector",
    "href": "posts/03_resources/선형대수/notes/01.html#vector",
    "title": "2-기초(1)",
    "section": "Vector",
    "text": "Vector\nvector는 크기와 방향을 가지고 있다.\n\nExample\n\\[\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n크기: \\(\\sqrt{9 + 4} = \\sqrt{13}\\)\n방향: \\(tan^{-1}(\\frac{2}{3})\\)\n\n크기와 방향이 같으면 같은 벡터이다.\n\n\n덧셈\n벡터의 덧셈을 기하학적으로 알아보자\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.\n\n\nScalar 배\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]\n마찬가지로 좌표평면으로 나타내는건 귀찮아서 생략하겠다.\n\n\n\n\n\n\nScalar 배를 한 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/01.html#전치-transpose",
    "href": "posts/03_resources/선형대수/notes/01.html#전치-transpose",
    "title": "2-기초(1)",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n행렬 \\(A\\)의 요소 \\(a_{ij}\\)는 A의 Transpose인 \\(A^T\\)의 \\(a_{ji}\\)가 된다. 즉, 행렬 \\(A\\)를 전치하면 diagnal(대각선 요소)를 제외한 모든 요소가 대각선을 기준으로 서로 뒤바뀐다.\n\nSymmetrix matrix: \\(A = A^T\\)인 행렬, 즉 대각선을 기준으로 값이 전부 같은 행렬 Hermitian matrix: \\((A^*)^T = A^H(conjugate transpose) = A\\)를 만족하는 행렬\n\nVector의 경우에는 Column Vector의 경우, Transpose시 Row Vector로, Row Vector의 경우도 반대로 작용한다.\n\nProperties\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\(\\color{red}{(AB)^T = B^TA^T}\\)\n\\((A^TA)^T\\)와 \\((AA^T)^T\\)의 결과는 항상 자기 자신이 된다. → Symmetrix matrix\n\\(C(A)^T = CA^T\\)\n\\(det(A^T) = det(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/01.html#inner-product-projection",
    "href": "posts/03_resources/선형대수/notes/01.html#inner-product-projection",
    "title": "2-기초(1)",
    "section": "Inner Product & Projection",
    "text": "Inner Product & Projection\n\\[\n\\underset{a}{\\begin{bmatrix}\n1 \\\\\n3\n\\end{bmatrix}} *\n\\underset{b}{\\begin{bmatrix}\n5 \\\\\n1\n\\end{bmatrix}} = 1 * 5 + 3 * 1 = 8 = a^Tb = b^Ta\n\\]\n갑자기 등장한 \\(a^Tb\\)가 의미하는건 아래와 같다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n\n||a||는 a 벡터의 크기를 의미한다.\n\n위의 식을 그림으로 표현해보자\n\n\n\n\n\n\n\n\n\n내적은 초록색 화살표와 파란색 화살표의 곱으로 표현할 수 있다.\n이는 a 벡터가 b 벡터의 방향에 대해 얼마나 투영되었는지를 나타낸다.\n두 벡터의 방향이 일치할 때 내적의 값이 가장 크고, 수직일 때 0 (안 닮음을 의미), 반대 방향일 때 가장 작은 값이 된다.\n\n단위 벡터(크기가 1인 벡터) 계산\n위의 식으로 부터 다음의 추론 과정을 통해 단위 벡터를 계산할 수 있다.\n\\(a^Ta = ||a||^2\\)\n∴ \\(||a|| = \\sqrt{a^Ta}\\)\n∴ 단위 벡터는 \\(\\frac{a}{||a||}\\) = \\(\\frac{a}{\\sqrt{a^Ta}}\\)\n\n\n정사형 벡터의 좌표 계산\n벡터의 좌표는 방향과 크기의 곱으로 표현할 수 있다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n정사형 벡터의 크기는 \\(\\frac{a^Tb}{||b||} = \\frac{a^Tb}{\\sqrt{b^Tb}}\\)\n장사형 벡터의 방향은 b의 단위 벡터와 같다.\n즉, 정사형 벡터의 좌표는 \\(\\frac{a^Tb}{\\sqrt{b^Tb}} * \\frac{b}{\\sqrt{b^Tb}} = \\frac{a^Tb}{b^Tb}b\\)\n\\(a^T\\frac{b}{\\sqrt{b^Tb}}*\\frac{b}{\\sqrt{b^Tb}}\\)로도 구할 수 있다.\n\na와 수직으로 연결되는 정사형 벡터 \\(\\hat{x}\\)\n\\((a-b\\hat{x})^Tb\\hat{x} = 0\\)\n\\(a^Tb - b^Tbb\\hat{x} = 0\\)\n\\(\\hat{x} = \\frac{a^Tb}{b^Tb}\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/01.html#norm",
    "href": "posts/03_resources/선형대수/notes/01.html#norm",
    "title": "2-기초(1)",
    "section": "Norm",
    "text": "Norm\n크기를 나타내는 것(0 포함, 양 음수 scalar)\n\n2-Norm (\\(l_2\\)-norm)\n벡터의 물리적인 길이.\n\\[\na = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\n\\(||a||_2 = \\sqrt{1^2+2^2+3^2} = (|1|^{\\color{red}{2}}+|2|^{\\color{red}{2}}+|3|^{\\color{red}{2}})^{\\color{red}{\\frac{1}{2}}}\\)\n2 제곱에, \\(\\frac{1}{2}\\)여서 2-norm이다.\n\n두 벡터 사이의 거리는 두 벡터의 차이의 2-norm이다.\n\n\n\n1-Norm (\\(l_1\\)-norm)\n1 제곱에 \\(\\frac{1}{1}\\)을 계산해주면 된다.\n\\(||a||_1 = (|1|^1+|2|^1+|3|^1)^{\\frac{1}{1}}\\)\n\n\np-Norm (\\(l_p\\)-norm)\n\\(||a||_p = (|x_1|^p+|x_2|^p+|x_3|^p+...)^{\\frac{1}{p}} = (\\underset{t}{\\Sigma} |x_t|^p)^{\\frac{1}{p}} \\quad (p ≥ 1)\\)\n\n\ninfinity-Norm\n\\(||a||_∞ = \\underset{t}{max}|x_t|\\)\n1-norm, 2-norm, infinity-norm의 값이 1이 되는 모든 벡터들을 좌표평면에 나타내면 다음과 같다.\n\n\n\n\n\n\n\n\n\n같은 벡터일 때, 1-norm ≥ 2-norm ≥ ∞-norm 순으로 크다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/10.html#metrix-vector-product",
    "href": "posts/03_resources/선형대수/notes/10.html#metrix-vector-product",
    "title": "Null space and Column space",
    "section": "Metrix vector product",
    "text": "Metrix vector product\n\nSee \\(A\\)’s column space as a set of vectors. → \\(A\\)’s column space is the set of all linear combinations of the columns of \\(A\\).\nSee \\(B\\)’s row space as a set of vectors.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/10.html#null-space",
    "href": "posts/03_resources/선형대수/notes/10.html#null-space",
    "title": "Null space and Column space",
    "section": "Null space",
    "text": "Null space\n\n\\(N = \\{x \\in \\mathbb{R}^n | Ax = 0\\}\\)\n\n\\(N\\) is Null space of \\(A\\).\n\n\n\nN(A) = N(rref(A))\nif N(A) = {0}, then column vector of \\(A\\) is linearly independent. → column vector of \\(A\\) is not a basis for C(A) → pivot variable의 합이 free variable을 만든다. (redundant column)\ndim(N(A)) = nullity of A = # of free variables in rref(A)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/10.html#column-space",
    "href": "posts/03_resources/선형대수/notes/10.html#column-space",
    "title": "Null space and Column space",
    "section": "Column Space",
    "text": "Column Space\n\nC(A) = span(columns of A)\nrref(A)의 pivot variable의 column vector가 C(A)의 basis이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/10.html#column-space의-평면의-방정식",
    "href": "posts/03_resources/선형대수/notes/10.html#column-space의-평면의-방정식",
    "title": "Null space and Column space",
    "section": "Column Space의 평면의 방정식",
    "text": "Column Space의 평면의 방정식\n\n\\({\\rightVectorBar{b} | A\\rightVectorBar{x} = \\rightVectorBar{b} ∧ \\rightVectorBar{x} ∈ R^n}\\)\n위를 만족하는 기약행렬을 만들어, 해가 존재하도록 방정식을 구성하면 평면의 방정식을 구할 수 있다.\n혹은 column space의 basis를 구하고, 이를 이용해 평면의 방정식을 구할 수 있다.\n\\(R^n\\)을 span하는 basis의 vector는 n개이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/04.html#what-is-vector",
    "href": "posts/03_resources/선형대수/notes/04.html#what-is-vector",
    "title": "벡터와 공간",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있고, 2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\n\nvector의 수학적 표현\nvector는 ordered list인 tuple 형태로 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\ndomain과 dimension에 따라 vector는 다음과 같이 표현할 수 있다.\n\\[\n\\vec{v} ∈ R^2\n\\]\n\n1차원: \\(R^1\\)\n2차원: \\(R^2\\)\n3차원: \\(R^3\\)\nn차원: \\(R^n\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/04.html#vector의-합",
    "href": "posts/03_resources/선형대수/notes/04.html#vector의-합",
    "title": "벡터와 공간",
    "section": "vector의 합",
    "text": "vector의 합\nvector의 합은 각 성분별로 더한 결과를 반환한다.\n\n기하학적 의미\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/04.html#vector의-scalar-곱",
    "href": "posts/03_resources/선형대수/notes/04.html#vector의-scalar-곱",
    "title": "벡터와 공간",
    "section": "vector의 scalar 곱",
    "text": "vector의 scalar 곱\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/04.html#vector의-차",
    "href": "posts/03_resources/선형대수/notes/04.html#vector의-차",
    "title": "벡터와 공간",
    "section": "vector의 차",
    "text": "vector의 차\nvector의 차는 각 성분별로 뺀 결과를 반환한다.\n기하학적으로는 두 벡터의 끝점을 연결한 벡터가 된다.\n\\(\\vec{x} - \\vec{y}\\)는 y에서 x를 연결한 벡터가 된다.\n\\(\\vec{y} - \\vec{x}\\)는 x에서 y를 연결한 벡터가 된다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/04.html#단위-벡터",
    "href": "posts/03_resources/선형대수/notes/04.html#단위-벡터",
    "title": "벡터와 공간",
    "section": "단위 벡터",
    "text": "단위 벡터\n\\[\n\\vec{v} = \\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\n위의 벡터를 단위 벡터의 합으로 만들면 다음과 같다.\n\\[\n\\hat{i} = \\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix},\n\\hat{j} = \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\\[\n\\vec{v} = 3\\hat{i} + 4\\hat{j}\n\\]\n\n\n\n\n\n\nScalar 배를 한 기저 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/07.html#subspaces",
    "href": "posts/03_resources/선형대수/notes/07.html#subspaces",
    "title": "Subspaces and the basis",
    "section": "Subspaces",
    "text": "Subspaces\n\n\\(S\\) is a subset of \\(V\\).\n\nS ⊆ V\nS is a vector space\n\ninclude zero vector\nclosed under addition\nclosed under scalar multiplication",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/07.html#basis",
    "href": "posts/03_resources/선형대수/notes/07.html#basis",
    "title": "Subspaces and the basis",
    "section": "Basis",
    "text": "Basis\n\nminimum set of vectors that spans the subset\n\\(S\\) is a basis of \\(V\\) ⟺\n\nelements of \\(S\\) are linearly independent\n\\(S\\) spans \\(V\\)\n\n특정 부분집합의 basis의 linear combination으로 표현되는 모든 벡터는 유일하다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/09.html#reduced-row-echelon-form",
    "href": "posts/03_resources/선형대수/notes/09.html#reduced-row-echelon-form",
    "title": "가감법으로 연립방정식을 풀기 위한 행렬",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\n각 행의 선행항을 1로 만들고, 그 열의 다른 항을 0으로 만드는 방법을 행렬로 표현한 것이다.\n이때 선행항의 변수를 pivot variable이라고 하고, 다른 변수들은 free variable이라고 한다.\n관행적으로 pivot entry는 우하향으로 이동하고, zeroed out 행은 맨 아래쪽에 위치한다.\n기약행렬을 이용해 연립방정식으로 풀 수 없는 식을 행렬의 선형 결합으로 표현할 수 있다.\n\n\n혹은 식이 유효하지 않다면, 해가 없는 경우이며, 이것은 평행한 조건이 존재할 경우 발생한다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "가감법으로 연립방정식을 풀기 위한 행렬"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "href": "posts/03_resources/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "title": "2-기초(2)",
    "section": "행렬의 곱을 바라보는 관점",
    "text": "행렬의 곱을 바라보는 관점\n\n내적으로 바라보기\n\n\\[\nA = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\quad (a_x = \\text{column vector})\n\\]\n\\[\nAB = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & b_3\n\\end{bmatrix} =\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3 \\\\\na_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\\na_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3\n\\end{bmatrix}\n\\]\n\nrank-1 matrix의 합\n\n\\[\nAB = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\nb_3^T\n\\end{bmatrix} =\na_1^Tb_1 + a_2^Tb_2 + a_3^Tb_3\n\\]\n\nColumn space로 바라보기\n\n\\[\nAx = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} = a_1x_1 + a_2x_2 + a_3x_3\n\\]\n\nRow space로 바라보기\n\n\\[\nx^TA = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix} = x_1a_1^T + x_2a_2^T + x_3a_3^T\n\\]",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#span과-column-space",
    "href": "posts/03_resources/선형대수/notes/02.html#span과-column-space",
    "title": "2-기초(2)",
    "section": "span과 column space",
    "text": "span과 column space\n\ncolumn space: column vector들이 span하는 영역\nspan: linear combination으로 만들어지는 모든 벡터들의 집합\nlinear combination: vector들을 scalar 배 하고 더한 것\nlinear independent: span하는 vector들이 서로 독립적인 경우\n수학적 정의: \\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\) 일 때 \\(a_1 = a_2 = \\cdots = a_n = 0\\) 인 경우\nbasis: 어떤 공간을 이루는 필수적인 구성요소 (linear independent, span)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#항등행렬",
    "href": "posts/03_resources/선형대수/notes/02.html#항등행렬",
    "title": "2-기초(2)",
    "section": "항등행렬",
    "text": "항등행렬\n\\(AI = IA = A\\)를 만족하는 행렬 \\(I\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#역행렬",
    "href": "posts/03_resources/선형대수/notes/02.html#역행렬",
    "title": "2-기초(2)",
    "section": "역행렬",
    "text": "역행렬\n\\(Ax = b\\)를 만족하는 \\(x\\)를 찾는 것은 \\(A^{-1}Ax = A^{-1}b\\)를 만족하는 \\(x\\)를 찾는 것과 같다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#대각-행렬",
    "href": "posts/03_resources/선형대수/notes/02.html#대각-행렬",
    "title": "2-기초(2)",
    "section": "대각 행렬",
    "text": "대각 행렬\ndiagonal을 제외한 모든 요소가 0인 행렬 (square, rectangular 모두 가능)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#orthogonal-행렬",
    "href": "posts/03_resources/선형대수/notes/02.html#orthogonal-행렬",
    "title": "2-기초(2)",
    "section": "Orthogonal 행렬",
    "text": "Orthogonal 행렬\n행렬의 모든 column들이 orthonormal vector인 경우\n\\(Q^{-1} = Q^T\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#행렬의-rank",
    "href": "posts/03_resources/선형대수/notes/02.html#행렬의-rank",
    "title": "2-기초(2)",
    "section": "행렬의 rank",
    "text": "행렬의 rank\nrank: 행렬이 가지는 independent한 column의 개수 → column space의 차원\nrank(A) = rank(A^T)\n\nfull-column rank: 해가 없거나 한 개 존재\nfull-row rank: 해가 무한하다\nfull rank: 해가 한 개 있다.\nrank-deficient: b가 column space에 속하지 않는 경우 해가 없고, 그렇지 않으면 해가 무한하다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/02.html#null-space",
    "href": "posts/03_resources/선형대수/notes/02.html#null-space",
    "title": "2-기초(2)",
    "section": "Null space",
    "text": "Null space\n\\(Ax = 0\\)을 만족하는 모든 \\(x\\)의 집합\nA가 m x n 행렬이라면, dim(N(A)) = n - rank(A)\nnull space와 row space는 orthogonal하다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/index.html",
    "href": "posts/03_resources/hadoop/index.html",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/index.html#details",
    "href": "posts/03_resources/hadoop/index.html#details",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/index.html#tasks",
    "href": "posts/03_resources/hadoop/index.html#tasks",
    "title": "Hadoop",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/index.html#참고-자료",
    "href": "posts/03_resources/hadoop/index.html#참고-자료",
    "title": "Hadoop",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/index.html#related-posts",
    "href": "posts/03_resources/hadoop/index.html#related-posts",
    "title": "Hadoop",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_resources/hadoop/notes/00.html",
    "href": "posts/03_resources/hadoop/notes/00.html",
    "title": "Hadoop Ecosystem",
    "section": "",
    "text": "HDFS: Hadoop Distributed File System\n\n클러스터의 하드 드라이브를 하나의 거대한 파일 시스템으로 통합\n자동 복제 및 장애 조치 기능 제공\n\nYARN: Yet Another Resource Negotiator\n\n클러스터의 리소스를 관리하고 작업을 스케줄링\n\nMapReduce: 데이터 처리 모델\n\n데이터를 분할하고 병렬로 처리하는 프레임워크\nMap 단계에서 데이터를 필터링하고 정렬, Reduce 단계에서 집계 및 요약\n\nPig: 데이터 흐름 언어\n\nSQL과 유사한 스크립트 언어로 MapReduce / TEZ를 위한 데이터 처리 작업을 작성\n\nHive: SQL과 유사한 쿼리 언어\n\n대규모 데이터 세트에 대한 쿼리 및 분석을 위한 SQL 인터페이스 제공\n\nAmbari: 데이터 시각화 도구\n\nHadoop 클러스터에서 데이터를 시각화하고 대시보드를 생성\n\nHBase: NoSQL 데이터베이스\n\n대규모 데이터 세트를 실시간으로 읽고 쓸 수 있는 분산형 데이터베이스\n\nstorm: 실시간 데이터 처리\n\n스트림 데이터를 실시간으로 처리하고 분석하는 프레임워크\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "Hadoop",
      "Notes",
      "Hadoop Ecosystem"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#dot-product",
    "href": "posts/03_resources/선형대수/notes/08.html#dot-product",
    "title": "vector dot product, cross product",
    "section": "Dot Product",
    "text": "Dot Product\n\n\\(v ⋅ w = \\sum_{i=1}^{n} v_i w_i\\)\n\n\nProperties\n\n\\(v ⋅ w = w ⋅ v\\)\n\\(v ⋅ (w + u) = v ⋅ w + v ⋅ u\\)\n\\(v ⋅ (c w) = c (v ⋅ w)\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#length-of-vector",
    "href": "posts/03_resources/선형대수/notes/08.html#length-of-vector",
    "title": "vector dot product, cross product",
    "section": "Length of vector",
    "text": "Length of vector\n\n\\(||v|| = \\sqrt{v ⋅ v}\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#some-properties",
    "href": "posts/03_resources/선형대수/notes/08.html#some-properties",
    "title": "vector dot product, cross product",
    "section": "Some properties",
    "text": "Some properties\nfor non-zero vectors \\(v\\) and \\(w\\):\n\ncauchy-schwarz inequality\n\n\\(|v ⋅ w| ≤ ||v|| ||w||\\)\n\\(|v ⋅ w| = ||v|| ||w||\\) ⟺ \\(v = cw\\)\n\n\n\nTriangle inequality\n\n\\(||v + w|| ≤ ||v|| + ||w||\\)\n\n\n\nAngle between vectors\n\n\\(cosθ = \\frac{v ⋅ w}{||v|| ||w||}\\)\nif \\(v\\) and \\(w\\) are orthogonal, then \\(v ⋅ w = 0\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#cross-product",
    "href": "posts/03_resources/선형대수/notes/08.html#cross-product",
    "title": "vector dot product, cross product",
    "section": "Cross Product",
    "text": "Cross Product\n\nonly for 3D vectors\nget a vector that is orthogonal to both \\(v\\) and \\(w\\)\n\\(v × w = (v_2 w_3 - v_3 w_2, v_3 w_1 - v_1 w_3, v_1 w_2 - v_2 w_1)\\)\n\\(sinθ = \\frac{||v × w||}{||v|| ||w||}\\)\n\\(v × w = 0\\) ⟺ \\(v\\) and \\(w\\) are parallel\nv와 w로 이루어진 평행사변형의 넓이는 \\(||v × w||\\)이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#triple-productlagrange-identity",
    "href": "posts/03_resources/선형대수/notes/08.html#triple-productlagrange-identity",
    "title": "vector dot product, cross product",
    "section": "Triple product(lagrange identity)",
    "text": "Triple product(lagrange identity)\n\n\\(a x (b x c) = b(a ⋅ c) - c(a ⋅ b)\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "href": "posts/03_resources/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "title": "vector dot product, cross product",
    "section": "3차원의 평면에서 직선의 방정식",
    "text": "3차원의 평면에서 직선의 방정식\n\n평면에 대한 법선벡터 \\((a, b, c)\\)와 평면 위의 한 점 \\((x_p, y_p, z_p)\\)이 주어졌을 때, 평면의 방정식은 다음과 같다.\n\\(ax + by + cz = D\\), \\(D = ax_p + by_p + cz_p\\)\n평행한 평면은 a, b, c의 계수가 같은 평면이다.\n\\(\\frac{Ax_0 + By_0 + Cz_0 + D}{\\sqrt{A^2 + B^2 + C^2}}\\)은 평면과 점 \\((x_0, y_0, z_0)\\) 사이의 거리이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "href": "posts/03_resources/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "title": "linear transformations",
    "section": "PreRequest for linear transformation",
    "text": "PreRequest for linear transformation\n\nT(a + b) = T(a) + T(b)\nT(ca) = cT(a)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/11.html#transformation",
    "href": "posts/03_resources/선형대수/notes/11.html#transformation",
    "title": "linear transformations",
    "section": "Transformation",
    "text": "Transformation\n\n표준 기저벡터가 어디로 이동하는지에 따라 변환을 정의한다.\nθ만큼 회전하는 A in \\(R^2\\)::\n\n\\(A = \\begin{bmatrix} \\cos θ & -\\sin θ \\\\ \\sin θ & \\cos θ \\end{bmatrix}\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/11.html#projection",
    "href": "posts/03_resources/선형대수/notes/11.html#projection",
    "title": "linear transformations",
    "section": "Projection",
    "text": "Projection\n\n\\(proj_L(x) = \\frac{x ⋅ v}{u ⋅ v} v\\)\n\nif v is unit vecotr, then \\(proj_L(x) = (x ⋅ v) v\\)\n\\(A = \\begin{bmatrix} μ_1^2 & μ_2μ_1 \\\\ μ_1μ_2 & μ_2^2 \\end{bmatrix}\\)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/11.html#합성곱",
    "href": "posts/03_resources/선형대수/notes/11.html#합성곱",
    "title": "linear transformations",
    "section": "합성곱",
    "text": "합성곱\n\n선형 변환의 합성곱 역시 선형 변환: 합성곱을 Ax로 표현 가능.\n행렬의 곱은 결합법칙이 성립한다.\n행렬의 곱은 교환법칙이 성립하지 않는다.\n행렬의 곱은 분배법칙이 성립한다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/00.html#what-is-linear-algebra",
    "href": "posts/03_resources/선형대수/notes/00.html#what-is-linear-algebra",
    "title": "what is linear algebra",
    "section": "what is linear algebra",
    "text": "what is linear algebra\n선형 방정식을 matrix와 vector로 표현해서 다루는 수학\n\\(ax^2 + bx + c = 0\\) (x)\n\\(ax_1 + bx_2 + c = 0\\) (0)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/00.html#what-is-vector",
    "href": "posts/03_resources/선형대수/notes/00.html#what-is-vector",
    "title": "what is linear algebra",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있다.\n2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\nvector는 수학적으로, 아래와 같이 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/00.html#example",
    "href": "posts/03_resources/선형대수/notes/00.html#example",
    "title": "what is linear algebra",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 연립 1차 방정식을 matrix와 vector로 표현해보자\n\\[\n\\underset{A}{\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}}\n\\underset{x}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} =\n\\begin{bmatrix}\n1x + 2y \\\\\n2x + 5y\n\\end{bmatrix} =\n\\underset{b}{\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}}\n\\]",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/12.html#함수의-역",
    "href": "posts/03_resources/선형대수/notes/12.html#함수의-역",
    "title": "역함수와 역변환",
    "section": "함수의 역",
    "text": "함수의 역\n\n\\(f\\) is invertible ⟺ ∀y ∈ Y, ∃!x ∈ X such that \\(f(x) = y\\), 즉 f는 전단사 함수이다.\n\n전사: \\(f(R^n) -&gt; R^m, f(x) = Ax, C(A) = R^M\\), 즉 rank(A) = m\n단사: N(A) = {0}, 즉 Ax = 0의 해는 유일하다. = C(A)가 linearly independent이다.\n즉, A는 정방행렬이다.\n즉, A의 기약행렬은 I이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/12.html#함수의-해집합",
    "href": "posts/03_resources/선형대수/notes/12.html#함수의-해집합",
    "title": "역함수와 역변환",
    "section": "함수의 해집합",
    "text": "함수의 해집합\n\nAx = b의 해집합은 Ax = 0의 해집합(Null space)을 특수해 만큼 평행 이동한 것과 같다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/12.html#행렬식",
    "href": "posts/03_resources/선형대수/notes/12.html#행렬식",
    "title": "역함수와 역변환",
    "section": "행렬식",
    "text": "행렬식\n\n선형 변환 시 부피의 변화율과 방향을 나타낸다. (기존 부피에 행렬식의 절댓값을 곱한 것이 새로운 부피)\nn x n: row를 정해서, 그 row를 기준으로, \\(a_{row 1}A_{row 1} - a_{row 2}A_{row 2} + ... + a_{row n}A_{row n}\\)의 형태로 나타낼 수 있다.\n부호는 checkerboard pattern을 따른다.\n행렬의 scalar 곱은, 행렬식의 \\(\\text{scalar}^n\\)\n행렬의 특정 row끼리 더한 행렬의 행렬식은 두 행렬식의 합과 같다.\nduplicated row는 행렬식이 0이 된다. 즉, 역행렬이 존재하지 않는다.\n\n따라서 한 행에 상수배를 해서 다른 행에 더해도 행렬식은 변하지 않는다.\n\n상삼각행렬의 행렬식은 대각선 원소의 곱이다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/13.html",
    "href": "posts/03_resources/선형대수/notes/13.html",
    "title": "전치행렬",
    "section": "",
    "text": "전치행렬의 행렬식은 원래 행렬의 행렬식과 같다.\n\\((ABC)^T = C^T B^T A^T\\)\n\\((A + B)^T = A^T + B^T\\)\n\\((A^{-1})^T = (A^T)^{-1}\\)\n\\(C(A^T) = N(A)^⟂\\)\n\\(N(A^T) = C(A)^⟂\\)\nrank(A) + nullity(Aᵀ) = n\n\\(P=A(A^TA)^{−1}A^T\\): projection matrix\n\\(Px\\)는 x를 A의 column space로 projection한 것이다.\n\\(A^TAx = A^Tb\\): || Ax = b ||의 최소제곱해를 구하는 식\n닮음 변환: \\(D=C^{-1}AC\\) (C는 invertible matrix)\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "전치행렬"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/05.html#선형결합",
    "href": "posts/03_resources/선형대수/notes/05.html#선형결합",
    "title": "선형결합과 생성",
    "section": "선형결합",
    "text": "선형결합\n벡터들의 상수배 합으로 만들 수 있는 벡터의 집합",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "선형결합과 생성"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/notes/06.html#linear-independence",
    "href": "posts/03_resources/선형대수/notes/06.html#linear-independence",
    "title": "linear independence",
    "section": "Linear independence",
    "text": "Linear independence\n\nDefinition\n\nDependence: one of the vectors in the set can be written as a linear combination of the others.\nIndependence: ⫬ dependence\n\n\n\nTheorem\nS = \\({v_1, v_2, ..., v_n}\\)\n\\(S\\) is linearly dependent ⟺ ∃(\\(c_i\\) is not 0) \\(c_1v_1 + c_2v_2 + ... + c_nv_n = 0\\) is \\(c_1 = c_2 = ... = c_n = 0\\).\n\nif \\(c_1 = c_2 = ... = c_n = 0\\), then \\(S\\) is linearly independent.\n벡터의 원소 수가 \\(n\\)인 경우, span(\\(S\\))의 차원은 \\(n\\) 이하이다. (나머지 벡터는 선형 결합으로 표현 가능)",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수",
      "Notes",
      "linear independence"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/index.html",
    "href": "posts/03_resources/선형대수/index.html",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/index.html#details",
    "href": "posts/03_resources/선형대수/index.html#details",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/index.html#tasks",
    "href": "posts/03_resources/선형대수/index.html#tasks",
    "title": "선형대수",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/index.html#참고-자료",
    "href": "posts/03_resources/선형대수/index.html#참고-자료",
    "title": "선형대수",
    "section": "참고 자료",
    "text": "참고 자료\n\nKhan Academy 강의\n3Blue1Brown 강의",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_resources/선형대수/index.html#related-posts",
    "href": "posts/03_resources/선형대수/index.html#related-posts",
    "title": "선형대수",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/index.html",
    "href": "posts/03_resources/helm/index.html",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/index.html#details",
    "href": "posts/03_resources/helm/index.html#details",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/index.html#tasks",
    "href": "posts/03_resources/helm/index.html#tasks",
    "title": "Helm",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/index.html#참고-자료",
    "href": "posts/03_resources/helm/index.html#참고-자료",
    "title": "Helm",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_resources/helm/index.html#related-posts",
    "href": "posts/03_resources/helm/index.html#related-posts",
    "title": "Helm",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Resources",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/00.html#what-is-airflow",
    "href": "posts/03_resources/air_flow/notes/00.html#what-is-airflow",
    "title": "Getting Started",
    "section": "What is Airflow",
    "text": "What is Airflow\n\nopen source platform to pragramatically author, schedule and monitor workflows\nNot a data processing framework\nNot a Real time streaming solution (only for batch processing)\nNot a data storage system\nand simple linear workflow might overkill",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/00.html#why-airflow",
    "href": "posts/03_resources/air_flow/notes/00.html#why-airflow",
    "title": "Getting Started",
    "section": "Why Airflow",
    "text": "Why Airflow\n\nautomation\nvisibility\nflexibility and scalability\nextensibility",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/00.html#core-components",
    "href": "posts/03_resources/air_flow/notes/00.html#core-components",
    "title": "Getting Started",
    "section": "Core Components",
    "text": "Core Components\n\nWebserver: provides UI\nScheduler: triggers tasks. ensure that task runs in correct time and order\nmeta database: memmory, communication between components\ntrigger: daemon that listens to external events and triggers tasks\nexecuter: traffic controller that decide how tasks are executed (sequential or parallel, local or remote)\nqueue\nworker",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/00.html#core-concepts",
    "href": "posts/03_resources/air_flow/notes/00.html#core-concepts",
    "title": "Getting Started",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nDAG\n\nDirected Acyclic Graph\ncollection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies\nno cycles in dependencies graph\n\n\n\nOperator\n\ndefines a single task in a workflow\ne.g. BashOperator, PythonOperator, EmailOperator, etc.\n\n\n\nTask / Task Instance\n\nspecific instance of an operator\nwhen operator assigned to a DAG, it becomes a task\n\n\n\nWorkflow\n\nentire process defined by DAG\nDAG = workflow",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_resources/air_flow/notes/00.html#arcitecture",
    "href": "posts/03_resources/air_flow/notes/00.html#arcitecture",
    "title": "Getting Started",
    "section": "Arcitecture",
    "text": "Arcitecture",
    "crumbs": [
      "PARA",
      "Resources",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/07.html#overview",
    "href": "posts/02_areas/deep_learning/notes/07.html#overview",
    "title": "word2vec",
    "section": "Overview",
    "text": "Overview\n\n이전에 봤던 방법은 통계 기반 기법, 모든 학습 데이터를 한꺼번에 처리하는 방식\nword2vec은 신경망 기반 기법, 미니배치 학습\n\n\nimport numpy as np\n\nc = np.array([[1, 0, 0, 0, 0, 0, 0]])\nW = np.random.randn(7, 3)\nh = np.dot(c, W)\nh\n\narray([[ 0.7871821 ,  0.94388187, -1.51434548]])",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "word2vec"
    ]
  },
  {
    "objectID": "posts/02_areas/deep_learning/notes/07.html#cbow-모델",
    "href": "posts/02_areas/deep_learning/notes/07.html#cbow-모델",
    "title": "word2vec",
    "section": "CBOW 모델",
    "text": "CBOW 모델\n\n딥러닝 학습을 진행\n\n말뭉치로부터 목표하는 단어를 타깃으로, 그 주변 단어를 맥락으로 뽑아냄.\n맥락을 one hot 인코딩 해서 입력으로 사용, 타깃을 정답 레이블로 사용\n\n입력 측의 가중치(단어의 분산 표현)를 이용해서 예측을 진행",
    "crumbs": [
      "PARA",
      "Areas",
      "Deep Learning",
      "Notes",
      "word2vec"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/index.html",
    "href": "posts/02_areas/ros/index.html",
    "title": "ROS",
    "section": "",
    "text": "ROS 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/index.html#details",
    "href": "posts/02_areas/ros/index.html#details",
    "title": "ROS",
    "section": "",
    "text": "ROS 관련 노트입니다.",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/index.html#tasks",
    "href": "posts/02_areas/ros/index.html#tasks",
    "title": "ROS",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/index.html#참고-자료",
    "href": "posts/02_areas/ros/index.html#참고-자료",
    "title": "ROS",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS"
    ]
  },
  {
    "objectID": "posts/02_areas/ros/index.html#related-posts",
    "href": "posts/02_areas/ros/index.html#related-posts",
    "title": "ROS",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Areas",
      "ROS"
    ]
  },
  {
    "objectID": "posts/04_archives/대학원/notes/01.html",
    "href": "posts/04_archives/대학원/notes/01.html",
    "title": "연구실",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Archives",
      "대학원",
      "Notes",
      "연구실"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/notes/00.html#데이터-모델의-이해",
    "href": "posts/01_projects/sqld/notes/00.html#데이터-모델의-이해",
    "title": "데이터 모델의 이해",
    "section": "데이터 모델의 이해",
    "text": "데이터 모델의 이해\n\nOverview\n\n모델링: 현실세계를 관리하고자 하는 데이터를 모델로 단순화해 표현한 것\n특징\n\n추상화\n단순화\n명확화\n\n모델링의 세 가지 관점\n\n데이터 관점: 어떤 데이터들이 업무와 얽혀있는지, 그리고 그 데이터 간에는 어떤 관계가 있는지에 대해서 모델링\n프로세스 관점: 이 업무가 실제로 처리하고 있는 일은 무엇인지, 앞으로 처리해야 하는 일은 무엇인지 모델링\n데이터와 프로세스의 상관 관점: 프로세스의 흐름에 따라 데이터가 어떤 영향을 받는지 모델링\n\n모델링의 세 가지 단계\n\n개념적 모델링: 전사적인 모델링\n논리적 모델링: key, 속성, 관계 등을 모두 표현하는 것\n물리적 모델링: 실제 물리적으로 데이터베이스를 설계\n\n데이터의 독립성\n\n외부 스키마: 각 사용자가 보는 데이터베이스의 스키마를 정의\n개념 스키마: 데이터베이스 싀마를 통합하여 전체 데이터베이스를 나타냄\n내부 스키마: 물리적인 저장 구조(칼럼, 인덱스 등)을 나타냄.\n논리적 독립성: 개념 스키마가 변경되어도 외부 스키마는 영향을 받지 않음.\n물리적 독립성: 내부 스키마가 변경되어도 외부 / 개념 스키마는 영향을 받지 않음.\n\nERD\n\nPeter Chen\nIDEF1X\nIE/Crow’s Foot\nMin-Max/ISO\nUML\nCaseMethod / Barker\n\n\n\n\nEntity\n\n특징\n\n업무에서 쓰이는 정보여야 함\n유니크함을 보장할 수 있는 식별자가 있어야 함\n2개 이상의 인스턴스를 가지고 있어야 함\n반드시 속성을 가지고 있어야 함\n다른 엔티티와 1개 이상의 관계를 가지고 있어야 함\n\n분류\n\n유형 vs 무형\n\n유형 엔티티: 물리적으로 존재\n개념 엔티티: 개념적으로 존재\n사건 엔티티: 행위를 함으로써 발생\n\n발생 시점\n\n기본 엔티티: 독립적으로 생성됨\n중심 엔티티: 기본 엔티티로부터 파생\n행위 엔티티: 2개 이상의 엔티티로부터 파생\n\n\n\n\n\nAttribute\n\n특성에 따른 분류\n\n기본속성\n설계속성: 필요하진 않지만 모델링을 위해 생긴 것\n파생속성: 계산하기 편하게 사용하려고 만든 것\n\n구성방식에 따른 분류\n\nPK\nFK\n일반 속성\n\n\n\n\nRelationship\n\n존재관계\n행위관계\n표기법\n\n관계명\n관계차수\n관계선택사양\n\n\n\n\nIdentifiers\n\n유일성\n식별성\n불변성\n존재성: NULL이 될 수 없음\n분류\n\n주식별자\n보조식별자\n내부식별자\n외부식별자\n단일식별자\n복합식별자\n원조식별자\n대리식별자",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비",
      "Notes",
      "데이터 모델의 이해"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/notes/00.html#데이터-모델과-sql",
    "href": "posts/01_projects/sqld/notes/00.html#데이터-모델과-sql",
    "title": "데이터 모델의 이해",
    "section": "데이터 모델과 SQL",
    "text": "데이터 모델과 SQL\n\n정규화\n\n제 1정규형: 모든 속성은 반드시 하나의 값만 가져야 한다.\n제 2정규형: 엔티티의 모든 일반 속성은 반드시 모든 주식별자에 종속되어야 한다.\n제 3정규형: 주 식별자가 아닌 모든 속성간에는 중속될 수 없다.\n\n\n\n반정규화\n\n테이블 반정규화\n\n테이블 병합\n테이블 분할\n테이블 추가\n\n칼럼 반정규화\n\n중복 칼럼 추가\n파생 칼럼 추가: 부하가 예상되는 칼럼을 미리 계산\n이력 테이블 칼럼 추가: 최신 데이터 여부 같은 조회 기준이 될 칼럼을 미리 만드는 것\n\n관계 반정규화\n\n\n\n트랜잭션\n\n\nNULL\n\n가로 연산은 NULL, 세로 연산은 NULL 무시",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비",
      "Notes",
      "데이터 모델의 이해"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/index.html",
    "href": "posts/01_projects/sqld/index.html",
    "title": "SQLD 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-08-19\n        종료일: 2025-08-23\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증sql",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/index.html#details",
    "href": "posts/01_projects/sqld/index.html#details",
    "title": "SQLD 준비",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/index.html#tasks",
    "href": "posts/01_projects/sqld/index.html#tasks",
    "title": "SQLD 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/index.html#참고-자료",
    "href": "posts/01_projects/sqld/index.html#참고-자료",
    "title": "SQLD 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/sqld/index.html#related-posts",
    "href": "posts/01_projects/sqld/index.html#related-posts",
    "title": "SQLD 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html",
    "href": "posts/01_projects/toeic/index.html",
    "title": "Toeic 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-08-22\n        종료일: 2025-10-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#details",
    "href": "posts/01_projects/toeic/index.html#details",
    "title": "Toeic 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#tasks",
    "href": "posts/01_projects/toeic/index.html#tasks",
    "title": "Toeic 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#참고-자료",
    "href": "posts/01_projects/toeic/index.html#참고-자료",
    "title": "Toeic 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#related-posts",
    "href": "posts/01_projects/toeic/index.html#related-posts",
    "title": "Toeic 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#한국은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#한국은행",
    "title": "관심 분야 JD",
    "section": "한국은행",
    "text": "한국은행\n\n\n\n채용 절차\n\n\n\n채용 인원: 컴퓨터공학 15명 이내\n어학 성적 우대: TOEIC, TOEFL(iBT), TEPS 중\n필기 출제:\n\n전공 학술: 소프트웨어공학, 데이터베이스, 컴퓨터구조, 데이터통신, 정보보호, 운영체제, 자료구조, 인공지능, 기계학습\n논술: 주요 경제 / 금융 이슈, 인문학 등\n기출\n\n면접 전형:\n\n1차 실무 면접: 집단 토론, 심층 면접\n2차 집행 간부 면접",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#한국-투자증권",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#한국-투자증권",
    "title": "관심 분야 JD",
    "section": "한국 투자증권",
    "text": "한국 투자증권\n\n자기소개서 문항\n\n성장과정:가족사항, 학창시절, 교우관계, 생활습관, 자신에게 크게 영향을 미친 사건 등을 포함하여 구체적으로 작성 (최대 500자)\n실패 혹은 좌절을 극복한 사례와 이를 통해 얻은 교훈은? (최대 300자)\n증권업을 선택하게된 이유와 증권사 중 당사를 선택한 이유 (최대 300자)\n지원한 분야는 어떤 일을 한다고 생각하는가? (최대 200자)\n지원분야에 본인이 적합한 이유를 증명하시오. (최대 500자)\n당신의 인생계획(Life Plan)에서 꿈은 무엇이고, 그 꿈을 이루기 위해 회사가 어떻게 도움을 줄 수 있는지 서술하시오. (최대 500자)",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#금융-감독원",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#금융-감독원",
    "title": "관심 분야 JD",
    "section": "금융 감독원",
    "text": "금융 감독원\n\n채용 인원: IT 7명\n\n - 어학 성적 우대: TOEIC 730, TOEFL(iBT) 79, TEPS 368 만점 - 1차 필기 시험: NCS 직업 기초 시험 - 2차 필기 시험 - 전공 시험 - 논술 - 1차 면접 전형 - 실무진, 외부위원 개별 면접, 집단 토론 - 2차 면접 전형 - 임원, 외부위원 개별 면접",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#ibk-기업은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#ibk-기업은행",
    "title": "관심 분야 JD",
    "section": "IBK 기업은행",
    "text": "IBK 기업은행\n  - ADP를 꼭 따자 - 산업공학 석사 우대 - 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n직무 소개\n\n\n\n\n\n전형 방법\n\n\n\n필기 시험\n\n직업 기초(객관식 40문항): 의사소통, 문제 해결, 자원관리, 조직 이해, 수리, 정보\n직무 수행(객관식 30문항, 주관식 5문항): 데이터베이스, 데이터분석, 인공지능 모델링, 블록체인, 시사 등\n\n실기 시험\n\n실기시험 전, AI역량검사 및 인성검사(온라인) 실시 예정\n(공통) 개인발표, 토론, 인터뷰\n(디지털, IT) 코딩테스트\n\n면접 시험\n\n多대多 질의응답을 통해 인성, 윤리의식, 직무·조직적합도 등의 평가항목을 기준으로 종합평가(평가위원 점수합계)\n\n\n\n자기소개 문항\n\nIBK와 미래를 함께해 나갈 지원자님을 환영합니다! 다양한 회사들 중에서 IBK를 선택하신 이유가 궁금한데요, 지원동기에 대해 편안하게 이야기해 주세요. (1500자 이내)\n지원자님의 여러 장점 중 “팀웍”에 대해 듣고 싶어요. 최선의 결과를 이끌어내기 위해 팀원으로서 했던 역할에 대해 구체적인 경험을 들어 말씀해 주세요. (1500자 이내)\n지원자님이 생각하는 본인의 단점에 대해 알고 싶어요. 그리고 그것을 극복하기 위해 기울이셨던 노력에 대해서도 자유롭게 전달해 주세요. (1500자 이내)\n은행원이라는 직업이 지원자님께 왜 어울리는지 궁금합니다. 지원자님만이 갖고 있는 차별화된 스토리를 저희에게 들려 주세요. (1500자 이내)",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#우리-은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#우리-은행",
    "title": "관심 분야 JD",
    "section": "우리 은행",
    "text": "우리 은행\n\n\n\n직무 내용\n\n\n\n신입 행원 연수: 약 1년 이내 영업점 근무 후, 유관 부서 및 본부 부서 배치. 인력 운영 및 업무상 필요에 따라 변동될 수 있음\n\n(이거 잘못하면 내 커리어 개같이 멸망할 수도 있겠는데)\n\nTECH 직무 관련 전공 우대 (구체적으로 무슨 전공인지 모르겠음)\n2025 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n\n우대 자격증\n\n\n\n\n\n공인 영어\n\n\n\n\n\n전형 방법\n\n\n\n서류 전형: 입행 지원서 심사 및 AI 역량검사\n\nAI 역량검사: 자기보고, 전략게임, 영상 면접\n\n1차 면접: 기본 역량 면접\n2차 면접:\n\n인사이트 면접(PT, 포트폴리오), 참여형 팀워크 프로그램, 직무 / 인성 면접\n코딩테스트: 알고리즘 및 SQL\n\n최종 면접: 심화 역량 면접 (blind 면접. 그럼 어떻게 본다는거지?)\n\n\n자기소개 문항\n\n우리은행 및 해당 부문에 지원한 동기와 입행 후 이루고 싶은 목표를 구체적으로 서술해 주세요. (800자 이내)\n본인 성격 중 은행원 업무와 가장 맞지 않는 부분이 무엇이라고 생각하며, 그 이유를 구체적으로 설명해 주세요. (800자 이내)\n과거에 했던 선택이나 행동 중, 후회하는 사례를 구체적으로 작성해 주세요.(800자 이내)\n본인이 IT 직무에서 다른 지원자와 차별화된 경쟁력을 갖추었다고 생각하는 부분을 구체적인 사례를 근거로 설명해 주세요. (800자 이내)",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#서울대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#카이스트",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#포항공대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#연세대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#고려대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "title": "관심 분야 JD",
    "section": "인프라 / 엔지니어",
    "text": "인프라 / 엔지니어",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "title": "관심 분야 JD",
    "section": "Computer Vision / 품질검사",
    "text": "Computer Vision / 품질검사\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 현대오토에버\n\n\n\n\n\n\n2025 상반기 현대트랜시스\n\n\n\n\n\n\n2025 상반기 에스엘",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#자율주행",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#자율주행",
    "title": "관심 분야 JD",
    "section": "자율주행",
    "text": "자율주행\n \n\n\n\n\n2025 상반기 KAI\n\n\n\n\n\n\n2025 상반기 현대트랜시스",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#llm",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#llm",
    "title": "관심 분야 JD",
    "section": "LLM",
    "text": "LLM\n\n\n\n2024 LG화학 기반기술 연구소",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#기타",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#기타",
    "title": "관심 분야 JD",
    "section": "기타",
    "text": "기타\n\n\n\n2025 상반기 HD한국조선해양\n\n\n\n \n\n \n\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 삼성물산\n\n\n\n\n\n\n2024 하반기 LG U+\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 네이버\n\n\n\n\n\n\n2025 상반기 네이버",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#의사결정-level",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#의사결정-level",
    "title": "SCM 의사결정",
    "section": "의사결정 level",
    "text": "의사결정 level\n\nsupply chain (strategic) design\n\n한 번 결정하면 오래 가고, 영향을 많이 미치는 장기적 의사결정\n기업의 우선경쟁역량1이나 경영전략을 종합적으로 고려\n설비의 수, 위치 선정, 제품설계 등\n수년 ~ 수십년 단위\n최고경영자 및 임원 수준에서 결정\n\nsupply chain (Tactical) planning\n\ninput: 공급사슬 설계, 예측 수요\n생산, 재고, 운송 등의 통합적 계획 수립\n수요-공급 계획, 생산 하청 계획, 판촉 규모와 시기 결정 등\n수주, 수개월 ~ 수년 단위 계획\n\nsupply chain operation\n\nSCO\nSCE(execution)\n시간, 수일 ~ 수주 단위 계획\n\n\n\n아래로 갈수록 범위가 좁아지고, 디테일해짐\n\n\nlead time이랑 time to market이 뭐가 다르지",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#프로세스-구조",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#프로세스-구조",
    "title": "SCM 의사결정",
    "section": "프로세스 구조",
    "text": "프로세스 구조\n\n주기(cycle) 프로세스\n\n인접한 단계간 활동이 주기적으로 반복된다고 보는 관점\n서로 다른 주기가 잘 synchornized 되어야 함\n\n\n\nPush-Pull 프로세스\n\npush: 예측 수요에 의해 공급사슬 활동이 촉발\npull: 실제 수요에 의해 공급사슬 활동이 촉발\n고객 주문을 접수하는 접점이 push-pull 경계\n접점을 어느 위치에 두느냐에 따라 cost-quality trade-off가 달라짐",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#footnotes",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#footnotes",
    "title": "SCM 의사결정",
    "section": "각주",
    "text": "각주\n\n\ncost, quality(성능, 일관성), time(time to market, lead-time, on time delivery), flexibility↩︎",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질이란",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질이란",
    "title": "품질관리의 기본개념",
    "section": "품질이란",
    "text": "품질이란\n\n상대적으로 뛰어난 정도 / 구분되는 속성\n품질 관자 입장에서 더 나은 품질을 만들기 위해 노력해야 함\n품질에 대한 다양한 관점이 존재한다.\n\n사용자 기반: 나한테 오는 재품이 얼마나 좋은가\n제조(시스템 관리자) 기반: 제품이 설계된 대로 잘 만들어졌는가\n제품 기반: 품질을 정확하고 측정 가능한 변수로 보는 것\n\n\n\n좋은 품질의 요건\n\n우수성(성능, 기능, 외관)\n일관성 (단품별, 사용환경, 내구성 등)\n둘을 만족할 때 고객 만족이 증가하고 조직의 이익이 증가함",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질관리의-정의",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질관리의-정의",
    "title": "품질관리의 기본개념",
    "section": "품질관리의 정의",
    "text": "품질관리의 정의\n\n\n\n전통적인 관점에서의 품질관리\n\n\n\n\n\n현대적인 관점에서의 품질관리\n\n\n\n시장성이 높은 제품 및 서비스를 경제적으로 생산하기 위한 일련의 체계적 관리\n전통적 관점(QC):\n\n표준을 설정할 뿐 지키기 위한 노력 경시\n불량품을 솎어내는 검사에만 의존\n불량의 발생 원인제거 및 개선 노력 부족\n\n현대적 관점 / 종합적 품질관리(TQC):\n\n품질변동의 원인을 파악하여, 변동을 지속적으로 감소시키고 성과 수준을 높이는 과정\nPDSA cycle: plan, do, study, act",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질의-분류",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질의-분류",
    "title": "품질관리의 기본개념",
    "section": "품질의 분류",
    "text": "품질의 분류\n\n\n\n품질의 분류\n\n\n\n요구품질: 시장에서 요구되는 품질. 정성적(시장조사, 경쟁제품 분석)인 방법으로 파악\n설계품질: 파악한 고객요구를 설계 언어로 잘 변환했는지\n제조품질: 설계된 사양을 실제로 잘 구현했는지 정량적인 방법으로 파악\n사용품질: 고객이 실제로 느끼는 품질. 만족도",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질-관리의-유명-지도자",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질-관리의-유명-지도자",
    "title": "품질관리의 기본개념",
    "section": "품질 관리의 유명 지도자",
    "text": "품질 관리의 유명 지도자\n\nF. W. Taylor: 품질에 대한 과학적(정량적) 관리법 제시\nW. A. Shewhart: 통계적 품질관리(SQC)의 시조. 품질 변동의 원인을 우연원인과 이상원인으로 구분\nW. E. Deming: PDMA 사이클",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질관리의-역사",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질관리의-역사",
    "title": "품질관리의 기본개념",
    "section": "품질관리의 역사",
    "text": "품질관리의 역사\n\n작업자에 의한 품질관리: 한 명이 장인정신으로 다 함.\n작업반장에 의한 품질관리: 산업혁명 이후 작업자를 그룹으로 묶고 이들을 통제하는 작업반장이 품질 책임. 공식적인 품질검사 도입\n검사에 의한 품질관리: 컨베이어 시스템 도입으로 작업자 수 증가. 작업이 전문화, 규격화 되면서 검사만 전문적으로 담당하는 품질부서 등장.\n통계적 품질관리\nTQC(Total Quality Control): 품질 관리. 품질 책임을 전사적으로 담당\nTQM(Total Quality Management): 품질 경영. 품질을 전체적으로 관리하는 전략\n\n\n품질 관리와 품질 경영의 차이\n\n적용 범위에 차이가 있다:\n\nPDSA를 기존에는 단순히 계획을 수립 및 설계하고 시스템을 운영하고 그 결과를 계획과 비교하여 통제하는데 집중함.\n생산자중심 시대에서는 시스템 운영상의 효율이 강조됨\n고객 중심 시대로 넘어옴에 따라 운영효율 뿐만 아니라 고객의 요구를 제대로 파악하고 이를 설계단계에 제대로 반영하는 계획 및 설계 단계의 중요성이 강조\n즉 품질 경영(선행형)은 과거 품질관리(대응형) 영역을 기본적으로 포함하며 계획 및 설계 단계로 확대한 개념",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질-비용",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질-비용",
    "title": "품질관리의 기본개념",
    "section": "품질 비용",
    "text": "품질 비용\n\n통제비용: 물품, 서비스의 품질과 관련해서 발생.\n\n예방비용: 계획, 교육, 훈련, 프로세스 개선 등 불량을 예방하는데 드는 비용\n평가비용: 불량을 검사하는데 드는 비용\n\n실패비용\n\n내적 실패비용: 생산 과정에서 발생하는 불량에 대한 비용\n외적 실패비용: 고객에게 전달된 후에 발생하는 불량에 대한 비용. 정량화가 매우 어려움\n\n\n\n\n\n품질 개선과 비용에 대한 견해\n\n\n\n전통적 견해: 최적 수준을 위해 일정량의 불량은 허용해야 한다고 봄\n무결점 견해: 통제비용이 올라가는 속도보다 실패비용이 감소하는 속도가 더 빠르기 때문에 품질 수준을 계속 높여야 한다고 봄",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#경영환경-변화",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#경영환경-변화",
    "title": "프로세스 경영 개요",
    "section": "경영환경 변화",
    "text": "경영환경 변화\n\n기술 발전과 경쟁 격화로 기업의 평균 수명은 지속적으로 단축되고 있음\n\n\n주요 요소\n\nGlobalization\n제품 수명 주기 단축 및 비용 압력\n제품과 서비스의 개인화 및 맞춤화\n기업의 인수합병\n\n기존의 업무 프로세스가 많이 바뀌어야 함\n변화에 대한 민첩한 적응력(agility) 요구\n\n가치사슬의 확대\n\n협업의 범위 및 레벨 확장/심화\n\nAI & 자동화 확산\nDX\n사이버 보안 및 개인정보 보호\nESG\nCX: 프로세스 설계의 기준이 고객 경험이 됨",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영이란",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영이란",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영이란?",
    "text": "프로세스 경영이란?\n\n고객가치 창출을 위해 내외부의 업무 프로세스를 최적화하는 경영체계\n내부의 고객과 외부의 고객에게 가치를 전달하는 시작점과 종료점까지의 업무 처리 전 과정을 통합적으로 관리함으로써 기업의 성공을 도모\n기업의 모든 활동을 프로세스를 중심으로 가시화함으로써 경영환경 변화에 민첩하게 대응할 수 있도록 하고, 이를 통해 가치사슬 전반에 걸쳐 비효율을 개선\nIT의 전략적 활용",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-중심적-사고방식",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-중심적-사고방식",
    "title": "프로세스 경영 개요",
    "section": "프로세스 중심적 사고방식",
    "text": "프로세스 중심적 사고방식\n\n사물을 구조보다는 프로세스로 인식하는 사고 방식\n기업 역시 기업 외부와의 상호작용, 내부의 업무 흐름을 비즈니스 프로세스의 집합으로 인식\n\n\n\n\nMichael Porter’s Value Chain",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#business-process-및-workflow",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#business-process-및-workflow",
    "title": "프로세스 경영 개요",
    "section": "Business process 및 workflow",
    "text": "Business process 및 workflow\n\nBusiness process: 고객 중심\n\n특정 고객또는 시장을 대상으로 필요한 산출물(output)을 생서하기 위해 정의된 구조화되고 측정가능한 활동들의 집합\n고객이나 기업에게 가치를 주는 결과(outcome)으로 이끄는 activities, events, decisions의 모임\nend-to-end 고객 여정\n\n\n\n\n\nBPMN에 따라 작성된 business process\n\n\n\nworkflow: 작업 중심\n\nbusiness process를 자동화한 것\n\n\n\nBusiness process 특징\n\n대상 고객이 있다.\ncross organizational boundaries이다.\n\n조직 경계와 무관하게 업무가 진행됨",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영-구축-프레임워크",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영-구축-프레임워크",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영 구축 프레임워크",
    "text": "프로세스 경영 구축 프레임워크\n\n\n\n프로세스 경영 구축 방법론 프레임워크\n\n\n\n비전수립\n프로세스 아키텍처 - Michael Porter's Value Chain\n프로세스 모델링 / 분석\n프로세스 자동화\n프로세스 성과 관리",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영의-역사",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#프로세스-경영의-역사",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영의 역사",
    "text": "프로세스 경영의 역사\n\n공정 기술\n\n1910년 초반 테일러의 과학적 관리\n\n공정을 분석하여 분업화, 전문화 하여 생산성 향상\n\n테일러 이론에 의한 공정기술의 발전\n구매, 회계, 배송, 고객 서비스 등 조직 전체 업무 영역으로 확대\n제조업을 넘어 여러 서비스 산업 전반으로 확산\n\n\n\n품질 경영\n\nSQM\nTQM\n6 시그마, ‘lean`’\n\n\n품질 경영: 오늘날의 프로세스 경영\n\n품질 뿐만 아니라 플세스를 활용하는 조직 전반의 포괄적 경영 패러다임으로 확장한 것",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#풍력에너지-실무-및-향후전망",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#풍력에너지-실무-및-향후전망",
    "title": "두산에너빌리티",
    "section": "풍력에너지 실무 및 향후전망",
    "text": "풍력에너지 실무 및 향후전망",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "두산에너빌리티"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/시뮬레이션/00.html#구조",
    "href": "posts/01_projects/bs_3_2/notes/시뮬레이션/00.html#구조",
    "title": "Intro",
    "section": "구조",
    "text": "구조\n\n\n\n\n\nflowchart LR\n    Entity((Entity)) --&gt; Queue\n    Queue[Queue] -&gt; Process(process)\n\n\n\n\n\n\n\nEntity: attribute를 가짐.\nprocess: 추상화된 개념. 여러 resource를 사용할 수 있음.\n물어볼거: 시작하자마자 error, mod 확장자?",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/00.html",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/00.html",
    "title": "intro",
    "section": "",
    "text": "중간: 10.23(목) 18:00 - 19:15\n기말: 12.09(화) 18:00 - 19:15\n나반임\n\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/00.html",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/00.html",
    "title": "intro",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/00.html#절대평가",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/00.html#절대평가",
    "title": "Intro",
    "section": "절대평가",
    "text": "절대평가\n\n85~100: A\n70~84: B\n~69: C\n\n시험\n\n강의실 미래관으로 옮김\n중간: 10/22 19:30 - 20:30\n기말:",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "Intro"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/index.html",
    "href": "posts/01_projects/bs_3_2/index.html",
    "title": "학부 3학년 2학기",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-09-01\n        종료일: 2025-12-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/index.html#details",
    "href": "posts/01_projects/bs_3_2/index.html#details",
    "title": "학부 3학년 2학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 2학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/index.html#tasks",
    "href": "posts/01_projects/bs_3_2/index.html#tasks",
    "title": "학부 3학년 2학기",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/index.html#참고-자료",
    "href": "posts/01_projects/bs_3_2/index.html#참고-자료",
    "title": "학부 3학년 2학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/index.html#related-posts",
    "href": "posts/01_projects/bs_3_2/index.html#related-posts",
    "title": "학부 3학년 2학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/01.html",
    "href": "posts/01_projects/진로준비/notes/대학원/01.html",
    "title": "연구실",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "연구실"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html",
    "href": "posts/01_projects/진로준비/index.html",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#details",
    "href": "posts/01_projects/진로준비/index.html#details",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#tasks",
    "href": "posts/01_projects/진로준비/index.html#tasks",
    "title": "진로 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#related-posts",
    "href": "posts/01_projects/진로준비/index.html#related-posts",
    "title": "진로 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "PARA",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/02.html#품질특성",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/02.html#품질특성",
    "title": "품질변동과 공정능력",
    "section": "품질특성",
    "text": "품질특성\n\n시장에서 고객의 요구사항을 충족시키는 정도를 평가하는 요소\n\n물리적 특성: 길이, 무게, 온도, 압력 등\n감각적 특성: 색상, 냄새, 맛, 촉감 등\n추상적 특성: 신뢰성, 내구성, 유지보수성 등\n\n좋은 품질: 고객이 만족하는 우수한 품질을 균일하게 달성\n품질은 변동이 존재.\n\n품질특성의 분포에서 설계품질 허용수준 이하의 값이 실현되면 고객은 이를 불량이라 인식함.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/02.html#설계규격과-불량",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/02.html#설계규격과-불량",
    "title": "품질변동과 공정능력",
    "section": "설계규격과 불량",
    "text": "설계규격과 불량\n\n\n설계규격: 기준치 + 허용차\n\n공차: 허용차의 범위. 규격한계\n\n치수공차: 길이, 너비, 두께, 등\n\nuni directional tolerance: 한쪽으로만 허용차가 있는 경우\nbi directional tolerance: 양쪽으로 허용차가 있는 경우\n\n기하공차: 형상의 정확성에 대한 허용 오차. 평면도, 직진도, 원형도 등",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/02.html#규격한계-공정능력-한계",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/02.html#규격한계-공정능력-한계",
    "title": "품질변동과 공정능력",
    "section": "규격한계, 공정능력 한계",
    "text": "규격한계, 공정능력 한계\n\n\n규격한계: 설계규격에서 엄격하게 정한 범위 (USL, LSL)\n공정능력 한계: 실제 공정에서 만들어지는 제품의 특성치가 분포하는 범위 6σ (UPCL, LPCL)\n\n공정능력: 공정능력 한계가 규격한계 내에 들어와야 한다\n\n이론 공정능력지수(Cp): 불량을 적게 생산하는 능력\n\nCp = (USL - LSL) / 6σ\n\n평균에 대한 고려가 없음\n\n\n\n\n\n\n공정능력지수\n\n\n\n실제 공정능력지수(Cpk): 평균이 중심에 있는지 고려\n\nCpk = min(\\(\\frac{USL - μ}{3σ} , \\frac{μ - LSL}{3σ}\\))\n평균은 시간이 지남에 따라 변동될 수 있음. 평균이 늘 공차의 중심에 있지 않을 수 있다.\n\n\n\n\n\n시그마 품질 수준\n\n\n\n\\(Cp_k\\)는 장기적 품질 이동 가능성을 고려하여 1.5σ인 0.5를 뺀 값으로 계산\n6σ 품질을 추구하려면 Cp = 2.0, Cpk = 1.5(1.5σ의 평균 이동 가정)가 되어야 함\n공정능력지수가 1이 되면 기대불량률은 0.27%\n\n하지만 실제로는 평균이 이동할 수 있으므로 기대불량률이 더 높아짐",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/02.html#시그마-프로그램",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/02.html#시그마-프로그램",
    "title": "품질변동과 공정능력",
    "section": "6 시그마 프로그램",
    "text": "6 시그마 프로그램\n\nDMAIC 프로세스 개선 모델:\n\nDefine: 핵심품질특성(CTQ) 규명\nMeasure: CTQ에 영향을 주는 독립변수와 값에 대한 데이터 수집\nAnalyze: 통계적 기법을 활용하여 분석\nImprove: 원인이 규명되면 개선할 아이디어를 찾아 실행\nControl: 개선사항을 표준화하여 절차 수립 및 작업자 교육\n\n적합품질 결정요소\n\n5M:\n\nmethod: 간단한 절차가 좋다.\nman: 품질관리에 숙련된 인력이 필요하다.\nmachine: 설비의 주기적인 점검과 고정이 필요\nmaterial: 원재료의 품질이 중요\nmeasurement: 올바른 측정이 중요\n\n\n\n\n\n\n공정 관리 준비 순서",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm이란",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm이란",
    "title": "BPM 개요",
    "section": "BPM이란?",
    "text": "BPM이란?\n\n프로세스 관점에서 기업을 경영하는 것\n소프트웨어 도구를 활용하여 조직의 업무와 프로세스를 끊임없이 최적화하는 구조적인 접근방법 및 관리역량이자 도구의 집합\n즉, 프로세스 경영 + IT 도구의 접목\n경영전략과 IT 기술의 alignment를 위한 방법론\n\n\n\n\nIT의 영향\n\n\n\nBPM의 영역 및 정의\n\n\nBPM의 영역\n\npeople: 공급자, 임직원, 고객\nprocess: PDCA\ntechnology: workflow, integration solution, UI solution",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#difficulties-in-scm",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/01.html#difficulties-in-scm",
    "title": "SCM 의사결정",
    "section": "Difficulties in SCM",
    "text": "Difficulties in SCM\n\n제품개발 프로세스와의 통합\n\n공급사슬 전략은 SCM뿐만 아니라 제품개발 프로세스에 의해서도 영향을 받음\n각 단계에서 서로 다른 사람들이 책임을 져서, misalignment이 발생할 수 있음\n\n\n\n전역최적화\n\ncentralized decision making이 어려움\n\n공급사슬의 조정이 필요\n\nbuy back contract: 재고를 공급자가 다시 사들이는 계약\nquantity discount: 대량 구매시 할인\n\n\n\n\n\n불확실성\n\n위험과 불확실성에 노출되어 있음\n채찍효과: 공급사슬의 상류로 갈수록 수요 변동 폭이 확대되는 현상",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html",
    "title": "개인 발표 - 탄소배출권: 측정 기술",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출권: 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm의-특징",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm의-특징",
    "title": "BPM 개요",
    "section": "BPM의 특징",
    "text": "BPM의 특징\n\n어플리케이션 로직에서 프로세스 로직을 분리하여 명시적으로 관리한다.\n\nagility와 visibility 제공",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm-system의-개념",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm-system의-개념",
    "title": "BPM 개요",
    "section": "BPM System의 개념",
    "text": "BPM System의 개념\n\n프로세스를 지속적으로 혁신시키고 실행시키는 것을 가능하게 하는 범용 소프트웨어 시스템\n\nBPM suite로 발전: BPM을 위한 모든 솔루션을 통합 제공",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm의-도입효과",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpm의-도입효과",
    "title": "BPM 개요",
    "section": "BPM의 도입효과",
    "text": "BPM의 도입효과\n\n프로세스 가시화를 통한 관리 능력의 향상\n자동화를 통한 프로세스 운영 효율성 및 성능 향상\n프로세스의 병렬처리 가능\naccountability 향상\n\n프로세스의 각 단계에서 실행 결과와 진행상태를 모두 기록 -&gt; 감사와 통제에 활용\n\n프로세스 모니터링 및 데이터 분석을 통한 최적화\n고객과 파트너와의 통합\n\n공동 목표를 대상으로 가상기업(VE)화 하여 생산성을 높임\n\n조직의 agility 제고\n\nRTE(Real Time Enterprise) 구현",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpr",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpr",
    "title": "BPM 개요",
    "section": "BPR",
    "text": "BPR\n\nFundamental radical dramatic improvements process redesign\n\n\n추진방향\n\nreduction input\ninnovation process\nvalue up output\n\n\n\n추진 원칙\n\n정보는 최초 발생지에서 1회 입력\n병렬 처리\n정보 생성 부서에서 바로 처리\n업무 단위가 아닌 결과 중심 설계\n\n\n\n방법론\n\n프로세스 선정\n\n대표적 증상 (data redundancy, bottleneck)을 제시하여 개선 대상 process 발굴\n\nAs-Is 이해\n\n현재 프로세스의 목적과 흐름을 고객 관점에서 분석\nwhat to do 정의\n\nTo-Be 설계\n\n프로세스 재설계 원칙을 적용\n새로운 프로세스 모델 설계\n\n변화 관리\n\n현 위치 파악 및 위기 의식 공유\nvision 제시, 구성원 참여 유도\n\n\n\n\nRPA 성공을 위한 요인\n\n경영진의 지원과 참여\n비전 및 명확한 목표 설정\n정보기술의 활용\n변화 관리\n\n\n\n\nBPR vs BPM",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpr-vs-bpm",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpr-vs-bpm",
    "title": "BPM 개요",
    "section": "BPR vs BPM",
    "text": "BPR vs BPM\n\n\n\nBPR vs BPM",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/03.html",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/03.html",
    "title": "으악",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "으악"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/02.html#수요관리-프로세스",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/02.html#수요관리-프로세스",
    "title": "수요 관리",
    "section": "수요관리 프로세스",
    "text": "수요관리 프로세스\n\n수요관리\n\n고객 요구사항과 주어진 공급사슬 능력 간의 조화를 이루기 위한 공급사슬관리 프로세스\n예상 가능한 수요에 대해 대체 + 예상치 못한 수요에 대한 반응성 향상\n\n수요예측의 정확도 향상, 변동성 완화, 운영 유연성 향상\n\n판매, 영업, 마케팅 부서, 제품 기획 부서 모든 고려가 필요\nS&OP (Sales and Operations Planning)\n\n수요와 공급의 균형을 맞추기 위한 통합적 계획 수립\n월단위로 책임자들이 만나서 계획 조정 및 수립\n\n\n\n\n수요관리 프로세스\n\n수요 계획\n\n주기적으로 재계획\n\n수요 공유\n\n회사 내 혹은 공급사슬 내 기업 간 공유\n적시성이 중요\n\nInfluencing Demand\n\n수요를 늘리거나 줄이기 위한 활동\nPDCA 사이클\n\nManaging and Prioritizing Demand\n\n예측 수요보다는 주문된 수요를 주로 관리",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "수요 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/02.html#수요예측-기법",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/02.html#수요예측-기법",
    "title": "수요 관리",
    "section": "수요예측 기법",
    "text": "수요예측 기법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "수요 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/00.html",
    "href": "posts/01_projects/bs_3_2/notes/cte/00.html",
    "title": "Introduction",
    "section": "",
    "text": "I’m self-motivated. During my time as a Soongsil University student, I thought that studying only my major would be insufficient for my future career. so I decided to develop my skills through extracurricular activities. I joined a programming camp, and I participated in various projects. Over one and half year, i have learned not only technical skills but also communication skills and teamwork skills. Now, I’m constantly looking for new challenges and opportunities.\nI’m diligent. I believe the success of a project comes from every member’s participation So I always try to be punctual and reliable. In my last team project, we had stand-up meetings every day to ensure everyone was aligned on our goals. We voluntarily participated in projects every days and the increased communication time helped us identify potential issues early and address them promptly. As a result, we completed the project ahead of schedule and secured 2nd place in the competition. from this experience, I learned that diligence is essential for achieving success and i keep this attitude in all my work.\nI aspire to be a man who helps people communicate more effectively. with my technical skills, I can build intuitive automated process and this will ensure the people get acces to information more transparently and reduce potential errors.\n\n\n\n 맨 위로",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Introduction"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#역할과-책임",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#역할과-책임",
    "title": "TFC 게임 개요",
    "section": "역할과 책임",
    "text": "역할과 책임\n\n판매 담당 인원: 고객과 협상 및 매출 증대 책임\nSCM 담당 인원: 재고 수준 관리 - 원재료 및 완제품\n\nWIP은 거의 없다고 본다.\n\n\n\n구매 담당\n\n공급업체 선정, 원자재 구매, 조달 관리 책임\n업체 발굴, 저렴하고 좋은 품질의 원자재 조달, 가용성에 문제가 없게 관리\n1, 2 라운드에서는 업체를 바꿀 수 없다.\n의사결정: 계약현황, 공급업체 시장, 핵심성과지표, …\n\n - 계약지수(구매 계약 지수): 1보다 낮으면 낮을수록 정상가보다 더 저렴하게 구매하는 것 - 구매액: 정상가 * 계약지수 - 품질: 좋음, 중간, 나쁨으로 선택 가능. 나쁨으로 하면 계약지수가 낮아지지만, 불량률이 높아져서 손해가 발생할 수 있다. - 리드타임: 공급업체가 원자재를 납품하는 데 걸리는 시간. 우리가 바꿀 수 없음. 계약할 때 가급적 짧은 리드타임을 선택할 시 운송비 절약 가능 - 인증: 우리가 인증하는거. - 운송수단: 트럭, 보트 (해상 육상 차이) - 여유 캐파: 우리, 다른 업체 다 하고도 남는 여유 - 지불 조건: 높게하면 계약지수가 높아지지만, 현금 흐름에 도움이 된다. - 거래단위: 파래트, 파래트 층, 풀트럭(30개 파레트 적재 가능)\n\n\n생산 운영\n\n창고 및 생산능력 관리\n혼합, bottling 두 공정 관리",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "TFC 게임 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#interface",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#interface",
    "title": "TFC 게임 개요",
    "section": "Interface",
    "text": "Interface\n\n왼쪽 메뉴\n\n - 위 4개의 부서에 들어가서 조회는 가능하나, 수정은 책임자만 가능하다. - 계산?\n - 4명 모두에게 올 수도, 한 명에게 올 수도 있다.\n - 회사의 제품에는 푸레시 오렌지, 푸레시 오렌지/망고, 푸레시 오렌지/C-파워 각각에 대한 1 리터, PET 제품이 총 6개 있다. - BOM을 다운 받을 수 있다.\n - 생산가격 최적화 도구?\n\n\n상단 메뉴\n - 처음부터 모든 항목이 활성화 되어 있지 않다.\n\n\n\n구매 (이전 라운드)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "TFC 게임 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#해야-하는-것",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#해야-하는-것",
    "title": "TFC 게임 개요",
    "section": "해야 하는 것",
    "text": "해야 하는 것\n\nThe mission과 The Company 읽기\n\n\n의사결정이 마무리되면, 6개월간 시뮬레이션이 진행되고, ROI를 확인할 수 있다.\n본사는 네덜란드에 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "TFC 게임 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/03.html#sop-프로세스",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/03.html#sop-프로세스",
    "title": "S&OP: Sales and Operations Planning",
    "section": "S&OP 프로세스",
    "text": "S&OP 프로세스\n\n수요 공급 균형",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "S&OP: Sales and Operations Planning"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/기공수/00.html#급수",
    "href": "posts/01_projects/bs_3_2/notes/기공수/00.html#급수",
    "title": "무한 급수",
    "section": "급수",
    "text": "급수\n\n유한급수: 유한한 항의 합\n무한급수: 무한한 항의 합\n\n부분합: n번째 항까지의 합\n\n\n\n부분합을 구하기 쉬운 급수\n\n기하급수\n\n|r| &lt; 1일때 \\(\\frac{a}{1-r}\\)로 수렴\n\np 급수: \\(\\Sigma \\frac{1}{n^p}\\)\n\np &gt; 1이면 수렴, p &lt;= 1이면 발산",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "무한 급수"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/기공수/00.html#급수의-수렴에-관한-정리",
    "href": "posts/01_projects/bs_3_2/notes/기공수/00.html#급수의-수렴에-관한-정리",
    "title": "무한 급수",
    "section": "급수의 수렴에 관한 정리",
    "text": "급수의 수렴에 관한 정리\n\n급수의 합과 scalar 곱은 급수가 수렴할 때만 정의된다.\n부분합을 구하지 않고 수렴 발산을 판별해보자\n\n\n\\(\\Sigma a_n\\)이 수렴하면 \\(\\limit_{n \\to \\infty} a_n = 0\\)이다.\n\n대우 성립\n역은 성립 x (예: 조화급수)\n\n양항급수 \\(\\Sigma a_n\\)의 부분합 \\(S_n\\)이 모든 n에 대하여 어떤 상수 k보다 작으면 이 급수는 수렴하며, 그 합은 k보다 크지 않다.\n적분 판정법\n\n\\(f(n) = a_n\\)가 [1, \\(\\infty\\))에서 양수이고 감소하는 함수라고 하자. 그러면 \\(\\Sigma_{n=1}^{\\infty} f(n)\\)가 수렴이면 \\(\\int_{1}^{\\infty} f(x) dx\\)는 수렴, 발산이면 발산한다.\n\n꼭 1부터 시작 안해도 된다.\n\n\n비교 판정법\n\n양항급수 \\(\\Sigma a_n\\), \\(\\Sigma b_n\\)에 대하여 모든 n에 대하여 \\(0 \\leq a_n \\leq b_n\\)이라고 하자.\n\n\\(\\Sigma b_n\\)이 수렴하면 \\(\\Sigma a_n\\)도 수렴한다.\n\\(\\Sigma a_n\\)이 발산하면 \\(\\Sigma b_n\\)도 발산한다.\n\n\n비판정법\n\n양항급수 \\(\\Sigma a_n\\)에서 \\(\\limit_{n \\to \\infty} \\frac{a_{n+1}}{a_n} = L\\)이라고 하자.\n\nL &lt; 1이면 급수는 수렴한다.\nL &gt; 1이면 급수는 발산한다.\nL = 1이면 판정 불가\n\n\n근판정법\n\n양항급수 \\(\\Sigma a_n\\)에서 \\(\\limit_{n \\to \\infty} \\sqrt[n]{a_n} = L\\)이라고 하자.\n\nL &lt; 1이면 급수는 수렴한다.\nL &gt; 1이면 급수는 발산한다.\nL = 1이면 판정 불가\n\n\n교대급수 판정법",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "무한 급수"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#표준의-개념-및-필요성",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#표준의-개념-및-필요성",
    "title": "BMP 표준 개요",
    "section": "표준의 개념 및 필요성",
    "text": "표준의 개념 및 필요성\n\n적용범위\n\n국제표준\n지역표준\n국가표준\n\n\n\n제정 기구\n\nDe Jure 표준: 공공기관에 의해 승인된 강제력 있는 표준\nDe Facto 표준: 시장에서 자연스럽게 채택된 표준\n\n\n\n목적\n\n상호운용성 및 사용의 편리성",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BMP 표준 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#정보통신-산업의-특징과-표준",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#정보통신-산업의-특징과-표준",
    "title": "BMP 표준 개요",
    "section": "정보통신 산업의 특징과 표준",
    "text": "정보통신 산업의 특징과 표준\n\n경제학적 특성\n\nNetwork Externality: 소비자군의 규모에 따라 가치가 달라짐\n\nDirect Network Effect\nCross Network Effect\nMetcalfe의 법칙: 지수함수적 가치 증가\n\nLock-in effect: 한번 표준을 채택하면 전환하기 어려움\nTipping effect: 비슷한 시장 점유율을 보이다가 어느 순간 급격히 한쪽으로 쏠림",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BMP 표준 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#bpm-표준의-필요성-및-유형",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#bpm-표준의-필요성-및-유형",
    "title": "BMP 표준 개요",
    "section": "BPM 표준의 필요성 및 유형",
    "text": "BPM 표준의 필요성 및 유형",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BMP 표준 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#bpmn-개요",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/04.html#bpmn-개요",
    "title": "BMP 표준 개요",
    "section": "BPMN 개요",
    "text": "BPMN 개요",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BMP 표준 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html",
    "title": "Question",
    "section": "",
    "text": "Suggest which of my sentences are confusing or hard to follow",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-issues",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-issues",
    "title": "Question",
    "section": "Writing A Issues:",
    "text": "Writing A Issues:\n\n“Last year, I joined in extracurricular education aboaut programming.”\n\n“joined in” should be “joined” (remove “in”)\n“aboaut” is misspelled (should be “about”)\nBetter: “Last year, I joined an extracurricular programming education program.”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-issues",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-issues",
    "title": "Question",
    "section": "Writing B Issues:",
    "text": "Writing B Issues:\n\n“I’m self-motivated. During my time as a Soongsil University student, I thought that studying only my major would be insufficient for any professional projects.”\n\nThe connection between these sentences is unclear\n“any professional projects” is vague\nBetter: “I’m self-motivated. As a Soongsil University student, I realized that studying only my major would be insufficient for future professional work.”\n\n“so I decided to develop my skills through extracurricular activities.”\n\nShould start with capital “S” as it begins a new sentence\nBetter: “So I decided to develop my skills through extracurricular activities.”\n\n“And I recieved top learner award for my performance in the camp.”\n\n“recieved” is misspelled (should be “received”)\nMissing article “the” before “top learner award”\nBetter: “I received the top learner award for my performance in the camp.”\n\n“Over one and half years, I had learned technical skills and communication skills.”\n\n“one and half” should be “one and a half”\nTense inconsistency (“had learned” vs. present context)\nBetter: “Over one and a half years, I learned both technical and communication skills.”\n\n“We voluntarily participated did them and the increased communication time helped us identify potential issues early and address them promptly.”\n\nThis sentence is very confusing with grammatical errors\n“participated did them” doesn’t make sense\nBetter: “We voluntarily participated in these meetings, and the increased communication time helped us identify potential issues early and address them promptly.”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-issues",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-issues",
    "title": "Question",
    "section": "Writing C Issues:",
    "text": "Writing C Issues:\n\n“With my technical skills, I can build intuitive automated process and this will ensure the people get access to information with more transparently and reduce potential errors.”\n\n“automated process” should be “automated processes” (plural) or “an automated process”\n“the people” should be “people”\n“with more transparently” is grammatically incorrect\nRun-on sentence that’s hard to follow\nBetter: “With my technical skills, I can build intuitive automated processes that will help people access information more transparently and reduce potential errors.”\n\n“Ultimately, I aim to become an engineer who can enable organization to achieve its maximum productivity with sophisticated system.”\n\n“organization” should be “organizations” (plural) or “an organization”\n“sophisticated system” should be “sophisticated systems” or “a sophisticated system”\nBetter: “Ultimately, I aim to become an engineer who can help organizations achieve maximum productivity through sophisticated systems.”\n\n\nThe main issues are: spelling errors, missing articles (a, an, the), grammatical inconsistencies, and some awkward sentence structures that make the meaning unclear.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a",
    "title": "Question",
    "section": "Writing A",
    "text": "Writing A\nYour first paragraph is already clear and simple - no changes needed!",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b",
    "title": "Question",
    "section": "Writing B",
    "text": "Writing B\nUnnecessary/Complicated words to replace:\n\n“insufficient” → “not enough”\n\nSimpler and more direct\n\n“extracurricular activities” → “activities outside class” or “extra activities”\n\nLess formal, easier to understand\n\n“voluntarily participated” → “joined willingly” or simply “participated”\n\n“Voluntarily” is redundant since participation implies choice\n\n“address them promptly” → “solve them quickly”\n\nMore straightforward language\n\n“From this experience, I learned that” → “This taught me that”\n\nMore concise",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c",
    "title": "Question",
    "section": "Writing C",
    "text": "Writing C\nVague/Complicated words to replace:\n\n“aspire to be a man” → “want to be someone”\n\nMore natural phrasing\n\n“communicate more effectively” → “communicate better”\n\nSimpler\n\n“intuitive automated processes” → “easy-to-use automated systems”\n\n“Intuitive” is vague; be more specific\n\n“access information more transparently” → “get information easily”\n\n“Transparently” is unclear in this context\n\n“sophisticated systems” → “advanced systems” or “well-designed systems”\n\nMore specific than “sophisticated”\n\n“achieve maximum productivity” → “work more efficiently”\n\nLess business jargon, more natural\n\n\nOverall tone suggestion: Your writing is generally good, but try to use more conversational, direct language instead of formal business terms. This will make your writing more engaging and easier to understand.\n✓ Task Completed\nI’ve completed the analysis of your English writing and provided specific suggestions for simplifying unnecessary, complicated, or vague words. The main recommendations include:\nWriting B improvements: - “insufficient” → “not enough” - “extracurricular activities” → “extra activities” - “voluntarily participated” → “participated” - “address them promptly” → “solve them quickly” - “From this experience, I learned that” → “This taught me that”\nWriting C improvements: - “aspire to be a man” → “want to be someone” - “communicate more effectively” → “communicate better” - “intuitive automated processes” → “easy-to-use automated systems” - “access information more transparently” → “get information easily” - “sophisticated systems” → “advanced systems” - “achieve maximum productivity” → “work more efficiently”\nYour Writing A was already clear and simple. The overall suggestion is to use more conversational, direct language instead of formal business terms to make your writing more engaging and accessible.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-1",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-1",
    "title": "Question",
    "section": "Writing A",
    "text": "Writing A\nAdd these specific details:\n\nWhat sparked your interest in programming?\n\nExample: “After struggling with repetitive data analysis in my Industrial Engineering coursework, I realized programming could automate these tasks.”\n\nName the specific program or organization\n\nInstead of “extracurricular programming education program,” mention the actual name or type (e.g., “42Seoul,” “Samsung Software Academy,” or “a 6-month Python bootcamp”)\n\nConnect your major to your programming interest\n\nExample: “I chose Industrial Engineering because I’m fascinated by optimizing systems, which naturally led me to discover how coding can solve efficiency problems.”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-1",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-1",
    "title": "Question",
    "section": "Writing B",
    "text": "Writing B\nAdd these concrete examples:\n\nSpecify what you built in the programming camp\n\nExample: “I developed a web application that helps students find study groups” or “I created an inventory management system using Python and SQL”\n\nGive details about the competition\n\nExample: “In a hackathon focused on solving campus problems” or “In a project showcase where 20 teams presented their solutions to local businesses”\n\nDescribe a specific challenge you overcame\n\nExample: “When our database crashed two days before the deadline, I spent the night learning backup recovery and managed to restore all our data”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-1",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-1",
    "title": "Question",
    "section": "Writing C",
    "text": "Writing C\nAdd these specific examples:\n\nGive a concrete example of the system you want to build\n\nExample: “For instance, I want to create chatbots that help elderly people easily access government services” or “I’d like to build scheduling systems that reduce wait times in hospitals”\n\nMention a specific industry or type of organization\n\nExample: “I’m particularly interested in helping small businesses streamline their operations” or “I want to work with healthcare organizations to improve patient care”\n\nReference a real problem you’ve observed\n\nExample: “I noticed how much time my part-time job colleagues waste on manual paperwork, and I want to solve problems like this on a larger scale”\n\n\nThese details will make your writing more memorable and help readers better understand your motivations and goals.\n✓ Task Completed\nI’ve completed the analysis and provided specific suggestions for adding interesting details to your writing. Here’s what I recommended for each section:\nWriting A: Add details about what sparked your programming interest, name the specific program you joined, and connect your Industrial Engineering major to your programming passion.\nWriting B: Include specifics about what you built in the programming camp, give details about the competition you won 2nd place in, and describe a specific challenge you overcame during your projects.\nWriting C: Provide concrete examples of systems you want to build, mention specific industries or organizations you want to help, and reference real problems you’ve observed that motivate your career goals.\nThese additions will make your writing more engaging, memorable, and help readers better understand your motivations and experiences.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#current-weak-adjectives-stronger-alternatives",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#current-weak-adjectives-stronger-alternatives",
    "title": "Question",
    "section": "Current Weak Adjectives → Stronger Alternatives:",
    "text": "Current Weak Adjectives → Stronger Alternatives:\n\nWriting A:\n\n“extracurricular education” → “intensive programming bootcamp” or “immersive coding program”\n\nMore specific and dynamic than generic “extracurricular”\n\n\n\n\nWriting B:\n\n“various projects” → “diverse projects” or “challenging projects”\n\n“Various” is bland; these alternatives show range or difficulty\n\n“technical skills” → “hands-on programming skills” or “practical coding skills”\n\nMore specific than generic “technical”\n\n“communication skills” → “collaborative teamwork skills” or “clear presentation skills”\n\nMore precise than vague “communication”\n\n“potential issues” → “critical bugs” or “major roadblocks”\n\nMuch more specific and urgent than “potential”\n\n“increased communication” → “frequent collaboration” or “constant coordination”\n\nMore dynamic and active\n\n\n\n\nWriting C:\n\n“intuitive automated process” → “seamless automated workflows” or “user-friendly automated systems”\n\n“Seamless” and “user-friendly” are more vivid than “intuitive”\n\n“sophisticated system” → “cutting-edge platforms” or “streamlined solutions”\n\nMore modern and specific than generic “sophisticated”\n\n“maximum productivity” → “peak performance” or “exceptional efficiency”\n\nMore energetic and specific",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#additional-adjective-opportunities",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#additional-adjective-opportunities",
    "title": "Question",
    "section": "Additional Adjective Opportunities:",
    "text": "Additional Adjective Opportunities:\nWhere you could ADD strong adjectives:\n\n“I’m self-motivated” → “I’m highly self-motivated” or “I’m incredibly driven”\n“I’m diligent” → “I’m exceptionally reliable” or “I’m consistently dedicated”\n“programming camp” → “intensive programming camp” or “rigorous coding bootcamp”\n“team project” → “collaborative final project” or “ambitious group project”\n“competition” → “prestigious competition” or “competitive hackathon”\n\nImpact: These stronger adjectives make your writing more engaging by being specific rather than generic, active rather than passive, and memorable rather than forgettable. They help paint a clearer picture of your experiences and abilities.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-3",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-a-3",
    "title": "Question",
    "section": "Writing A:",
    "text": "Writing A:\n\nBetween sentences 2-3: “My university is Soongsil university, and I’m majoring in Industrial Engineering. Additionally, last year, I joined an extracurricular programming education program.”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-3",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-b-3",
    "title": "Question",
    "section": "Writing B:",
    "text": "Writing B:\n\nAfter “not enough for future professional work”: “So I decided to develop my skills through extra activities. Therefore, I joined a programming camp, and I participated in various projects.”\nBefore receiving the award: “I participated in various projects. As a result, I received the top learner award for my performance in the camp.”\nBetween learning skills and looking for challenges: “Over one and a half years, I learned both technical and communication skills. Because of this experience, I’m constantly looking for another challenges and opportunities.”\nBefore the team meeting example: “So I always try to be punctual and reliable. For example, in my last team project, our team had meetings every day.”\nAfter describing the meetings: “We joined willingly in these meetings, and the increased communication time helped us identify potential issues early and solve them quickly. As a result, we completed the project ahead of schedule and won 2nd place in the competition.”\nBefore the final lesson: “As a result, we completed the project ahead of schedule and won 2nd place in the competition. This experience taught me that diligence is essential for achieving success and I keep this attitude in all my work.”",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-3",
    "href": "posts/01_projects/bs_3_2/notes/cte/01.html#writing-c-3",
    "title": "Question",
    "section": "Writing C:",
    "text": "Writing C:\n\nBetween the two paragraphs: “With my technical skills, I can build easy-to-use automated systems that will help people get information easily and reduce potential errors. Building on this foundation, I aim to become an engineer who can help organizations work more efficiently through advanced systems.”\n\nThese linking phrases will help create smoother transitions between ideas, clarify cause-and-effect relationships, and make the writing more cohesive and professional.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Question"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-a.",
    "href": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-a.",
    "title": "Before",
    "section": "Writing A.",
    "text": "Writing A.\nHi, my name is HyungHoon Kim and I’m living in Yongin. My university is Soongsil university, and I’m majoring in Industrial Engineering. Last year, I joined in extracurricular education aboaut programming.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Before"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-b.",
    "href": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-b.",
    "title": "Before",
    "section": "Writing B.",
    "text": "Writing B.\nI’m self-motivated. During my time as a Soongsil University student, I thought that studying only my major would be insufficient for any professional projects. so I decided to develop my skills through extracurricular activities. I joined a programming camp, and I participated in various projects. And I recieved top learner award for my performance in the camp. Over one and half years, I had learned technical skills and communication skills. Now, for my growth, I’m constantly looking for another challenges and opportunities.\nI’m diligent. I believe the success of projects comes from every member’s participation. So I always try to be punctual and reliable. In my last team project, our team had meetings every day. We voluntarily participated did them and the increased communication time helped us identify potential issues early and address them promptly. As a result, we completed the project ahead of schedule and won 2nd place in the competition. From this experience, I learned that diligence is essential for achieving success and I keep this attitude in all my work.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Before"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-c.",
    "href": "posts/01_projects/bs_3_2/notes/cte/00.html#writing-c.",
    "title": "Before",
    "section": "Writing C.",
    "text": "Writing C.\nI aspire to be a man who helps people communicate more effectively. With my technical skills, I can build intuitive automated process and this will ensure the people get access to information with more transparently and reduce potential errors. Ultimately, I aim to become an engineer who can enable organization to achieve its maximum productivity with sophisticated system.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Before"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#question",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#question",
    "title": "After",
    "section": "Question",
    "text": "Question\nSuggest which of my sentences are confusing or hard to follow",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#answer",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#answer",
    "title": "After",
    "section": "Answer",
    "text": "Answer\n🤔 Thoughts\n\nThe user is asking me to review the English writing in the selected file and identify sentences that are confusing or hard to follow. I need to analyze the three writing sections (A, B, and C) and provide feedback on clarity, grammar, and readability issues.\n\nHere are the sentences that are confusing or hard to follow in your writing:\n\nWriting A Issues:\n\n“Last year, I joined in extracurricular education aboaut programming.”\n\n“joined in” should be “joined” (remove “in”)\n“aboaut” is misspelled (should be “about”)\nBetter: “Last year, I joined an extracurricular programming education program.”\n\n\n\n\nWriting B Issues:\n\n“I’m self-motivated. During my time as a Soongsil University student, I thought that studying only my major would be insufficient for any professional projects.”\n\nThe connection between these sentences is unclear\n“any professional projects” is vague\nBetter: “I’m self-motivated. As a Soongsil University student, I realized that studying only my major would be insufficient for future professional work.”\n\n“so I decided to develop my skills through extracurricular activities.”\n\nShould start with capital “S” as it begins a new sentence\nBetter: “So I decided to develop my skills through extracurricular activities.”\n\n“And I recieved top learner award for my performance in the camp.”\n\n“recieved” is misspelled (should be “received”)\nMissing article “the” before “top learner award”\nBetter: “I received the top learner award for my performance in the camp.”\n\n“Over one and half years, I had learned technical skills and communication skills.”\n\n“one and half” should be “one and a half”\nTense inconsistency (“had learned” vs. present context)\nBetter: “Over one and a half years, I learned both technical and communication skills.”\n\n“We voluntarily participated did them and the increased communication time helped us identify potential issues early and address them promptly.”\n\nThis sentence is very confusing with grammatical errors\n“participated did them” doesn’t make sense\nBetter: “We voluntarily participated in these meetings, and the increased communication time helped us identify potential issues early and address them promptly.”\n\n\n\n\nWriting C Issues:\n\n“With my technical skills, I can build intuitive automated process and this will ensure the people get access to information with more transparently and reduce potential errors.”\n\n“automated process” should be “automated processes” (plural) or “an automated process”\n“the people” should be “people”\n“with more transparently” is grammatically incorrect\nRun-on sentence that’s hard to follow\nBetter: “With my technical skills, I can build intuitive automated processes that will help people access information more transparently and reduce potential errors.”\n\n“Ultimately, I aim to become an engineer who can enable organization to achieve its maximum productivity with sophisticated system.”\n\n“organization” should be “organizations” (plural) or “an organization”\n“sophisticated system” should be “sophisticated systems” or “a sophisticated system”\nBetter: “Ultimately, I aim to become an engineer who can help organizations achieve maximum productivity through sophisticated systems.”\n\n\nThe main issues are: spelling errors, missing articles (a, an, the), grammatical inconsistencies, and some awkward sentence structures that make the meaning unclear.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#question-1",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#question-1",
    "title": "After",
    "section": "Question",
    "text": "Question\nSuggest 2–3 extra details I could add to make my writing more interesting.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#answer-1",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#answer-1",
    "title": "After",
    "section": "Answer",
    "text": "Answer\n🤔 Thoughts\n\nThe user wants me to suggest 2-3 extra details they could add to make their writing more interesting. I need to analyze each writing section and think about what specific, concrete details would make the content more engaging and memorable. Let me think about each section:\nWriting A: This is very basic biographical information. It could use more specific details about their interests, what drew them to Industrial Engineering, or what specific programming education program they joined.\nWriting B: This talks about being self-motivated and diligent, but lacks specific examples and concrete details. It mentions a programming camp and team project but could be more specific about what they built, what technologies they used, what the competition was about, etc.\nWriting C: This talks about career goals but is quite vague. It could use more specific examples of what kinds of systems they want to build, what organizations they want to help, or what specific problems they want to solve.\nI should provide concrete, specific suggestions that would make each section more vivid and memorable.\n\nHere are my suggestions for 2-3 extra details you could add to make each section more interesting:\n\nWriting A\nAdd these specific details:\n\nWhat sparked your interest in programming?\n\nExample: “After struggling with repetitive data analysis in my Industrial Engineering coursework, I realized programming could automate these tasks.”\n\nName the specific program or organization\n\nInstead of “extracurricular programming education program,” mention the actual name or type (e.g., “42Seoul,” “Samsung Software Academy,” or “a 6-month Python bootcamp”)\n\nConnect your major to your programming interest\n\nExample: “I chose Industrial Engineering because I’m fascinated by optimizing systems, which naturally led me to discover how coding can solve efficiency problems.”\n\n\n\n\nWriting B\nAdd these concrete examples:\n\nSpecify what you built in the programming camp\n\nExample: “I developed a web application that helps students find study groups” or “I created an inventory management system using Python and SQL”\n\nGive details about the competition\n\nExample: “In a hackathon focused on solving campus problems” or “In a project showcase where 20 teams presented their solutions to local businesses”\n\nDescribe a specific challenge you overcame\n\nExample: “When our database crashed two days before the deadline, I spent the night learning backup recovery and managed to restore all our data”\n\n\n\n\nWriting C\nAdd these specific examples:\n\nGive a concrete example of the system you want to build\n\nExample: “For instance, I want to create chatbots that help elderly people easily access government services” or “I’d like to build scheduling systems that reduce wait times in hospitals”\n\nMention a specific industry or type of organization\n\nExample: “I’m particularly interested in helping small businesses streamline their operations” or “I want to work with healthcare organizations to improve patient care”\n\nReference a real problem you’ve observed\n\nExample: “I noticed how much time my part-time job colleagues waste on manual paperwork, and I want to solve problems like this on a larger scale”\n\n\nThese details will make your writing more memorable and help readers better understand your motivations and goals.\n✓ Task Completed\nI’ve completed the analysis and provided specific suggestions for adding interesting details to your writing. Here’s what I recommended for each section:\nWriting A: Add details about what sparked your programming interest, name the specific program you joined, and connect your Industrial Engineering major to your programming passion.\nWriting B: Include specifics about what you built in the programming camp, give details about the competition you won 2nd place in, and describe a specific challenge you overcame during your projects.\nWriting C: Provide concrete examples of systems you want to build, mention specific industries or organizations you want to help, and reference real problems you’ve observed that motivate your career goals.\nThese additions will make your writing more engaging, memorable, and help readers better understand your motivations and experiences.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-a.",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-a.",
    "title": "After",
    "section": "Writing A.",
    "text": "Writing A.\nHi, my name is HyungHoon Kim and I’m living in Yongin. My university is Soongsil university, and I’m majoring in Industrial Engineering. Additionally, last year, I joined an extracurricular programming education program.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-b.",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-b.",
    "title": "After",
    "section": "Writing B.",
    "text": "Writing B.\nI’m self-motivated. As a Soongsil University student, I realized that studying only my major would be not enough for future professional work. So I decided to develop my skills through extra activities. I joined a programming camp, and I participated in various projects. As a result, I received the top learner award for my performance in the camp. Over one and a half years, I learned both technical and communication skills. Now, for my growth, I’m constantly looking for another challenges and opportunities.\nI’m diligent. I believe the success of projects comes from every member’s participation. So I always try to be punctual and reliable. In my last team project, our team had meetings every day. We joined willingly in these meetings, and the increased communication time helped us identify potential issues early and solve them quickly. As a result, we completed the project ahead of schedule and won 2nd place in the competition. This experience taught me that diligence is essential for achieving success and I keep this attitude in all my work.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-c.",
    "href": "posts/01_projects/bs_3_2/notes/cte/02.html#writing-c.",
    "title": "After",
    "section": "Writing C.",
    "text": "Writing C.\nI want to be a person who helps people communicate better. With my technical skills, I can build easy-to-use automated systems that will help people get information easily and reduce potential errors. Building on this foundation, I aim to become an engineer who can help organizations work more efficiently through advanced systems.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "After"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#정의",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#정의",
    "title": "개인 발표 - 탄소배출 측정 기술",
    "section": "정의",
    "text": "정의\n\n탄소 배출권은 일정량의 이산화탄소 등 온실가스를 배출할 수 있는 권리를 뜻하며, 이는 정부가 기업에 배출량을 할당하고 남거나 부족한 배출권은 시장에서 거래할 수 있도록 한 배출권거래제(ETS)의 핵심 요소입니다.\n기업은 할당량보다 적게 배출하면 남은 배출권을 팔아 수익을 얻을 수 있고, 할당량을 초과하면 다른 기업으로부터 배출권을 구매하여 규제를 충족해야 하므로, 이를 통해 기업의 자발적인 온실가스 감축을 유도하고 기후변화에 대응하는 제도입니다.\n탄소배출권은 GWP(Global Warming Potential, 지구온난화지수)에 따라 CO2 환산톤(tCO2eq) 단위로 거래됩니다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#배출권거래제ets-운영",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#배출권거래제ets-운영",
    "title": "개인 발표 - 탄소배출 측정 기술",
    "section": "배출권거래제(ETS) 운영",
    "text": "배출권거래제(ETS) 운영\n\n정부가 기업에 총배출허용량(cap)을 설정하여 배출권을 할당하고, 기업들이 시장에서 배출권을 거래할 수 있도록 하는 제도입니다.\n기후변화 대응 수단: 지구온난화를 막기 위해 국제적으로 합의된 기후변화협약의 목표를 달성하기 위한 효과적인 방법 중 하나입니다.\n계획기간 안에서 이월과 차입이 가능하다.\n\n계획기간이 지나면 남은 배출권은 일정량만 이월할 수 있다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#배출권-할당",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#배출권-할당",
    "title": "개인 발표 - 탄소배출 측정 기술",
    "section": "배출권 할당",
    "text": "배출권 할당\n\n무상할당\n\nGFI(Grandfathering): 과거 배출량을 기준으로 할당\nBM(Benchmarking): 산업별 배출량 효율성을 기준으로 할당\n\n유상할당\n\n경매방식: 기업들이 경매에 참여하여 배출권을 구매",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#기술적-이슈",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#기술적-이슈",
    "title": "개인 발표 - 탄소배출 측정 기술",
    "section": "기술적 이슈",
    "text": "기술적 이슈\n\n탄소 배출량 측정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-모델링-분석",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-모델링-분석",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 모델링 / 분석",
    "text": "프로세스 모델링 / 분석\n\nAs-Is 모델링\n\n프로세스의 논리적 흐름을 정의\n현재의 업무 프로세스를 분석하여 문제점을 도출\n\n\n\n\n\n표기법(BPMN)\n\n\n\n단계\n\n대상 프로세스의 목표와 고객 확인\n프로세스 정보 취합과 태스크 정의\n\n프로세스 수행 조직 및 참여자\n기동되는 이벤트\n다른 프로세스와의 연관관계\n관계되는 역할들\n관련 전문 용어\n\n프로세스 맵 작성\n프로세스 분석\n\n품질 향상, 리드타임 단축, 생산성 향상, 원가 절감\n\n프로세스 맵 검증 및 워크숍 등을 통한 확정",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-자동화",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-자동화",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 자동화",
    "text": "프로세스 자동화\n\n분석 설계 대상\n\n프로세스\n어플리케이션\n인터페이스",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-성과관리",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-성과관리",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 성과관리",
    "text": "프로세스 성과관리\n\n프로세스의 End-to-End 관점에서 진단/분석/대책 수립\n결과뿐만 아니라 과정에 대한 지표를 포함\n\n\n전사 성과관리 체계 수립\n성과관리 대상 프로세스의 선정\n성과관리 지표 및 성과측정 모형의 설계\n성과지표 모니터링\n지속적 성과창출을 위한 전략 수립",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#공급사슬에서-재고의-의미",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#공급사슬에서-재고의-의미",
    "title": "재고 관리",
    "section": "공급사슬에서 재고의 의미",
    "text": "공급사슬에서 재고의 의미\n\n미래 사용에 대비하여 보관중인 재화\n\n다양한 형태로 다양한 위치에 존재\n\n종류\n\n원자재 및 구매 부품\n재공품(WIP)\n완제품(FGI)\n운송중 재고(GIT)",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고-유지비용",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고-유지비용",
    "title": "재고 관리",
    "section": "재고 유지비용",
    "text": "재고 유지비용\n\n자본비용: 기회비용\n보관 및 취급 비용: 창고 임대료\n세금: 관세?\n보험료\n가치상실\n물리적 파손, 도난, 분실",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고의-필요성",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고의-필요성",
    "title": "재고 관리",
    "section": "재고의 필요성",
    "text": "재고의 필요성\n\n재고비용은 늘어나지만, 공급사실의 반응성에 영향을 준다.",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고-관리",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/05.html#재고-관리",
    "title": "재고 관리",
    "section": "재고 관리",
    "text": "재고 관리\n\n재고 비용을 낮추면서 고객의 서비스 수준을 유지하는 것이 목표\n\n\n고객 서비스 수준 지표\n\n재고 충족률(In-stock rate)\n\n고객 주문에 대해 재고로부터 즉시 출고할 수 있는 비율\n\n백오더(Backorders)\n\n재고 부족으로 인해 고객 주문을 즉시 출고하지 못한 주문량\n\n재고 회전율(Inventory Turnover)\n\n\n\n재고 비용\n\n주문 비용\n보관 비용\n부족 비용",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#주차",
    "href": "posts/01_projects/bs_3_2/notes/공급사슬관리/04.html#주차",
    "title": "TFC 게임 개요",
    "section": "1주차",
    "text": "1주차\n\nFood & Groceries\n\n푸레시 오렌지 1L\n푸레시 오렌지/C-파워 1L\n푸레시 오렌지/망고 1L\n푸레시 오렌지 PET\n푸레시 오렌지/C-파워 PET\n푸레시 오렌지/망고 PET\n\nLAND Market\n\n푸레시 오렌지 1L\n푸레시 오렌지/C-파워 1L\n푸레시 오렌지/망고 1L\n푸레시 오렌지 PET\n푸레시 오렌지/C-파워 PET\n푸레시 오렌지/망고 PET\n\nDominick’s\n\n푸레시 오렌지 PET\n푸레시 오렌지/C-파워 PET\n푸레시 오렌지/망고 PET\n\nFOOD & GROCERIES의 수요가 가장 높은데 판매단가가 낮다.\n\nLAND Market에 비해 수요가 높지만, 판매단가가 낮은게 문제.\n\n\n\n고객 우선순위\n\nDominick’s\nFOOD & Groceries\nLAND Market",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "TFC 게임 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#탄소배출-측정-기술의-혁신과-최신-동향-2-3분-발표-대사",
    "href": "posts/01_projects/bs_3_2/notes/신재생에너지/00.html#탄소배출-측정-기술의-혁신과-최신-동향-2-3분-발표-대사",
    "title": "개인 발표 - 탄소배출권: 측정 기술",
    "section": "탄소배출 측정 기술의 혁신과 최신 동향 (2-3분 발표 대사)",
    "text": "탄소배출 측정 기술의 혁신과 최신 동향 (2-3분 발표 대사)\n안녕하세요. 오늘은 탄소배출 측정 기술의 최신 동향에 대해 말씀드리겠습니다.\n\n도입 (30초)\n먼저 탄소배출권이 무엇인지 간단히 설명하겠습니다. 탄소배출권은 기업이나 국가가 배출할 수 있는 이산화탄소의 양을 권리로 만든 것입니다. 마치 “오염할 수 있는 쿠폰”이라고 생각하시면 됩니다. 문제는 이 탄소배출량을 정확히 측정하는 것이 매우 어려웠다는 점입니다.\n\n\n기존 측정 방식의 한계 (30초)\n과거에는 공장 굴뚝에 센서를 달거나, 기업이 자체적으로 보고하는 방식에 의존했습니다. 하지만 이런 방식은 부정확했고, 일부 기업들이 거짓 보고를 하는 경우도 있었습니다. 특히 숲이나 바다처럼 넓은 지역의 탄소 흡수량을 측정하기는 거의 불가능했죠.\n\n\n최신 기술 혁신 (60초)\n그런데 최근 몇 년간 놀라운 기술 발전이 일어났습니다.\n첫째, 위성 기술입니다. 인공위성을 통해 지구 전체의 이산화탄소 농도를 실시간으로 관찰할 수 있게 되었습니다. 마치 우주에서 지구의 “탄소 지도”를 그리는 것입니다.\n둘째, AI와 빅데이터 기술입니다. 인공지능이 공장, 교통, 건물 등 다양한 데이터를 분석해서 탄소배출량을 정확히 예측할 수 있습니다.\n셋째, 디지털 트윈 기술입니다. 실제 공장이나 도시를 컴퓨터 안에 그대로 재현해서, 탄소배출량을 시뮬레이션으로 정확히 계산할 수 있게 되었습니다.\n넷째, IoT 센서망입니다. 도시 곳곳에 설치된 수많은 센서들이 실시간으로 대기 중 이산화탄소를 측정합니다.\n\n\n현재의 이슈와 전망 (30초)\n이러한 기술 발전으로 탄소배출 측정이 훨씬 정확해지고 있지만, 여전히 과제가 있습니다. 국가별로 측정 기준이 다르고, 기술 격차가 존재합니다. 하지만 2025년 현재, 전 세계적으로 표준화된 디지털 측정 시스템을 구축하려는 노력이 활발히 진행되고 있습니다.\n\n\n결론 (30초)\n결국 정확한 측정 기술이 발전할수록, 탄소배출권 거래가 더 공정해지고, 실질적인 온실가스 감축에 기여할 수 있을 것입니다. 기술의 힘으로 기후변화 대응이 한 단계 업그레이드되고 있는 셈입니다.\n감사합니다.\n\n발표 팁: - 전체 분량: 약 2분 30초 - 각 섹션별 시간 배분을 지켜주세요 - 어려운 기술 용어 사용 시 쉬운 비유로 설명 - 손짓이나 시각 자료 활용하면 더 효과적",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 탄소배출권: 측정 기술"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질과-전략",
    "href": "posts/01_projects/bs_3_2/notes/품질경영/01.html#품질과-전략",
    "title": "품질관리의 기본개념",
    "section": "품질과 전략",
    "text": "품질과 전략\n\n품질의 향상은 판매 이익 증가와 비용 절감에 기여함.\n좋은 품질 향상을 위해 전사적인 품질 경영(TQM)의 달성이 필요.\n\n\n\n\nTQM을 달성하기 위한 흐름\n\n\n\nTQM의 3요소\n\n고객 중심: 고객과 시장의 요구에 민첩하게 대응하고, 고객의 목소리를 설계 규격에 반영\n전사적 참여\n지속적 개선",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-경영을-위한-전략visioning",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-경영을-위한-전략visioning",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 경영을 위한 전략(Visioning)",
    "text": "프로세스 경영을 위한 전략(Visioning)\n\n1. 대내외 환경 분석\n\n\n\nSWOT\n\n\n\n\n\n5-Forces 모델\n\n\n\n\n2. 기업 비전 수립\n\n기업의 총체적인 방향과 목표 설정\n보통 2~5년 주기로 변화관리 수행\n다음의 요소를 정의해야 함\n\n비전: 기업이 달성하고자 하는 미래상\n미션 또는 사명: 기업의 존재 이유\n가치 제안: 경쟁사와 차별화되는 요소\n목적(Goal): 실현하고자 하는 일\n목표(Objective): 목적을 이루기 위해 구체적으로 해야하는 일\n\n\n\n\n3. 기업 전략 탐색\n\n가치규율모델(value discipline model): 세 가지 전략 중 하나를 선택하여 집중\n\n운영 우수성: 제조 프로세스 및 업무 절차를 자동화하여 운영 업무를 간소화하고 비용을 절감하는데 집중\n고객 친밀도: 서비스의 개인화와 맞춤화를 통해 각기 다른 고객의 요구를 충족하는데 집중\n제품 리더십: 프리미엄 전략\n\nBSC(Balanced Score Card)\n\n재무적인 지표에만 치중하지 않고, 균형된 성과 지표의 조합을 보는거\n4가지 영역\n\n재무\n고객\n프로세스\n학습과 성장\n\n\n\n\n\n4. 전략측정지표 정의(KPI)\n\n상위 수준의 전략 성과 측정지표 정의",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-아키텍처",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/03.html#프로세스-아키텍처",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 아키텍처",
    "text": "프로세스 아키텍처\n\n1. 프로세스 관리 목표 수립\n\n비전수립 단계에서 수립된 기업 전략을 프로세스 관점에서 해석하여 프로세스 관리 목표 수립\n\n\n\n2. 프로세스 관리 원칙 수립\n\n\n3. 프로세스 모델 수립\n\n분류 체계: 프로세스 간 계층관계\n스키마\n콘텐츠\n\n\n\n4. 프로세스 통합 및 검증\n\n\n5. 프로세스 아키텍처 운영",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#경영환경-변화와-무한-경쟁",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/01.html#경영환경-변화와-무한-경쟁",
    "title": "프로세스 경영 개요",
    "section": "경영환경 변화와 무한 경쟁",
    "text": "경영환경 변화와 무한 경쟁\n\n기술 발전과 경쟁 격화로 기업의 평균 수명은 지속적으로 단축되고 있음\n아래의 주요 변화 요소들에 적응하고 뛰어난 성과를 창출하기 위해 프로세스 경영이 필요\n\n\n주요 요소\n\nGlobalization\n제품 수명 주기 단축 및 비용 압력\n제품과 서비스의 개인화 및 맞춤화\n기업의 인수합병\n\n기존의 업무 프로세스가 많이 바뀌어야 함\n변화에 대한 민첩한 적응력(agility) 요구\n\n가치사슬의 확대\n\n협업의 범위 및 레벨 확장/심화\n\nAI & 자동화 확산\nDX\n사이버 보안 및 개인정보 보호\nESG\nCX: 프로세스 설계의 기준이 고객 경험이 됨",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpms-bpm-system의-개념",
    "href": "posts/01_projects/bs_3_2/notes/프로세스경영/02.html#bpms-bpm-system의-개념",
    "title": "BPM 개요",
    "section": "BPMS (BPM System)의 개념",
    "text": "BPMS (BPM System)의 개념\n\n프로세스를 지속적으로 혁신시키고 실행시키는 것을 가능하게 하는 범용 소프트웨어 시스템\n\nBPM suite로 발전: BPM을 위한 모든 솔루션을 통합 제공\n\n\n\n\n\nBPMS Life Cycle",
    "crumbs": [
      "PARA",
      "Projects",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  }
]